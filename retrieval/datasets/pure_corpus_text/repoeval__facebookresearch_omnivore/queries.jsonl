{"_id": "facebookresearch_omnivore/0", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:", "metadata": {"task_id": "facebookresearch_omnivore/0", "ground_truth": "    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 23, "function_name": "_unix_pattern_to_parameter_names", "line_no": 23}}
{"_id": "facebookresearch_omnivore/1", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"", "metadata": {"task_id": "facebookresearch_omnivore/1", "ground_truth": "        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 52, "function_name": "__call__", "line_no": 52}}
{"_id": "facebookresearch_omnivore/2", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"", "metadata": {"task_id": "facebookresearch_omnivore/2", "ground_truth": "        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 81, "function_name": "__call__", "line_no": 81}}
{"_id": "facebookresearch_omnivore/3", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):", "metadata": {"task_id": "facebookresearch_omnivore/3", "ground_truth": "        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 168, "function_name": "__init__", "line_no": 168}}
{"_id": "facebookresearch_omnivore/4", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences", "metadata": {"task_id": "facebookresearch_omnivore/4", "ground_truth": "        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 0, "lineno": 179, "function_name": "__call__", "line_no": 179}}
{"_id": "facebookresearch_omnivore/5", "text": "            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:\n        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"", "metadata": {"task_id": "facebookresearch_omnivore/5", "ground_truth": "    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 53, "lineno": 266, "function_name": "load_checkpoint_and_apply_kernels", "line_no": 266}}
{"_id": "facebookresearch_omnivore/6", "text": " \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:\n        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n\n\ndef load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"", "metadata": {"task_id": "facebookresearch_omnivore/6", "ground_truth": "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:\n            raise KeyError(err)\n    return model\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "context_start_lineno": 100, "lineno": 305, "function_name": "load_state_dict_into_model", "line_no": 305}}
{"_id": "facebookresearch_omnivore/7", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):", "metadata": {"task_id": "facebookresearch_omnivore/7", "ground_truth": "                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 0, "lineno": 170, "function_name": "hook_fn", "line_no": 170}}
{"_id": "facebookresearch_omnivore/8", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):", "metadata": {"task_id": "facebookresearch_omnivore/8", "ground_truth": "        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 0, "lineno": 187, "function_name": "_get_trunk_fields", "line_no": 187}}
{"_id": "facebookresearch_omnivore/9", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):", "metadata": {"task_id": "facebookresearch_omnivore/9", "ground_truth": "        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 0, "lineno": 201, "function_name": "forward_sub_batch", "line_no": 201}}
{"_id": "facebookresearch_omnivore/10", "text": "\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n\n    def forward(self, batch, *args, **kwargs) -> Dict:", "metadata": {"task_id": "facebookresearch_omnivore/10", "ground_truth": "        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):\n                # FIXME: this only handles list inputs for the field \"vision\"\n                assert len(batch) == 1\n                out_vals = []\n                for e in sub_batch.vision:\n                    e_batch = copy.copy(sub_batch)\n                    e_batch.vision = e\n                    self.forward_sub_batch(e_batch, *args, **kwargs)\n                    assert len(self.outputs) == 1\n                    out_key, out_val = self.outputs.popitem()\n                    out_vals.append(out_val)\n                return {out_key: torch.cat(out_vals)}\n            else:\n                self.forward_sub_batch(sub_batch, *args, **kwargs)\n        outputs = self.outputs\n        self.input_key = None\n        self.outputs = {}\n        return outputs\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "context_start_lineno": 18, "lineno": 210, "function_name": "forward", "line_no": 210}}
{"_id": "facebookresearch_omnivore/11", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field, fields, is_dataclass, make_dataclass\nfrom typing import Any, Callable, Dict\n\nfrom torch.utils.data.dataloader import default_collate\n\n\n@dataclass\nclass Batch:\n    # the following are per batch args which are passed to the trainer\n    # and are set to reasonable defaults\n    model_fwd_kwargs: Dict = field(default_factory=dict)\n    accum_steps: int = 1\n\n\ndef create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"", "metadata": {"task_id": "facebookresearch_omnivore/11", "ground_truth": "    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "create_batch_sample_cls", "line_no": 25}}
{"_id": "facebookresearch_omnivore/12", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom hydra.utils import instantiate\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import Sample\nfrom omnivision.data.concat_dataset import ConcatDataset\nfrom omnivision.data.torch_dataset import TorchDataset\nfrom omnivision.losses import wrap_base_loss\nfrom omnivision.optim import construct_optimizer\nfrom omnivision.utils.train import (\n    AverageMeter,\n    copy_data_to_device,\n    get_amp_type,\n    get_machine_local_and_dist_rank,\n    get_resume_checkpoint,\n    is_dist_avail_and_initialized,\n    makedir,\n    ProgressMeter,\n    set_seeds,\n    setup_distributed_backend,\n    setup_logging,\n)\n\n\ndef chunk_batch_for_accum_steps(batch, accum_steps):\n    return [get_chunk_from_data(batch, i, accum_steps) for i in range(accum_steps)]\n\n\ndef get_chunk_from_data(data, chunk_id, num_chunks):\n    \"\"\"\n    Recursively splits all the tensors inside the passed data object into num_chunks.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        assert len(data) % num_chunks == 0\n        start = (len(data) // num_chunks) * chunk_id\n        end = (len(data) // num_chunks) * (chunk_id + 1)\n        return data[start:end]\n    elif isinstance(data, Mapping):\n        return {\n            key: get_chunk_from_data(value, chunk_id, num_chunks)\n            for key, value in data.items()\n        }\n    elif isinstance(data, Sequence):\n        return [get_chunk_from_data(value, chunk_id, num_chunks) for value in data]\n    elif isinstance(data, Sample):\n        data_cls = type(data)\n        data = data.__dict__\n        return data_cls(**get_chunk_from_data(data, chunk_id, num_chunks))\n    else:\n        return data\n\n\n@dataclass\nclass OmnivisionOptimAMPConf:\n    enabled: bool = False\n    amp_dtype: str = \"float16\"\n\n\n@dataclass\nclass OmnivisionOptimConf:\n    optimizer: torch.optim.Optimizer = None\n    options: Optional[Dict[str, Any]] = None\n    param_group_modifiers: Optional[List] = None\n    amp: Optional[Dict[str, Any]] = None\n    gradient_clip: Any = None\n\n    def __post_init__(self):\n        # amp", "metadata": {"task_id": "facebookresearch_omnivore/12", "ground_truth": "        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "__post_init__", "line_no": 86}}
{"_id": "facebookresearch_omnivore/13", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):", "metadata": {"task_id": "facebookresearch_omnivore/13", "ground_truth": "    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 28, "function_name": "validate_param_group_params", "line_no": 28}}
{"_id": "facebookresearch_omnivore/14", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:", "metadata": {"task_id": "facebookresearch_omnivore/14", "ground_truth": "    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 40, "function_name": "unix_pattern_to_parameter_names", "line_no": 40}}
{"_id": "facebookresearch_omnivore/15", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):", "metadata": {"task_id": "facebookresearch_omnivore/15", "ground_truth": "    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "get_full_parameter_name", "line_no": 48}}
{"_id": "facebookresearch_omnivore/16", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:", "metadata": {"task_id": "facebookresearch_omnivore/16", "ground_truth": "    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "unix_param_pattern_to_parameter_names", "line_no": 86}}
{"_id": "facebookresearch_omnivore/17", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:", "metadata": {"task_id": "facebookresearch_omnivore/17", "ground_truth": "    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 104, "function_name": "set_default_parameters", "line_no": 104}}
{"_id": "facebookresearch_omnivore/18", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n\n\ndef name_constraints_to_parameters(\n    param_constraints: List[Set[str]], model: torch.nn.Module\n) -> List[torch.nn.Parameter]:\n    matching_names = set.intersection(*param_constraints)\n    return [value for name, value in model.named_parameters() if name in matching_names]\n\n\ndef map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:", "metadata": {"task_id": "facebookresearch_omnivore/18", "ground_truth": "    schedulers = []\n    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]\n            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "context_start_lineno": 0, "lineno": 134, "function_name": "map_scheduler_cfgs_to_param_groups", "line_no": 134}}
{"_id": "facebookresearch_omnivore/19", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:", "metadata": {"task_id": "facebookresearch_omnivore/19", "ground_truth": "        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "context_start_lineno": 0, "lineno": 9, "function_name": "__init__", "line_no": 9}}
{"_id": "facebookresearch_omnivore/20", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n\n    def _validate_optimizer_schedulers(self):", "metadata": {"task_id": "facebookresearch_omnivore/20", "ground_truth": "        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "context_start_lineno": 0, "lineno": 15, "function_name": "_validate_optimizer_schedulers", "line_no": 15}}
{"_id": "facebookresearch_omnivore/21", "text": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n\n    def _validate_optimizer_schedulers(self):\n        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n\n    def step_schedulers(self, where: float) -> None:", "metadata": {"task_id": "facebookresearch_omnivore/21", "ground_truth": "        if self.schedulers is None:\n            return\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            for option, scheduler in self.schedulers[i].items():\n                new_value = scheduler(where)\n                param_group[option] = new_value\n", "fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "context_start_lineno": 0, "lineno": 26, "function_name": "step_schedulers", "line_no": 26}}
