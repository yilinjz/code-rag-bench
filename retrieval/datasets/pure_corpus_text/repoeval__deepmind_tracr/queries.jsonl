{"_id": "deepmind_tracr/0", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP Evaluator which applies causal masks to selectors.\"\"\"\n\nfrom typing import Sequence, Union\n\nimport numpy as np\nfrom tracr.rasp import rasp\n\n\nclass CausalEvaluator(rasp.DefaultRASPEvaluator):\n  \"\"\"Evaluates RASP with causal masking.\"\"\"\n\n  def evaluate(\n      self, expr: rasp.RASPExpr, xs: Sequence[rasp.Value]\n  ) -> Union[Sequence[rasp.Value], rasp.SelectorValue]:", "metadata": {"task_id": "deepmind_tracr/0", "ground_truth": "    out = super().evaluate(expr, xs)\n\n    if not isinstance(expr, rasp.Selector):\n      return out\n\n    out = np.array(out)\n    causal_mask = np.tril(np.full(out.shape, 1))\n    return np.logical_and(causal_mask, out).tolist()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "causal_eval.py"], "context_start_lineno": 0, "lineno": 28, "function_name": "evaluate", "line_no": 28}}
{"_id": "deepmind_tracr/1", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP program objects.\n\nEvery object in the RASP language is a function.\n\nThe most important type is S-Op, which is a function List[Value] -> List[Value].\n\nAn S-Op represents a state inside the residual stream of the transformer.\nTherefore, any RASP program that represents a transformer computation must\ndefine a final S-Op that represents the state of the residual stream at the\nend of the computation. In particular, given an S-Op `x`,\n`x([1, 2, 3])` represents something like the state of the residual stream\nat location `x` when the transformer is fed [1, 2, 3] as input.\n\nA secondary (but still important) type is Selector, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:", "metadata": {"task_id": "deepmind_tracr/1", "ground_truth": "    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 0, "lineno": 87, "function_name": "__getitem__", "line_no": 87}}
{"_id": "deepmind_tracr/2", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP program objects.\n\nEvery object in the RASP language is a function.\n\nThe most important type is S-Op, which is a function List[Value] -> List[Value].\n\nAn S-Op represents a state inside the residual stream of the transformer.\nTherefore, any RASP program that represents a transformer computation must\ndefine a final S-Op that represents the state of the residual stream at the\nend of the computation. In particular, given an S-Op `x`,\n`x([1, 2, 3])` represents something like the state of the residual stream\nat location `x` when the transformer is fed [1, 2, 3] as input.\n\nA secondary (but still important) type is Selector, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"", "metadata": {"task_id": "deepmind_tracr/2", "ground_truth": "  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 0, "lineno": 158, "function_name": "annotate", "line_no": 158}}
{"_id": "deepmind_tracr/3", "text": ".\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP program objects.\n\nEvery object in the RASP language is a function.\n\nThe most important type is S-Op, which is a function List[Value] -> List[Value].\n\nAn S-Op represents a state inside the residual stream of the transformer.\nTherefore, any RASP program that represents a transformer computation must\ndefine a final S-Op that represents the state of the residual stream at the\nend of the computation. In particular, given an S-Op `x`,\n`x([1, 2, 3])` represents something like the state of the residual stream\nat location `x` when the transformer is fed [1, 2, 3] as input.\n\nA secondary (but still important) type is Selector, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"", "metadata": {"task_id": "deepmind_tracr/3", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 0, "lineno": 203, "function_name": "__add__", "line_no": 203}}
{"_id": "deepmind_tracr/4", "text": " applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP program objects.\n\nEvery object in the RASP language is a function.\n\nThe most important type is S-Op, which is a function List[Value] -> List[Value].\n\nAn S-Op represents a state inside the residual stream of the transformer.\nTherefore, any RASP program that represents a transformer computation must\ndefine a final S-Op that represents the state of the residual stream at the\nend of the computation. In particular, given an S-Op `x`,\n`x([1, 2, 3])` represents something like the state of the residual stream\nat location `x` when the transformer is fed [1, 2, 3] as input.\n\nA secondary (but still important) type is Selector, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"", "metadata": {"task_id": "deepmind_tracr/4", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 8, "lineno": 209, "function_name": "__radd__", "line_no": 209}}
{"_id": "deepmind_tracr/5", "text": "\n\nEvery object in the RASP language is a function.\n\nThe most important type is S-Op, which is a function List[Value] -> List[Value].\n\nAn S-Op represents a state inside the residual stream of the transformer.\nTherefore, any RASP program that represents a transformer computation must\ndefine a final S-Op that represents the state of the residual stream at the\nend of the computation. In particular, given an S-Op `x`,\n`x([1, 2, 3])` represents something like the state of the residual stream\nat location `x` when the transformer is fed [1, 2, 3] as input.\n\nA secondary (but still important) type is Selector, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"", "metadata": {"task_id": "deepmind_tracr/5", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 14, "lineno": 215, "function_name": "__sub__", "line_no": 215}}
{"_id": "deepmind_tracr/6", "text": " the state of the residual stream at the\nend of the computation. In particular, given an S-Op `x`,\n`x([1, 2, 3])` represents something like the state of the residual stream\nat location `x` when the transformer is fed [1, 2, 3] as input.\n\nA secondary (but still important) type is Selector, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"", "metadata": {"task_id": "deepmind_tracr/6", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 22, "lineno": 221, "function_name": "__rsub__", "line_no": 221}}
{"_id": "deepmind_tracr/7", "text": "or, which is a function\nList[Value] -> List[List[bool]]. Given a Selector `sel`, sel([1, 2, 3])\nrepresents something like an attention matrix in the transformer.\n\nFor a full reference on RASP, see https://arxiv.org/abs/2106.06981.\n\"\"\"\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"", "metadata": {"task_id": "deepmind_tracr/7", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 27, "lineno": 227, "function_name": "__mul__", "line_no": 227}}
{"_id": "deepmind_tracr/8", "text": "\n\nimport abc\nimport collections.abc\nimport copy\nimport enum\nimport functools\nimport itertools\nfrom typing import (Any, Callable, Dict, Generic, List, Mapping, Optional,\n                    Sequence, TypeVar, Union)\n\nfrom absl import logging\nimport numpy as np\nfrom typing_extensions import Protocol\n\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"", "metadata": {"task_id": "deepmind_tracr/8", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 32, "lineno": 233, "function_name": "__rmul__", "line_no": 233}}
{"_id": "deepmind_tracr/9", "text": "\nSelectorValue = List[List[bool]]\nNumericValue = Union[int, float]\nValue = Union[None, int, float, str, bool]\nVT = TypeVar(\"VT\", bound=Value)\nRASPExprT = TypeVar(\"RASPExprT\", bound=\"RASPExpr\")\nSOpT = TypeVar(\"SOpT\", bound=\"SOp\")\nT = TypeVar(\"T\")\n\n_NAME_KEY = \"name\"\n_ENCODING_KEY = \"encoding\"\n\n# These are run on every expression when it's initialised.\n# Add your own annotators to this dict to add custom default annotations.\n#\n# For example, DEFAULT_ANNOTATORS['foo'] will provide the default value for\n# expr.annotations['foo]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"", "metadata": {"task_id": "deepmind_tracr/9", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 46, "lineno": 239, "function_name": "__truediv__", "line_no": 239}}
{"_id": "deepmind_tracr/10", "text": "]. The annotator will get called lazily the first time\n# that key is accessed.\n#\n# See the `default_name` annotator for a full example.\nDEFAULT_ANNOTATORS: Dict[str, \"Annotator\"] = {}\n\n\nclass Annotator(Protocol):\n\n  def __call__(self, expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"", "metadata": {"task_id": "deepmind_tracr/10", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 62, "lineno": 254, "function_name": "__and__", "line_no": 254}}
{"_id": "deepmind_tracr/11", "text": " expr: \"RASPExpr\") -> Any:\n    \"\"\"What annotation to add to `expr`.\"\"\"\n\n\nclass _Annotations(collections.abc.Mapping):\n  \"\"\"Holds the expression's annotations.\n\n  It's immutable to the user, but will attempt to generate default values\n  lazily when missing keys are requested.\n  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"", "metadata": {"task_id": "deepmind_tracr/11", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 71, "lineno": 260, "function_name": "__or__", "line_no": 260}}
{"_id": "deepmind_tracr/12", "text": "  \"\"\"\n\n  def __init__(self, expr, **kwargs: Any):\n    self._expr = expr\n    self._inner_dict: Dict[str, Any] = {**kwargs}\n\n  def __getitem__(self, key: str) -> Any:\n    if key not in self._inner_dict:\n      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"", "metadata": {"task_id": "deepmind_tracr/12", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 80, "lineno": 266, "function_name": "__rand__", "line_no": 266}}
{"_id": "deepmind_tracr/13", "text": "      if key not in DEFAULT_ANNOTATORS:\n        raise KeyError(\n            f\"No annotation exists for key '{key}'. \"\n            f\"Available keys: {list(*self.keys(), *DEFAULT_ANNOTATORS.keys())}\")\n      self._inner_dict[key] = DEFAULT_ANNOTATORS[key](self._expr)\n\n    return self._inner_dict[key]\n\n  def __iter__(self):\n    return iter(self._inner_dict)\n\n  def __len__(self):\n    return len(self._inner_dict)\n\n\nclass RASPExpr(abc.ABC):\n  \"\"\"A class distinguishing RASP expressions from other objects.\"\"\"\n  _ids = itertools.count(1)\n\n  def __init__(self):\n    self._annotations: Mapping[str, Any] = _Annotations(self)\n\n  @abc.abstractmethod\n  def __call__(self,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASPExpr using the standard evaluator.\"\"\"\n\n  @property\n  def annotations(self) -> Mapping[str, Any]:\n    \"\"\"The annotations of this expression instance.\"\"\"\n    return self._annotations\n\n  @annotations.setter\n  def annotations(self, annotations: Mapping[str, Any]):\n    self._annotations = _Annotations(self, **annotations)\n\n  @property\n  def name(self) -> str:\n    \"\"\"The name of this expression.\"\"\"\n    return self.annotations[_NAME_KEY]\n\n  @property\n  @abc.abstractmethod\n  def children(self) -> Sequence[\"RASPExpr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"", "metadata": {"task_id": "deepmind_tracr/13", "ground_truth": "    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 88, "lineno": 272, "function_name": "__ror__", "line_no": 272}}
{"_id": "deepmind_tracr/14", "text": "pr\"]:\n    \"\"\"Direct dependencies of this expression.\"\"\"\n\n  @functools.cached_property\n  def unique_id(self):\n    \"\"\"A unique id for every expression instance.\"\"\"\n    return next(self._ids)\n\n  def copy(self: RASPExprT) -> RASPExprT:\n    \"\"\"Returns a shallow copy of this RASPExpr with a new ID.\"\"\"\n    return copy.copy(self)\n\n  @property\n  def label(self) -> str:\n    return f\"{self.name}_{self.unique_id}\"\n\n  def named(self: RASPExprT, name: str) -> RASPExprT:\n    \"\"\"Convenience method for adding a name.\"\"\"\n    return annotate(self, name=name)\n\n  def annotated(self: RASPExprT, **annotations) -> RASPExprT:\n    \"\"\"Convenience method for adding annotations.\"\"\"\n    return annotate(self, **annotations)\n\n\ndef annotate(expr: RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):", "metadata": {"task_id": "deepmind_tracr/14", "ground_truth": "    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 131, "lineno": 334, "function_name": "__init__", "line_no": 334}}
{"_id": "deepmind_tracr/15", "text": " RASPExprT, **annotations) -> RASPExprT:\n  \"\"\"Creates a new expr with added annotations.\"\"\"\n  new = expr.copy()\n  # Note that new annotations will overwrite existing ones with matching keys.\n  new.annotations = {**expr.annotations, **annotations}\n  return new\n\n\n### S-Ops.\n\n\nclass SOp(RASPExpr):\n  \"\"\"A Sequence Operation.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> Sequence[Value]:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of SOps using numeric operators with constant values.\n  # Note: if inheriting SOp by a dataclass, make sure to disable eq and order,\n  # as they will override these.\n\n  def __lt__(self, other: Value) -> \"SOp\":\n    \"\"\"self < other.\"\"\"\n    return Map(lambda x: x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):", "metadata": {"task_id": "deepmind_tracr/15", "ground_truth": "    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 156, "lineno": 359, "function_name": "__init__", "line_no": 359}}
{"_id": "deepmind_tracr/16", "text": " x < other, self)\n\n  def __le__(self, other: Value) -> \"SOp\":\n    \"\"\"self <= other.\"\"\"\n    return Map(lambda x: x <= other, self)\n\n  def __eq__(self, other: Value) -> \"SOp\":\n    \"\"\"self == other.\"\"\"\n    return Map(lambda x: x == other, self)\n\n  def __ne__(self, other: Value) -> \"SOp\":\n    \"\"\"self != other.\"\"\"\n    return Map(lambda x: x != other, self)\n\n  def __gt__(self, other: Value) -> \"SOp\":\n    \"\"\"self > other.\"\"\"\n    return Map(lambda x: x > other, self)\n\n  def __ge__(self, other: Value) -> \"SOp\":\n    \"\"\"self >= other.\"\"\"\n    return Map(lambda x: x >= other, self)\n\n  def __add__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):", "metadata": {"task_id": "deepmind_tracr/16", "ground_truth": "    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 179, "lineno": 381, "function_name": "__init__", "line_no": 381}}
{"_id": "deepmind_tracr/17", "text": "\n    \"\"\"self + other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, self, other)\n    return Map(lambda x: x + other, self)\n\n  def __radd__(self, other: Union[\"SOp\", Value]) -> \"SOp\":\n    \"\"\"other + self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x + y, other, self)\n    return Map(lambda x: other + x, self)\n\n  def __sub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self - other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, self, other)\n    return Map(lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):", "metadata": {"task_id": "deepmind_tracr/17", "ground_truth": "    super().__init__()\n    self.value = value\n    self.check_length = check_length\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 201, "lineno": 406, "function_name": "__init__", "line_no": 406}}
{"_id": "deepmind_tracr/18", "text": "lambda x: x - other, self)\n\n  def __rsub__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other - self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x - y, other, self)\n    return Map(lambda x: other - x, self)\n\n  def __mul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self * other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, self, other)\n    return Map(lambda x: x * other, self)\n\n  def __rmul__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other * self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x * y, other, self)\n    return Map(lambda x: other * x, self)\n\n  def __truediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self / other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, self, other)\n    return Map(lambda x: x / other, self)\n\n  def __rtruediv__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other / self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x / y, other, self)\n    return Map(lambda x: other / x, self)\n\n  def __invert__(self) -> \"SOp\":\n    return Map(lambda x: not x, self)\n\n  def __and__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self & other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, self, other)\n    return Map(lambda x: x and other, self)\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\n### Selectors.\n\n\nclass Predicate(Protocol):\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    \"\"\"Applies the predicate.\"\"\"\n\n\nclass Comparison(enum.Enum):\n  \"\"\"A two-place boolean comparison predicate for use in Select.\"\"\"\n  EQ = \"==\"\n  LT = \"<\"\n  LEQ = \"<=\"\n  GT = \">\"\n  GEQ = \">=\"\n  NEQ = \"!=\"\n  TRUE = \"True\"\n  FALSE = \"False\"\n\n  def __call__(self, key: Value, query: Value) -> bool:", "metadata": {"task_id": "deepmind_tracr/18", "ground_truth": "    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 217, "lineno": 436, "function_name": "__call__", "line_no": 436}}
{"_id": "deepmind_tracr/19", "text": "\n\n  def __or__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"self | other.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, self, other)\n    return Map(lambda x: x or other, self)\n\n  def __rand__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other & self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x and y, other, self)\n    return Map(lambda x: other and x, self)\n\n  def __ror__(self, other: Union[\"SOp\", NumericValue]) -> \"SOp\":\n    \"\"\"other | self.\"\"\"\n    if isinstance(other, SOp):\n      return SequenceMap(lambda x, y: x or y, other, self)\n    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\n### Selectors.\n\n\nclass Predicate(Protocol):\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    \"\"\"Applies the predicate.\"\"\"\n\n\nclass Comparison(enum.Enum):\n  \"\"\"A two-place boolean comparison predicate for use in Select.\"\"\"\n  EQ = \"==\"\n  LT = \"<\"\n  LEQ = \"<=\"\n  GT = \">\"\n  GEQ = \">=\"\n  NEQ = \"!=\"\n  TRUE = \"True\"\n  FALSE = \"False\"\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n\n\n_comparison_table = {\n    Comparison.EQ: lambda key, query: key == query,\n    Comparison.LT: lambda key, query: key < query,\n    Comparison.LEQ: lambda key, query: key <= query,\n    Comparison.GT: lambda key, query: key > query,\n    Comparison.GEQ: lambda key, query: key >= query,\n    Comparison.NEQ: lambda key, query: key != query,\n    Comparison.TRUE: lambda key, query: True,\n    Comparison.FALSE: lambda key, query: False,\n}\n\n\nclass Selector(RASPExpr):\n  \"\"\"RASP Selector. Represents something like an attention head's weights.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> SelectorValue:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of Selector combinations using Python logical operators.\n  def __and__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self & other.\"\"\"\n    return selector_and(self, other)\n\n  def __rand__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other & self.\"\"\"\n    return selector_and(other, self)\n\n  def __or__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self | other.\"\"\"\n    return selector_or(self, other)\n\n  def __ror__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other | self.\"\"\"\n    return selector_or(other, self)\n\n  def __invert__(self) -> \"Selector\":\n    \"\"\"~self.\"\"\"\n    return selector_not(self)\n\n\nclass Select(Selector):\n  \"\"\"Primitive that creates a Selector.\"\"\"\n\n  def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):", "metadata": {"task_id": "deepmind_tracr/19", "ground_truth": "    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 256, "lineno": 487, "function_name": "__init__", "line_no": 487}}
{"_id": "deepmind_tracr/20", "text": "    return Map(lambda x: x or other, self)\n\n\nclass TokensType(SOp):\n  \"\"\"Primitive SOp returning the original input tokens.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"tokens\"\n\n  def __repr__(self):\n    return \"tokens\"\n\n\nclass IndicesType(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\n### Selectors.\n\n\nclass Predicate(Protocol):\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    \"\"\"Applies the predicate.\"\"\"\n\n\nclass Comparison(enum.Enum):\n  \"\"\"A two-place boolean comparison predicate for use in Select.\"\"\"\n  EQ = \"==\"\n  LT = \"<\"\n  LEQ = \"<=\"\n  GT = \">\"\n  GEQ = \">=\"\n  NEQ = \"!=\"\n  TRUE = \"True\"\n  FALSE = \"False\"\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n\n\n_comparison_table = {\n    Comparison.EQ: lambda key, query: key == query,\n    Comparison.LT: lambda key, query: key < query,\n    Comparison.LEQ: lambda key, query: key <= query,\n    Comparison.GT: lambda key, query: key > query,\n    Comparison.GEQ: lambda key, query: key >= query,\n    Comparison.NEQ: lambda key, query: key != query,\n    Comparison.TRUE: lambda key, query: True,\n    Comparison.FALSE: lambda key, query: False,\n}\n\n\nclass Selector(RASPExpr):\n  \"\"\"RASP Selector. Represents something like an attention head's weights.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> SelectorValue:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of Selector combinations using Python logical operators.\n  def __and__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self & other.\"\"\"\n    return selector_and(self, other)\n\n  def __rand__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other & self.\"\"\"\n    return selector_and(other, self)\n\n  def __or__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self | other.\"\"\"\n    return selector_or(self, other)\n\n  def __ror__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other | self.\"\"\"\n    return selector_or(other, self)\n\n  def __invert__(self) -> \"Selector\":\n    \"\"\"~self.\"\"\"\n    return selector_not(self)\n\n\nclass Select(Selector):\n  \"\"\"Primitive that creates a Selector.\"\"\"\n\n  def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):\n    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.keys, self.queries]\n\n\nclass ConstantSelector(Selector):\n  \"\"\"A constant selector for testing purposes.\"\"\"\n\n  def __init__(self, value: SelectorValue, check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\nclass SelectorWidth(SOp):\n  \"\"\"SelectorWidth primitive.\"\"\"\n\n  def __init__(self, selector: Selector):", "metadata": {"task_id": "deepmind_tracr/20", "ground_truth": "    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 274, "lineno": 516, "function_name": "__init__", "line_no": 516}}
{"_id": "deepmind_tracr/21", "text": "(SOp):\n  \"\"\"Primitive SOp returning the position index at each token.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"indices\"\n\n  def __repr__(self):\n    return \"indices\"\n\n\nclass LengthType(SOp):\n  \"\"\"Primitive SOp returning the total length of the input.\"\"\"\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n  @property\n  def label(self) -> str:\n    return \"length\"\n\n  def __repr__(self):\n    return \"length\"\n\n\ntokens = TokensType()\nindices = IndicesType()\nlength = LengthType()\n\n\nclass Map(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the input SOp.\n\n  Map(lambda x: x + 1, tokens).eval([1, 2, 3]) == [2, 3, 4]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value], Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\n### Selectors.\n\n\nclass Predicate(Protocol):\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    \"\"\"Applies the predicate.\"\"\"\n\n\nclass Comparison(enum.Enum):\n  \"\"\"A two-place boolean comparison predicate for use in Select.\"\"\"\n  EQ = \"==\"\n  LT = \"<\"\n  LEQ = \"<=\"\n  GT = \">\"\n  GEQ = \">=\"\n  NEQ = \"!=\"\n  TRUE = \"True\"\n  FALSE = \"False\"\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n\n\n_comparison_table = {\n    Comparison.EQ: lambda key, query: key == query,\n    Comparison.LT: lambda key, query: key < query,\n    Comparison.LEQ: lambda key, query: key <= query,\n    Comparison.GT: lambda key, query: key > query,\n    Comparison.GEQ: lambda key, query: key >= query,\n    Comparison.NEQ: lambda key, query: key != query,\n    Comparison.TRUE: lambda key, query: True,\n    Comparison.FALSE: lambda key, query: False,\n}\n\n\nclass Selector(RASPExpr):\n  \"\"\"RASP Selector. Represents something like an attention head's weights.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> SelectorValue:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of Selector combinations using Python logical operators.\n  def __and__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self & other.\"\"\"\n    return selector_and(self, other)\n\n  def __rand__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other & self.\"\"\"\n    return selector_and(other, self)\n\n  def __or__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self | other.\"\"\"\n    return selector_or(self, other)\n\n  def __ror__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other | self.\"\"\"\n    return selector_or(other, self)\n\n  def __invert__(self) -> \"Selector\":\n    \"\"\"~self.\"\"\"\n    return selector_not(self)\n\n\nclass Select(Selector):\n  \"\"\"Primitive that creates a Selector.\"\"\"\n\n  def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):\n    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.keys, self.queries]\n\n\nclass ConstantSelector(Selector):\n  \"\"\"A constant selector for testing purposes.\"\"\"\n\n  def __init__(self, value: SelectorValue, check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\nclass SelectorWidth(SOp):\n  \"\"\"SelectorWidth primitive.\"\"\"\n\n  def __init__(self, selector: Selector):\n    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector]\n\n\nclass SelectorAnd(Selector):\n  \"\"\"Implements elementwise `and` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):", "metadata": {"task_id": "deepmind_tracr/21", "ground_truth": "    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 292, "lineno": 529, "function_name": "__init__", "line_no": 529}}
{"_id": "deepmind_tracr/22", "text": " Value], inner: SOp):\n    super().__init__()\n    self.f = f\n    self.inner = inner\n\n    assert isinstance(self.inner, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n    if isinstance(self.inner, Map):\n      # Combine the functions into just one.\n      inner_f = self.inner.f\n      self.f = lambda t: f(inner_f(t))\n      self.inner = self.inner.inner\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\nclass SequenceMap(SOp):\n  \"\"\"SOp that evaluates the function elementwise on the two given SOp's.\n\n  SequenceMap(lambda x, y: x - y, length, tokens).eval([1, 2, 3]) == [2, 1, 0]\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\n### Selectors.\n\n\nclass Predicate(Protocol):\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    \"\"\"Applies the predicate.\"\"\"\n\n\nclass Comparison(enum.Enum):\n  \"\"\"A two-place boolean comparison predicate for use in Select.\"\"\"\n  EQ = \"==\"\n  LT = \"<\"\n  LEQ = \"<=\"\n  GT = \">\"\n  GEQ = \">=\"\n  NEQ = \"!=\"\n  TRUE = \"True\"\n  FALSE = \"False\"\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n\n\n_comparison_table = {\n    Comparison.EQ: lambda key, query: key == query,\n    Comparison.LT: lambda key, query: key < query,\n    Comparison.LEQ: lambda key, query: key <= query,\n    Comparison.GT: lambda key, query: key > query,\n    Comparison.GEQ: lambda key, query: key >= query,\n    Comparison.NEQ: lambda key, query: key != query,\n    Comparison.TRUE: lambda key, query: True,\n    Comparison.FALSE: lambda key, query: False,\n}\n\n\nclass Selector(RASPExpr):\n  \"\"\"RASP Selector. Represents something like an attention head's weights.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> SelectorValue:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of Selector combinations using Python logical operators.\n  def __and__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self & other.\"\"\"\n    return selector_and(self, other)\n\n  def __rand__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other & self.\"\"\"\n    return selector_and(other, self)\n\n  def __or__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self | other.\"\"\"\n    return selector_or(self, other)\n\n  def __ror__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other | self.\"\"\"\n    return selector_or(other, self)\n\n  def __invert__(self) -> \"Selector\":\n    \"\"\"~self.\"\"\"\n    return selector_not(self)\n\n\nclass Select(Selector):\n  \"\"\"Primitive that creates a Selector.\"\"\"\n\n  def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):\n    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.keys, self.queries]\n\n\nclass ConstantSelector(Selector):\n  \"\"\"A constant selector for testing purposes.\"\"\"\n\n  def __init__(self, value: SelectorValue, check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\nclass SelectorWidth(SOp):\n  \"\"\"SelectorWidth primitive.\"\"\"\n\n  def __init__(self, selector: Selector):\n    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector]\n\n\nclass SelectorAnd(Selector):\n  \"\"\"Implements elementwise `and` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorOr(Selector):\n  \"\"\"Implements elementwise `or` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorNot(Selector):\n  \"\"\"Implements elementwise `not` on a selector.\"\"\"\n\n  def __init__(self, inner: Selector):", "metadata": {"task_id": "deepmind_tracr/22", "ground_truth": "    self.inner = inner\n    super().__init__()\n    assert isinstance(self.inner, Selector)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 333, "lineno": 559, "function_name": "__init__", "line_no": 559}}
{"_id": "deepmind_tracr/23", "text": "\n  \"\"\"\n\n  def __init__(self, f: Callable[[Value, Value], Value], fst: SOp, snd: SOp):\n    super().__init__()\n\n    if fst == snd:\n      logging.warning(\"Creating a SequenceMap with both inputs being the same \"\n                      \"SOp is discouraged. You should use a Map instead.\")\n\n    self.f = f\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, SOp)\n    assert isinstance(self.snd, SOp)\n    assert callable(self.f) and not isinstance(self.f, RASPExpr)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass LinearSequenceMap(SequenceMap):\n  \"\"\"SOp that evaluates a linear function elementwise on the two given SOp's.\"\"\"\n\n  def __init__(self, fst: SOp, snd: SOp, fst_fac: float, snd_fac: float):\n    super().__init__(fst=fst, snd=snd, f=lambda x, y: fst_fac * x + snd_fac * y)\n    self.fst_fac = fst_fac\n    self.snd_fac = snd_fac\n\n\nclass Full(SOp):\n  \"\"\"A SOp evaluating to [fill]*len(input_values).\"\"\"\n\n  def __init__(self, fill: Value):\n    super().__init__()\n    self.fill = fill\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\ndef sop_not(sop: SOp) -> SOp:\n  return Map(lambda t: not t, sop)\n\n\nclass ConstantSOp(SOp, Generic[VT]):\n  \"\"\"A constant S-Op for testing purposes.\"\"\"\n\n  def __init__(self, value: Sequence[VT], check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\n### Selectors.\n\n\nclass Predicate(Protocol):\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    \"\"\"Applies the predicate.\"\"\"\n\n\nclass Comparison(enum.Enum):\n  \"\"\"A two-place boolean comparison predicate for use in Select.\"\"\"\n  EQ = \"==\"\n  LT = \"<\"\n  LEQ = \"<=\"\n  GT = \">\"\n  GEQ = \">=\"\n  NEQ = \"!=\"\n  TRUE = \"True\"\n  FALSE = \"False\"\n\n  def __call__(self, key: Value, query: Value) -> bool:\n    if key is None:\n      raise ValueError(\"key is None!\")\n    if query is None:\n      raise ValueError(\"query is None!\")\n    return _comparison_table[self](key, query)\n\n\n_comparison_table = {\n    Comparison.EQ: lambda key, query: key == query,\n    Comparison.LT: lambda key, query: key < query,\n    Comparison.LEQ: lambda key, query: key <= query,\n    Comparison.GT: lambda key, query: key > query,\n    Comparison.GEQ: lambda key, query: key >= query,\n    Comparison.NEQ: lambda key, query: key != query,\n    Comparison.TRUE: lambda key, query: True,\n    Comparison.FALSE: lambda key, query: False,\n}\n\n\nclass Selector(RASPExpr):\n  \"\"\"RASP Selector. Represents something like an attention head's weights.\"\"\"\n\n  def __call__(self, xs: Sequence[Value]) -> SelectorValue:\n    return evaluate(self, xs)  # pytype: disable=bad-return-type\n\n  # Allow construction of Selector combinations using Python logical operators.\n  def __and__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self & other.\"\"\"\n    return selector_and(self, other)\n\n  def __rand__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other & self.\"\"\"\n    return selector_and(other, self)\n\n  def __or__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self | other.\"\"\"\n    return selector_or(self, other)\n\n  def __ror__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other | self.\"\"\"\n    return selector_or(other, self)\n\n  def __invert__(self) -> \"Selector\":\n    \"\"\"~self.\"\"\"\n    return selector_not(self)\n\n\nclass Select(Selector):\n  \"\"\"Primitive that creates a Selector.\"\"\"\n\n  def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):\n    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.keys, self.queries]\n\n\nclass ConstantSelector(Selector):\n  \"\"\"A constant selector for testing purposes.\"\"\"\n\n  def __init__(self, value: SelectorValue, check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\nclass SelectorWidth(SOp):\n  \"\"\"SelectorWidth primitive.\"\"\"\n\n  def __init__(self, selector: Selector):\n    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector]\n\n\nclass SelectorAnd(Selector):\n  \"\"\"Implements elementwise `and` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorOr(Selector):\n  \"\"\"Implements elementwise `or` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorNot(Selector):\n  \"\"\"Implements elementwise `not` on a selector.\"\"\"\n\n  def __init__(self, inner: Selector):\n    self.inner = inner\n    super().__init__()\n    assert isinstance(self.inner, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\ndef selector_not(\n    inner: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorNot, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(inner, Select):\n    predicate = lambda k, q: not inner.predicate(k, q)\n    return Select(inner.keys, inner.queries, predicate=predicate)\n\n  return SelectorNot(inner)\n\n\ndef selector_and(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorAnd, or a Select if simplifying is possible.\"\"\"", "metadata": {"task_id": "deepmind_tracr/23", "ground_truth": "  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l and r)\n    if simplified:\n      return simplified\n\n  return SelectorAnd(fst, snd)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 355, "lineno": 586, "function_name": "selector_and", "line_no": 586}}
{"_id": "deepmind_tracr/24", "text": "=bad-return-type\n\n  # Allow construction of Selector combinations using Python logical operators.\n  def __and__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self & other.\"\"\"\n    return selector_and(self, other)\n\n  def __rand__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other & self.\"\"\"\n    return selector_and(other, self)\n\n  def __or__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"self | other.\"\"\"\n    return selector_or(self, other)\n\n  def __ror__(self, other: \"Selector\") -> \"Selector\":\n    \"\"\"other | self.\"\"\"\n    return selector_or(other, self)\n\n  def __invert__(self) -> \"Selector\":\n    \"\"\"~self.\"\"\"\n    return selector_not(self)\n\n\nclass Select(Selector):\n  \"\"\"Primitive that creates a Selector.\"\"\"\n\n  def __init__(self, keys: SOp, queries: SOp, predicate: Predicate):\n    super().__init__()\n    self.keys = keys\n    self.queries = queries\n    self.predicate = predicate\n    assert isinstance(self.keys, SOp)\n    assert isinstance(self.queries, SOp)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.keys, self.queries]\n\n\nclass ConstantSelector(Selector):\n  \"\"\"A constant selector for testing purposes.\"\"\"\n\n  def __init__(self, value: SelectorValue, check_length: bool = True):\n    super().__init__()\n    self.value = value\n    self.check_length = check_length\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return []\n\n\nclass SelectorWidth(SOp):\n  \"\"\"SelectorWidth primitive.\"\"\"\n\n  def __init__(self, selector: Selector):\n    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector]\n\n\nclass SelectorAnd(Selector):\n  \"\"\"Implements elementwise `and` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorOr(Selector):\n  \"\"\"Implements elementwise `or` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorNot(Selector):\n  \"\"\"Implements elementwise `not` on a selector.\"\"\"\n\n  def __init__(self, inner: Selector):\n    self.inner = inner\n    super().__init__()\n    assert isinstance(self.inner, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\ndef selector_not(\n    inner: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorNot, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(inner, Select):\n    predicate = lambda k, q: not inner.predicate(k, q)\n    return Select(inner.keys, inner.queries, predicate=predicate)\n\n  return SelectorNot(inner)\n\n\ndef selector_and(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorAnd, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l and r)\n    if simplified:\n      return simplified\n\n  return SelectorAnd(fst, snd)\n\n\ndef selector_or(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorOr, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l or r)\n    if simplified:\n      return simplified\n\n  return SelectorOr(fst, snd)\n\n\ndef _attempt_simplify(\n    fst: Select,\n    snd: Select,\n    combine: Callable[[bool, bool], bool],\n) -> Optional[Select]:\n  \"\"\"Simplifies two Selects if possible.\n\n  If two Selects in a compound Selector have matching keys and queries, they can\n  be simplified into one Select with a compound predicate:\n\n  lambda k,q: combine(fst.predicate(k,q), snd.predicate(k,q))\n\n  This function returns a Select with this predicate if possible,\n  and None otherwise.\n\n  A Full SOp in a key or query position is a special case that always matches\n  any SOp in the corresponding position in the other selector. In that case,\n  we bake in the fill value into the corresponding Select's predicate before\n  combining. This allows us to use the other SOp as the input to the simplified\n  Select.\n\n  Args:\n    fst: the first Select.\n    snd: the second Select.\n    combine: how to combine the outputs of the individual predicates.\n\n  Returns:\n    A combined Select, if possible.\n  \"\"\"\n  fst_predicate = fst.predicate\n  snd_predicate = snd.predicate\n  common_keys = None\n  common_queries = None\n\n  if isinstance(fst.keys, Full):\n    common_keys = snd.keys\n    # We pass the predicate in as a default arg to avoid unintended recursion.\n    fst_predicate = lambda key, query, p=fst_predicate: p(fst.keys.fill, query)\n  if isinstance(snd.keys, Full):\n    common_keys = fst.keys\n    snd_predicate = lambda key, query, p=snd_predicate: p(snd.keys.fill, query)\n  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"", "metadata": {"task_id": "deepmind_tracr/24", "ground_truth": "    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 459, "lineno": 677, "function_name": "__init__", "line_no": 677}}
{"_id": "deepmind_tracr/25", "text": " return []\n\n\nclass SelectorWidth(SOp):\n  \"\"\"SelectorWidth primitive.\"\"\"\n\n  def __init__(self, selector: Selector):\n    super().__init__()\n    self.selector = selector\n    assert isinstance(self.selector, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector]\n\n\nclass SelectorAnd(Selector):\n  \"\"\"Implements elementwise `and` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorOr(Selector):\n  \"\"\"Implements elementwise `or` between selectors.\"\"\"\n\n  def __init__(self, fst: Selector, snd: Selector):\n    super().__init__()\n    self.fst = fst\n    self.snd = snd\n    assert isinstance(self.fst, Selector)\n    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorNot(Selector):\n  \"\"\"Implements elementwise `not` on a selector.\"\"\"\n\n  def __init__(self, inner: Selector):\n    self.inner = inner\n    super().__init__()\n    assert isinstance(self.inner, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\ndef selector_not(\n    inner: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorNot, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(inner, Select):\n    predicate = lambda k, q: not inner.predicate(k, q)\n    return Select(inner.keys, inner.queries, predicate=predicate)\n\n  return SelectorNot(inner)\n\n\ndef selector_and(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorAnd, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l and r)\n    if simplified:\n      return simplified\n\n  return SelectorAnd(fst, snd)\n\n\ndef selector_or(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorOr, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l or r)\n    if simplified:\n      return simplified\n\n  return SelectorOr(fst, snd)\n\n\ndef _attempt_simplify(\n    fst: Select,\n    snd: Select,\n    combine: Callable[[bool, bool], bool],\n) -> Optional[Select]:\n  \"\"\"Simplifies two Selects if possible.\n\n  If two Selects in a compound Selector have matching keys and queries, they can\n  be simplified into one Select with a compound predicate:\n\n  lambda k,q: combine(fst.predicate(k,q), snd.predicate(k,q))\n\n  This function returns a Select with this predicate if possible,\n  and None otherwise.\n\n  A Full SOp in a key or query position is a special case that always matches\n  any SOp in the corresponding position in the other selector. In that case,\n  we bake in the fill value into the corresponding Select's predicate before\n  combining. This allows us to use the other SOp as the input to the simplified\n  Select.\n\n  Args:\n    fst: the first Select.\n    snd: the second Select.\n    combine: how to combine the outputs of the individual predicates.\n\n  Returns:\n    A combined Select, if possible.\n  \"\"\"\n  fst_predicate = fst.predicate\n  snd_predicate = snd.predicate\n  common_keys = None\n  common_queries = None\n\n  if isinstance(fst.keys, Full):\n    common_keys = snd.keys\n    # We pass the predicate in as a default arg to avoid unintended recursion.\n    fst_predicate = lambda key, query, p=fst_predicate: p(fst.keys.fill, query)\n  if isinstance(snd.keys, Full):\n    common_keys = fst.keys\n    snd_predicate = lambda key, query, p=snd_predicate: p(snd.keys.fill, query)\n  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"", "metadata": {"task_id": "deepmind_tracr/25", "ground_truth": "  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 509, "lineno": 724, "function_name": "default_encoding", "line_no": 724}}
{"_id": "deepmind_tracr/26", "text": "    assert isinstance(self.snd, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.fst, self.snd]\n\n\nclass SelectorNot(Selector):\n  \"\"\"Implements elementwise `not` on a selector.\"\"\"\n\n  def __init__(self, inner: Selector):\n    self.inner = inner\n    super().__init__()\n    assert isinstance(self.inner, Selector)\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.inner]\n\n\ndef selector_not(\n    inner: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorNot, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(inner, Select):\n    predicate = lambda k, q: not inner.predicate(k, q)\n    return Select(inner.keys, inner.queries, predicate=predicate)\n\n  return SelectorNot(inner)\n\n\ndef selector_and(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorAnd, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l and r)\n    if simplified:\n      return simplified\n\n  return SelectorAnd(fst, snd)\n\n\ndef selector_or(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorOr, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l or r)\n    if simplified:\n      return simplified\n\n  return SelectorOr(fst, snd)\n\n\ndef _attempt_simplify(\n    fst: Select,\n    snd: Select,\n    combine: Callable[[bool, bool], bool],\n) -> Optional[Select]:\n  \"\"\"Simplifies two Selects if possible.\n\n  If two Selects in a compound Selector have matching keys and queries, they can\n  be simplified into one Select with a compound predicate:\n\n  lambda k,q: combine(fst.predicate(k,q), snd.predicate(k,q))\n\n  This function returns a Select with this predicate if possible,\n  and None otherwise.\n\n  A Full SOp in a key or query position is a special case that always matches\n  any SOp in the corresponding position in the other selector. In that case,\n  we bake in the fill value into the corresponding Select's predicate before\n  combining. This allows us to use the other SOp as the input to the simplified\n  Select.\n\n  Args:\n    fst: the first Select.\n    snd: the second Select.\n    combine: how to combine the outputs of the individual predicates.\n\n  Returns:\n    A combined Select, if possible.\n  \"\"\"\n  fst_predicate = fst.predicate\n  snd_predicate = snd.predicate\n  common_keys = None\n  common_queries = None\n\n  if isinstance(fst.keys, Full):\n    common_keys = snd.keys\n    # We pass the predicate in as a default arg to avoid unintended recursion.\n    fst_predicate = lambda key, query, p=fst_predicate: p(fst.keys.fill, query)\n  if isinstance(snd.keys, Full):\n    common_keys = fst.keys\n    snd_predicate = lambda key, query, p=snd_predicate: p(snd.keys.fill, query)\n  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:", "metadata": {"task_id": "deepmind_tracr/26", "ground_truth": "  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 548, "lineno": 762, "function_name": "default_name", "line_no": 762}}
{"_id": "deepmind_tracr/27", "text": "or:\n  \"\"\"Returns a SelectorAnd, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l and r)\n    if simplified:\n      return simplified\n\n  return SelectorAnd(fst, snd)\n\n\ndef selector_or(\n    fst: Selector,\n    snd: Selector,\n    simplify: bool = True,\n) -> Selector:\n  \"\"\"Returns a SelectorOr, or a Select if simplifying is possible.\"\"\"\n  if simplify and isinstance(fst, Select) and isinstance(snd, Select):\n    simplified = _attempt_simplify(fst, snd, lambda l, r: l or r)\n    if simplified:\n      return simplified\n\n  return SelectorOr(fst, snd)\n\n\ndef _attempt_simplify(\n    fst: Select,\n    snd: Select,\n    combine: Callable[[bool, bool], bool],\n) -> Optional[Select]:\n  \"\"\"Simplifies two Selects if possible.\n\n  If two Selects in a compound Selector have matching keys and queries, they can\n  be simplified into one Select with a compound predicate:\n\n  lambda k,q: combine(fst.predicate(k,q), snd.predicate(k,q))\n\n  This function returns a Select with this predicate if possible,\n  and None otherwise.\n\n  A Full SOp in a key or query position is a special case that always matches\n  any SOp in the corresponding position in the other selector. In that case,\n  we bake in the fill value into the corresponding Select's predicate before\n  combining. This allows us to use the other SOp as the input to the simplified\n  Select.\n\n  Args:\n    fst: the first Select.\n    snd: the second Select.\n    combine: how to combine the outputs of the individual predicates.\n\n  Returns:\n    A combined Select, if possible.\n  \"\"\"\n  fst_predicate = fst.predicate\n  snd_predicate = snd.predicate\n  common_keys = None\n  common_queries = None\n\n  if isinstance(fst.keys, Full):\n    common_keys = snd.keys\n    # We pass the predicate in as a default arg to avoid unintended recursion.\n    fst_predicate = lambda key, query, p=fst_predicate: p(fst.keys.fill, query)\n  if isinstance(snd.keys, Full):\n    common_keys = fst.keys\n    snd_predicate = lambda key, query, p=snd_predicate: p(snd.keys.fill, query)\n  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):", "metadata": {"task_id": "deepmind_tracr/27", "ground_truth": "    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 584, "lineno": 792, "function_name": "__init__", "line_no": 792}}
{"_id": "deepmind_tracr/28", "text": "\n  Select.\n\n  Args:\n    fst: the first Select.\n    snd: the second Select.\n    combine: how to combine the outputs of the individual predicates.\n\n  Returns:\n    A combined Select, if possible.\n  \"\"\"\n  fst_predicate = fst.predicate\n  snd_predicate = snd.predicate\n  common_keys = None\n  common_queries = None\n\n  if isinstance(fst.keys, Full):\n    common_keys = snd.keys\n    # We pass the predicate in as a default arg to avoid unintended recursion.\n    fst_predicate = lambda key, query, p=fst_predicate: p(fst.keys.fill, query)\n  if isinstance(snd.keys, Full):\n    common_keys = fst.keys\n    snd_predicate = lambda key, query, p=snd_predicate: p(snd.keys.fill, query)\n  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:", "metadata": {"task_id": "deepmind_tracr/28", "ground_truth": "    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 626, "lineno": 831, "function_name": "eval_sequence_map", "line_no": 831}}
{"_id": "deepmind_tracr/29", "text": " common_keys = snd.keys\n    # We pass the predicate in as a default arg to avoid unintended recursion.\n    fst_predicate = lambda key, query, p=fst_predicate: p(fst.keys.fill, query)\n  if isinstance(snd.keys, Full):\n    common_keys = fst.keys\n    snd_predicate = lambda key, query, p=snd_predicate: p(snd.keys.fill, query)\n  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:", "metadata": {"task_id": "deepmind_tracr/29", "ground_truth": "    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 643, "lineno": 839, "function_name": "eval_map", "line_no": 839}}
{"_id": "deepmind_tracr/30", "text": "  if isinstance(fst.queries, Full):\n    common_queries = snd.queries\n    fst_predicate = lambda key, query, p=fst_predicate: p(key, fst.queries.fill)\n  if isinstance(snd.queries, Full):\n    common_queries = fst.queries\n    snd_predicate = lambda key, query, p=snd_predicate: p(key, snd.queries.fill)\n  if fst.keys is snd.keys:\n    common_keys = fst.keys\n  if fst.queries is snd.queries:\n    common_queries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n\n  def eval_full(self, sop: Full, xs: Sequence[Value]) -> Sequence[Value]:\n    return [sop.fill] * len(xs)\n\n  def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:", "metadata": {"task_id": "deepmind_tracr/30", "ground_truth": "    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 649, "lineno": 849, "function_name": "eval_constant_sop", "line_no": 849}}
{"_id": "deepmind_tracr/31", "text": "eries = fst.queries\n\n  if not common_keys or not common_queries:\n    return None\n\n  def predicate(key, query):\n    return combine(fst_predicate(key, query), snd_predicate(key, query))\n\n  return Select(common_keys, common_queries, predicate=predicate)\n\n\nclass Aggregate(SOp, Generic[VT]):\n  \"\"\"Aggregate primitive.\"\"\"\n\n  def __init__(self,\n               selector: Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n\n  def eval_full(self, sop: Full, xs: Sequence[Value]) -> Sequence[Value]:\n    return [sop.fill] * len(xs)\n\n  def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n\n  def eval_selector_width(self, sop: SelectorWidth,\n                          xs: Sequence[Value]) -> Sequence[Value]:\n    selector_values = self.evaluate(sop.selector, xs)\n    return [sum(row) for row in selector_values]\n\n  def eval_aggregate(self, sop: Aggregate,\n                     xs: Sequence[Value]) -> Sequence[Value]:", "metadata": {"task_id": "deepmind_tracr/31", "ground_truth": "    selector_value = self.evaluate(sop.selector, xs)\n    values = self.evaluate(sop.sop, xs)\n    default = sop.default\n\n    return [\n        _mean(_get_selected(row, values), default) for row in selector_value\n    ]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 658, "lineno": 861, "function_name": "eval_aggregate", "line_no": 861}}
{"_id": "deepmind_tracr/32", "text": ": Selector,\n               sop: SOp,\n               default: Optional[VT] = None):\n    \"\"\"Initialises. The default is used where nothing is selected.\"\"\"\n    super().__init__()\n    self.selector = selector\n    self.sop = sop\n    self.default = default\n    assert isinstance(self.selector, Selector)\n    assert isinstance(self.sop, SOp)\n    assert (self.default is None or isinstance(self.default,\n                                               (str, float, bool, int)))\n\n  @property\n  def children(self) -> Sequence[RASPExpr]:\n    return [self.selector, self.sop]\n\n\n### SOp encodings.\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n\n  def eval_full(self, sop: Full, xs: Sequence[Value]) -> Sequence[Value]:\n    return [sop.fill] * len(xs)\n\n  def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n\n  def eval_selector_width(self, sop: SelectorWidth,\n                          xs: Sequence[Value]) -> Sequence[Value]:\n    selector_values = self.evaluate(sop.selector, xs)\n    return [sum(row) for row in selector_values]\n\n  def eval_aggregate(self, sop: Aggregate,\n                     xs: Sequence[Value]) -> Sequence[Value]:\n    selector_value = self.evaluate(sop.selector, xs)\n    values = self.evaluate(sop.sop, xs)\n    default = sop.default\n\n    return [\n        _mean(_get_selected(row, values), default) for row in selector_value\n    ]\n\n  def eval_select(self, sel: Select, xs: Sequence[Value]) -> SelectorValue:\n    \"\"\"Evaluates a Select on `xs`.\"\"\"", "metadata": {"task_id": "deepmind_tracr/32", "ground_truth": "    key_values = self.evaluate(sel.keys, xs)\n    query_values = self.evaluate(sel.queries, xs)\n\n    key_len = len(key_values)\n    query_len = len(query_values)\n    out = np.zeros((query_len, key_len), dtype=bool).tolist()\n    for row, query in enumerate(query_values):\n      for col, key in enumerate(key_values):\n        out[row][col] = bool(sel.predicate(key, query))\n    return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 673, "lineno": 871, "function_name": "eval_select", "line_no": 871}}
{"_id": "deepmind_tracr/33", "text": "\n\n\nclass Encoding(enum.Enum):\n  \"\"\"The encoding used by a SOp. Only number-valued SOps support numerical.\"\"\"\n  CATEGORICAL = \"categorical\"\n  NUMERICAL = \"numerical\"\n\n\ndef numerical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.NUMERICAL)\n\n\ndef categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n\n  def eval_full(self, sop: Full, xs: Sequence[Value]) -> Sequence[Value]:\n    return [sop.fill] * len(xs)\n\n  def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n\n  def eval_selector_width(self, sop: SelectorWidth,\n                          xs: Sequence[Value]) -> Sequence[Value]:\n    selector_values = self.evaluate(sop.selector, xs)\n    return [sum(row) for row in selector_values]\n\n  def eval_aggregate(self, sop: Aggregate,\n                     xs: Sequence[Value]) -> Sequence[Value]:\n    selector_value = self.evaluate(sop.selector, xs)\n    values = self.evaluate(sop.sop, xs)\n    default = sop.default\n\n    return [\n        _mean(_get_selected(row, values), default) for row in selector_value\n    ]\n\n  def eval_select(self, sel: Select, xs: Sequence[Value]) -> SelectorValue:\n    \"\"\"Evaluates a Select on `xs`.\"\"\"\n    key_values = self.evaluate(sel.keys, xs)\n    query_values = self.evaluate(sel.queries, xs)\n\n    key_len = len(key_values)\n    query_len = len(query_values)\n    out = np.zeros((query_len, key_len), dtype=bool).tolist()\n    for row, query in enumerate(query_values):\n      for col, key in enumerate(key_values):\n        out[row][col] = bool(sel.predicate(key, query))\n    return out\n\n  def eval_constant_selector(self, sel: ConstantSelector,\n                             xs: Sequence[Value]) -> SelectorValue:", "metadata": {"task_id": "deepmind_tracr/33", "ground_truth": "    if sel.check_length and (len(xs) != len(sel.value)):\n      raise ValueError(\n          f\"Constant len {len(xs)} doesn't match input len {len(sel.value)}.\")\n    return sel.value\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 691, "lineno": 884, "function_name": "eval_constant_selector", "line_no": 884}}
{"_id": "deepmind_tracr/34", "text": " categorical(sop: SOpT) -> SOpT:\n  return annotate(sop, encoding=Encoding.CATEGORICAL)\n\n\ndef get_encoding(sop: SOp) -> Encoding:\n  return sop.annotations[\"encoding\"]\n\n\ndef is_numerical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is numerically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.NUMERICAL\n\n\ndef is_categorical(sop: SOp) -> bool:\n  \"\"\"Check if the SOp is categorically encoded.\"\"\"\n  return get_encoding(sop) == Encoding.CATEGORICAL\n\n\ndef default_encoding(expr: RASPExpr) -> Optional[Encoding]:\n  \"\"\"Adds an 'encoding' annotation, default is Categorical.\"\"\"\n  if not isinstance(expr, SOp):\n    raise TypeError(f\"expr {expr} is not a SOp.\")\n\n  return Encoding.CATEGORICAL\n\n\nDEFAULT_ANNOTATORS[_ENCODING_KEY] = default_encoding\n\n### naming.\n\n# Subclasses must appear here before superclasses in order for\n# the most specific entry to be used.\n\n_default_name_by_class = {\n    # Primitives\n    TokensType: \"tokens\",\n    IndicesType: \"indices\",\n    LengthType: \"length\",\n    # SOps\n    LinearSequenceMap: \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n\n  def eval_full(self, sop: Full, xs: Sequence[Value]) -> Sequence[Value]:\n    return [sop.fill] * len(xs)\n\n  def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n\n  def eval_selector_width(self, sop: SelectorWidth,\n                          xs: Sequence[Value]) -> Sequence[Value]:\n    selector_values = self.evaluate(sop.selector, xs)\n    return [sum(row) for row in selector_values]\n\n  def eval_aggregate(self, sop: Aggregate,\n                     xs: Sequence[Value]) -> Sequence[Value]:\n    selector_value = self.evaluate(sop.selector, xs)\n    values = self.evaluate(sop.sop, xs)\n    default = sop.default\n\n    return [\n        _mean(_get_selected(row, values), default) for row in selector_value\n    ]\n\n  def eval_select(self, sel: Select, xs: Sequence[Value]) -> SelectorValue:\n    \"\"\"Evaluates a Select on `xs`.\"\"\"\n    key_values = self.evaluate(sel.keys, xs)\n    query_values = self.evaluate(sel.queries, xs)\n\n    key_len = len(key_values)\n    query_len = len(query_values)\n    out = np.zeros((query_len, key_len), dtype=bool).tolist()\n    for row, query in enumerate(query_values):\n      for col, key in enumerate(key_values):\n        out[row][col] = bool(sel.predicate(key, query))\n    return out\n\n  def eval_constant_selector(self, sel: ConstantSelector,\n                             xs: Sequence[Value]) -> SelectorValue:\n    if sel.check_length and (len(xs) != len(sel.value)):\n      raise ValueError(\n          f\"Constant len {len(xs)} doesn't match input len {len(sel.value)}.\")\n    return sel.value\n\n  def eval_selector_and(self, sel: SelectorAnd,\n                        xs: Sequence[Value]) -> SelectorValue:", "metadata": {"task_id": "deepmind_tracr/34", "ground_truth": "    fst_values = self.evaluate(sel.fst, xs)\n    snd_values = self.evaluate(sel.snd, xs)\n    return np.logical_and(np.array(fst_values), np.array(snd_values)).tolist()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 704, "lineno": 891, "function_name": "eval_selector_and", "line_no": 891}}
{"_id": "deepmind_tracr/35", "text": ": \"linear_sequence_map\",\n    SequenceMap: \"sequence_map\",\n    Map: \"map\",\n    Full: \"full\",\n    ConstantSOp: \"constant_sop\",\n    SelectorWidth: \"selector_width\",\n    Aggregate: \"aggregate\",\n    SOp: \"sop\",\n    # Selectors\n    Select: \"select\",\n    SelectorAnd: \"selector_and\",\n    SelectorOr: \"selector_or\",\n    SelectorNot: \"selector_not\",\n    ConstantSelector: \"constant_selector\",\n    Selector: \"selector\",\n}\n\n\ndef default_name(expr: RASPExpr) -> Dict[str, str]:\n  for cls, name in _default_name_by_class.items():\n    if isinstance(expr, cls):\n      return name\n\n  raise NotImplementedError(f\"{expr} was not given a default name!\")\n\n\nDEFAULT_ANNOTATORS[_NAME_KEY] = default_name\n\n### evaluation.\n\n\nclass RASPEvaluator(abc.ABC):\n  \"\"\"ABC for RASP evaluators.\"\"\"\n\n  @abc.abstractmethod\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n\n\nclass DefaultRASPEvaluator(abc.ABC):\n  \"\"\"Default evaluator for RASP.\"\"\"\n\n  def evaluate(self, expr: RASPExpr,\n               xs: Sequence[Value]) -> Union[Sequence[Value], SelectorValue]:\n    \"\"\"Evaluates the RASP expression on input `xs`.\"\"\"\n    return self._eval_fn_by_expr_type[type(expr)](expr, xs)\n\n  def __init__(self):\n    self._eval_fn_by_expr_type = {\n        # Primitives\n        TokensType: self.eval_tokens,\n        IndicesType: self.eval_indices,\n        LengthType: self.eval_length,\n        # SOps\n        LinearSequenceMap: self.eval_sequence_map,\n        SequenceMap: self.eval_sequence_map,\n        Map: self.eval_map,\n        Full: self.eval_full,\n        ConstantSOp: self.eval_constant_sop,\n        SelectorWidth: self.eval_selector_width,\n        Aggregate: self.eval_aggregate,\n        SOp: _raise_not_implemented,\n        # Selectors\n        Select: self.eval_select,\n        SelectorAnd: self.eval_selector_and,\n        SelectorOr: self.eval_selector_or,\n        SelectorNot: self.eval_selector_not,\n        ConstantSelector: self.eval_constant_selector,\n        Selector: _raise_not_implemented,\n    }\n\n  def eval_tokens(self, sop: TokensType,\n                  xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(xs)\n\n  def eval_indices(self, sop: IndicesType,\n                   xs: Sequence[Value]) -> Sequence[Value]:\n    del sop\n    return list(range(len(xs)))\n\n  def eval_length(self, sop: LengthType, xs: Sequence[Value]) -> Sequence[int]:\n    del sop\n    return [len(xs)] * len(xs)\n\n  def eval_sequence_map(self, sop: SequenceMap,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    fst_values = self.evaluate(sop.fst, xs)\n    snd_values = self.evaluate(sop.snd, xs)\n    return [\n        sop.f(x, y) if None not in [x, y] else None\n        for x, y in zip(fst_values, snd_values)\n    ]\n\n  def eval_map(self, sop: Map, xs: Sequence[Value]) -> Sequence[Value]:\n    return [\n        sop.f(x) if x is not None else None\n        for x in self.evaluate(sop.inner, xs)\n    ]\n\n  def eval_full(self, sop: Full, xs: Sequence[Value]) -> Sequence[Value]:\n    return [sop.fill] * len(xs)\n\n  def eval_constant_sop(self, sop: ConstantSOp,\n                        xs: Sequence[Value]) -> Sequence[Value]:\n    if sop.check_length and (len(xs) != len(sop.value)):\n      raise ValueError(\n          f\"Constant len {len(sop.value)} doesn't match input len {len(xs)}.\")\n    return sop.value\n\n  def eval_selector_width(self, sop: SelectorWidth,\n                          xs: Sequence[Value]) -> Sequence[Value]:\n    selector_values = self.evaluate(sop.selector, xs)\n    return [sum(row) for row in selector_values]\n\n  def eval_aggregate(self, sop: Aggregate,\n                     xs: Sequence[Value]) -> Sequence[Value]:\n    selector_value = self.evaluate(sop.selector, xs)\n    values = self.evaluate(sop.sop, xs)\n    default = sop.default\n\n    return [\n        _mean(_get_selected(row, values), default) for row in selector_value\n    ]\n\n  def eval_select(self, sel: Select, xs: Sequence[Value]) -> SelectorValue:\n    \"\"\"Evaluates a Select on `xs`.\"\"\"\n    key_values = self.evaluate(sel.keys, xs)\n    query_values = self.evaluate(sel.queries, xs)\n\n    key_len = len(key_values)\n    query_len = len(query_values)\n    out = np.zeros((query_len, key_len), dtype=bool).tolist()\n    for row, query in enumerate(query_values):\n      for col, key in enumerate(key_values):\n        out[row][col] = bool(sel.predicate(key, query))\n    return out\n\n  def eval_constant_selector(self, sel: ConstantSelector,\n                             xs: Sequence[Value]) -> SelectorValue:\n    if sel.check_length and (len(xs) != len(sel.value)):\n      raise ValueError(\n          f\"Constant len {len(xs)} doesn't match input len {len(sel.value)}.\")\n    return sel.value\n\n  def eval_selector_and(self, sel: SelectorAnd,\n                        xs: Sequence[Value]) -> SelectorValue:\n    fst_values = self.evaluate(sel.fst, xs)\n    snd_values = self.evaluate(sel.snd, xs)\n    return np.logical_and(np.array(fst_values), np.array(snd_values)).tolist()\n\n  def eval_selector_or(self, sel: SelectorOr,\n                       xs: Sequence[Value]) -> SelectorValue:\n    fst_values = self.evaluate(sel.fst, xs)\n    snd_values = self.evaluate(sel.snd, xs)\n    return np.logical_or(np.array(fst_values), np.array(snd_values)).tolist()\n\n  def eval_selector_not(self, sel: SelectorNot,\n                        xs: Sequence[Value]) -> SelectorValue:\n    values = self.evaluate(sel.inner, xs)\n    return np.logical_not(np.array(values)).tolist()\n\n\ndef _get_selected(\n    selector_row: List[bool],\n    values: Sequence[VT],\n) -> Sequence[VT]:\n  \"\"\"Helper for aggregate. [T T F], [a b c] -> [a b].\"\"\"\n  return [v for s, v in zip(selector_row, values) if s]\n\n\ndef _mean(xs: Sequence[VT], default: VT) -> VT:\n  \"\"\"Takes the mean for numbers and concats for strings.\"\"\"", "metadata": {"task_id": "deepmind_tracr/35", "ground_truth": "  if not xs:\n    return default\n  exemplar = xs[0]\n  if isinstance(exemplar, (int, bool)):\n    return sum(xs) / len(xs)\n  elif len(xs) == 1:\n    return exemplar\n  else:\n    raise ValueError(f\"Unsupported type for aggregation: {xs}\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp.py"], "context_start_lineno": 743, "lineno": 917, "function_name": "_mean", "line_no": 917}}
{"_id": "deepmind_tracr/36", "text": "indices, rasp.Comparison.GEQ),\n        rasp.Select(rasp.indices, rasp.tokens, rasp.Comparison.LEQ),\n    )\n    self.assertIsInstance(selector, rasp.SelectorAnd)\n\n  def test_selector_and_gets_simplified_when_keys_are_full(self):\n    selector = rasp.selector_and(\n        rasp.Select(rasp.Full(1), rasp.indices, rasp.Comparison.GEQ),\n        rasp.Select(rasp.tokens, rasp.indices, rasp.Comparison.LEQ),\n    )\n    self.assertIsInstance(selector, rasp.Select)\n    self.assertIs(selector.keys, rasp.tokens)\n    self.assertIs(selector.queries, rasp.indices)\n\n  def test_selector_and_gets_simplified_when_queries_are_full(self):\n    selector = rasp.selector_and(\n        rasp.Select(rasp.tokens, rasp.indices, rasp.Comparison.GEQ),\n        rasp.Select(rasp.tokens, rasp.Full(1), rasp.Comparison.LEQ),\n    )\n    self.assertIsInstance(selector, rasp.Select)\n    self.assertIs(selector.keys, rasp.tokens)\n    self.assertIs(selector.queries, rasp.indices)\n\n  @parameterized.parameters(\n      itertools.product(\n          (rasp.tokens, rasp.indices, rasp.Full(1)),\n          (rasp.tokens, rasp.indices, rasp.Full(1)),\n          list(rasp.Comparison),\n          (rasp.tokens, rasp.indices, rasp.Full(1)),\n          (rasp.tokens, rasp.indices, rasp.Full(1)),\n          list(rasp.Comparison),\n      ))\n  def test_simplified_selector_and_works_the_same_way_as_not(\n      self, fst_k, fst_q, fst_p, snd_k, snd_q, snd_p):\n    fst = rasp.Select(fst_k, fst_q, fst_p)\n    snd = rasp.Select(snd_k, snd_q, snd_p)\n\n    simplified = rasp.selector_and(fst, snd)([0, 1, 2, 3])\n    not_simplified = rasp.selector_and(fst, snd, simplify=False)([0, 1, 2, 3])\n\n    np.testing.assert_array_equal(\n        np.array(simplified),\n        np.array(not_simplified),\n    )\n\n  def test_select_is_selector(self):\n    self.assertIsInstance(\n        rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.EQ),\n        rasp.Selector,\n    )\n\n  def test_select_is_raspexpr(self):\n    self.assertIsInstance(\n        rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.EQ),\n        rasp.RASPExpr,\n    )\n\n  def test_constant_selector(self):\n    self.assertEqual(\n        rasp.ConstantSelector([[True, True], [False, False]])([1, 2]),\n        [[True, True], [False, False]],\n    )\n\n\nclass CopyTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(*_ALL_EXAMPLES())\n  def test_copy_preserves_name(self, expr: rasp.RASPExpr):\n    expr = expr.named(\"foo\")\n    self.assertEqual(expr.copy().name, expr.name)\n\n  @parameterized.named_parameters(*_ALL_EXAMPLES())\n  def test_renaming_copy_doesnt_rename_original(self, expr: rasp.RASPExpr):\n    expr = expr.named(\"foo\")\n    expr.copy().named(\"bar\")\n    self.assertEqual(expr.name, \"foo\")\n\n  @parameterized.named_parameters(*_ALL_EXAMPLES())\n  def test_renaming_original_doesnt_rename_copy(self, expr: rasp.RASPExpr):\n    expr = expr.named(\"foo\")\n    copy = expr.copy()\n    expr.named(\"bar\")\n    self.assertEqual(copy.name, \"foo\")\n\n  @parameterized.named_parameters(*_ALL_EXAMPLES())\n  def test_copy_changes_id(self, expr: rasp.RASPExpr):\n    self.assertNotEqual(expr.copy().unique_id, expr.unique_id)\n\n  @parameterized.named_parameters(*_ALL_EXAMPLES())\n  def test_copy_preserves_child_ids(self, expr: rasp.RASPExpr):\n    copy_child_ids = [c.unique_id for c in expr.copy().children]\n    child_ids = [c.unique_id for c in expr.children]\n    for child_id, copy_child_id in zip(child_ids, copy_child_ids):\n      self.assertEqual(child_id, copy_child_id)\n\n\nclass AggregateTest(parameterized.TestCase):\n  \"\"\"Tests for Aggregate.\"\"\"\n\n  @parameterized.parameters(\n      dict(\n          selector=rasp.ConstantSelector([\n              [True, False],\n              [False, True],\n          ]),\n          sop=rasp.ConstantSOp([\"h\", \"e\"]),\n          default=None,\n          expected_value=[\"h\", \"e\"],\n      ),\n      dict(\n          selector=rasp.ConstantSelector([\n              [False, True],\n              [False, False],\n          ]),\n          sop=rasp.ConstantSOp([\"h\", \"e\"]),\n          default=None,\n          expected_value=[\"e\", None],\n      ),\n      dict(\n          selector=rasp.ConstantSelector([\n              [True, False],\n              [False, False],\n          ]),\n          sop=rasp.ConstantSOp([\"h\", \"e\"]),\n          default=None,\n          expected_value=[\"h\", None],\n      ),\n      dict(\n          selector=rasp.ConstantSelector([\n              [True, True],\n              [False, True],\n          ]),\n          sop=rasp.ConstantSOp([0, 1]),\n          default=0,\n          expected_value=[0.5, 1],\n      ),\n      dict(\n          selector=rasp.ConstantSelector([\n              [False, False],\n              [True, True],\n          ]),\n          sop=rasp.ConstantSOp([0, 1]),\n          default=0,\n          expected_value=[0, 0.5],\n      ),\n      dict(\n          selector=rasp.ConstantSelector([\n              [False, False],\n              [True, True],\n          ]),\n          sop=rasp.ConstantSOp([0, 1]),\n          default=None,\n          expected_value=[None, 0.5],\n      ),\n  )\n  def test_aggregate_on_size_2_inputs(self, selector, sop, default,\n                                      expected_value):\n    # The 0, 0 input is ignored as it's overridden by the constant SOps.\n    self.assertEqual(\n        rasp.Aggregate(selector, sop, default)([0, 0]),\n        expected_value,\n    )\n\n\nclass RaspProgramTest(parameterized.TestCase):\n  \"\"\"Each testcase implements and tests a RASP program.\"\"\"\n\n  def test_has_prev(self):\n\n    def has_prev(seq: rasp.SOp) -> rasp.SOp:", "metadata": {"task_id": "deepmind_tracr/36", "ground_truth": "      prev_copy = rasp.SelectorAnd(\n          rasp.Select(seq, seq, rasp.Comparison.EQ),\n          rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LT),\n      )\n      return rasp.Aggregate(prev_copy, rasp.Full(1), default=0) > 0\n", "fpath_tuple": ["deepmind_tracr", "tracr", "rasp", "rasp_test.py"], "context_start_lineno": 386, "lineno": 556, "function_name": "has_prev", "line_no": 556}}
{"_id": "deepmind_tracr/37", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Assemble weights of a transformer model from a craft residual stack.\"\"\"\n\nimport dataclasses\nfrom typing import Any, Callable, Optional, List, Tuple\n\nimport chex\nimport einops\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\nfrom tracr.transformer import encoder\nfrom tracr.transformer import model\nfrom typing_extensions import Protocol\n\n\n@chex.dataclass\nclass AssembledTransformerModelOutput:\n  decoded: List[Any]  # length T.\n  unembedded: jax.Array  # [B, T]     B = 1 always.\n  layer_outputs: List[jax.Array]  # [B, T, D]\n  residuals: List[jax.Array]  # [B, T, D]\n  attn_logits: List[jax.Array]  # [B, T, T, H]\n  transformer_output: jax.Array  # [B, T, D]\n  input_embeddings: jax.Array\n\n\nclass ModelForward(Protocol):\n\n  def __call__(\n      self,\n      params: hk.Params,\n      emb: jax.Array,\n  ) -> model.CompiledTransformerModelOutput:\n    \"\"\"A hk-transformed forward pass through the compiled model.\"\"\"\n\n\n@dataclasses.dataclass\nclass AssembledTransformerModel:\n  \"\"\"Model architecture and parameters from assembling a model.\"\"\"\n  forward: ModelForward\n  get_compiled_model: Callable[[], model.CompiledTransformerModel]\n  params: hk.Params\n  model_config: model.TransformerConfig\n  residual_labels: List[str]\n  input_encoder: Optional[encoder.Encoder] = None\n  output_encoder: Optional[encoder.Encoder] = None\n\n  def apply(self, tokens: List[bases.Value]) -> AssembledTransformerModelOutput:\n    \"\"\"Returns output from running the model on a set of input tokens.\"\"\"", "metadata": {"task_id": "deepmind_tracr/37", "ground_truth": "    if self.input_encoder:\n      tokens = self.input_encoder.encode(tokens)\n    tokens = jnp.array([tokens])\n    output = self.forward(self.params, tokens)\n    decoded = output.unembedded_output[0].tolist()\n    if self.output_encoder:\n      decoded = self.output_encoder.decode(decoded)\n\n    if self.input_encoder.bos_token:\n      # Special case for decoding the bos token position, for which the output\n      # decoder might have unspecified behavior.\n      decoded = [self.input_encoder.bos_token] + decoded[1:]\n\n    return AssembledTransformerModelOutput(\n        decoded=decoded,\n        unembedded=output.unembedded_output,\n        layer_outputs=output.transformer_output.layer_outputs,\n        residuals=output.transformer_output.residuals,\n        attn_logits=output.transformer_output.attn_logits,\n        transformer_output=output.transformer_output.output,\n        input_embeddings=output.transformer_output.input_embeddings)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "apply", "line_no": 67}}
{"_id": "deepmind_tracr/38", "text": "chex.dataclass\nclass AssembledTransformerModelOutput:\n  decoded: List[Any]  # length T.\n  unembedded: jax.Array  # [B, T]     B = 1 always.\n  layer_outputs: List[jax.Array]  # [B, T, D]\n  residuals: List[jax.Array]  # [B, T, D]\n  attn_logits: List[jax.Array]  # [B, T, T, H]\n  transformer_output: jax.Array  # [B, T, D]\n  input_embeddings: jax.Array\n\n\nclass ModelForward(Protocol):\n\n  def __call__(\n      self,\n      params: hk.Params,\n      emb: jax.Array,\n  ) -> model.CompiledTransformerModelOutput:\n    \"\"\"A hk-transformed forward pass through the compiled model.\"\"\"\n\n\n@dataclasses.dataclass\nclass AssembledTransformerModel:\n  \"\"\"Model architecture and parameters from assembling a model.\"\"\"\n  forward: ModelForward\n  get_compiled_model: Callable[[], model.CompiledTransformerModel]\n  params: hk.Params\n  model_config: model.TransformerConfig\n  residual_labels: List[str]\n  input_encoder: Optional[encoder.Encoder] = None\n  output_encoder: Optional[encoder.Encoder] = None\n\n  def apply(self, tokens: List[bases.Value]) -> AssembledTransformerModelOutput:\n    \"\"\"Returns output from running the model on a set of input tokens.\"\"\"\n    if self.input_encoder:\n      tokens = self.input_encoder.encode(tokens)\n    tokens = jnp.array([tokens])\n    output = self.forward(self.params, tokens)\n    decoded = output.unembedded_output[0].tolist()\n    if self.output_encoder:\n      decoded = self.output_encoder.decode(decoded)\n\n    if self.input_encoder.bos_token:\n      # Special case for decoding the bos token position, for which the output\n      # decoder might have unspecified behavior.\n      decoded = [self.input_encoder.bos_token] + decoded[1:]\n\n    return AssembledTransformerModelOutput(\n        decoded=decoded,\n        unembedded=output.unembedded_output,\n        layer_outputs=output.transformer_output.layer_outputs,\n        residuals=output.transformer_output.residuals,\n        attn_logits=output.transformer_output.attn_logits,\n        transformer_output=output.transformer_output.output,\n        input_embeddings=output.transformer_output.input_embeddings)\n\n\n@dataclasses.dataclass\nclass EmbeddingModules:\n  \"\"\"Modules for embedding and tokens and positions and unembedding results.\"\"\"\n  token_embed: model.CallableHaikuModule\n  pos_embed: model.CallableHaikuModule\n  unembed: model.CallableHaikuModule\n\n\ndef _get_model_config_and_module_names(\n    craft_model: transformers.SeriesWithResiduals\n) -> Tuple[model.TransformerConfig, List[str]]:\n  \"\"\"Returns model config and locations (in params) for halflayers.\"\"\"\n\n  multi_attn_heads: List[List[transformers.AttentionHead]] = []\n  mlps: List[transformers.MLP] = []\n  module_names: List[str] = []\n\n  candidate_module_names = []\n  for layer in range(len(craft_model.blocks)):\n    candidate_module_names.append(f\"transformer/layer_{layer}/attn\")\n    candidate_module_names.append(f\"transformer/layer_{layer}/mlp\")\n  candidate_module_names = iter(candidate_module_names)\n\n  for module in craft_model.blocks:\n    if isinstance(module, transformers.MLP):\n      mlps.append(module)\n      layer_type = \"mlp\"\n    else:\n      multi_attn_heads.append(list(module.as_multi().heads()))\n      layer_type = \"attn\"\n    # Find next layer with the necessary type. Modules in-between, that are not\n    # added to module_names will be disabled later by setting all weights to 0.\n    module_name = next(candidate_module_names)\n    while layer_type not in module_name:\n      module_name = next(candidate_module_names)\n    module_names.append(module_name)\n\n  num_layers = int(module_names[-1].split(\"_\")[1].split(\"/\")[0]) + 1\n  heads = sum(multi_attn_heads, [])\n\n  if multi_attn_heads:\n    num_heads = max(len(heads) for heads in multi_attn_heads)\n    key_size = max(max(head.w_qk.matrix.shape) for head in heads)\n  else:\n    num_heads, key_size = 1, 1\n\n  if mlps:\n    mlp_hidden_size = max(mlp.fst.output_space.num_dims for mlp in mlps)\n  else:\n    mlp_hidden_size = 1\n\n  model_config = model.TransformerConfig(\n      num_heads=num_heads,\n      num_layers=num_layers,\n      key_size=key_size,\n      mlp_hidden_size=mlp_hidden_size,\n      dropout_rate=0.,\n      activation_function=jax.nn.relu,\n      layer_norm=False,\n      causal=False,\n  )\n\n  return model_config, module_names\n\n\ndef _make_embedding_modules(\n    residual_space: bases.VectorSpaceWithBasis,\n    tokens_space: bases.VectorSpaceWithBasis,\n    indices_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis) -> EmbeddingModules:\n  \"\"\"Creates embedding and unembedding modules from vector spaces.\n\n  Args:\n    residual_space: Full residual space of the model.\n    tokens_space: Subspace to embed tokens to.\n    indices_space: Subspace to embed indices/position embeddings to.\n    output_space: Subspace to unembed outputs from.\n\n  Returns:\n    EmbeddingModules containing modules for token embeddings, position\n      embeddings and unembeddings.\n  \"\"\"\n  tokens_to_res = vectorspace_fns.project(tokens_space, residual_space)\n\n  # If we use the 'one' direction, make sure all inputs have a 1 here\n  one_dir = bases.BasisDirection(\"one\")\n  if one_dir in residual_space:\n    one_to_res = vectorspace_fns.Linear.from_action(\n        tokens_space, residual_space,\n        lambda x: residual_space.vector_from_basis_direction(one_dir))\n    tokens_to_res = vectorspace_fns.Linear.combine_in_parallel(\n        [tokens_to_res, one_to_res])\n\n  # Token embeddings.\n  res_to_out = vectorspace_fns.project(residual_space, output_space)\n  token_embed = hk.Embed(\n      embedding_matrix=tokens_to_res.matrix, name=\"token_embed\")\n\n  # Positional embeddings.\n  index_to_res = vectorspace_fns.project(indices_space, residual_space)\n  # The zeroth position should not have any positional embeddings,\n  # so we add one line of padding at the zeroth position.\n  pos_matrix = np.concatenate(\n      [np.zeros((1, residual_space.num_dims)), index_to_res.matrix], axis=0)\n  pos_embed = hk.Embed(embedding_matrix=pos_matrix, name=\"pos_embed\")\n\n  def unembed(x, use_unembed_argmax):", "metadata": {"task_id": "deepmind_tracr/38", "ground_truth": "    out = x @ res_to_out.matrix\n    if use_unembed_argmax:\n      return jnp.argmax(out, axis=-1)\n    elif out.shape[-1] == 1:\n      return out.squeeze(-1)\n    return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble.py"], "context_start_lineno": 33, "lineno": 197, "function_name": "unembed", "line_no": 197}}
{"_id": "deepmind_tracr/39", "text": "\n        unembedded=output.unembedded_output,\n        layer_outputs=output.transformer_output.layer_outputs,\n        residuals=output.transformer_output.residuals,\n        attn_logits=output.transformer_output.attn_logits,\n        transformer_output=output.transformer_output.output,\n        input_embeddings=output.transformer_output.input_embeddings)\n\n\n@dataclasses.dataclass\nclass EmbeddingModules:\n  \"\"\"Modules for embedding and tokens and positions and unembedding results.\"\"\"\n  token_embed: model.CallableHaikuModule\n  pos_embed: model.CallableHaikuModule\n  unembed: model.CallableHaikuModule\n\n\ndef _get_model_config_and_module_names(\n    craft_model: transformers.SeriesWithResiduals\n) -> Tuple[model.TransformerConfig, List[str]]:\n  \"\"\"Returns model config and locations (in params) for halflayers.\"\"\"\n\n  multi_attn_heads: List[List[transformers.AttentionHead]] = []\n  mlps: List[transformers.MLP] = []\n  module_names: List[str] = []\n\n  candidate_module_names = []\n  for layer in range(len(craft_model.blocks)):\n    candidate_module_names.append(f\"transformer/layer_{layer}/attn\")\n    candidate_module_names.append(f\"transformer/layer_{layer}/mlp\")\n  candidate_module_names = iter(candidate_module_names)\n\n  for module in craft_model.blocks:\n    if isinstance(module, transformers.MLP):\n      mlps.append(module)\n      layer_type = \"mlp\"\n    else:\n      multi_attn_heads.append(list(module.as_multi().heads()))\n      layer_type = \"attn\"\n    # Find next layer with the necessary type. Modules in-between, that are not\n    # added to module_names will be disabled later by setting all weights to 0.\n    module_name = next(candidate_module_names)\n    while layer_type not in module_name:\n      module_name = next(candidate_module_names)\n    module_names.append(module_name)\n\n  num_layers = int(module_names[-1].split(\"_\")[1].split(\"/\")[0]) + 1\n  heads = sum(multi_attn_heads, [])\n\n  if multi_attn_heads:\n    num_heads = max(len(heads) for heads in multi_attn_heads)\n    key_size = max(max(head.w_qk.matrix.shape) for head in heads)\n  else:\n    num_heads, key_size = 1, 1\n\n  if mlps:\n    mlp_hidden_size = max(mlp.fst.output_space.num_dims for mlp in mlps)\n  else:\n    mlp_hidden_size = 1\n\n  model_config = model.TransformerConfig(\n      num_heads=num_heads,\n      num_layers=num_layers,\n      key_size=key_size,\n      mlp_hidden_size=mlp_hidden_size,\n      dropout_rate=0.,\n      activation_function=jax.nn.relu,\n      layer_norm=False,\n      causal=False,\n  )\n\n  return model_config, module_names\n\n\ndef _make_embedding_modules(\n    residual_space: bases.VectorSpaceWithBasis,\n    tokens_space: bases.VectorSpaceWithBasis,\n    indices_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis) -> EmbeddingModules:\n  \"\"\"Creates embedding and unembedding modules from vector spaces.\n\n  Args:\n    residual_space: Full residual space of the model.\n    tokens_space: Subspace to embed tokens to.\n    indices_space: Subspace to embed indices/position embeddings to.\n    output_space: Subspace to unembed outputs from.\n\n  Returns:\n    EmbeddingModules containing modules for token embeddings, position\n      embeddings and unembeddings.\n  \"\"\"\n  tokens_to_res = vectorspace_fns.project(tokens_space, residual_space)\n\n  # If we use the 'one' direction, make sure all inputs have a 1 here\n  one_dir = bases.BasisDirection(\"one\")\n  if one_dir in residual_space:\n    one_to_res = vectorspace_fns.Linear.from_action(\n        tokens_space, residual_space,\n        lambda x: residual_space.vector_from_basis_direction(one_dir))\n    tokens_to_res = vectorspace_fns.Linear.combine_in_parallel(\n        [tokens_to_res, one_to_res])\n\n  # Token embeddings.\n  res_to_out = vectorspace_fns.project(residual_space, output_space)\n  token_embed = hk.Embed(\n      embedding_matrix=tokens_to_res.matrix, name=\"token_embed\")\n\n  # Positional embeddings.\n  index_to_res = vectorspace_fns.project(indices_space, residual_space)\n  # The zeroth position should not have any positional embeddings,\n  # so we add one line of padding at the zeroth position.\n  pos_matrix = np.concatenate(\n      [np.zeros((1, residual_space.num_dims)), index_to_res.matrix], axis=0)\n  pos_embed = hk.Embed(embedding_matrix=pos_matrix, name=\"pos_embed\")\n\n  def unembed(x, use_unembed_argmax):\n    out = x @ res_to_out.matrix\n    if use_unembed_argmax:\n      return jnp.argmax(out, axis=-1)\n    elif out.shape[-1] == 1:\n      return out.squeeze(-1)\n    return out\n\n  unembed_mod = hk.to_module(unembed)()\n  return EmbeddingModules(\n      token_embed=token_embed, pos_embed=pos_embed, unembed=unembed_mod)\n\n\ndef assemble_craft_model(\n    craft_model: transformers.SeriesWithResiduals,\n    tokens_space: bases.VectorSpaceWithBasis,\n    indices_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    categorical_output: bool,\n    causal: bool = False,\n) -> AssembledTransformerModel:\n  \"\"\"Assembles the given components into a Haiku model with parameters.\n\n  Args:\n    craft_model: Model to assemble weights for.\n    tokens_space: Vectorspace to embed the input tokens to.\n    indices_space: Vectorspace to embed the indices to (position encodings).\n    output_space: Vectorspace that the model will write outputs to that should\n      be unembedded.\n    categorical_output: Whether the output is categorical. If True, we take an\n      argmax when unembedding.\n    causal: Whether to output a causally-masked model.\n\n  Returns:\n    An AssembledTransformerModel that contains the model and parameters of the\n      assembled transformer.\n  \"\"\"\n  # TODO(b/255936413): Make embeddings only retain the tokens and indices that\n  #   are actually used.\n  # TODO(b/255936496): Think about enabling layer norm and reversing it somehow\n\n  model_config, module_names = _get_model_config_and_module_names(craft_model)\n  model_config.causal = causal\n\n  residual_space = bases.join_vector_spaces(craft_model.residual_space,\n                                            tokens_space, indices_space,\n                                            output_space)\n  residual_labels = [str(basis_dir) for basis_dir in residual_space.basis]\n\n  # Build model with embedding and unembedding layers\n  def get_compiled_model():", "metadata": {"task_id": "deepmind_tracr/39", "ground_truth": "    transformer = model.Transformer(model_config)\n    embed_modules = _make_embedding_modules(\n        residual_space=residual_space,\n        tokens_space=tokens_space,\n        indices_space=indices_space,\n        output_space=output_space)\n    return model.CompiledTransformerModel(\n        transformer=transformer,\n        token_embed=embed_modules.token_embed,\n        position_embed=embed_modules.pos_embed,\n        unembed=embed_modules.unembed,\n        use_unembed_argmax=categorical_output)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble.py"], "context_start_lineno": 81, "lineno": 247, "function_name": "get_compiled_model", "line_no": 247}}
{"_id": "deepmind_tracr/40", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Integration tests for the full RASP -> transformer compilation.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport numpy as np\n\nfrom tracr.compiler import compiling\nfrom tracr.compiler import lib\nfrom tracr.compiler import test_cases\nfrom tracr.craft import tests_common\nfrom tracr.rasp import rasp\n\n_COMPILER_BOS = \"rasp_to_transformer_integration_test_BOS\"\n_COMPILER_PAD = \"rasp_to_transformer_integration_test_PAD\"\n\n# Force float32 precision on TPU, which otherwise defaults to float16.\njax.config.update(\"jax_default_matmul_precision\", \"float32\")\n\n\nclass CompilerIntegrationTest(tests_common.VectorFnTestCase):\n\n  def assertSequenceEqualWhenExpectedIsNotNone(self, actual_seq, expected_seq):", "metadata": {"task_id": "deepmind_tracr/40", "ground_truth": "    for actual, expected in zip(actual_seq, expected_seq):\n      if expected is not None and actual != expected:\n        self.fail(f\"{actual_seq} does not match (ignoring Nones) \"\n                  f\"expected_seq={expected_seq}\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_transformer_integration_test.py"], "context_start_lineno": 0, "lineno": 37, "function_name": "assertSequenceEqualWhenExpectedIsNotNone", "line_no": 37}}
{"_id": "deepmind_tracr/41", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for compiler.expr_to_craft_graph.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom tracr.compiler import basis_inference\nfrom tracr.compiler import expr_to_craft_graph\nfrom tracr.compiler import lib\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\n\nclass ExprToCraftGraphTest(parameterized.TestCase):\n\n  def _check_block_types_are_correct(self, graph):", "metadata": {"task_id": "deepmind_tracr/41", "ground_truth": "    for _, node in graph.nodes.items():\n      expr = node[nodes.EXPR]\n      if isinstance(expr, rasp.SOp):\n        block = node[nodes.MODEL_BLOCK]\n        if isinstance(expr, (rasp.Map, rasp.SequenceMap)):\n          self.assertIsInstance(block, transformers.MLP)\n        elif isinstance(expr, rasp.Aggregate):\n          self.assertIsInstance(block, transformers.AttentionHead)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph_test.py"], "context_start_lineno": 0, "lineno": 31, "function_name": "_check_block_types_are_correct", "line_no": 31}}
{"_id": "deepmind_tracr/42", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for compiler.expr_to_craft_graph.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom tracr.compiler import basis_inference\nfrom tracr.compiler import expr_to_craft_graph\nfrom tracr.compiler import lib\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\n\nclass ExprToCraftGraphTest(parameterized.TestCase):\n\n  def _check_block_types_are_correct(self, graph):\n    for _, node in graph.nodes.items():\n      expr = node[nodes.EXPR]\n      if isinstance(expr, rasp.SOp):\n        block = node[nodes.MODEL_BLOCK]\n        if isinstance(expr, (rasp.Map, rasp.SequenceMap)):\n          self.assertIsInstance(block, transformers.MLP)\n        elif isinstance(expr, rasp.Aggregate):\n          self.assertIsInstance(block, transformers.AttentionHead)\n\n  def _get_input_space_from_node(self, node):", "metadata": {"task_id": "deepmind_tracr/42", "ground_truth": "    block = node[nodes.MODEL_BLOCK]\n    if isinstance(block, transformers.MLP):\n      return block.fst.input_space\n    elif isinstance(block, transformers.AttentionHead):\n      return bases.join_vector_spaces(block.w_qk.left_space,\n                                      block.w_qk.right_space,\n                                      block.w_ov.input_space)\n    else:\n      return None\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph_test.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "_get_input_space_from_node", "line_no": 41}}
{"_id": "deepmind_tracr/43", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for compiler.expr_to_craft_graph.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom tracr.compiler import basis_inference\nfrom tracr.compiler import expr_to_craft_graph\nfrom tracr.compiler import lib\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\n\nclass ExprToCraftGraphTest(parameterized.TestCase):\n\n  def _check_block_types_are_correct(self, graph):\n    for _, node in graph.nodes.items():\n      expr = node[nodes.EXPR]\n      if isinstance(expr, rasp.SOp):\n        block = node[nodes.MODEL_BLOCK]\n        if isinstance(expr, (rasp.Map, rasp.SequenceMap)):\n          self.assertIsInstance(block, transformers.MLP)\n        elif isinstance(expr, rasp.Aggregate):\n          self.assertIsInstance(block, transformers.AttentionHead)\n\n  def _get_input_space_from_node(self, node):\n    block = node[nodes.MODEL_BLOCK]\n    if isinstance(block, transformers.MLP):\n      return block.fst.input_space\n    elif isinstance(block, transformers.AttentionHead):\n      return bases.join_vector_spaces(block.w_qk.left_space,\n                                      block.w_qk.right_space,\n                                      block.w_ov.input_space)\n    else:\n      return None\n\n  def _check_spaces_are_consistent(self, graph):\n    \"\"\"Check that for each edge the output is a subspace of the input.\"\"\"", "metadata": {"task_id": "deepmind_tracr/43", "ground_truth": "    for u, v in graph.edges:\n      u_node, v_node = graph.nodes[u], graph.nodes[v]\n      if isinstance(u_node[nodes.EXPR], rasp.SOp) and isinstance(\n          v_node[nodes.EXPR], rasp.SOp):\n        u_out_basis = u_node[nodes.OUTPUT_BASIS]\n        u_out_space = bases.VectorSpaceWithBasis(u_out_basis)\n        v_in_space = self._get_input_space_from_node(v_node)\n        self.assertTrue(u_out_space.issubspace(v_in_space))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph_test.py"], "context_start_lineno": 0, "lineno": 53, "function_name": "_check_spaces_are_consistent", "line_no": 53}}
{"_id": "deepmind_tracr/44", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Create a craft model from a computational graph.\"\"\"\n\nimport collections\nfrom typing import Dict, List, Sequence\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\ndef _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/44", "ground_truth": "  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n\n  result = -1\n  for source in sources:\n    all_paths = nx.all_simple_paths(graph, source[nodes.ID], node[nodes.ID])\n    longest_path_len = max(map(num_sops, all_paths), default=-1) - 1\n    if longest_path_len > result:\n      result = longest_path_len\n  return result\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 44, "function_name": "_get_longest_path_length_to_node", "line_no": 44}}
{"_id": "deepmind_tracr/45", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Create a craft model from a computational graph.\"\"\"\n\nimport collections\nfrom typing import Dict, List, Sequence\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\ndef _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"\n  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:", "metadata": {"task_id": "deepmind_tracr/45", "ground_truth": "    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "num_sops", "line_no": 48}}
{"_id": "deepmind_tracr/46", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Create a craft model from a computational graph.\"\"\"\n\nimport collections\nfrom typing import Dict, List, Sequence\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\ndef _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"\n  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n\n  result = -1\n  for source in sources:\n    all_paths = nx.all_simple_paths(graph, source[nodes.ID], node[nodes.ID])\n    longest_path_len = max(map(num_sops, all_paths), default=-1) - 1\n    if longest_path_len > result:\n      result = longest_path_len\n  return result\n\n\ndef _node_is_attn(node: Node) -> bool:\n  \"\"\"Returns True if node is an attention layer.\"\"\"", "metadata": {"task_id": "deepmind_tracr/46", "ground_truth": "  return nodes.MODEL_BLOCK in node and isinstance(\n      node[nodes.MODEL_BLOCK],\n      (transformers.AttentionHead, transformers.MultiAttentionHead))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "_node_is_attn", "line_no": 65}}
{"_id": "deepmind_tracr/47", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Create a craft model from a computational graph.\"\"\"\n\nimport collections\nfrom typing import Dict, List, Sequence\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\ndef _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"\n  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n\n  result = -1\n  for source in sources:\n    all_paths = nx.all_simple_paths(graph, source[nodes.ID], node[nodes.ID])\n    longest_path_len = max(map(num_sops, all_paths), default=-1) - 1\n    if longest_path_len > result:\n      result = longest_path_len\n  return result\n\n\ndef _node_is_attn(node: Node) -> bool:\n  \"\"\"Returns True if node is an attention layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(\n      node[nodes.MODEL_BLOCK],\n      (transformers.AttentionHead, transformers.MultiAttentionHead))\n\n\ndef _node_is_mlp(node: Node) -> bool:\n  \"\"\"Returns True if node is an MLP layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(node[nodes.MODEL_BLOCK],\n                                                  transformers.MLP)\n\n\ndef _node_is_residual_block(node: Node) -> bool:\n  \"\"\"Returns True if node is a valid residual block (Attn followed by MLP).\"\"\"", "metadata": {"task_id": "deepmind_tracr/47", "ground_truth": "  block = node[nodes.MODEL_BLOCK] if nodes.MODEL_BLOCK in node else None\n  if block and isinstance(block, transformers.SeriesWithResiduals):\n    if len(block.blocks) == 2:\n      attn, mlp = block.blocks\n      if (isinstance(\n          attn,\n          (transformers.AttentionHead, transformers.MultiAttentionHead)) and\n          isinstance(mlp, transformers.MLP)):\n        return True\n  return False\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 78, "function_name": "_node_is_residual_block", "line_no": 78}}
{"_id": "deepmind_tracr/48", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Create a craft model from a computational graph.\"\"\"\n\nimport collections\nfrom typing import Dict, List, Sequence\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\ndef _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"\n  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n\n  result = -1\n  for source in sources:\n    all_paths = nx.all_simple_paths(graph, source[nodes.ID], node[nodes.ID])\n    longest_path_len = max(map(num_sops, all_paths), default=-1) - 1\n    if longest_path_len > result:\n      result = longest_path_len\n  return result\n\n\ndef _node_is_attn(node: Node) -> bool:\n  \"\"\"Returns True if node is an attention layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(\n      node[nodes.MODEL_BLOCK],\n      (transformers.AttentionHead, transformers.MultiAttentionHead))\n\n\ndef _node_is_mlp(node: Node) -> bool:\n  \"\"\"Returns True if node is an MLP layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(node[nodes.MODEL_BLOCK],\n                                                  transformers.MLP)\n\n\ndef _node_is_residual_block(node: Node) -> bool:\n  \"\"\"Returns True if node is a valid residual block (Attn followed by MLP).\"\"\"\n  block = node[nodes.MODEL_BLOCK] if nodes.MODEL_BLOCK in node else None\n  if block and isinstance(block, transformers.SeriesWithResiduals):\n    if len(block.blocks) == 2:\n      attn, mlp = block.blocks\n      if (isinstance(\n          attn,\n          (transformers.AttentionHead, transformers.MultiAttentionHead)) and\n          isinstance(mlp, transformers.MLP)):\n        return True\n  return False\n\n\ndef _all_attn_nodes(node_list: Sequence[Node]) -> bool:\n  \"\"\"Returns True iff all nodes are attention layers (or nodes is empty).\"\"\"", "metadata": {"task_id": "deepmind_tracr/48", "ground_truth": "  for node in node_list:\n    if not _node_is_attn(node):\n      return False\n  return True\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 92, "function_name": "_all_attn_nodes", "line_no": 92}}
{"_id": "deepmind_tracr/49", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Create a craft model from a computational graph.\"\"\"\n\nimport collections\nfrom typing import Dict, List, Sequence\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\ndef _get_longest_path_length_to_node(graph: nx.DiGraph, sources: Sequence[Node],\n                                     node: Node) -> int:\n  \"\"\"Returns the lengths of the longest path from sources to node.\n\n  Only SOps count towards the length of a path.\n\n  Args:\n    graph: DAG to compute longest path in.\n    sources: List of starting nodes, longest path will be a maximum over all.\n    node: Target node.\n\n  Returns:\n    Number of steps needed for the longest path from the source to the node, or\n    -1 if there is no path from any of the sources to the target node.\n  \"\"\"\n  if node in sources:\n    return 0\n\n  def num_sops(path: Sequence[NodeID]) -> int:\n    num = 0\n    for node_id in path:\n      if isinstance(graph.nodes[node_id][nodes.EXPR], rasp.SOp):\n        num += 1\n    return num\n\n  result = -1\n  for source in sources:\n    all_paths = nx.all_simple_paths(graph, source[nodes.ID], node[nodes.ID])\n    longest_path_len = max(map(num_sops, all_paths), default=-1) - 1\n    if longest_path_len > result:\n      result = longest_path_len\n  return result\n\n\ndef _node_is_attn(node: Node) -> bool:\n  \"\"\"Returns True if node is an attention layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(\n      node[nodes.MODEL_BLOCK],\n      (transformers.AttentionHead, transformers.MultiAttentionHead))\n\n\ndef _node_is_mlp(node: Node) -> bool:\n  \"\"\"Returns True if node is an MLP layer.\"\"\"\n  return nodes.MODEL_BLOCK in node and isinstance(node[nodes.MODEL_BLOCK],\n                                                  transformers.MLP)\n\n\ndef _node_is_residual_block(node: Node) -> bool:\n  \"\"\"Returns True if node is a valid residual block (Attn followed by MLP).\"\"\"\n  block = node[nodes.MODEL_BLOCK] if nodes.MODEL_BLOCK in node else None\n  if block and isinstance(block, transformers.SeriesWithResiduals):\n    if len(block.blocks) == 2:\n      attn, mlp = block.blocks\n      if (isinstance(\n          attn,\n          (transformers.AttentionHead, transformers.MultiAttentionHead)) and\n          isinstance(mlp, transformers.MLP)):\n        return True\n  return False\n\n\ndef _all_attn_nodes(node_list: Sequence[Node]) -> bool:\n  \"\"\"Returns True iff all nodes are attention layers (or nodes is empty).\"\"\"\n  for node in node_list:\n    if not _node_is_attn(node):\n      return False\n  return True\n\n\ndef _all_mlp_nodes(node_list: Sequence[Node]) -> bool:\n  \"\"\"Returns True iff all nodes are MLP layers (or nodes is empty).\"\"\"", "metadata": {"task_id": "deepmind_tracr/49", "ground_truth": "  for node in node_list:\n    if not _node_is_mlp(node):\n      return False\n  return True\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model.py"], "context_start_lineno": 0, "lineno": 100, "function_name": "_all_mlp_nodes", "line_no": 100}}
{"_id": "deepmind_tracr/50", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Add craft model blocks to graph of RASPExpr.\"\"\"\n\nfrom typing import Any, Callable, Optional\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft.chamber import categorical_attn\nfrom tracr.craft.chamber import categorical_mlp\nfrom tracr.craft.chamber import numerical_mlp\nfrom tracr.craft.chamber import selector_width\nfrom tracr.rasp import rasp\n\n\ndef _transform_fun_to_basis_fun(\n    fun: Callable[..., Any],\n    output_direction_name: Optional[str] = None) -> Callable[..., Any]:\n  \"\"\"Transforms a function acting on values into one acting on directions.\"\"\"", "metadata": {"task_id": "deepmind_tracr/50", "ground_truth": "  def bases_fun(*args):\n    values = [d.value for d in args]\n    result = fun(*values)\n    if output_direction_name:\n      return bases.BasisDirection(output_direction_name, result)\n    return result\n\n  return bases_fun\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph.py"], "context_start_lineno": 0, "lineno": 33, "function_name": "_transform_fun_to_basis_fun", "line_no": 33}}
{"_id": "deepmind_tracr/51", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Add craft model blocks to graph of RASPExpr.\"\"\"\n\nfrom typing import Any, Callable, Optional\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.craft import bases\nfrom tracr.craft.chamber import categorical_attn\nfrom tracr.craft.chamber import categorical_mlp\nfrom tracr.craft.chamber import numerical_mlp\nfrom tracr.craft.chamber import selector_width\nfrom tracr.rasp import rasp\n\n\ndef _transform_fun_to_basis_fun(\n    fun: Callable[..., Any],\n    output_direction_name: Optional[str] = None) -> Callable[..., Any]:\n  \"\"\"Transforms a function acting on values into one acting on directions.\"\"\"\n\n  def bases_fun(*args):", "metadata": {"task_id": "deepmind_tracr/51", "ground_truth": "    values = [d.value for d in args]\n    result = fun(*values)\n    if output_direction_name:\n      return bases.BasisDirection(output_direction_name, result)\n    return result\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "expr_to_craft_graph.py"], "context_start_lineno": 0, "lineno": 34, "function_name": "bases_fun", "line_no": 34}}
{"_id": "deepmind_tracr/52", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for compiler.craft_graph_to_model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport networkx as nx\nfrom tracr.compiler import craft_graph_to_model\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.craft import bases\nfrom tracr.craft.chamber import categorical_attn\nfrom tracr.craft.chamber import categorical_mlp\nfrom tracr.rasp import rasp\n\n\nclass CraftAllocateModulesToLayersTest(parameterized.TestCase):\n\n  def _get_dummy_block(self, block_type):", "metadata": {"task_id": "deepmind_tracr/52", "ground_truth": "    if block_type == \"ATTN\":\n      return categorical_attn.categorical_attn(\n          query_space=bases.VectorSpaceWithBasis.from_names([\"query\"]),\n          key_space=bases.VectorSpaceWithBasis.from_names([\"bos\", \"key\"]),\n          value_space=bases.VectorSpaceWithBasis.from_names([\"bos\", \"value\"]),\n          output_space=bases.VectorSpaceWithBasis.from_names([\"output\"]),\n          bos_space=bases.VectorSpaceWithBasis.from_names([\"bos\"]),\n          one_space=bases.VectorSpaceWithBasis.from_names([\"one\"]),\n          attn_fn=lambda x, y: True,\n      )\n    elif block_type == \"MLP\":\n      return categorical_mlp.map_categorical_mlp(\n          input_space=bases.VectorSpaceWithBasis.from_names([\"input\"]),\n          output_space=bases.VectorSpaceWithBasis.from_names([\"output\"]),\n          operation=lambda x: x,\n      )\n    else:\n      return None\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "craft_graph_to_model_test.py"], "context_start_lineno": 0, "lineno": 31, "function_name": "_get_dummy_block", "line_no": 31}}
{"_id": "deepmind_tracr/53", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Converting a RaspExpr to a graph.\"\"\"\n\nimport dataclasses\nimport queue\nfrom typing import List\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\n@dataclasses.dataclass\nclass ExtractRaspGraphOutput:\n  graph: nx.DiGraph\n  sink: Node  # the program's output.\n  sources: List[Node]  # the primitive S-Ops.\n\n\ndef extract_rasp_graph(tip: rasp.SOp) -> ExtractRaspGraphOutput:\n  \"\"\"Converts a RASP program into a graph representation.\"\"\"\n  expr_queue = queue.Queue()\n  graph = nx.DiGraph()\n  sources: List[NodeID] = []\n\n  def ensure_node(expr: rasp.RASPExpr) -> NodeID:\n    \"\"\"Finds or creates a graph node corresponding to expr; returns its ID.\"\"\"", "metadata": {"task_id": "deepmind_tracr/53", "ground_truth": "    node_id = expr.label\n    if node_id not in graph:\n      graph.add_node(node_id, **{nodes.ID: node_id, nodes.EXPR: expr})\n\n    return node_id\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_graph.py"], "context_start_lineno": 0, "lineno": 43, "function_name": "ensure_node", "line_no": 43}}
{"_id": "deepmind_tracr/54", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Converting a RaspExpr to a graph.\"\"\"\n\nimport dataclasses\nimport queue\nfrom typing import List\n\nimport networkx as nx\nfrom tracr.compiler import nodes\nfrom tracr.rasp import rasp\n\nNode = nodes.Node\nNodeID = nodes.NodeID\n\n\n@dataclasses.dataclass\nclass ExtractRaspGraphOutput:\n  graph: nx.DiGraph\n  sink: Node  # the program's output.\n  sources: List[Node]  # the primitive S-Ops.\n\n\ndef extract_rasp_graph(tip: rasp.SOp) -> ExtractRaspGraphOutput:\n  \"\"\"Converts a RASP program into a graph representation.\"\"\"\n  expr_queue = queue.Queue()\n  graph = nx.DiGraph()\n  sources: List[NodeID] = []\n\n  def ensure_node(expr: rasp.RASPExpr) -> NodeID:\n    \"\"\"Finds or creates a graph node corresponding to expr; returns its ID.\"\"\"\n    node_id = expr.label\n    if node_id not in graph:\n      graph.add_node(node_id, **{nodes.ID: node_id, nodes.EXPR: expr})\n\n    return node_id\n\n  # Breadth-first search over the RASP expression graph.\n\n  def visit_raspexpr(expr: rasp.RASPExpr):", "metadata": {"task_id": "deepmind_tracr/54", "ground_truth": "    parent_id = ensure_node(expr)\n\n    for child_expr in expr.children:\n      expr_queue.put(child_expr)\n      child_id = ensure_node(child_expr)\n      graph.add_edge(child_id, parent_id)\n\n    if not expr.children:\n      sources.append(graph.nodes[parent_id])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_graph.py"], "context_start_lineno": 0, "lineno": 52, "function_name": "visit_raspexpr", "line_no": 52}}
{"_id": "deepmind_tracr/55", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Integration tests for the RASP -> craft stages of the compiler.\"\"\"\n\nimport unittest\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nfrom tracr.compiler import basis_inference\nfrom tracr.compiler import craft_graph_to_model\nfrom tracr.compiler import expr_to_craft_graph\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.compiler import test_cases\nfrom tracr.craft import bases\nfrom tracr.craft import tests_common\nfrom tracr.rasp import rasp\n\n_BOS_DIRECTION = \"rasp_to_transformer_integration_test_BOS\"\n_ONE_DIRECTION = \"rasp_to_craft_integration_test_ONE\"\n\n\ndef _make_input_space(vocab, max_seq_len):", "metadata": {"task_id": "deepmind_tracr/55", "ground_truth": "  tokens_space = bases.VectorSpaceWithBasis.from_values(\"tokens\", vocab)\n  indices_space = bases.VectorSpaceWithBasis.from_values(\n      \"indices\", range(max_seq_len))\n  one_space = bases.VectorSpaceWithBasis.from_names([_ONE_DIRECTION])\n  bos_space = bases.VectorSpaceWithBasis.from_names([_BOS_DIRECTION])\n  input_space = bases.join_vector_spaces(tokens_space, indices_space, one_space,\n                                         bos_space)\n\n  return input_space\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_craft_integration_test.py"], "context_start_lineno": 0, "lineno": 36, "function_name": "_make_input_space", "line_no": 36}}
{"_id": "deepmind_tracr/56", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Integration tests for the RASP -> craft stages of the compiler.\"\"\"\n\nimport unittest\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nfrom tracr.compiler import basis_inference\nfrom tracr.compiler import craft_graph_to_model\nfrom tracr.compiler import expr_to_craft_graph\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.compiler import test_cases\nfrom tracr.craft import bases\nfrom tracr.craft import tests_common\nfrom tracr.rasp import rasp\n\n_BOS_DIRECTION = \"rasp_to_transformer_integration_test_BOS\"\n_ONE_DIRECTION = \"rasp_to_craft_integration_test_ONE\"\n\n\ndef _make_input_space(vocab, max_seq_len):\n  tokens_space = bases.VectorSpaceWithBasis.from_values(\"tokens\", vocab)\n  indices_space = bases.VectorSpaceWithBasis.from_values(\n      \"indices\", range(max_seq_len))\n  one_space = bases.VectorSpaceWithBasis.from_names([_ONE_DIRECTION])\n  bos_space = bases.VectorSpaceWithBasis.from_names([_BOS_DIRECTION])\n  input_space = bases.join_vector_spaces(tokens_space, indices_space, one_space,\n                                         bos_space)\n\n  return input_space\n\n\ndef _embed_input(input_seq, input_space):", "metadata": {"task_id": "deepmind_tracr/56", "ground_truth": "  bos_vec = input_space.vector_from_basis_direction(\n      bases.BasisDirection(_BOS_DIRECTION))\n  one_vec = input_space.vector_from_basis_direction(\n      bases.BasisDirection(_ONE_DIRECTION))\n  embedded_input = [bos_vec + one_vec]\n  for i, val in enumerate(input_seq):\n    i_vec = input_space.vector_from_basis_direction(\n        bases.BasisDirection(\"indices\", i))\n    val_vec = input_space.vector_from_basis_direction(\n        bases.BasisDirection(\"tokens\", val))\n    embedded_input.append(i_vec + val_vec + one_vec)\n  return bases.VectorInBasis.stack(embedded_input)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_craft_integration_test.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "_embed_input", "line_no": 48}}
{"_id": "deepmind_tracr/57", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Integration tests for the RASP -> craft stages of the compiler.\"\"\"\n\nimport unittest\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nfrom tracr.compiler import basis_inference\nfrom tracr.compiler import craft_graph_to_model\nfrom tracr.compiler import expr_to_craft_graph\nfrom tracr.compiler import nodes\nfrom tracr.compiler import rasp_to_graph\nfrom tracr.compiler import test_cases\nfrom tracr.craft import bases\nfrom tracr.craft import tests_common\nfrom tracr.rasp import rasp\n\n_BOS_DIRECTION = \"rasp_to_transformer_integration_test_BOS\"\n_ONE_DIRECTION = \"rasp_to_craft_integration_test_ONE\"\n\n\ndef _make_input_space(vocab, max_seq_len):\n  tokens_space = bases.VectorSpaceWithBasis.from_values(\"tokens\", vocab)\n  indices_space = bases.VectorSpaceWithBasis.from_values(\n      \"indices\", range(max_seq_len))\n  one_space = bases.VectorSpaceWithBasis.from_names([_ONE_DIRECTION])\n  bos_space = bases.VectorSpaceWithBasis.from_names([_BOS_DIRECTION])\n  input_space = bases.join_vector_spaces(tokens_space, indices_space, one_space,\n                                         bos_space)\n\n  return input_space\n\n\ndef _embed_input(input_seq, input_space):\n  bos_vec = input_space.vector_from_basis_direction(\n      bases.BasisDirection(_BOS_DIRECTION))\n  one_vec = input_space.vector_from_basis_direction(\n      bases.BasisDirection(_ONE_DIRECTION))\n  embedded_input = [bos_vec + one_vec]\n  for i, val in enumerate(input_seq):\n    i_vec = input_space.vector_from_basis_direction(\n        bases.BasisDirection(\"indices\", i))\n    val_vec = input_space.vector_from_basis_direction(\n        bases.BasisDirection(\"tokens\", val))\n    embedded_input.append(i_vec + val_vec + one_vec)\n  return bases.VectorInBasis.stack(embedded_input)\n\n\ndef _embed_output(output_seq, output_space, categorical_output):", "metadata": {"task_id": "deepmind_tracr/57", "ground_truth": "  embedded_output = []\n  output_label = output_space.basis[0].name\n  for x in output_seq:\n    if x is None:\n      out_vec = output_space.null_vector()\n    elif categorical_output:\n      out_vec = output_space.vector_from_basis_direction(\n          bases.BasisDirection(output_label, x))\n    else:\n      out_vec = x * output_space.vector_from_basis_direction(\n          output_space.basis[0])\n    embedded_output.append(out_vec)\n  return bases.VectorInBasis.stack(embedded_output)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "rasp_to_craft_integration_test.py"], "context_start_lineno": 0, "lineno": 63, "function_name": "_embed_output", "line_no": 63}}
{"_id": "deepmind_tracr/58", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP programs only using the subset of RASP supported by the compiler.\"\"\"\n\nfrom typing import List, Sequence\n\nfrom tracr.rasp import rasp\n\n### Programs that work only under non-causal evaluation.\n\n\ndef make_length() -> rasp.SOp:\n  \"\"\"Creates the `length` SOp using selector width primitive.\n\n  Example usage:\n    length = make_length()\n    length(\"abcdefg\")\n    >> [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n\n  Returns:\n    length: SOp mapping an input to a sequence, where every element\n      is the length of that sequence.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/58", "ground_truth": "  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 35, "function_name": "make_length", "line_no": 35}}
{"_id": "deepmind_tracr/59", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP programs only using the subset of RASP supported by the compiler.\"\"\"\n\nfrom typing import List, Sequence\n\nfrom tracr.rasp import rasp\n\n### Programs that work only under non-causal evaluation.\n\n\ndef make_length() -> rasp.SOp:\n  \"\"\"Creates the `length` SOp using selector width primitive.\n\n  Example usage:\n    length = make_length()\n    length(\"abcdefg\")\n    >> [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n\n  Returns:\n    length: SOp mapping an input to a sequence, where every element\n      is the length of that sequence.\n  \"\"\"\n  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n\n\nlength = make_length()\n\n\ndef make_reverse(sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Create an SOp that reverses a sequence, using length primitive.\n\n  Example usage:\n    reverse = make_reverse(rasp.tokens)\n    reverse(\"Hello\")\n    >> ['o', 'l', 'l', 'e', 'H']\n\n  Args:\n    sop: an SOp\n\n  Returns:\n    reverse : SOp that reverses the input sequence.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/59", "ground_truth": "  opp_idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 57, "function_name": "make_reverse", "line_no": 57}}
{"_id": "deepmind_tracr/60", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP programs only using the subset of RASP supported by the compiler.\"\"\"\n\nfrom typing import List, Sequence\n\nfrom tracr.rasp import rasp\n\n### Programs that work only under non-causal evaluation.\n\n\ndef make_length() -> rasp.SOp:\n  \"\"\"Creates the `length` SOp using selector width primitive.\n\n  Example usage:\n    length = make_length()\n    length(\"abcdefg\")\n    >> [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n\n  Returns:\n    length: SOp mapping an input to a sequence, where every element\n      is the length of that sequence.\n  \"\"\"\n  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n\n\nlength = make_length()\n\n\ndef make_reverse(sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Create an SOp that reverses a sequence, using length primitive.\n\n  Example usage:\n    reverse = make_reverse(rasp.tokens)\n    reverse(\"Hello\")\n    >> ['o', 'l', 'l', 'e', 'H']\n\n  Args:\n    sop: an SOp\n\n  Returns:\n    reverse : SOp that reverses the input sequence.\n  \"\"\"\n  opp_idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n\n\ndef make_pair_balance(sop: rasp.SOp, open_token: str,\n                      close_token: str) -> rasp.SOp:\n  \"\"\"Return fraction of previous open tokens minus the fraction of close tokens.\n\n   (As implemented in the RASP paper.)\n\n  If the outputs are always non-negative and end in 0, that implies the input\n  has balanced parentheses.\n\n  Example usage:\n    num_l = make_pair_balance(rasp.tokens, \"(\", \")\")\n    num_l(\"a()b(c))\")\n    >> [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/60", "ground_truth": "  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 87, "function_name": "make_pair_balance", "line_no": 87}}
{"_id": "deepmind_tracr/61", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP programs only using the subset of RASP supported by the compiler.\"\"\"\n\nfrom typing import List, Sequence\n\nfrom tracr.rasp import rasp\n\n### Programs that work only under non-causal evaluation.\n\n\ndef make_length() -> rasp.SOp:\n  \"\"\"Creates the `length` SOp using selector width primitive.\n\n  Example usage:\n    length = make_length()\n    length(\"abcdefg\")\n    >> [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n\n  Returns:\n    length: SOp mapping an input to a sequence, where every element\n      is the length of that sequence.\n  \"\"\"\n  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n\n\nlength = make_length()\n\n\ndef make_reverse(sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Create an SOp that reverses a sequence, using length primitive.\n\n  Example usage:\n    reverse = make_reverse(rasp.tokens)\n    reverse(\"Hello\")\n    >> ['o', 'l', 'l', 'e', 'H']\n\n  Args:\n    sop: an SOp\n\n  Returns:\n    reverse : SOp that reverses the input sequence.\n  \"\"\"\n  opp_idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n\n\ndef make_pair_balance(sop: rasp.SOp, open_token: str,\n                      close_token: str) -> rasp.SOp:\n  \"\"\"Return fraction of previous open tokens minus the fraction of close tokens.\n\n   (As implemented in the RASP paper.)\n\n  If the outputs are always non-negative and end in 0, that implies the input\n  has balanced parentheses.\n\n  Example usage:\n    num_l = make_pair_balance(rasp.tokens, \"(\", \")\")\n    num_l(\"a()b(c))\")\n    >> [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"\n  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n\n\ndef make_shuffle_dyck(pairs: List[str]) -> rasp.SOp:\n  \"\"\"Returns 1 if a set of parentheses are balanced, 0 else.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    shuffle_dyck2 = make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n    shuffle_dyck2(\"({)}\")\n    >> [1, 1, 1, 1]\n    shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/61", "ground_truth": "  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 165, "function_name": "make_hist", "line_no": 165}}
{"_id": "deepmind_tracr/62", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RASP programs only using the subset of RASP supported by the compiler.\"\"\"\n\nfrom typing import List, Sequence\n\nfrom tracr.rasp import rasp\n\n### Programs that work only under non-causal evaluation.\n\n\ndef make_length() -> rasp.SOp:\n  \"\"\"Creates the `length` SOp using selector width primitive.\n\n  Example usage:\n    length = make_length()\n    length(\"abcdefg\")\n    >> [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n\n  Returns:\n    length: SOp mapping an input to a sequence, where every element\n      is the length of that sequence.\n  \"\"\"\n  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n\n\nlength = make_length()\n\n\ndef make_reverse(sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Create an SOp that reverses a sequence, using length primitive.\n\n  Example usage:\n    reverse = make_reverse(rasp.tokens)\n    reverse(\"Hello\")\n    >> ['o', 'l', 'l', 'e', 'H']\n\n  Args:\n    sop: an SOp\n\n  Returns:\n    reverse : SOp that reverses the input sequence.\n  \"\"\"\n  opp_idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n\n\ndef make_pair_balance(sop: rasp.SOp, open_token: str,\n                      close_token: str) -> rasp.SOp:\n  \"\"\"Return fraction of previous open tokens minus the fraction of close tokens.\n\n   (As implemented in the RASP paper.)\n\n  If the outputs are always non-negative and end in 0, that implies the input\n  has balanced parentheses.\n\n  Example usage:\n    num_l = make_pair_balance(rasp.tokens, \"(\", \")\")\n    num_l(\"a()b(c))\")\n    >> [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"\n  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n\n\ndef make_shuffle_dyck(pairs: List[str]) -> rasp.SOp:\n  \"\"\"Returns 1 if a set of parentheses are balanced, 0 else.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    shuffle_dyck2 = make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n    shuffle_dyck2(\"({)}\")\n    >> [1, 1, 1, 1]\n    shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/62", "ground_truth": "  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 0, "lineno": 184, "function_name": "make_sort_unique", "line_no": 184}}
{"_id": "deepmind_tracr/63", "text": " \"\"\"\n  all_true_selector = rasp.Select(\n      rasp.tokens, rasp.tokens, rasp.Comparison.TRUE).named(\"all_true_selector\")\n  return rasp.SelectorWidth(all_true_selector).named(\"length\")\n\n\nlength = make_length()\n\n\ndef make_reverse(sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Create an SOp that reverses a sequence, using length primitive.\n\n  Example usage:\n    reverse = make_reverse(rasp.tokens)\n    reverse(\"Hello\")\n    >> ['o', 'l', 'l', 'e', 'H']\n\n  Args:\n    sop: an SOp\n\n  Returns:\n    reverse : SOp that reverses the input sequence.\n  \"\"\"\n  opp_idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n\n\ndef make_pair_balance(sop: rasp.SOp, open_token: str,\n                      close_token: str) -> rasp.SOp:\n  \"\"\"Return fraction of previous open tokens minus the fraction of close tokens.\n\n   (As implemented in the RASP paper.)\n\n  If the outputs are always non-negative and end in 0, that implies the input\n  has balanced parentheses.\n\n  Example usage:\n    num_l = make_pair_balance(rasp.tokens, \"(\", \")\")\n    num_l(\"a()b(c))\")\n    >> [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"\n  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n\n\ndef make_shuffle_dyck(pairs: List[str]) -> rasp.SOp:\n  \"\"\"Returns 1 if a set of parentheses are balanced, 0 else.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    shuffle_dyck2 = make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n    shuffle_dyck2(\"({)}\")\n    >> [1, 1, 1, 1]\n    shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/63", "ground_truth": "  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 34, "lineno": 216, "function_name": "make_sort", "line_no": 216}}
{"_id": "deepmind_tracr/64", "text": "idx = (length - rasp.indices).named(\"opp_idx\")\n  opp_idx = (opp_idx - 1).named(\"opp_idx-1\")\n  reverse_selector = rasp.Select(rasp.indices, opp_idx,\n                                 rasp.Comparison.EQ).named(\"reverse_selector\")\n  return rasp.Aggregate(reverse_selector, sop).named(\"reverse\")\n\n\ndef make_pair_balance(sop: rasp.SOp, open_token: str,\n                      close_token: str) -> rasp.SOp:\n  \"\"\"Return fraction of previous open tokens minus the fraction of close tokens.\n\n   (As implemented in the RASP paper.)\n\n  If the outputs are always non-negative and end in 0, that implies the input\n  has balanced parentheses.\n\n  Example usage:\n    num_l = make_pair_balance(rasp.tokens, \"(\", \")\")\n    num_l(\"a()b(c))\")\n    >> [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"\n  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n\n\ndef make_shuffle_dyck(pairs: List[str]) -> rasp.SOp:\n  \"\"\"Returns 1 if a set of parentheses are balanced, 0 else.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    shuffle_dyck2 = make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n    shuffle_dyck2(\"({)}\")\n    >> [1, 1, 1, 1]\n    shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n\n\ndef make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/64", "ground_truth": "  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 57, "lineno": 235, "function_name": "make_sort_freq", "line_no": 235}}
{"_id": "deepmind_tracr/65", "text": " [0, 1/2, 0, 0, 1/5, 1/6, 0, -1/8]\n\n  Args:\n    sop: Input SOp.\n    open_token: Token that counts positive.\n    close_token: Token that counts negative.\n\n  Returns:\n    pair_balance: SOp mapping an input to a sequence, where every element\n      is the fraction of previous open tokens minus previous close tokens.\n  \"\"\"\n  bools_open = rasp.numerical(sop == open_token).named(\"bools_open\")\n  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n\n\ndef make_shuffle_dyck(pairs: List[str]) -> rasp.SOp:\n  \"\"\"Returns 1 if a set of parentheses are balanced, 0 else.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    shuffle_dyck2 = make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n    shuffle_dyck2(\"({)}\")\n    >> [1, 1, 1, 1]\n    shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n\n\ndef make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"\n  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n\n\n### Programs that work under both causal and regular evaluation.\n\n\ndef make_frac_prevs(bools: rasp.SOp) -> rasp.SOp:\n  \"\"\"Count the fraction of previous tokens where a specific condition was True.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    num_l = make_frac_prevs(rasp.tokens==\"l\")\n    num_l(\"hello\")\n    >> [0, 0, 1/3, 1/2, 2/5]\n\n  Args:\n    bools: SOp mapping a sequence to a sequence of booleans.\n\n  Returns:\n    frac_prevs: SOp mapping an input to a sequence, where every element\n      is the fraction of previous \"True\" tokens.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/65", "ground_truth": "  bools = rasp.numerical(bools)\n  prevs = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LEQ)\n  return rasp.numerical(rasp.Aggregate(prevs, bools,\n                                       default=0)).named(\"frac_prevs\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 76, "lineno": 260, "function_name": "make_frac_prevs", "line_no": 260}}
{"_id": "deepmind_tracr/66", "text": "  opens = rasp.numerical(make_frac_prevs(bools_open)).named(\"opens\")\n\n  bools_close = rasp.numerical(sop == close_token).named(\"bools_close\")\n  closes = rasp.numerical(make_frac_prevs(bools_close)).named(\"closes\")\n\n  pair_balance = rasp.numerical(rasp.LinearSequenceMap(opens, closes, 1, -1))\n  return pair_balance.named(\"pair_balance\")\n\n\ndef make_shuffle_dyck(pairs: List[str]) -> rasp.SOp:\n  \"\"\"Returns 1 if a set of parentheses are balanced, 0 else.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    shuffle_dyck2 = make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n    shuffle_dyck2(\"({)}\")\n    >> [1, 1, 1, 1]\n    shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n\n\ndef make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"\n  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n\n\n### Programs that work under both causal and regular evaluation.\n\n\ndef make_frac_prevs(bools: rasp.SOp) -> rasp.SOp:\n  \"\"\"Count the fraction of previous tokens where a specific condition was True.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    num_l = make_frac_prevs(rasp.tokens==\"l\")\n    num_l(\"hello\")\n    >> [0, 0, 1/3, 1/2, 2/5]\n\n  Args:\n    bools: SOp mapping a sequence to a sequence of booleans.\n\n  Returns:\n    frac_prevs: SOp mapping an input to a sequence, where every element\n      is the fraction of previous \"True\" tokens.\n  \"\"\"\n  bools = rasp.numerical(bools)\n  prevs = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LEQ)\n  return rasp.numerical(rasp.Aggregate(prevs, bools,\n                                       default=0)).named(\"frac_prevs\")\n\n\ndef shift_by(offset: int, /, sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns the sop, shifted by `offset`, None-padded.\"\"\"", "metadata": {"task_id": "deepmind_tracr/66", "ground_truth": "  select_off_by_offset = rasp.Select(rasp.indices, rasp.indices,\n                                     lambda k, q: q == k + offset)\n  out = rasp.Aggregate(select_off_by_offset, sop, default=None)\n  return out.named(f\"shift_by({offset})\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 88, "lineno": 268, "function_name": "shift_by", "line_no": 268}}
{"_id": "deepmind_tracr/67", "text": " shuffle_dyck2(\"(){)}\")\n    >> [0, 0, 0, 0, 0]\n\n  Args:\n    pairs: List of pairs of open and close tokens that each should be balanced.\n  \"\"\"\n  assert len(pairs) >= 1\n\n  # Compute running balance of each type of parenthesis\n  balances = []\n  for pair in pairs:\n    assert len(pair) == 2\n    open_token, close_token = pair\n    balance = make_pair_balance(\n        rasp.tokens, open_token=open_token,\n        close_token=close_token).named(f\"balance_{pair}\")\n    balances.append(balance)\n\n  # Check if balances where negative anywhere -> parentheses not balanced\n  any_negative = balances[0] < 0\n  for balance in balances[1:]:\n    any_negative = any_negative | (balance < 0)\n\n  # Convert to numerical SOp\n  any_negative = rasp.numerical(rasp.Map(lambda x: x,\n                                         any_negative)).named(\"any_negative\")\n\n  select_all = rasp.Select(rasp.indices, rasp.indices,\n                           rasp.Comparison.TRUE).named(\"select_all\")\n  has_neg = rasp.numerical(rasp.Aggregate(select_all, any_negative,\n                                          default=0)).named(\"has_neg\")\n\n  # Check if all balances are 0 at the end -> closed all parentheses\n  all_zero = balances[0] == 0\n  for balance in balances[1:]:\n    all_zero = all_zero & (balance == 0)\n\n  select_last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n\n\ndef make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"\n  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n\n\n### Programs that work under both causal and regular evaluation.\n\n\ndef make_frac_prevs(bools: rasp.SOp) -> rasp.SOp:\n  \"\"\"Count the fraction of previous tokens where a specific condition was True.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    num_l = make_frac_prevs(rasp.tokens==\"l\")\n    num_l(\"hello\")\n    >> [0, 0, 1/3, 1/2, 2/5]\n\n  Args:\n    bools: SOp mapping a sequence to a sequence of booleans.\n\n  Returns:\n    frac_prevs: SOp mapping an input to a sequence, where every element\n      is the fraction of previous \"True\" tokens.\n  \"\"\"\n  bools = rasp.numerical(bools)\n  prevs = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LEQ)\n  return rasp.numerical(rasp.Aggregate(prevs, bools,\n                                       default=0)).named(\"frac_prevs\")\n\n\ndef shift_by(offset: int, /, sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns the sop, shifted by `offset`, None-padded.\"\"\"\n  select_off_by_offset = rasp.Select(rasp.indices, rasp.indices,\n                                     lambda k, q: q == k + offset)\n  out = rasp.Aggregate(select_off_by_offset, sop, default=None)\n  return out.named(f\"shift_by({offset})\")\n\n\ndef detect_pattern(sop: rasp.SOp, pattern: Sequence[rasp.Value]) -> rasp.SOp:\n  \"\"\"Returns an SOp which is True at the final element of the pattern.\n\n  The first len(pattern) - 1 elements of the output SOp are None-padded.\n\n  detect_pattern(tokens, \"abc\")(\"abcabc\") == [None, None, T, F, F, T]\n\n  Args:\n    sop: the SOp in which to look for patterns.\n    pattern: a sequence of values to look for.\n\n  Returns:\n    a sop which detects the pattern.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/67", "ground_truth": "  if len(pattern) < 1:\n    raise ValueError(f\"Length of `pattern` must be at least 1. Got {pattern}\")\n\n  # detectors[i] will be a boolean-valued SOp which is true at position j iff\n  # the i'th (from the end) element of the pattern was detected at position j-i.\n  detectors = []\n  for i, element in enumerate(reversed(pattern)):\n    detector = sop == element\n    if i != 0:\n      detector = shift_by(i, detector)\n    detectors.append(detector)\n\n  # All that's left is to take the AND over all detectors.\n  pattern_detected = detectors.pop()\n  while detectors:\n    pattern_detected = pattern_detected & detectors.pop()\n\n  return pattern_detected.named(f\"detect_pattern({pattern})\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 106, "lineno": 289, "function_name": "detect_pattern", "line_no": 289}}
{"_id": "deepmind_tracr/68", "text": "last = rasp.Select(rasp.indices, length - 1,\n                            rasp.Comparison.EQ).named(\"select_last\")\n  last_zero = rasp.Aggregate(select_last, all_zero).named(\"last_zero\")\n\n  not_has_neg = (~has_neg).named(\"not_has_neg\")\n  return (last_zero & not_has_neg).named(\"shuffle_dyck\")\n\n\ndef make_shuffle_dyck2() -> rasp.SOp:\n  return make_shuffle_dyck(pairs=[\"()\", \"{}\"]).named(\"shuffle_dyck2\")\n\n\ndef make_hist() -> rasp.SOp:\n  \"\"\"Returns the number of times each token occurs in the input.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    hist = make_hist()\n    hist(\"abac\")\n    >> [2, 1, 2, 1]\n  \"\"\"\n  same_tok = rasp.Select(rasp.tokens, rasp.tokens,\n                         rasp.Comparison.EQ).named(\"same_tok\")\n  return rasp.SelectorWidth(same_tok).named(\"hist\")\n\n\ndef make_sort_unique(vals: rasp.SOp, keys: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys.\n\n  Only supports unique keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n  \"\"\"\n  smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n\n\ndef make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"\n  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n\n\n### Programs that work under both causal and regular evaluation.\n\n\ndef make_frac_prevs(bools: rasp.SOp) -> rasp.SOp:\n  \"\"\"Count the fraction of previous tokens where a specific condition was True.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    num_l = make_frac_prevs(rasp.tokens==\"l\")\n    num_l(\"hello\")\n    >> [0, 0, 1/3, 1/2, 2/5]\n\n  Args:\n    bools: SOp mapping a sequence to a sequence of booleans.\n\n  Returns:\n    frac_prevs: SOp mapping an input to a sequence, where every element\n      is the fraction of previous \"True\" tokens.\n  \"\"\"\n  bools = rasp.numerical(bools)\n  prevs = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LEQ)\n  return rasp.numerical(rasp.Aggregate(prevs, bools,\n                                       default=0)).named(\"frac_prevs\")\n\n\ndef shift_by(offset: int, /, sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns the sop, shifted by `offset`, None-padded.\"\"\"\n  select_off_by_offset = rasp.Select(rasp.indices, rasp.indices,\n                                     lambda k, q: q == k + offset)\n  out = rasp.Aggregate(select_off_by_offset, sop, default=None)\n  return out.named(f\"shift_by({offset})\")\n\n\ndef detect_pattern(sop: rasp.SOp, pattern: Sequence[rasp.Value]) -> rasp.SOp:\n  \"\"\"Returns an SOp which is True at the final element of the pattern.\n\n  The first len(pattern) - 1 elements of the output SOp are None-padded.\n\n  detect_pattern(tokens, \"abc\")(\"abcabc\") == [None, None, T, F, F, T]\n\n  Args:\n    sop: the SOp in which to look for patterns.\n    pattern: a sequence of values to look for.\n\n  Returns:\n    a sop which detects the pattern.\n  \"\"\"\n\n  if len(pattern) < 1:\n    raise ValueError(f\"Length of `pattern` must be at least 1. Got {pattern}\")\n\n  # detectors[i] will be a boolean-valued SOp which is true at position j iff\n  # the i'th (from the end) element of the pattern was detected at position j-i.\n  detectors = []\n  for i, element in enumerate(reversed(pattern)):\n    detector = sop == element\n    if i != 0:\n      detector = shift_by(i, detector)\n    detectors.append(detector)\n\n  # All that's left is to take the AND over all detectors.\n  pattern_detected = detectors.pop()\n  while detectors:\n    pattern_detected = pattern_detected & detectors.pop()\n\n  return pattern_detected.named(f\"detect_pattern({pattern})\")\n\n\ndef make_count_less_freq(n: int) -> rasp.SOp:\n  \"\"\"Returns how many tokens appear fewer than n times in the input.\n\n  The output sequence contains this count in each position.\n\n  Example usage:\n    count_less_freq = make_count_less_freq(2)\n    count_less_freq([\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"])\n    >> [3, 3, 3, 3, 3, 3]\n    count_less_freq([\"a\", \"a\", \"c\", \"b\", \"b\", \"c\"])\n    >> [6, 6, 6, 6, 6, 6]\n\n  Args:\n    n: Integer to compare token frequences to.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/68", "ground_truth": "  hist = make_hist().named(\"hist\")\n  select_less = rasp.Select(hist, hist,\n                            lambda x, y: x <= n).named(\"select_less\")\n  return rasp.SelectorWidth(select_less).named(\"count_less_freq\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 143, "lineno": 324, "function_name": "make_count_less_freq", "line_no": 324}}
{"_id": "deepmind_tracr/69", "text": ".LT).named(\"smaller\")\n  target_pos = rasp.SelectorWidth(smaller).named(\"target_pos\")\n  sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)\n  return rasp.Aggregate(sel_new, vals).named(\"sort\")\n\n\ndef make_sort(vals: rasp.SOp, keys: rasp.SOp, *, max_seq_len: int,\n              min_key: float) -> rasp.SOp:\n  \"\"\"Returns vals sorted by < relation on keys, which don't need to be unique.\n\n  The implementation differs from the RASP paper, as it avoids using\n  compositions of selectors to break ties. Instead, it uses the arguments\n  max_seq_len and min_key to ensure the keys are unique.\n\n  Note that this approach only works for numerical keys.\n\n  Example usage:\n    sort = make_sort(rasp.tokens, rasp.tokens, 5, 1)\n    sort([2, 4, 3, 1])\n    >> [1, 2, 3, 4]\n    sort([2, 4, 1, 2])\n    >> [1, 2, 2, 4]\n\n  Args:\n    vals: Values to sort.\n    keys: Keys for sorting.\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n    min_key: Minimum key value (used to ensure keys are unique)\n\n  Returns:\n    Output SOp of sort program.\n  \"\"\"\n  keys = rasp.SequenceMap(lambda x, i: x + min_key * i / max_seq_len, keys,\n                          rasp.indices)\n  return make_sort_unique(vals, keys)\n\n\ndef make_sort_freq(max_seq_len: int) -> rasp.SOp:\n  \"\"\"Returns tokens sorted by the frequency they appear in the input.\n\n  Tokens the appear the same amount of times are output in the same order as in\n  the input.\n\n  Example usage:\n    sort = make_sort_freq(rasp.tokens, rasp.tokens, 5)\n    sort([2, 4, 2, 1])\n    >> [2, 2, 4, 1]\n\n  Args:\n    max_seq_len: Maximum sequence length (used to ensure keys are unique)\n  \"\"\"\n  hist = -1 * make_hist().named(\"hist\")\n  return make_sort(\n      rasp.tokens, hist, max_seq_len=max_seq_len, min_key=1).named(\"sort_freq\")\n\n\n### Programs that work under both causal and regular evaluation.\n\n\ndef make_frac_prevs(bools: rasp.SOp) -> rasp.SOp:\n  \"\"\"Count the fraction of previous tokens where a specific condition was True.\n\n   (As implemented in the RASP paper.)\n\n  Example usage:\n    num_l = make_frac_prevs(rasp.tokens==\"l\")\n    num_l(\"hello\")\n    >> [0, 0, 1/3, 1/2, 2/5]\n\n  Args:\n    bools: SOp mapping a sequence to a sequence of booleans.\n\n  Returns:\n    frac_prevs: SOp mapping an input to a sequence, where every element\n      is the fraction of previous \"True\" tokens.\n  \"\"\"\n  bools = rasp.numerical(bools)\n  prevs = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.LEQ)\n  return rasp.numerical(rasp.Aggregate(prevs, bools,\n                                       default=0)).named(\"frac_prevs\")\n\n\ndef shift_by(offset: int, /, sop: rasp.SOp) -> rasp.SOp:\n  \"\"\"Returns the sop, shifted by `offset`, None-padded.\"\"\"\n  select_off_by_offset = rasp.Select(rasp.indices, rasp.indices,\n                                     lambda k, q: q == k + offset)\n  out = rasp.Aggregate(select_off_by_offset, sop, default=None)\n  return out.named(f\"shift_by({offset})\")\n\n\ndef detect_pattern(sop: rasp.SOp, pattern: Sequence[rasp.Value]) -> rasp.SOp:\n  \"\"\"Returns an SOp which is True at the final element of the pattern.\n\n  The first len(pattern) - 1 elements of the output SOp are None-padded.\n\n  detect_pattern(tokens, \"abc\")(\"abcabc\") == [None, None, T, F, F, T]\n\n  Args:\n    sop: the SOp in which to look for patterns.\n    pattern: a sequence of values to look for.\n\n  Returns:\n    a sop which detects the pattern.\n  \"\"\"\n\n  if len(pattern) < 1:\n    raise ValueError(f\"Length of `pattern` must be at least 1. Got {pattern}\")\n\n  # detectors[i] will be a boolean-valued SOp which is true at position j iff\n  # the i'th (from the end) element of the pattern was detected at position j-i.\n  detectors = []\n  for i, element in enumerate(reversed(pattern)):\n    detector = sop == element\n    if i != 0:\n      detector = shift_by(i, detector)\n    detectors.append(detector)\n\n  # All that's left is to take the AND over all detectors.\n  pattern_detected = detectors.pop()\n  while detectors:\n    pattern_detected = pattern_detected & detectors.pop()\n\n  return pattern_detected.named(f\"detect_pattern({pattern})\")\n\n\ndef make_count_less_freq(n: int) -> rasp.SOp:\n  \"\"\"Returns how many tokens appear fewer than n times in the input.\n\n  The output sequence contains this count in each position.\n\n  Example usage:\n    count_less_freq = make_count_less_freq(2)\n    count_less_freq([\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"])\n    >> [3, 3, 3, 3, 3, 3]\n    count_less_freq([\"a\", \"a\", \"c\", \"b\", \"b\", \"c\"])\n    >> [6, 6, 6, 6, 6, 6]\n\n  Args:\n    n: Integer to compare token frequences to.\n  \"\"\"\n  hist = make_hist().named(\"hist\")\n  select_less = rasp.Select(hist, hist,\n                            lambda x, y: x <= n).named(\"select_less\")\n  return rasp.SelectorWidth(select_less).named(\"count_less_freq\")\n\n\ndef make_count(sop, token):\n  \"\"\"Returns the count of `token` in `sop`.\n\n  The output sequence contains this count in each position.\n\n  Example usage:\n    count = make_count(tokens, \"a\")\n    count([\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"])\n    >> [3, 3, 3, 3, 3, 3]\n    count([\"c\", \"a\", \"b\", \"c\"])\n    >> [1, 1, 1, 1]\n\n  Args:\n    sop: Sop to count tokens in.\n    token: Token to count.\n  \"\"\"\n  return rasp.SelectorWidth(rasp.Select(\n      sop, sop, lambda k, q: k == token)).named(f\"count_{token}\")\n\n\ndef make_nary_sequencemap(f, *sops):\n  \"\"\"Returns an SOp that simulates an n-ary SequenceMap.\n\n  Uses multiple binary SequenceMaps to convert n SOps x_1, x_2, ..., x_n\n  into a single SOp arguments that takes n-tuples as value. The n-ary sequence\n  map implementing f is then a Map on this resulting SOp.\n\n  Note that the intermediate variables representing tuples of varying length\n  will be encoded categorically, and can become very high-dimensional. So,\n  using this function might lead to very large compiled models.\n\n  Args:\n    f: Function with n arguments.\n    *sops: Sequence of SOps, one for each argument of f.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/69", "ground_truth": "  values, *sops = sops\n  for sop in sops:\n    # x is a single entry in the first iteration but a tuple in later iterations\n    values = rasp.SequenceMap(\n        lambda x, y: (*x, y) if isinstance(x, tuple) else (x, y), values, sop)\n  return rasp.Map(lambda args: f(*args), values)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "lib.py"], "context_start_lineno": 184, "lineno": 365, "function_name": "make_nary_sequencemap", "line_no": 365}}
{"_id": "deepmind_tracr/70", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.assemble.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.compiler import assemble\nfrom tracr.craft import bases\n\n\nclass AssembleTest(parameterized.TestCase):\n\n  def test_token_embedding_produces_correct_embedding(self):\n    # Token embeddings should be one-hot embeddings of the input integers\n    # into the token subspace of residual_space\n    input_space = bases.VectorSpaceWithBasis.from_values(\"0inp\", range(2))\n    indices_space = bases.VectorSpaceWithBasis.from_values(\"1ind\", range(3))\n    output_space = bases.VectorSpaceWithBasis.from_values(\"2out\", range(2))\n    residual_space = bases.join_vector_spaces(input_space, indices_space,\n                                              output_space)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def token_pos_embed(tokens):", "metadata": {"task_id": "deepmind_tracr/70", "ground_truth": "      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.token_embed(tokens)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble_test.py"], "context_start_lineno": 0, "lineno": 40, "function_name": "token_pos_embed", "line_no": 40}}
{"_id": "deepmind_tracr/71", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.assemble.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.compiler import assemble\nfrom tracr.craft import bases\n\n\nclass AssembleTest(parameterized.TestCase):\n\n  def test_token_embedding_produces_correct_embedding(self):\n    # Token embeddings should be one-hot embeddings of the input integers\n    # into the token subspace of residual_space\n    input_space = bases.VectorSpaceWithBasis.from_values(\"0inp\", range(2))\n    indices_space = bases.VectorSpaceWithBasis.from_values(\"1ind\", range(3))\n    output_space = bases.VectorSpaceWithBasis.from_values(\"2out\", range(2))\n    residual_space = bases.join_vector_spaces(input_space, indices_space,\n                                              output_space)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def token_pos_embed(tokens):\n      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.token_embed(tokens)\n\n    tokens = jnp.array([0, 0, 1])\n    expected_token_embeddings = jnp.array([[1, 0, 0, 0, 0, 0, 0],\n                                           [1, 0, 0, 0, 0, 0, 0],\n                                           [0, 1, 0, 0, 0, 0, 0]])\n\n    params = token_pos_embed.init(jax.random.PRNGKey(0), tokens)\n    embeddings = token_pos_embed.apply(params, tokens)\n    np.testing.assert_allclose(embeddings, expected_token_embeddings)\n\n  def test_position_embedding_produces_correct_embedding(self):\n    # Position embeddings should be one-hot embeddings of the input integers\n    # (representing indices) into the indices subspace of residual_space\n    input_space = bases.VectorSpaceWithBasis.from_values(\"0inp\", range(2))\n    indices_space = bases.VectorSpaceWithBasis.from_values(\"1ind\", range(3))\n    output_space = bases.VectorSpaceWithBasis.from_values(\"2out\", range(2))\n    residual_space = bases.join_vector_spaces(input_space, indices_space,\n                                              output_space)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def token_pos_embed(tokens):", "metadata": {"task_id": "deepmind_tracr/71", "ground_truth": "      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.pos_embed(jnp.indices(tokens.shape)[-1])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble_test.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "token_pos_embed", "line_no": 68}}
{"_id": "deepmind_tracr/72", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.assemble.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.compiler import assemble\nfrom tracr.craft import bases\n\n\nclass AssembleTest(parameterized.TestCase):\n\n  def test_token_embedding_produces_correct_embedding(self):\n    # Token embeddings should be one-hot embeddings of the input integers\n    # into the token subspace of residual_space\n    input_space = bases.VectorSpaceWithBasis.from_values(\"0inp\", range(2))\n    indices_space = bases.VectorSpaceWithBasis.from_values(\"1ind\", range(3))\n    output_space = bases.VectorSpaceWithBasis.from_values(\"2out\", range(2))\n    residual_space = bases.join_vector_spaces(input_space, indices_space,\n                                              output_space)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def token_pos_embed(tokens):\n      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.token_embed(tokens)\n\n    tokens = jnp.array([0, 0, 1])\n    expected_token_embeddings = jnp.array([[1, 0, 0, 0, 0, 0, 0],\n                                           [1, 0, 0, 0, 0, 0, 0],\n                                           [0, 1, 0, 0, 0, 0, 0]])\n\n    params = token_pos_embed.init(jax.random.PRNGKey(0), tokens)\n    embeddings = token_pos_embed.apply(params, tokens)\n    np.testing.assert_allclose(embeddings, expected_token_embeddings)\n\n  def test_position_embedding_produces_correct_embedding(self):\n    # Position embeddings should be one-hot embeddings of the input integers\n    # (representing indices) into the indices subspace of residual_space\n    input_space = bases.VectorSpaceWithBasis.from_values(\"0inp\", range(2))\n    indices_space = bases.VectorSpaceWithBasis.from_values(\"1ind\", range(3))\n    output_space = bases.VectorSpaceWithBasis.from_values(\"2out\", range(2))\n    residual_space = bases.join_vector_spaces(input_space, indices_space,\n                                              output_space)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def token_pos_embed(tokens):\n      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.pos_embed(jnp.indices(tokens.shape)[-1])\n\n    tokens = jnp.array([3, 0, 0, 1])\n    expected_pos_embeddings = jnp.array([[0, 0, 0, 0, 0, 0, 0],\n                                         [0, 0, 1, 0, 0, 0, 0],\n                                         [0, 0, 0, 1, 0, 0, 0],\n                                         [0, 0, 0, 0, 1, 0, 0]])\n\n    params = token_pos_embed.init(jax.random.PRNGKey(0), tokens)\n    embeddings = token_pos_embed.apply(params, tokens)\n    np.testing.assert_allclose(embeddings, expected_pos_embeddings)\n\n  def test_unembedding(self):\n    # Prepend numbers to preserve basis order [input, index, output]\n    input_space = bases.VectorSpaceWithBasis.from_values(\"0inp\", range(2))\n    indices_space = bases.VectorSpaceWithBasis.from_values(\"1ind\", range(3))\n    output_space = bases.VectorSpaceWithBasis.from_values(\"2out\", range(2))\n    residual_space = bases.join_vector_spaces(input_space, indices_space,\n                                              output_space)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def unembed(embeddings):", "metadata": {"task_id": "deepmind_tracr/72", "ground_truth": "      embed_modules = assemble._make_embedding_modules(\n          residual_space=residual_space,\n          tokens_space=input_space,\n          indices_space=indices_space,\n          output_space=output_space)\n      return embed_modules.unembed(embeddings, use_unembed_argmax=True)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "compiler", "assemble_test.py"], "context_start_lineno": 0, "lineno": 96, "function_name": "unembed", "line_no": 96}}
{"_id": "deepmind_tracr/73", "text": ".\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Attention head for categorical inputs.\"\"\"\n\nfrom typing import Optional\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\nfrom typing_extensions import Protocol\n\n\nclass QueryKeyToAttnLogit(Protocol):\n\n  def __call__(self, query: bases.BasisDirection,\n               key: bases.BasisDirection) -> bool:\n    pass\n\n\ndef categorical_attn(\n    query_space: bases.VectorSpaceWithBasis,\n    key_space: bases.VectorSpaceWithBasis,\n    value_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    bos_space: bases.VectorSpaceWithBasis,\n    one_space: bases.VectorSpaceWithBasis,\n    attn_fn: QueryKeyToAttnLogit,\n    default_output: Optional[bases.VectorInBasis] = None,\n    causal: bool = False,\n    always_attend_to_bos: bool = False,\n    use_bos_for_default_output: bool = True,\n    softmax_coldness: float = 100.,\n) -> transformers.AttentionHead:\n  \"\"\"Returns an attention head for categorical inputs.\n\n  Assumes the existence of a beginning of sequence token and attends to it\n  always with strength 0.5*softmax_coldness. This allows to implement an\n  arbitrary default value for rows in the attention pattern that are all-zero.\n\n  Attends to the BOS token if all other key-query pairs have zero attention.\n  Hence, the first value in the value sequence will be the default output for\n  such cases.\n\n  Args:\n    query_space: Vector space containing (categorical) query input.\n    key_space: Vector space containing (categorical) key input.\n    value_space: Vector space containing (numerical) value input.\n    output_space: Vector space which will contain (numerical) output.\n    bos_space: 1-d space used to identify the beginning of sequence token.\n    one_space: 1-d space which contains 1 at every position.\n    attn_fn: A selector function f(query, key) operating on the query/key basis\n      directions that defines the attention pattern.\n    default_output: Output to return if attention pattern is all zero.\n    causal: If True, use masked attention.\n    always_attend_to_bos: If True, always attend to the BOS token. If False,\n      only attend to BOS when attending to nothing else.\n    use_bos_for_default_output: If True, assume BOS is not in the value space\n      and output a default value when attending to BOS. If False, assume BOS is\n      in the value space, and map it to the output space like any other token.\n    softmax_coldness: The inverse temperature of the softmax. Default value is\n      high which makes the attention close to a hard maximum.\n  \"\"\"\n  bases.ensure_dims(bos_space, num_dims=1, name=\"bos_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n  bos_direction = bos_space.basis[0]\n  one_direction = one_space.basis[0]\n\n  # Add bos direction to query, key, and value spaces in case it is missing\n  query_space = bases.join_vector_spaces(query_space, bos_space, one_space)\n  key_space = bases.join_vector_spaces(key_space, bos_space)\n  value_space = bases.join_vector_spaces(value_space, bos_space)\n\n  if always_attend_to_bos:\n    value_basis = value_space.basis\n  else:\n    value_basis = [v for v in value_space.basis if v != bos_direction]\n  assert len(value_basis) == output_space.num_dims\n  value_to_output = dict(zip(value_basis, output_space.basis))\n\n  if default_output is None:\n    default_output = output_space.null_vector()\n  assert default_output in output_space\n\n  def qk_fun(query: bases.BasisDirection, key: bases.BasisDirection) -> float:\n\n    # We want to enforce the following property on our attention patterns:\n    # - if nothing else is attended to, attend to the BOS token.\n    # - otherwise, don't attend to the BOS token.\n    #\n    # We assume that the BOS position always only contains the vector bos + one,\n    # and that any other position has bos coefficient 0.\n    #\n    # We do this as follows:\n    # Let Q and K be subspaces of V containing the query and key vectors,\n    # both disjoint with the BOS space {bos} or the one space {one}.\n    # Suppose we have an attn_fn which defines a bilinear W_QK: V x V -> ℝ,\n    # s.t. W_QK(q, k) = 0 whenever either q or k are bos or one.\n    #\n    # Then define W_new: V x V -> ℝ st:\n    # W_new(one, bos) = 0.5, otherwise 0.\n    #\n    # Now set W_QK' = W_QK + W_new.\n    #\n    # To evaluate the attention to the BOS position:\n    # W_QK'(q, bos + one)\n    # = W_QK'(q, bos) + W_QK'(q, one)\n    # = W_QK(q, bos) + W_QK(q, one) + W_new(q, bos) + W_new(q, one)\n    # = 0            + 0            + W_new(q, bos) + W_new(q, one)\n    # = W_new(q, bos) + W_new(q, one)\n    # = W_new(q' + one, bos) + W_new(q' + one, one)  where q = one + q'\n    # = W_new(q', bos) + W_new(one, bos) + W_new(q', one) + W_new(one, one)\n    # = 0              + 0.5             + 0              + 0\n    # = 0.5\n    #\n    # To evaluate the attention to a non-BOS position:\n    # W_QK'(0 * bos + q, 0 * bos + k)  # s.t. q ∈ Q+{one}, k ∈ K+{one}\n    # = 0*W_QK'(bos, 0*bos + k) + W_QK'(q, 0*bos + k)\n    # = W_QK'(q, 0*bos + k)\n    # = 0*W_QK'(q, bos) + W_QK'(q, k)\n    # = W_QK'(q, k)\n    # = W_QK(q, k)    since W_QK' = W_QK on inputs not containing bos.\n    # = W_QK(q', k')  since W_QK(x, y) = 0 whenever x or y are one.\n    #\n    # Since W_QK(q, k) takes values in 0, 1, a sufficiently high softmax\n    # coldness will give us the desired property.                            QED\n    #\n    # The following implements this idea.\n    # By replacing 0.5 with 1, we can instead enforce a different property: that\n    # the BOS token is always attended to in addition to whatever else.", "metadata": {"task_id": "deepmind_tracr/73", "ground_truth": "    if key == bos_direction and query == one_direction:\n      c = 1. if always_attend_to_bos else 0.5\n      return c * softmax_coldness\n    elif {key, query}.intersection({one_direction, bos_direction}):\n      return 0\n\n    return softmax_coldness * attn_fn(query, key)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_attn.py"], "context_start_lineno": 0, "lineno": 142, "function_name": "qk_fun", "line_no": 142}}
{"_id": "deepmind_tracr/74", "text": " typing import Optional\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\nfrom typing_extensions import Protocol\n\n\nclass QueryKeyToAttnLogit(Protocol):\n\n  def __call__(self, query: bases.BasisDirection,\n               key: bases.BasisDirection) -> bool:\n    pass\n\n\ndef categorical_attn(\n    query_space: bases.VectorSpaceWithBasis,\n    key_space: bases.VectorSpaceWithBasis,\n    value_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    bos_space: bases.VectorSpaceWithBasis,\n    one_space: bases.VectorSpaceWithBasis,\n    attn_fn: QueryKeyToAttnLogit,\n    default_output: Optional[bases.VectorInBasis] = None,\n    causal: bool = False,\n    always_attend_to_bos: bool = False,\n    use_bos_for_default_output: bool = True,\n    softmax_coldness: float = 100.,\n) -> transformers.AttentionHead:\n  \"\"\"Returns an attention head for categorical inputs.\n\n  Assumes the existence of a beginning of sequence token and attends to it\n  always with strength 0.5*softmax_coldness. This allows to implement an\n  arbitrary default value for rows in the attention pattern that are all-zero.\n\n  Attends to the BOS token if all other key-query pairs have zero attention.\n  Hence, the first value in the value sequence will be the default output for\n  such cases.\n\n  Args:\n    query_space: Vector space containing (categorical) query input.\n    key_space: Vector space containing (categorical) key input.\n    value_space: Vector space containing (numerical) value input.\n    output_space: Vector space which will contain (numerical) output.\n    bos_space: 1-d space used to identify the beginning of sequence token.\n    one_space: 1-d space which contains 1 at every position.\n    attn_fn: A selector function f(query, key) operating on the query/key basis\n      directions that defines the attention pattern.\n    default_output: Output to return if attention pattern is all zero.\n    causal: If True, use masked attention.\n    always_attend_to_bos: If True, always attend to the BOS token. If False,\n      only attend to BOS when attending to nothing else.\n    use_bos_for_default_output: If True, assume BOS is not in the value space\n      and output a default value when attending to BOS. If False, assume BOS is\n      in the value space, and map it to the output space like any other token.\n    softmax_coldness: The inverse temperature of the softmax. Default value is\n      high which makes the attention close to a hard maximum.\n  \"\"\"\n  bases.ensure_dims(bos_space, num_dims=1, name=\"bos_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n  bos_direction = bos_space.basis[0]\n  one_direction = one_space.basis[0]\n\n  # Add bos direction to query, key, and value spaces in case it is missing\n  query_space = bases.join_vector_spaces(query_space, bos_space, one_space)\n  key_space = bases.join_vector_spaces(key_space, bos_space)\n  value_space = bases.join_vector_spaces(value_space, bos_space)\n\n  if always_attend_to_bos:\n    value_basis = value_space.basis\n  else:\n    value_basis = [v for v in value_space.basis if v != bos_direction]\n  assert len(value_basis) == output_space.num_dims\n  value_to_output = dict(zip(value_basis, output_space.basis))\n\n  if default_output is None:\n    default_output = output_space.null_vector()\n  assert default_output in output_space\n\n  def qk_fun(query: bases.BasisDirection, key: bases.BasisDirection) -> float:\n\n    # We want to enforce the following property on our attention patterns:\n    # - if nothing else is attended to, attend to the BOS token.\n    # - otherwise, don't attend to the BOS token.\n    #\n    # We assume that the BOS position always only contains the vector bos + one,\n    # and that any other position has bos coefficient 0.\n    #\n    # We do this as follows:\n    # Let Q and K be subspaces of V containing the query and key vectors,\n    # both disjoint with the BOS space {bos} or the one space {one}.\n    # Suppose we have an attn_fn which defines a bilinear W_QK: V x V -> ℝ,\n    # s.t. W_QK(q, k) = 0 whenever either q or k are bos or one.\n    #\n    # Then define W_new: V x V -> ℝ st:\n    # W_new(one, bos) = 0.5, otherwise 0.\n    #\n    # Now set W_QK' = W_QK + W_new.\n    #\n    # To evaluate the attention to the BOS position:\n    # W_QK'(q, bos + one)\n    # = W_QK'(q, bos) + W_QK'(q, one)\n    # = W_QK(q, bos) + W_QK(q, one) + W_new(q, bos) + W_new(q, one)\n    # = 0            + 0            + W_new(q, bos) + W_new(q, one)\n    # = W_new(q, bos) + W_new(q, one)\n    # = W_new(q' + one, bos) + W_new(q' + one, one)  where q = one + q'\n    # = W_new(q', bos) + W_new(one, bos) + W_new(q', one) + W_new(one, one)\n    # = 0              + 0.5             + 0              + 0\n    # = 0.5\n    #\n    # To evaluate the attention to a non-BOS position:\n    # W_QK'(0 * bos + q, 0 * bos + k)  # s.t. q ∈ Q+{one}, k ∈ K+{one}\n    # = 0*W_QK'(bos, 0*bos + k) + W_QK'(q, 0*bos + k)\n    # = W_QK'(q, 0*bos + k)\n    # = 0*W_QK'(q, bos) + W_QK'(q, k)\n    # = W_QK'(q, k)\n    # = W_QK(q, k)    since W_QK' = W_QK on inputs not containing bos.\n    # = W_QK(q', k')  since W_QK(x, y) = 0 whenever x or y are one.\n    #\n    # Since W_QK(q, k) takes values in 0, 1, a sufficiently high softmax\n    # coldness will give us the desired property.                            QED\n    #\n    # The following implements this idea.\n    # By replacing 0.5 with 1, we can instead enforce a different property: that\n    # the BOS token is always attended to in addition to whatever else.\n\n    if key == bos_direction and query == one_direction:\n      c = 1. if always_attend_to_bos else 0.5\n      return c * softmax_coldness\n    elif {key, query}.intersection({one_direction, bos_direction}):\n      return 0\n\n    return softmax_coldness * attn_fn(query, key)\n\n  w_qk = vectorspace_fns.ScalarBilinear.from_action(\n      query_space,\n      key_space,\n      qk_fun,\n  )\n\n  def ov_fun(input_dir: bases.BasisDirection) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/74", "ground_truth": "    if use_bos_for_default_output and input_dir == bos_direction:\n      return default_output\n    return output_space.vector_from_basis_direction(value_to_output[input_dir])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_attn.py"], "context_start_lineno": 16, "lineno": 157, "function_name": "ov_fun", "line_no": 157}}
{"_id": "deepmind_tracr/75", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLPs to compute arbitrary numerical functions by discretising.\"\"\"\n\nimport dataclasses\n\nfrom typing import Callable, Iterable, List\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\nfrom tracr.utils import errors\n\n\n@dataclasses.dataclass\nclass DiscretisingLayerMaterials:\n  \"\"\"Provides components for a hidden layer that discretises the input.\n\n  Attributes:\n    action: Function acting on basis directions that defines the computation.\n    hidden_space: Vector space of the hidden representation of the layer.\n    output_values: Set of output values that correspond to the discretisation.\n  \"\"\"\n  action: Callable[[bases.BasisDirection], bases.VectorInBasis]\n  hidden_space: bases.VectorSpaceWithBasis\n  output_values: List[float]\n\n\ndef _get_discretising_layer(input_value_set: Iterable[float],\n                            f: Callable[[float],\n                                        float], hidden_name: bases.Name,\n                            one_direction: bases.BasisDirection,\n                            large_number: float) -> DiscretisingLayerMaterials:\n  \"\"\"Creates a hidden layer that discretises the input of f(x) into a value set.\n\n  The input is split up into a distinct region around each value in\n  `input_value_set`:\n\n  elements of value set:  v0   |  v1  |  v2  |  v3  |  v4  | ...\n  thresholds:                  t0     t1     t2     t3     t4\n\n  The hidden layer has two activations per threshold:\n    hidden_k_1 = ReLU(L * (x - threshold[k]) + 1)\n    hidden_k_2 = ReLU(L * (x - threshold[k]))\n\n  Note that hidden_k_1 - hidden_k_2 is:\n    1                 if x >= threshold[k] + 1/L\n    0                 if x <= threshold[k]\n    between 0 and 1   if threshold[k] < x < threshold[k] + 1/L\n\n  So as long as we choose L a big enough number, we have\n    hidden_k_1 - hidden_k_2 = 1 if x >= threshold[k].\n  i.e. we know in which region the input value is.\n\n  Args:\n    input_value_set: Set of discrete input values.\n    f: Function to approximate.\n    hidden_name: Name for hidden dimensions.\n    one_direction: Auxiliary dimension that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n\n  Returns:\n    DiscretisingLayerMaterials containing all components for the layer.\n  \"\"\"\n  output_values, sorted_values = [], []\n  for x in sorted(input_value_set):\n    res = errors.ignoring_arithmetic_errors(f)(x)\n    if res is not None:\n      output_values.append(res)\n      sorted_values.append(x)\n\n  num_vals = len(sorted_values)\n  value_thresholds = [\n      (sorted_values[i] + sorted_values[i + 1]) / 2 for i in range(num_vals - 1)\n  ]\n\n  hidden_directions = [bases.BasisDirection(f\"{hidden_name}start\")]\n  for k in range(1, num_vals):\n    dir0 = bases.BasisDirection(hidden_name, (k, 0))\n    dir1 = bases.BasisDirection(hidden_name, (k, 1))\n    hidden_directions.extend([dir0, dir1])\n  hidden_space = bases.VectorSpaceWithBasis(hidden_directions)\n\n  def action(direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # hidden_k_0 = ReLU(L * (x - threshold[k]) + 1)\n    # hidden_k_1 = ReLU(L * (x - threshold[k]))", "metadata": {"task_id": "deepmind_tracr/75", "ground_truth": "    if direction == one_direction:\n      hidden = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(f\"{hidden_name}start\"))\n    else:\n      hidden = hidden_space.null_vector()\n    for k in range(1, num_vals):\n      vec0 = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(hidden_name, (k, 0)))\n      vec1 = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(hidden_name, (k, 1)))\n      if direction == one_direction:\n        hidden += (1 - large_number * value_thresholds[k - 1]) * vec0\n        hidden -= large_number * value_thresholds[k - 1] * vec1\n      else:\n        hidden += large_number * vec0 + large_number * vec1\n    return hidden\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "action", "line_no": 98}}
{"_id": "deepmind_tracr/76", "text": "                        float], hidden_name: bases.Name,\n                            one_direction: bases.BasisDirection,\n                            large_number: float) -> DiscretisingLayerMaterials:\n  \"\"\"Creates a hidden layer that discretises the input of f(x) into a value set.\n\n  The input is split up into a distinct region around each value in\n  `input_value_set`:\n\n  elements of value set:  v0   |  v1  |  v2  |  v3  |  v4  | ...\n  thresholds:                  t0     t1     t2     t3     t4\n\n  The hidden layer has two activations per threshold:\n    hidden_k_1 = ReLU(L * (x - threshold[k]) + 1)\n    hidden_k_2 = ReLU(L * (x - threshold[k]))\n\n  Note that hidden_k_1 - hidden_k_2 is:\n    1                 if x >= threshold[k] + 1/L\n    0                 if x <= threshold[k]\n    between 0 and 1   if threshold[k] < x < threshold[k] + 1/L\n\n  So as long as we choose L a big enough number, we have\n    hidden_k_1 - hidden_k_2 = 1 if x >= threshold[k].\n  i.e. we know in which region the input value is.\n\n  Args:\n    input_value_set: Set of discrete input values.\n    f: Function to approximate.\n    hidden_name: Name for hidden dimensions.\n    one_direction: Auxiliary dimension that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n\n  Returns:\n    DiscretisingLayerMaterials containing all components for the layer.\n  \"\"\"\n  output_values, sorted_values = [], []\n  for x in sorted(input_value_set):\n    res = errors.ignoring_arithmetic_errors(f)(x)\n    if res is not None:\n      output_values.append(res)\n      sorted_values.append(x)\n\n  num_vals = len(sorted_values)\n  value_thresholds = [\n      (sorted_values[i] + sorted_values[i + 1]) / 2 for i in range(num_vals - 1)\n  ]\n\n  hidden_directions = [bases.BasisDirection(f\"{hidden_name}start\")]\n  for k in range(1, num_vals):\n    dir0 = bases.BasisDirection(hidden_name, (k, 0))\n    dir1 = bases.BasisDirection(hidden_name, (k, 1))\n    hidden_directions.extend([dir0, dir1])\n  hidden_space = bases.VectorSpaceWithBasis(hidden_directions)\n\n  def action(direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # hidden_k_0 = ReLU(L * (x - threshold[k]) + 1)\n    # hidden_k_1 = ReLU(L * (x - threshold[k]))\n    if direction == one_direction:\n      hidden = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(f\"{hidden_name}start\"))\n    else:\n      hidden = hidden_space.null_vector()\n    for k in range(1, num_vals):\n      vec0 = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(hidden_name, (k, 0)))\n      vec1 = hidden_space.vector_from_basis_direction(\n          bases.BasisDirection(hidden_name, (k, 1)))\n      if direction == one_direction:\n        hidden += (1 - large_number * value_thresholds[k - 1]) * vec0\n        hidden -= large_number * value_thresholds[k - 1] * vec1\n      else:\n        hidden += large_number * vec0 + large_number * vec1\n    return hidden\n\n  return DiscretisingLayerMaterials(\n      action=action, hidden_space=hidden_space, output_values=output_values)\n\n\ndef map_numerical_mlp(\n    f: Callable[[float], float],\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    input_value_set: Iterable[float],\n    one_space: bases.VectorSpaceWithBasis,\n    large_number: float = 100,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any function of a single variable f(x).\n\n  This is implemented by discretising the input according to input_value_set\n  and defining thresholds that determine which part of the input range will\n  is allocated to which value in input_value_set.\n\n  elements of value set:  v0   |  v1  |  v2  |  v3  |  v4  | ...\n  thresholds:                  t0     t1     t2     t3     t4\n\n  The MLP computes two hidden activations per threshold:\n    hidden_k_0 = ReLU(L * (x - threshold[k]) + 1)\n    hidden_k_1 = ReLU(L * (x - threshold[k]))\n\n  Note that hidden_k_1 - hidden_k_2 is:\n    1                 if x >= threshold[k] + 1/L\n    0                 if x <= threshold[k]\n    between 0 and 1   if threshold[k] < x < threshold[k] + 1/L\n\n  So as long as we choose L a big enough number, we have\n    hidden_k_0 - hidden_k_1 = 1 if x >= threshold[k].\n\n  The MLP then computes the output as:\n    output = f(input[0]) +\n      sum((hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n        for all k=0,1,...)\n\n  This sum will be (by a telescoping sums argument)\n    f(input[0])      if x <= threshold[0]\n    f(input[k])      if threshold[k-1] < x <= threshold[k] for some other k\n    f(input[-1])     if x > threshold[-1]\n  which approximates f() up to an accuracy given by input_value_set and L.\n\n  Args:\n    f: Function to approximate.\n    input_space: 1-d vector space that encodes the input x.\n    output_space: 1-d vector space to write the output to.\n    input_value_set: Set of values the input can take.\n    one_space: Auxiliary 1-d vector space that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n      Note that too large values of L can lead to numerical issues, particularly\n      during inference on GPU/TPU.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(input_space, num_dims=1, name=\"input_space\")\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # output = sum(\n    #     (hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n    #   for all k)", "metadata": {"task_id": "deepmind_tracr/76", "ground_truth": "    if direction.name == f\"{hidden_name}start\":\n      return discretising_layer.output_values[0] * out_vec\n    k, i = direction.value\n    # add hidden_k_0 and subtract hidden_k_1\n    sign = {0: 1, 1: -1}[i]\n    return sign * (discretising_layer.output_values[k] -\n                   discretising_layer.output_values[k - 1]) * out_vec\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 42, "lineno": 192, "function_name": "second_layer_action", "line_no": 192}}
{"_id": "deepmind_tracr/77", "text": " += (1 - large_number * value_thresholds[k - 1]) * vec0\n        hidden -= large_number * value_thresholds[k - 1] * vec1\n      else:\n        hidden += large_number * vec0 + large_number * vec1\n    return hidden\n\n  return DiscretisingLayerMaterials(\n      action=action, hidden_space=hidden_space, output_values=output_values)\n\n\ndef map_numerical_mlp(\n    f: Callable[[float], float],\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    input_value_set: Iterable[float],\n    one_space: bases.VectorSpaceWithBasis,\n    large_number: float = 100,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any function of a single variable f(x).\n\n  This is implemented by discretising the input according to input_value_set\n  and defining thresholds that determine which part of the input range will\n  is allocated to which value in input_value_set.\n\n  elements of value set:  v0   |  v1  |  v2  |  v3  |  v4  | ...\n  thresholds:                  t0     t1     t2     t3     t4\n\n  The MLP computes two hidden activations per threshold:\n    hidden_k_0 = ReLU(L * (x - threshold[k]) + 1)\n    hidden_k_1 = ReLU(L * (x - threshold[k]))\n\n  Note that hidden_k_1 - hidden_k_2 is:\n    1                 if x >= threshold[k] + 1/L\n    0                 if x <= threshold[k]\n    between 0 and 1   if threshold[k] < x < threshold[k] + 1/L\n\n  So as long as we choose L a big enough number, we have\n    hidden_k_0 - hidden_k_1 = 1 if x >= threshold[k].\n\n  The MLP then computes the output as:\n    output = f(input[0]) +\n      sum((hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n        for all k=0,1,...)\n\n  This sum will be (by a telescoping sums argument)\n    f(input[0])      if x <= threshold[0]\n    f(input[k])      if threshold[k-1] < x <= threshold[k] for some other k\n    f(input[-1])     if x > threshold[-1]\n  which approximates f() up to an accuracy given by input_value_set and L.\n\n  Args:\n    f: Function to approximate.\n    input_space: 1-d vector space that encodes the input x.\n    output_space: 1-d vector space to write the output to.\n    input_value_set: Set of values the input can take.\n    one_space: Auxiliary 1-d vector space that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n      Note that too large values of L can lead to numerical issues, particularly\n      during inference on GPU/TPU.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(input_space, num_dims=1, name=\"input_space\")\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # output = sum(\n    #     (hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n    #   for all k)\n    if direction.name == f\"{hidden_name}start\":\n      return discretising_layer.output_values[0] * out_vec\n    k, i = direction.value\n    # add hidden_k_0 and subtract hidden_k_1\n    sign = {0: 1, 1: -1}[i]\n    return sign * (discretising_layer.output_values[k] -\n                   discretising_layer.output_values[k - 1]) * out_vec\n\n  second_layer = vectorspace_fns.Linear.from_action(\n      discretising_layer.hidden_space, output_space, second_layer_action)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_numerical_to_categorical_mlp(\n    f: Callable[[float], float],\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    input_value_set: Iterable[float],\n    one_space: bases.VectorSpaceWithBasis,\n    large_number: float = 100,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a numerical to a categorical variable.\n\n  Uses a set of possible output values, and rounds f(x) to the closest value\n  in this set to create a categorical output variable.\n\n  The output is discretised the same way as in `map_numerical_mlp`.\n\n  Args:\n    f: Function to approximate.\n    input_space: 1-d vector space that encodes the input x.\n    output_space: n-d vector space to write categorical output to. The output\n      directions need to encode the possible output values.\n    input_value_set: Set of values the input can take.\n    one_space: Auxiliary 1-d space that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(input_space, num_dims=1, name=\"input_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n\n  vec_by_out_val = dict()\n  for d in output_space.basis:\n    # TODO(b/255937603): Do a similar assert in other places where we expect\n    # categorical basis directions to encode values.\n    assert d.value is not None, (\"output directions need to encode \"\n                                 \"possible output values\")\n    vec_by_out_val[d.value] = output_space.vector_from_basis_direction(d)\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n\n  assert set(discretising_layer.output_values).issubset(\n      set(vec_by_out_val.keys()))\n\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    \"\"\"Computes output value and returns corresponding output direction.\"\"\"", "metadata": {"task_id": "deepmind_tracr/77", "ground_truth": "    if direction.name == f\"{hidden_name}start\":\n      return vec_by_out_val[discretising_layer.output_values[0]]\n    else:\n      k, i = direction.value\n      # add hidden_k_0 and subtract hidden_k_1\n      sign = {0: 1, 1: -1}[i]\n      out_k = discretising_layer.output_values[k]\n      out_k_m_1 = discretising_layer.output_values[k - 1]\n      return sign * (vec_by_out_val[out_k] - vec_by_out_val[out_k_m_1])\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 109, "lineno": 261, "function_name": "second_layer_action", "line_no": 261}}
{"_id": "deepmind_tracr/78", "text": " contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n      Note that too large values of L can lead to numerical issues, particularly\n      during inference on GPU/TPU.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(input_space, num_dims=1, name=\"input_space\")\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # output = sum(\n    #     (hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n    #   for all k)\n    if direction.name == f\"{hidden_name}start\":\n      return discretising_layer.output_values[0] * out_vec\n    k, i = direction.value\n    # add hidden_k_0 and subtract hidden_k_1\n    sign = {0: 1, 1: -1}[i]\n    return sign * (discretising_layer.output_values[k] -\n                   discretising_layer.output_values[k - 1]) * out_vec\n\n  second_layer = vectorspace_fns.Linear.from_action(\n      discretising_layer.hidden_space, output_space, second_layer_action)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_numerical_to_categorical_mlp(\n    f: Callable[[float], float],\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    input_value_set: Iterable[float],\n    one_space: bases.VectorSpaceWithBasis,\n    large_number: float = 100,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a numerical to a categorical variable.\n\n  Uses a set of possible output values, and rounds f(x) to the closest value\n  in this set to create a categorical output variable.\n\n  The output is discretised the same way as in `map_numerical_mlp`.\n\n  Args:\n    f: Function to approximate.\n    input_space: 1-d vector space that encodes the input x.\n    output_space: n-d vector space to write categorical output to. The output\n      directions need to encode the possible output values.\n    input_value_set: Set of values the input can take.\n    one_space: Auxiliary 1-d space that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(input_space, num_dims=1, name=\"input_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n\n  vec_by_out_val = dict()\n  for d in output_space.basis:\n    # TODO(b/255937603): Do a similar assert in other places where we expect\n    # categorical basis directions to encode values.\n    assert d.value is not None, (\"output directions need to encode \"\n                                 \"possible output values\")\n    vec_by_out_val[d.value] = output_space.vector_from_basis_direction(d)\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n\n  assert set(discretising_layer.output_values).issubset(\n      set(vec_by_out_val.keys()))\n\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    \"\"\"Computes output value and returns corresponding output direction.\"\"\"\n    if direction.name == f\"{hidden_name}start\":\n      return vec_by_out_val[discretising_layer.output_values[0]]\n    else:\n      k, i = direction.value\n      # add hidden_k_0 and subtract hidden_k_1\n      sign = {0: 1, 1: -1}[i]\n      out_k = discretising_layer.output_values[k]\n      out_k_m_1 = discretising_layer.output_values[k - 1]\n      return sign * (vec_by_out_val[out_k] - vec_by_out_val[out_k_m_1])\n\n  second_layer = vectorspace_fns.Linear.from_action(\n      discretising_layer.hidden_space, output_space, second_layer_action)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef linear_sequence_map_numerical_mlp(\n    input1_basis_direction: bases.BasisDirection,\n    input2_basis_direction: bases.BasisDirection,\n    output_basis_direction: bases.BasisDirection,\n    input1_factor: float,\n    input2_factor: float,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes a linear function f(x, y) = a*x + b*y.\n\n  Args:\n    input1_basis_direction: Basis direction that encodes the input x.\n    input2_basis_direction: Basis direction that encodes the input y.\n    output_basis_direction: Basis direction to write the output to.\n    input1_factor: Linear factor a for input x.\n    input2_factor: Linear factor a for input y.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  input_space = bases.VectorSpaceWithBasis(\n      [input1_basis_direction, input2_basis_direction])\n  output_space = bases.VectorSpaceWithBasis([output_basis_direction])\n  out_vec = output_space.vector_from_basis_direction(output_basis_direction)\n\n  hidden_directions = [\n      bases.BasisDirection(f\"{hidden_name}x\", 1),\n      bases.BasisDirection(f\"{hidden_name}x\", -1),\n      bases.BasisDirection(f\"{hidden_name}y\", 1),\n      bases.BasisDirection(f\"{hidden_name}y\", -1)\n  ]\n  hidden_space = bases.VectorSpaceWithBasis(hidden_directions)\n  x_pos_vec, x_neg_vec, y_pos_vec, y_neg_vec = (\n      hidden_space.vector_from_basis_direction(d) for d in hidden_directions)\n\n  def first_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/78", "ground_truth": "    output = hidden_space.null_vector()\n    if direction == input1_basis_direction:\n      output += x_pos_vec - x_neg_vec\n    if direction == input2_basis_direction:\n      output += y_pos_vec - y_neg_vec\n    return output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 165, "lineno": 312, "function_name": "first_layer_action", "line_no": 312}}
{"_id": "deepmind_tracr/79", "text": " name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    # output = sum(\n    #     (hidden_k_0 - hidden_k_1) * (f(input[k]) - f(input[k-1]))\n    #   for all k)\n    if direction.name == f\"{hidden_name}start\":\n      return discretising_layer.output_values[0] * out_vec\n    k, i = direction.value\n    # add hidden_k_0 and subtract hidden_k_1\n    sign = {0: 1, 1: -1}[i]\n    return sign * (discretising_layer.output_values[k] -\n                   discretising_layer.output_values[k - 1]) * out_vec\n\n  second_layer = vectorspace_fns.Linear.from_action(\n      discretising_layer.hidden_space, output_space, second_layer_action)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_numerical_to_categorical_mlp(\n    f: Callable[[float], float],\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    input_value_set: Iterable[float],\n    one_space: bases.VectorSpaceWithBasis,\n    large_number: float = 100,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a numerical to a categorical variable.\n\n  Uses a set of possible output values, and rounds f(x) to the closest value\n  in this set to create a categorical output variable.\n\n  The output is discretised the same way as in `map_numerical_mlp`.\n\n  Args:\n    f: Function to approximate.\n    input_space: 1-d vector space that encodes the input x.\n    output_space: n-d vector space to write categorical output to. The output\n      directions need to encode the possible output values.\n    input_value_set: Set of values the input can take.\n    one_space: Auxiliary 1-d space that must contain 1 in the input.\n    large_number: Large number L that determines accuracy of the computation.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(input_space, num_dims=1, name=\"input_space\")\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  input_space = bases.join_vector_spaces(input_space, one_space)\n\n  vec_by_out_val = dict()\n  for d in output_space.basis:\n    # TODO(b/255937603): Do a similar assert in other places where we expect\n    # categorical basis directions to encode values.\n    assert d.value is not None, (\"output directions need to encode \"\n                                 \"possible output values\")\n    vec_by_out_val[d.value] = output_space.vector_from_basis_direction(d)\n\n  discretising_layer = _get_discretising_layer(\n      input_value_set=input_value_set,\n      f=f,\n      hidden_name=hidden_name,\n      one_direction=one_space.basis[0],\n      large_number=large_number)\n\n  assert set(discretising_layer.output_values).issubset(\n      set(vec_by_out_val.keys()))\n\n  first_layer = vectorspace_fns.Linear.from_action(\n      input_space, discretising_layer.hidden_space, discretising_layer.action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    \"\"\"Computes output value and returns corresponding output direction.\"\"\"\n    if direction.name == f\"{hidden_name}start\":\n      return vec_by_out_val[discretising_layer.output_values[0]]\n    else:\n      k, i = direction.value\n      # add hidden_k_0 and subtract hidden_k_1\n      sign = {0: 1, 1: -1}[i]\n      out_k = discretising_layer.output_values[k]\n      out_k_m_1 = discretising_layer.output_values[k - 1]\n      return sign * (vec_by_out_val[out_k] - vec_by_out_val[out_k_m_1])\n\n  second_layer = vectorspace_fns.Linear.from_action(\n      discretising_layer.hidden_space, output_space, second_layer_action)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef linear_sequence_map_numerical_mlp(\n    input1_basis_direction: bases.BasisDirection,\n    input2_basis_direction: bases.BasisDirection,\n    output_basis_direction: bases.BasisDirection,\n    input1_factor: float,\n    input2_factor: float,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes a linear function f(x, y) = a*x + b*y.\n\n  Args:\n    input1_basis_direction: Basis direction that encodes the input x.\n    input2_basis_direction: Basis direction that encodes the input y.\n    output_basis_direction: Basis direction to write the output to.\n    input1_factor: Linear factor a for input x.\n    input2_factor: Linear factor a for input y.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  input_space = bases.VectorSpaceWithBasis(\n      [input1_basis_direction, input2_basis_direction])\n  output_space = bases.VectorSpaceWithBasis([output_basis_direction])\n  out_vec = output_space.vector_from_basis_direction(output_basis_direction)\n\n  hidden_directions = [\n      bases.BasisDirection(f\"{hidden_name}x\", 1),\n      bases.BasisDirection(f\"{hidden_name}x\", -1),\n      bases.BasisDirection(f\"{hidden_name}y\", 1),\n      bases.BasisDirection(f\"{hidden_name}y\", -1)\n  ]\n  hidden_space = bases.VectorSpaceWithBasis(hidden_directions)\n  x_pos_vec, x_neg_vec, y_pos_vec, y_neg_vec = (\n      hidden_space.vector_from_basis_direction(d) for d in hidden_directions)\n\n  def first_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:\n    output = hidden_space.null_vector()\n    if direction == input1_basis_direction:\n      output += x_pos_vec - x_neg_vec\n    if direction == input2_basis_direction:\n      output += y_pos_vec - y_neg_vec\n    return output\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, hidden_space,\n                                                   first_layer_action)\n\n  def second_layer_action(\n      direction: bases.BasisDirection) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/79", "ground_truth": "    if direction.name == f\"{hidden_name}x\":\n      return input1_factor * direction.value * out_vec\n    if direction.name == f\"{hidden_name}y\":\n      return input2_factor * direction.value * out_vec\n    return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "numerical_mlp.py"], "context_start_lineno": 173, "lineno": 324, "function_name": "second_layer_action", "line_no": 324}}
{"_id": "deepmind_tracr/80", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for chamber.categorical_attn.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nfrom tracr.craft import bases\nfrom tracr.craft import tests_common\nfrom tracr.craft.chamber import categorical_attn\n\n\nclass CategoricalAttnTest(tests_common.VectorFnTestCase):\n\n  @parameterized.parameters([\n      dict(causal=False, input_seq=[1, 2, 3, 4, 5], result_seq=[3, 3, 3, 3, 3]),\n      dict(\n          causal=True,\n          input_seq=[1, 2, 3, 4, 5],\n          result_seq=[1, 1.5, 2, 2.5, 3]),\n      dict(causal=False, input_seq=[10], result_seq=[10]),\n      dict(causal=True, input_seq=[10], result_seq=[10]),\n      dict(causal=False, input_seq=[-1, 0, 1], result_seq=[0, 0, 0]),\n      dict(causal=True, input_seq=[-1, 0, 1], result_seq=[-1, -0.5, 0]),\n  ])\n  def test_categorical_attn_can_implement_select_all(self, causal, input_seq,\n                                                     result_seq):\n    vocab = range(-20, 20)\n    input_space = bases.VectorSpaceWithBasis.from_values(\"input\", vocab)\n\n    output_dir = bases.BasisDirection(\"output\")\n    output_space = bases.VectorSpaceWithBasis([output_dir])\n    output_vec = output_space.vector_from_basis_direction(output_dir)\n\n    bos_dir = bases.BasisDirection(\"bos_dimension\")\n    bos_space = bases.VectorSpaceWithBasis([bos_dir])\n\n    one_dir = bases.BasisDirection(\"one\")\n    one_space = bases.VectorSpaceWithBasis([one_dir])\n\n    value_dir = bases.BasisDirection(\"value\")\n    value_space = bases.VectorSpaceWithBasis([value_dir])\n\n    input_space = bases.join_vector_spaces(input_space, bos_space, one_space)\n    value_space = bases.join_vector_spaces(value_space, bos_space)\n    residual_space = bases.join_vector_spaces(input_space, value_space,\n                                              output_space)\n    one_vec = residual_space.vector_from_basis_direction(one_dir)\n    bos_vec = residual_space.vector_from_basis_direction(bos_dir)\n    value_vec = residual_space.vector_from_basis_direction(value_dir)\n\n    attn = categorical_attn.categorical_attn(\n        key_space=input_space,\n        query_space=input_space,\n        value_space=value_space,\n        output_space=output_space,\n        bos_space=bos_space,\n        one_space=one_space,\n        attn_fn=lambda x, y: True,\n        causal=causal)\n\n    test_inputs = [bos_vec + one_vec]\n    for x in input_seq:\n      test_inputs.append(\n          residual_space.vector_from_basis_direction(\n              bases.BasisDirection(\"input\", x)) + x * value_vec)\n    test_inputs = bases.VectorInBasis.stack(test_inputs)\n\n    # Expect the average of all (previous) tokens\n    expected_results = [x * output_vec for x in result_seq]\n    expected_results = bases.VectorInBasis.stack(expected_results)\n\n    test_outputs = attn.apply(test_inputs).project(output_space)\n\n    self.assertVectorAllClose(\n        tests_common.strip_bos_token(test_outputs), expected_results)\n\n  @parameterized.parameters([\n      dict(causal=False, input_seq=[1, 2, 3, 4, 5], default=0),\n      dict(causal=True, input_seq=[1, 2, 3, 4, 5], default=1),\n      dict(causal=False, input_seq=[10], default=2),\n      dict(causal=True, input_seq=[10], default=-3),\n      dict(causal=False, input_seq=[-1, 0, 1], default=-2),\n      dict(causal=True, input_seq=[-1, 0, 1], default=-1),\n  ])\n  def test_categorical_attn_can_implement_select_none(self, causal, input_seq,\n                                                      default):\n    vocab = range(-20, 20)\n    input_space = bases.VectorSpaceWithBasis.from_values(\"input\", vocab)\n\n    output_dir = bases.BasisDirection(\"output\")\n    output_space = bases.VectorSpaceWithBasis([output_dir])\n    default_vec = default * output_space.vector_from_basis_direction(output_dir)\n\n    bos_dir = bases.BasisDirection(\"bos_dimension\")\n    bos_space = bases.VectorSpaceWithBasis([bos_dir])\n\n    one_dir = bases.BasisDirection(\"one\")\n    one_space = bases.VectorSpaceWithBasis([one_dir])\n\n    value_dir = bases.BasisDirection(\"value\")\n    value_space = bases.VectorSpaceWithBasis([value_dir])\n\n    input_space = bases.join_vector_spaces(input_space, bos_space, one_space)\n    value_space = bases.join_vector_spaces(value_space, bos_space)\n    residual_space = bases.join_vector_spaces(input_space, value_space,\n                                              output_space)\n    value_vec = residual_space.vector_from_basis_direction(value_dir)\n    bos_vec = residual_space.vector_from_basis_direction(bos_dir)\n    one_vec = residual_space.vector_from_basis_direction(one_dir)\n\n    attn = categorical_attn.categorical_attn(\n        key_space=input_space,\n        query_space=input_space,\n        value_space=value_space,\n        output_space=output_space,\n        bos_space=bos_space,\n        one_space=one_space,\n        attn_fn=lambda x, y: False,\n        default_output=default_vec,\n        causal=causal,\n        always_attend_to_bos=False,\n        use_bos_for_default_output=True)\n\n    def make_input(x):", "metadata": {"task_id": "deepmind_tracr/80", "ground_truth": "      return (one_vec + x * value_vec +\n              residual_space.vector_from_basis_direction(\n                  bases.BasisDirection(\"input\", x)))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_attn_test.py"], "context_start_lineno": 0, "lineno": 137, "function_name": "make_input", "line_no": 137}}
{"_id": "deepmind_tracr/81", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/81", "ground_truth": "  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 43, "function_name": "map_categorical_mlp", "line_no": 43}}
{"_id": "deepmind_tracr/82", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n\n  def operation_fn(direction):", "metadata": {"task_id": "deepmind_tracr/82", "ground_truth": "    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 44, "function_name": "operation_fn", "line_no": 44}}
{"_id": "deepmind_tracr/83", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n\n  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_categorical_to_numerical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.Value], float],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a categorical to a numerical variable.\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: Vector space containing the input x.\n    output_space: Vector space to write the numerical output to.\n    operation: A function operating on basis directions.\n  \"\"\"", "metadata": {"task_id": "deepmind_tracr/83", "ground_truth": "  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  def operation_fn(direction):\n    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 73, "function_name": "map_categorical_to_numerical_mlp", "line_no": 73}}
{"_id": "deepmind_tracr/84", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n\n  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_categorical_to_numerical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.Value], float],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a categorical to a numerical variable.\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: Vector space containing the input x.\n    output_space: Vector space to write the numerical output to.\n    operation: A function operating on basis directions.\n  \"\"\"\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  def operation_fn(direction):", "metadata": {"task_id": "deepmind_tracr/84", "ground_truth": "    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "operation_fn", "line_no": 77}}
{"_id": "deepmind_tracr/85", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n\n  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_categorical_to_numerical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.Value], float],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a categorical to a numerical variable.\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: Vector space containing the input x.\n    output_space: Vector space to write the numerical output to.\n    operation: A function operating on basis directions.\n  \"\"\"\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  def operation_fn(direction):\n    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef sequence_map_categorical_mlp(\n    input1_space: bases.VectorSpaceWithBasis,\n    input2_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection, bases.BasisDirection],\n                        bases.BasisDirection],\n    one_space: bases.VectorSpaceWithBasis = _ONE_SPACE,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes a categorical function of two variables f(x, y).\n\n  The hidden layer of the MLP computes the logical and of all input directions\n    hidden_i_j = ReLU(x_i+x_j-1)\n\n  And the output combines this with a lookup table\n    output_k = sum(f(i, j)*hidden_i_j for all i,j in input space)\n\n  Args:\n    input1_space: Vector space containing the input x.\n    input2_space: Vector space containing the input y.\n    output_space: Vector space to write outputs to.\n    operation: A function operating on basis directions.\n    one_space: a reserved 1-d space that always contains a 1.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  if not set(input1_space.basis).isdisjoint(input2_space.basis):\n    raise ValueError(\"Input spaces to a SequenceMap must be disjoint. \"\n                     \"If input spaces are the same, use Map instead!\")\n\n  input_space = bases.direct_sum(input1_space, input2_space, one_space)\n\n  def to_hidden(x, y):\n    return bases.BasisDirection(hidden_name, (x.name, x.value, y.name, y.value))\n\n  def from_hidden(h):", "metadata": {"task_id": "deepmind_tracr/85", "ground_truth": "    x_name, x_value, y_name, y_value = h.value\n    x_dir = bases.BasisDirection(x_name, x_value)\n    y_dir = bases.BasisDirection(y_name, y_value)\n    return x_dir, y_dir\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 126, "function_name": "from_hidden", "line_no": 126}}
{"_id": "deepmind_tracr/86", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n\n  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_categorical_to_numerical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.Value], float],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a categorical to a numerical variable.\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: Vector space containing the input x.\n    output_space: Vector space to write the numerical output to.\n    operation: A function operating on basis directions.\n  \"\"\"\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  def operation_fn(direction):\n    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef sequence_map_categorical_mlp(\n    input1_space: bases.VectorSpaceWithBasis,\n    input2_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection, bases.BasisDirection],\n                        bases.BasisDirection],\n    one_space: bases.VectorSpaceWithBasis = _ONE_SPACE,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes a categorical function of two variables f(x, y).\n\n  The hidden layer of the MLP computes the logical and of all input directions\n    hidden_i_j = ReLU(x_i+x_j-1)\n\n  And the output combines this with a lookup table\n    output_k = sum(f(i, j)*hidden_i_j for all i,j in input space)\n\n  Args:\n    input1_space: Vector space containing the input x.\n    input2_space: Vector space containing the input y.\n    output_space: Vector space to write outputs to.\n    operation: A function operating on basis directions.\n    one_space: a reserved 1-d space that always contains a 1.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  if not set(input1_space.basis).isdisjoint(input2_space.basis):\n    raise ValueError(\"Input spaces to a SequenceMap must be disjoint. \"\n                     \"If input spaces are the same, use Map instead!\")\n\n  input_space = bases.direct_sum(input1_space, input2_space, one_space)\n\n  def to_hidden(x, y):\n    return bases.BasisDirection(hidden_name, (x.name, x.value, y.name, y.value))\n\n  def from_hidden(h):\n    x_name, x_value, y_name, y_value = h.value\n    x_dir = bases.BasisDirection(x_name, x_value)\n    y_dir = bases.BasisDirection(y_name, y_value)\n    return x_dir, y_dir\n\n  hidden_dir = []\n  for dir1 in input1_space.basis:\n    for dir2 in input2_space.basis:\n      hidden_dir.append(to_hidden(dir1, dir2))\n  hidden_space = bases.VectorSpaceWithBasis(hidden_dir)\n\n  def logical_and(direction):", "metadata": {"task_id": "deepmind_tracr/86", "ground_truth": "    if direction in one_space:\n      out = bases.VectorInBasis(hidden_space.basis,\n                                -np.ones(hidden_space.num_dims))\n    elif direction in input1_space:\n      dir1 = direction\n      out = hidden_space.null_vector()\n      for dir2 in input2_space.basis:\n        out += hidden_space.vector_from_basis_direction(to_hidden(dir1, dir2))\n    else:\n      dir2 = direction\n      out = hidden_space.null_vector()\n      for dir1 in input1_space.basis:\n        out += hidden_space.vector_from_basis_direction(to_hidden(dir1, dir2))\n    return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 138, "function_name": "logical_and", "line_no": 138}}
{"_id": "deepmind_tracr/87", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"MLP to compute basic linear functions of one-hot encoded integers.\"\"\"\n\nfrom typing import Callable\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import transformers\nfrom tracr.craft import vectorspace_fns\n\n_ONE_SPACE = bases.VectorSpaceWithBasis.from_names([\"one\"])\n\n\ndef map_categorical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection], bases.BasisDirection],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes any categorical function of a single variable f(x).\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output_k = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: space containing the input x.\n    output_space: space containing possible outputs.\n    operation: A function operating on basis directions.\n  \"\"\"\n\n  def operation_fn(direction):\n    if direction in input_space:\n      output_direction = operation(direction)\n      if output_direction in output_space:\n        return output_space.vector_from_basis_direction(output_direction)\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef map_categorical_to_numerical_mlp(\n    input_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.Value], float],\n) -> transformers.MLP:\n  \"\"\"Returns an MLP to compute f(x) from a categorical to a numerical variable.\n\n  The hidden layer is the identity and output combines this with a lookup table\n    output = sum(f(i)*input_i for all i in input space)\n\n  Args:\n    input_space: Vector space containing the input x.\n    output_space: Vector space to write the numerical output to.\n    operation: A function operating on basis directions.\n  \"\"\"\n  bases.ensure_dims(output_space, num_dims=1, name=\"output_space\")\n  out_vec = output_space.vector_from_basis_direction(output_space.basis[0])\n\n  def operation_fn(direction):\n    if direction in input_space:\n      return operation(direction.value) * out_vec\n    return output_space.null_vector()\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, output_space,\n                                                   operation_fn)\n\n  second_layer = vectorspace_fns.project(output_space, output_space)\n\n  return transformers.MLP(first_layer, second_layer)\n\n\ndef sequence_map_categorical_mlp(\n    input1_space: bases.VectorSpaceWithBasis,\n    input2_space: bases.VectorSpaceWithBasis,\n    output_space: bases.VectorSpaceWithBasis,\n    operation: Callable[[bases.BasisDirection, bases.BasisDirection],\n                        bases.BasisDirection],\n    one_space: bases.VectorSpaceWithBasis = _ONE_SPACE,\n    hidden_name: bases.Name = \"__hidden__\",\n) -> transformers.MLP:\n  \"\"\"Returns an MLP that encodes a categorical function of two variables f(x, y).\n\n  The hidden layer of the MLP computes the logical and of all input directions\n    hidden_i_j = ReLU(x_i+x_j-1)\n\n  And the output combines this with a lookup table\n    output_k = sum(f(i, j)*hidden_i_j for all i,j in input space)\n\n  Args:\n    input1_space: Vector space containing the input x.\n    input2_space: Vector space containing the input y.\n    output_space: Vector space to write outputs to.\n    operation: A function operating on basis directions.\n    one_space: a reserved 1-d space that always contains a 1.\n    hidden_name: Name for hidden dimensions.\n  \"\"\"\n  bases.ensure_dims(one_space, num_dims=1, name=\"one_space\")\n\n  if not set(input1_space.basis).isdisjoint(input2_space.basis):\n    raise ValueError(\"Input spaces to a SequenceMap must be disjoint. \"\n                     \"If input spaces are the same, use Map instead!\")\n\n  input_space = bases.direct_sum(input1_space, input2_space, one_space)\n\n  def to_hidden(x, y):\n    return bases.BasisDirection(hidden_name, (x.name, x.value, y.name, y.value))\n\n  def from_hidden(h):\n    x_name, x_value, y_name, y_value = h.value\n    x_dir = bases.BasisDirection(x_name, x_value)\n    y_dir = bases.BasisDirection(y_name, y_value)\n    return x_dir, y_dir\n\n  hidden_dir = []\n  for dir1 in input1_space.basis:\n    for dir2 in input2_space.basis:\n      hidden_dir.append(to_hidden(dir1, dir2))\n  hidden_space = bases.VectorSpaceWithBasis(hidden_dir)\n\n  def logical_and(direction):\n    if direction in one_space:\n      out = bases.VectorInBasis(hidden_space.basis,\n                                -np.ones(hidden_space.num_dims))\n    elif direction in input1_space:\n      dir1 = direction\n      out = hidden_space.null_vector()\n      for dir2 in input2_space.basis:\n        out += hidden_space.vector_from_basis_direction(to_hidden(dir1, dir2))\n    else:\n      dir2 = direction\n      out = hidden_space.null_vector()\n      for dir1 in input1_space.basis:\n        out += hidden_space.vector_from_basis_direction(to_hidden(dir1, dir2))\n    return out\n\n  first_layer = vectorspace_fns.Linear.from_action(input_space, hidden_space,\n                                                   logical_and)\n\n  def operation_fn(direction):", "metadata": {"task_id": "deepmind_tracr/87", "ground_truth": "    dir1, dir2 = from_hidden(direction)\n    output_direction = operation(dir1, dir2)\n    if output_direction in output_space:\n      return output_space.vector_from_basis_direction(output_direction)\n    else:\n      return output_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "chamber", "categorical_mlp.py"], "context_start_lineno": 0, "lineno": 157, "function_name": "operation_fn", "line_no": 157}}
{"_id": "deepmind_tracr/88", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"", "metadata": {"task_id": "deepmind_tracr/88", "ground_truth": "    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 56, "function_name": "__init__", "line_no": 56}}
{"_id": "deepmind_tracr/89", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/89", "ground_truth": "    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 66, "function_name": "__call__", "line_no": 66}}
{"_id": "deepmind_tracr/90", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"", "metadata": {"task_id": "deepmind_tracr/90", "ground_truth": "    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 82, "function_name": "from_action", "line_no": 82}}
{"_id": "deepmind_tracr/91", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"", "metadata": {"task_id": "deepmind_tracr/91", "ground_truth": "    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 95, "function_name": "combine_in_parallel", "line_no": 95}}
{"_id": "deepmind_tracr/92", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/92", "ground_truth": "      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 101, "function_name": "action", "line_no": 101}}
{"_id": "deepmind_tracr/93", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n\n\ndef project(\n    from_space: VectorSpaceWithBasis,\n    to_space: VectorSpaceWithBasis,\n) -> Linear:\n  \"\"\"Creates a projection.\"\"\"", "metadata": {"task_id": "deepmind_tracr/93", "ground_truth": "  def action(direction: bases.BasisDirection) -> VectorInBasis:\n    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n\n  return Linear.from_action(from_space, to_space, action=action)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 117, "function_name": "project", "line_no": 117}}
{"_id": "deepmind_tracr/94", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n\n\ndef project(\n    from_space: VectorSpaceWithBasis,\n    to_space: VectorSpaceWithBasis,\n) -> Linear:\n  \"\"\"Creates a projection.\"\"\"\n\n  def action(direction: bases.BasisDirection) -> VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/94", "ground_truth": "    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 118, "function_name": "action", "line_no": 118}}
{"_id": "deepmind_tracr/95", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n\n\ndef project(\n    from_space: VectorSpaceWithBasis,\n    to_space: VectorSpaceWithBasis,\n) -> Linear:\n  \"\"\"Creates a projection.\"\"\"\n\n  def action(direction: bases.BasisDirection) -> VectorInBasis:\n    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n\n  return Linear.from_action(from_space, to_space, action=action)\n\n\n@dataclasses.dataclass\nclass ScalarBilinear:\n  \"\"\"A scalar-valued bilinear operator.\"\"\"\n  left_space: VectorSpaceWithBasis\n  right_space: VectorSpaceWithBasis\n  matrix: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Ensure matrix acts in sorted bases and typecheck sizes.\"\"\"", "metadata": {"task_id": "deepmind_tracr/95", "ground_truth": "    left_size, right_size = self.matrix.shape\n    assert left_size == self.left_space.num_dims\n    assert right_size == self.right_space.num_dims\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 135, "function_name": "__post_init__", "line_no": 135}}
{"_id": "deepmind_tracr/96", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n\n\ndef project(\n    from_space: VectorSpaceWithBasis,\n    to_space: VectorSpaceWithBasis,\n) -> Linear:\n  \"\"\"Creates a projection.\"\"\"\n\n  def action(direction: bases.BasisDirection) -> VectorInBasis:\n    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n\n  return Linear.from_action(from_space, to_space, action=action)\n\n\n@dataclasses.dataclass\nclass ScalarBilinear:\n  \"\"\"A scalar-valued bilinear operator.\"\"\"\n  left_space: VectorSpaceWithBasis\n  right_space: VectorSpaceWithBasis\n  matrix: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Ensure matrix acts in sorted bases and typecheck sizes.\"\"\"\n    left_size, right_size = self.matrix.shape\n    assert left_size == self.left_space.num_dims\n    assert right_size == self.right_space.num_dims\n\n  def __call__(self, x: VectorInBasis, y: VectorInBasis) -> float:\n    \"\"\"Describes the action of the operator on vectors.\"\"\"", "metadata": {"task_id": "deepmind_tracr/96", "ground_truth": "    if x not in self.left_space:\n      raise TypeError(f\"x={x} not in self.left_space={self.left_space}.\")\n    if y not in self.right_space:\n      raise TypeError(f\"y={y} not in self.right_space={self.right_space}.\")\n    return (x.magnitudes.T @ self.matrix @ y.magnitudes).item()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 141, "function_name": "__call__", "line_no": 141}}
{"_id": "deepmind_tracr/97", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functions on vector spaces.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Callable, Sequence\n\nimport numpy as np\n\nfrom tracr.craft import bases\n\nVectorSpaceWithBasis = bases.VectorSpaceWithBasis\nVectorInBasis = bases.VectorInBasis\nBasisDirection = bases.BasisDirection\n\n\nclass VectorFunction(abc.ABC):\n  \"\"\"A function that acts on vectors.\"\"\"\n\n  input_space: VectorSpaceWithBasis\n  output_space: VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    \"\"\"Evaluates the function.\"\"\"\n\n\nclass Linear(VectorFunction):\n  \"\"\"A linear function.\"\"\"\n\n  def __init__(\n      self,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      matrix: np.ndarray,\n  ):\n    \"\"\"Initialises.\n\n    Args:\n      input_space: The input vector space.\n      output_space: The output vector space.\n      matrix: a [input, output] matrix acting in a (sorted) basis.\n    \"\"\"\n    self.input_space = input_space\n    self.output_space = output_space\n    self.matrix = matrix\n\n  def __post_init__(self) -> None:\n    output_size, input_size = self.matrix.shape\n    assert input_size == self.input_space.num_dims\n    assert output_size == self.output_space.num_dims\n\n  def __call__(self, x: VectorInBasis) -> VectorInBasis:\n    if x not in self.input_space:\n      raise TypeError(f\"x={x} not in self.input_space={self.input_space}.\")\n    return VectorInBasis(\n        basis_directions=sorted(self.output_space.basis),\n        magnitudes=x.magnitudes @ self.matrix,\n    )\n\n  @classmethod\n  def from_action(\n      cls,\n      input_space: VectorSpaceWithBasis,\n      output_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection], VectorInBasis],\n  ) -> \"Linear\":\n    \"\"\"from_action(i, o)(action) creates a Linear.\"\"\"\n\n    matrix = np.zeros((input_space.num_dims, output_space.num_dims))\n    for i, direction in enumerate(input_space.basis):\n      out_vector = action(direction)\n      if out_vector not in output_space:\n        raise TypeError(f\"image of {direction} from input_space={input_space} \"\n                        f\"is not in output_space={output_space}\")\n      matrix[i, :] = out_vector.magnitudes\n\n    return Linear(input_space, output_space, matrix)\n\n  @classmethod\n  def combine_in_parallel(cls, fns: Sequence[\"Linear\"]) -> \"Linear\":\n    \"\"\"Combines multiple parallel linear functions into a single one.\"\"\"\n    joint_input_space = bases.join_vector_spaces(\n        *[fn.input_space for fn in fns])\n    joint_output_space = bases.join_vector_spaces(\n        *[fn.output_space for fn in fns])\n\n    def action(x: bases.BasisDirection) -> bases.VectorInBasis:\n      out = joint_output_space.null_vector()\n      for fn in fns:\n        if x in fn.input_space:\n          x_vec = fn.input_space.vector_from_basis_direction(x)\n          out += fn(x_vec).project(joint_output_space)\n      return out\n\n    return cls.from_action(joint_input_space, joint_output_space, action)\n\n\ndef project(\n    from_space: VectorSpaceWithBasis,\n    to_space: VectorSpaceWithBasis,\n) -> Linear:\n  \"\"\"Creates a projection.\"\"\"\n\n  def action(direction: bases.BasisDirection) -> VectorInBasis:\n    if direction in to_space:\n      return to_space.vector_from_basis_direction(direction)\n    else:\n      return to_space.null_vector()\n\n  return Linear.from_action(from_space, to_space, action=action)\n\n\n@dataclasses.dataclass\nclass ScalarBilinear:\n  \"\"\"A scalar-valued bilinear operator.\"\"\"\n  left_space: VectorSpaceWithBasis\n  right_space: VectorSpaceWithBasis\n  matrix: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Ensure matrix acts in sorted bases and typecheck sizes.\"\"\"\n    left_size, right_size = self.matrix.shape\n    assert left_size == self.left_space.num_dims\n    assert right_size == self.right_space.num_dims\n\n  def __call__(self, x: VectorInBasis, y: VectorInBasis) -> float:\n    \"\"\"Describes the action of the operator on vectors.\"\"\"\n    if x not in self.left_space:\n      raise TypeError(f\"x={x} not in self.left_space={self.left_space}.\")\n    if y not in self.right_space:\n      raise TypeError(f\"y={y} not in self.right_space={self.right_space}.\")\n    return (x.magnitudes.T @ self.matrix @ y.magnitudes).item()\n\n  @classmethod\n  def from_action(\n      cls,\n      left_space: VectorSpaceWithBasis,\n      right_space: VectorSpaceWithBasis,\n      action: Callable[[BasisDirection, BasisDirection], float],\n  ) -> \"ScalarBilinear\":\n    \"\"\"from_action(l, r)(action) creates a ScalarBilinear.\"\"\"", "metadata": {"task_id": "deepmind_tracr/97", "ground_truth": "    matrix = np.zeros((left_space.num_dims, right_space.num_dims))\n    for i, left_direction in enumerate(left_space.basis):\n      for j, right_direction in enumerate(right_space.basis):\n        matrix[i, j] = action(left_direction, right_direction)\n\n    return ScalarBilinear(left_space, right_space, matrix)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "vectorspace_fns.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "from_action", "line_no": 156}}
{"_id": "deepmind_tracr/98", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):", "metadata": {"task_id": "deepmind_tracr/98", "ground_truth": "    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "__str__", "line_no": 41}}
{"_id": "deepmind_tracr/99", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:", "metadata": {"task_id": "deepmind_tracr/99", "ground_truth": "    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 46, "function_name": "__lt__", "line_no": 46}}
{"_id": "deepmind_tracr/100", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"", "metadata": {"task_id": "deepmind_tracr/100", "ground_truth": "    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "__post_init__", "line_no": 65}}
{"_id": "deepmind_tracr/101", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":", "metadata": {"task_id": "deepmind_tracr/101", "ground_truth": "    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 76, "function_name": "__add__", "line_no": 76}}
{"_id": "deepmind_tracr/102", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":", "metadata": {"task_id": "deepmind_tracr/102", "ground_truth": "    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 87, "function_name": "__sub__", "line_no": 87}}
{"_id": "deepmind_tracr/103", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __rsub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {other} - {self}\")\n    magnitudes = other.magnitudes - self.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __mul__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes * scalar)\n\n  def __rmul__(self, scalar: float) -> \"VectorInBasis\":\n    return self * scalar\n\n  def __truediv__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes / scalar)\n\n  def __neg__(self) -> \"VectorInBasis\":\n    return (-1) * self\n\n  def __eq__(self, other: \"VectorInBasis\") -> bool:", "metadata": {"task_id": "deepmind_tracr/103", "ground_truth": "    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 111, "function_name": "__eq__", "line_no": 111}}
{"_id": "deepmind_tracr/104", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __rsub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {other} - {self}\")\n    magnitudes = other.magnitudes - self.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __mul__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes * scalar)\n\n  def __rmul__(self, scalar: float) -> \"VectorInBasis\":\n    return self * scalar\n\n  def __truediv__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes / scalar)\n\n  def __neg__(self) -> \"VectorInBasis\":\n    return (-1) * self\n\n  def __eq__(self, other: \"VectorInBasis\") -> bool:\n    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n\n  @classmethod\n  def sum(cls, vectors: Sequence[\"VectorInBasis\"]) -> \"VectorInBasis\":\n    return cls(vectors[0].basis_directions,\n               np.sum([x.magnitudes for x in vectors], axis=0))\n\n  @classmethod\n  def stack(cls,\n            vectors: Sequence[\"VectorInBasis\"],\n            axis: int = 0) -> \"VectorInBasis\":", "metadata": {"task_id": "deepmind_tracr/104", "ground_truth": "    for v in vectors[1:]:\n      if v.basis_directions != vectors[0].basis_directions:\n        raise TypeError(f\"Stacking incompatible bases: {vectors[0]} + {v}\")\n    return cls(vectors[0].basis_directions,\n               np.stack([v.magnitudes for v in vectors], axis=axis))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 124, "function_name": "stack", "line_no": 124}}
{"_id": "deepmind_tracr/105", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __rsub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {other} - {self}\")\n    magnitudes = other.magnitudes - self.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __mul__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes * scalar)\n\n  def __rmul__(self, scalar: float) -> \"VectorInBasis\":\n    return self * scalar\n\n  def __truediv__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes / scalar)\n\n  def __neg__(self) -> \"VectorInBasis\":\n    return (-1) * self\n\n  def __eq__(self, other: \"VectorInBasis\") -> bool:\n    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n\n  @classmethod\n  def sum(cls, vectors: Sequence[\"VectorInBasis\"]) -> \"VectorInBasis\":\n    return cls(vectors[0].basis_directions,\n               np.sum([x.magnitudes for x in vectors], axis=0))\n\n  @classmethod\n  def stack(cls,\n            vectors: Sequence[\"VectorInBasis\"],\n            axis: int = 0) -> \"VectorInBasis\":\n    for v in vectors[1:]:\n      if v.basis_directions != vectors[0].basis_directions:\n        raise TypeError(f\"Stacking incompatible bases: {vectors[0]} + {v}\")\n    return cls(vectors[0].basis_directions,\n               np.stack([v.magnitudes for v in vectors], axis=axis))\n\n  def project(\n      self, basis: Union[\"VectorSpaceWithBasis\", Sequence[BasisDirection]]\n  ) -> \"VectorInBasis\":\n    \"\"\"Projects to the basis.\"\"\"", "metadata": {"task_id": "deepmind_tracr/105", "ground_truth": "    if isinstance(basis, VectorSpaceWithBasis):\n      basis = basis.basis\n    components = []\n    for direction in basis:\n      if direction in self.basis_directions:\n        components.append(\n            self.magnitudes[..., self.basis_directions.index(direction)])\n      else:\n        components.append(np.zeros_like(self.magnitudes[..., 0]))\n    return VectorInBasis(list(basis), np.stack(components, axis=-1))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 134, "function_name": "project", "line_no": 134}}
{"_id": "deepmind_tracr/106", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Vectors and bases.\"\"\"\n\nimport dataclasses\nfrom typing import Sequence, Union, Optional, Iterable\n\nimport numpy as np\n\nName = Union[int, str]\nValue = Union[int, float, bool, str, tuple]\n\n\n@dataclasses.dataclass(frozen=True)\nclass BasisDirection:\n  \"\"\"Represents a basis direction (no magnitude) in a vector space.\n\n  Attributes:\n    name: a unique name for this direction.\n    value: used to hold a value one-hot-encoded by this direction. e.g.,\n      [BasisDirection(\"vs_1\", True), BasisDirection(\"vs_1\", False)] would be\n      basis directions of a subspace called \"vs_1\" which one-hot-encodes the\n      values True and False. If provided, considered part of the name for the\n      purpose of disambiguating directions.\n  \"\"\"\n  name: Name\n  value: Optional[Value] = None\n\n  def __str__(self):\n    if self.value is None:\n      return str(self.name)\n    return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __rsub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {other} - {self}\")\n    magnitudes = other.magnitudes - self.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __mul__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes * scalar)\n\n  def __rmul__(self, scalar: float) -> \"VectorInBasis\":\n    return self * scalar\n\n  def __truediv__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes / scalar)\n\n  def __neg__(self) -> \"VectorInBasis\":\n    return (-1) * self\n\n  def __eq__(self, other: \"VectorInBasis\") -> bool:\n    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n\n  @classmethod\n  def sum(cls, vectors: Sequence[\"VectorInBasis\"]) -> \"VectorInBasis\":\n    return cls(vectors[0].basis_directions,\n               np.sum([x.magnitudes for x in vectors], axis=0))\n\n  @classmethod\n  def stack(cls,\n            vectors: Sequence[\"VectorInBasis\"],\n            axis: int = 0) -> \"VectorInBasis\":\n    for v in vectors[1:]:\n      if v.basis_directions != vectors[0].basis_directions:\n        raise TypeError(f\"Stacking incompatible bases: {vectors[0]} + {v}\")\n    return cls(vectors[0].basis_directions,\n               np.stack([v.magnitudes for v in vectors], axis=axis))\n\n  def project(\n      self, basis: Union[\"VectorSpaceWithBasis\", Sequence[BasisDirection]]\n  ) -> \"VectorInBasis\":\n    \"\"\"Projects to the basis.\"\"\"\n    if isinstance(basis, VectorSpaceWithBasis):\n      basis = basis.basis\n    components = []\n    for direction in basis:\n      if direction in self.basis_directions:\n        components.append(\n            self.magnitudes[..., self.basis_directions.index(direction)])\n      else:\n        components.append(np.zeros_like(self.magnitudes[..., 0]))\n    return VectorInBasis(list(basis), np.stack(components, axis=-1))\n\n\n@dataclasses.dataclass\nclass VectorSpaceWithBasis:\n  \"\"\"A vector subspace in a given basis.\"\"\"\n  basis: Sequence[BasisDirection]\n\n  def __post_init__(self):\n    \"\"\"Keep basis directions sorted.\"\"\"\n    self.basis = sorted(self.basis)\n\n  @property\n  def num_dims(self) -> int:\n    return len(self.basis)\n\n  def __contains__(self, item: Union[VectorInBasis, BasisDirection]) -> bool:", "metadata": {"task_id": "deepmind_tracr/106", "ground_truth": "    if isinstance(item, BasisDirection):\n      return item in self.basis\n\n    return set(self.basis) == set(item.basis_directions)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 0, "lineno": 160, "function_name": "__contains__", "line_no": 160}}
{"_id": "deepmind_tracr/107", "text": " return f\"{self.name}:{self.value}\"\n\n  def __lt__(self, other: \"BasisDirection\") -> bool:\n    try:\n      return (self.name, self.value) < (other.name, other.value)\n    except TypeError:\n      return str(self) < str(other)\n\n\n@dataclasses.dataclass\nclass VectorInBasis:\n  \"\"\"A vector (or array of vectors) in a given basis.\n\n  When magnitudes are 1-d, this is a vector.\n  When magnitudes are (n+1)-d, this is an array of vectors,\n  where the -1th dimension is the basis dimension.\n  \"\"\"\n  basis_directions: Sequence[BasisDirection]\n  magnitudes: np.ndarray\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __rsub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {other} - {self}\")\n    magnitudes = other.magnitudes - self.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __mul__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes * scalar)\n\n  def __rmul__(self, scalar: float) -> \"VectorInBasis\":\n    return self * scalar\n\n  def __truediv__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes / scalar)\n\n  def __neg__(self) -> \"VectorInBasis\":\n    return (-1) * self\n\n  def __eq__(self, other: \"VectorInBasis\") -> bool:\n    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n\n  @classmethod\n  def sum(cls, vectors: Sequence[\"VectorInBasis\"]) -> \"VectorInBasis\":\n    return cls(vectors[0].basis_directions,\n               np.sum([x.magnitudes for x in vectors], axis=0))\n\n  @classmethod\n  def stack(cls,\n            vectors: Sequence[\"VectorInBasis\"],\n            axis: int = 0) -> \"VectorInBasis\":\n    for v in vectors[1:]:\n      if v.basis_directions != vectors[0].basis_directions:\n        raise TypeError(f\"Stacking incompatible bases: {vectors[0]} + {v}\")\n    return cls(vectors[0].basis_directions,\n               np.stack([v.magnitudes for v in vectors], axis=axis))\n\n  def project(\n      self, basis: Union[\"VectorSpaceWithBasis\", Sequence[BasisDirection]]\n  ) -> \"VectorInBasis\":\n    \"\"\"Projects to the basis.\"\"\"\n    if isinstance(basis, VectorSpaceWithBasis):\n      basis = basis.basis\n    components = []\n    for direction in basis:\n      if direction in self.basis_directions:\n        components.append(\n            self.magnitudes[..., self.basis_directions.index(direction)])\n      else:\n        components.append(np.zeros_like(self.magnitudes[..., 0]))\n    return VectorInBasis(list(basis), np.stack(components, axis=-1))\n\n\n@dataclasses.dataclass\nclass VectorSpaceWithBasis:\n  \"\"\"A vector subspace in a given basis.\"\"\"\n  basis: Sequence[BasisDirection]\n\n  def __post_init__(self):\n    \"\"\"Keep basis directions sorted.\"\"\"\n    self.basis = sorted(self.basis)\n\n  @property\n  def num_dims(self) -> int:\n    return len(self.basis)\n\n  def __contains__(self, item: Union[VectorInBasis, BasisDirection]) -> bool:\n    if isinstance(item, BasisDirection):\n      return item in self.basis\n\n    return set(self.basis) == set(item.basis_directions)\n\n  def issubspace(self, other: \"VectorSpaceWithBasis\") -> bool:\n    return set(self.basis).issubset(set(other.basis))\n\n  def basis_vectors(self) -> Sequence[VectorInBasis]:\n    basis_vector_magnitudes = list(np.eye(self.num_dims))\n    return [VectorInBasis(self.basis, m) for m in basis_vector_magnitudes]\n\n  def vector_from_basis_direction(\n      self, basis_direction: BasisDirection) -> VectorInBasis:\n    i = self.basis.index(basis_direction)\n    return VectorInBasis(self.basis, np.eye(self.num_dims)[i])\n\n  def null_vector(self) -> VectorInBasis:\n    return VectorInBasis(self.basis, np.zeros(self.num_dims))\n\n  @classmethod\n  def from_names(cls, names: Sequence[Name]) -> \"VectorSpaceWithBasis\":\n    \"\"\"Creates a VectorSpace from a list of names for its basis directions.\"\"\"\n    return cls([BasisDirection(n) for n in names])\n\n  @classmethod\n  def from_values(\n      cls,\n      name: Name,\n      values: Iterable[Value],\n  ) -> \"VectorSpaceWithBasis\":\n    \"\"\"Creates a VectorSpace from a list of values for its basis directions.\"\"\"\n    return cls([BasisDirection(name, v) for v in values])\n\n\ndef direct_sum(*vs: VectorSpaceWithBasis) -> VectorSpaceWithBasis:\n  \"\"\"Create a direct sum of the vector spaces.\n\n  Assumes the basis elements of all input vector spaces are\n  orthogonal to each other. Maintains the order of the bases.\n\n  Args:\n    *vs: the vector spaces to sum.\n\n  Returns:\n    the combined vector space.\n\n  Raises:\n    Value error in case of overlapping bases.\n  \"\"\"\n  # Take the union of all the bases:", "metadata": {"task_id": "deepmind_tracr/107", "ground_truth": "  total_basis = sum([v.basis for v in vs], [])\n\n  if len(total_basis) != len(set(total_basis)):\n    raise ValueError(\"Overlapping bases!\")\n\n  return VectorSpaceWithBasis(total_basis)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 43, "lineno": 211, "function_name": "direct_sum", "line_no": 211}}
{"_id": "deepmind_tracr/108", "text": "\n\n  def __post_init__(self):\n    \"\"\"Sort basis directions.\"\"\"\n    if len(self.basis_directions) != self.magnitudes.shape[-1]:\n      raise ValueError(\n          \"Last dimension of magnitudes must be the same as number \"\n          f\"of basis directions. Was {len(self.basis_directions)} \"\n          f\"and {self.magnitudes.shape[-1]}.\")\n\n    sort_idx = np.argsort(self.basis_directions)\n    self.basis_directions = [self.basis_directions[i] for i in sort_idx]\n    self.magnitudes = np.take(self.magnitudes, sort_idx, -1)\n\n  def __add__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {self} + {other}\")\n    magnitudes = self.magnitudes + other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __radd__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Adding incompatible bases: {other} + {self}\")\n    return self + other\n\n  def __sub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {self} - {other}\")\n    magnitudes = self.magnitudes - other.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __rsub__(self, other: \"VectorInBasis\") -> \"VectorInBasis\":\n    if self.basis_directions != other.basis_directions:\n      raise TypeError(f\"Subtracting incompatible bases: {other} - {self}\")\n    magnitudes = other.magnitudes - self.magnitudes\n    return VectorInBasis(self.basis_directions, magnitudes)\n\n  def __mul__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes * scalar)\n\n  def __rmul__(self, scalar: float) -> \"VectorInBasis\":\n    return self * scalar\n\n  def __truediv__(self, scalar: float) -> \"VectorInBasis\":\n    return VectorInBasis(self.basis_directions, self.magnitudes / scalar)\n\n  def __neg__(self) -> \"VectorInBasis\":\n    return (-1) * self\n\n  def __eq__(self, other: \"VectorInBasis\") -> bool:\n    return ((self.basis_directions == other.basis_directions) and\n            (self.magnitudes.shape == other.magnitudes.shape) and\n            (np.all(self.magnitudes == other.magnitudes)))\n\n  @classmethod\n  def sum(cls, vectors: Sequence[\"VectorInBasis\"]) -> \"VectorInBasis\":\n    return cls(vectors[0].basis_directions,\n               np.sum([x.magnitudes for x in vectors], axis=0))\n\n  @classmethod\n  def stack(cls,\n            vectors: Sequence[\"VectorInBasis\"],\n            axis: int = 0) -> \"VectorInBasis\":\n    for v in vectors[1:]:\n      if v.basis_directions != vectors[0].basis_directions:\n        raise TypeError(f\"Stacking incompatible bases: {vectors[0]} + {v}\")\n    return cls(vectors[0].basis_directions,\n               np.stack([v.magnitudes for v in vectors], axis=axis))\n\n  def project(\n      self, basis: Union[\"VectorSpaceWithBasis\", Sequence[BasisDirection]]\n  ) -> \"VectorInBasis\":\n    \"\"\"Projects to the basis.\"\"\"\n    if isinstance(basis, VectorSpaceWithBasis):\n      basis = basis.basis\n    components = []\n    for direction in basis:\n      if direction in self.basis_directions:\n        components.append(\n            self.magnitudes[..., self.basis_directions.index(direction)])\n      else:\n        components.append(np.zeros_like(self.magnitudes[..., 0]))\n    return VectorInBasis(list(basis), np.stack(components, axis=-1))\n\n\n@dataclasses.dataclass\nclass VectorSpaceWithBasis:\n  \"\"\"A vector subspace in a given basis.\"\"\"\n  basis: Sequence[BasisDirection]\n\n  def __post_init__(self):\n    \"\"\"Keep basis directions sorted.\"\"\"\n    self.basis = sorted(self.basis)\n\n  @property\n  def num_dims(self) -> int:\n    return len(self.basis)\n\n  def __contains__(self, item: Union[VectorInBasis, BasisDirection]) -> bool:\n    if isinstance(item, BasisDirection):\n      return item in self.basis\n\n    return set(self.basis) == set(item.basis_directions)\n\n  def issubspace(self, other: \"VectorSpaceWithBasis\") -> bool:\n    return set(self.basis).issubset(set(other.basis))\n\n  def basis_vectors(self) -> Sequence[VectorInBasis]:\n    basis_vector_magnitudes = list(np.eye(self.num_dims))\n    return [VectorInBasis(self.basis, m) for m in basis_vector_magnitudes]\n\n  def vector_from_basis_direction(\n      self, basis_direction: BasisDirection) -> VectorInBasis:\n    i = self.basis.index(basis_direction)\n    return VectorInBasis(self.basis, np.eye(self.num_dims)[i])\n\n  def null_vector(self) -> VectorInBasis:\n    return VectorInBasis(self.basis, np.zeros(self.num_dims))\n\n  @classmethod\n  def from_names(cls, names: Sequence[Name]) -> \"VectorSpaceWithBasis\":\n    \"\"\"Creates a VectorSpace from a list of names for its basis directions.\"\"\"\n    return cls([BasisDirection(n) for n in names])\n\n  @classmethod\n  def from_values(\n      cls,\n      name: Name,\n      values: Iterable[Value],\n  ) -> \"VectorSpaceWithBasis\":\n    \"\"\"Creates a VectorSpace from a list of values for its basis directions.\"\"\"\n    return cls([BasisDirection(name, v) for v in values])\n\n\ndef direct_sum(*vs: VectorSpaceWithBasis) -> VectorSpaceWithBasis:\n  \"\"\"Create a direct sum of the vector spaces.\n\n  Assumes the basis elements of all input vector spaces are\n  orthogonal to each other. Maintains the order of the bases.\n\n  Args:\n    *vs: the vector spaces to sum.\n\n  Returns:\n    the combined vector space.\n\n  Raises:\n    Value error in case of overlapping bases.\n  \"\"\"\n  # Take the union of all the bases:\n  total_basis = sum([v.basis for v in vs], [])\n\n  if len(total_basis) != len(set(total_basis)):\n    raise ValueError(\"Overlapping bases!\")\n\n  return VectorSpaceWithBasis(total_basis)\n\n\ndef join_vector_spaces(*vs: VectorSpaceWithBasis) -> VectorSpaceWithBasis:\n  \"\"\"Joins a set of vector spaces allowing them to overlap.\n\n  Assumes the basis elements of all input vector spaces are\n  orthogonal to each other. Does not maintain the order of the bases but\n  sorts them.\n\n  Args:\n    *vs: the vector spaces to sum.\n\n  Returns:\n    the combined vector space.\n  \"\"\"\n  # Take the union of all the bases:", "metadata": {"task_id": "deepmind_tracr/108", "ground_truth": "  total_basis = list(set().union(*[set(v.basis) for v in vs]))\n  total_basis = sorted(total_basis)\n  return VectorSpaceWithBasis(total_basis)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "bases.py"], "context_start_lineno": 61, "lineno": 233, "function_name": "join_vector_spaces", "line_no": 233}}
{"_id": "deepmind_tracr/109", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"", "metadata": {"task_id": "deepmind_tracr/109", "ground_truth": "    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "__post_init__", "line_no": 65}}
{"_id": "deepmind_tracr/110", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/110", "ground_truth": "    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "apply", "line_no": 77}}
{"_id": "deepmind_tracr/111", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"", "metadata": {"task_id": "deepmind_tracr/111", "ground_truth": "    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "w_ov_residual", "line_no": 98}}
{"_id": "deepmind_tracr/112", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n\n  @property\n  def num_heads(self) -> int:\n    return 1\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return MultiAttentionHead([self])\n\n\n@dataclasses.dataclass\nclass MultiAttentionHead(Block):\n  \"\"\"Applies attention heads in parallel.\"\"\"\n  sub_blocks: List[Union[AttentionHead, \"MultiAttentionHead\"]]\n\n  def __post_init__(self):", "metadata": {"task_id": "deepmind_tracr/112", "ground_truth": "    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "__post_init__", "line_no": 116}}
{"_id": "deepmind_tracr/113", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n\n  @property\n  def num_heads(self) -> int:\n    return 1\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return MultiAttentionHead([self])\n\n\n@dataclasses.dataclass\nclass MultiAttentionHead(Block):\n  \"\"\"Applies attention heads in parallel.\"\"\"\n  sub_blocks: List[Union[AttentionHead, \"MultiAttentionHead\"]]\n\n  def __post_init__(self):\n    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    # each element is seq_len x embedding\n    outs = [block.apply(x) for block in self.sub_blocks]\n    return bases.VectorInBasis.sum(outs)  # seq_len x embedding\n\n  @property\n  def num_heads(self) -> int:\n    return sum(sub_block.num_heads for sub_block in self.sub_blocks)\n\n  def heads(self) -> Iterable[AttentionHead]:", "metadata": {"task_id": "deepmind_tracr/113", "ground_truth": "    for sub_block in self.sub_blocks:\n      if isinstance(sub_block, AttentionHead):\n        yield sub_block\n      elif isinstance(sub_block, MultiAttentionHead):\n        yield from sub_block.heads()\n      else:\n        raise NotImplementedError()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 130, "function_name": "heads", "line_no": 130}}
{"_id": "deepmind_tracr/114", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n\n  @property\n  def num_heads(self) -> int:\n    return 1\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return MultiAttentionHead([self])\n\n\n@dataclasses.dataclass\nclass MultiAttentionHead(Block):\n  \"\"\"Applies attention heads in parallel.\"\"\"\n  sub_blocks: List[Union[AttentionHead, \"MultiAttentionHead\"]]\n\n  def __post_init__(self):\n    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    # each element is seq_len x embedding\n    outs = [block.apply(x) for block in self.sub_blocks]\n    return bases.VectorInBasis.sum(outs)  # seq_len x embedding\n\n  @property\n  def num_heads(self) -> int:\n    return sum(sub_block.num_heads for sub_block in self.sub_blocks)\n\n  def heads(self) -> Iterable[AttentionHead]:\n    for sub_block in self.sub_blocks:\n      if isinstance(sub_block, AttentionHead):\n        yield sub_block\n      elif isinstance(sub_block, MultiAttentionHead):\n        yield from sub_block.heads()\n      else:\n        raise NotImplementedError()\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return self\n\n\n@dataclasses.dataclass\nclass MLP(Block):\n  \"\"\"A transformer MLP block.\"\"\"\n  fst: vectorspace_fns.Linear\n  snd: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n\n  def __post_init__(self):\n    \"\"\"Typecheck subspaces.\"\"\"", "metadata": {"task_id": "deepmind_tracr/114", "ground_truth": "    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.fst.input_space,\n                                                     self.snd.output_space)\n\n    assert self.fst.output_space == self.snd.input_space\n    assert self.fst.input_space.issubspace(self.residual_space)\n    assert self.snd.output_space.issubspace(self.residual_space)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 151, "function_name": "__post_init__", "line_no": 151}}
{"_id": "deepmind_tracr/115", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n\n  @property\n  def num_heads(self) -> int:\n    return 1\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return MultiAttentionHead([self])\n\n\n@dataclasses.dataclass\nclass MultiAttentionHead(Block):\n  \"\"\"Applies attention heads in parallel.\"\"\"\n  sub_blocks: List[Union[AttentionHead, \"MultiAttentionHead\"]]\n\n  def __post_init__(self):\n    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    # each element is seq_len x embedding\n    outs = [block.apply(x) for block in self.sub_blocks]\n    return bases.VectorInBasis.sum(outs)  # seq_len x embedding\n\n  @property\n  def num_heads(self) -> int:\n    return sum(sub_block.num_heads for sub_block in self.sub_blocks)\n\n  def heads(self) -> Iterable[AttentionHead]:\n    for sub_block in self.sub_blocks:\n      if isinstance(sub_block, AttentionHead):\n        yield sub_block\n      elif isinstance(sub_block, MultiAttentionHead):\n        yield from sub_block.heads()\n      else:\n        raise NotImplementedError()\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return self\n\n\n@dataclasses.dataclass\nclass MLP(Block):\n  \"\"\"A transformer MLP block.\"\"\"\n  fst: vectorspace_fns.Linear\n  snd: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n\n  def __post_init__(self):\n    \"\"\"Typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.fst.input_space,\n                                                     self.snd.output_space)\n\n    assert self.fst.output_space == self.snd.input_space\n    assert self.fst.input_space.issubspace(self.residual_space)\n    assert self.snd.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/115", "ground_truth": "    assert x in self.residual_space\n\n    x = project(self.residual_space, self.fst.input_space)(x)\n    hidden = self.fst(x)\n    hidden = relu(hidden)\n    out = self.snd(hidden)\n    return project(self.snd.output_space, self.residual_space)(out)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 160, "function_name": "apply", "line_no": 160}}
{"_id": "deepmind_tracr/116", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Pieces for making transformers.\"\"\"\n\nimport abc\nimport dataclasses\nfrom typing import Iterable, List, Optional, Sequence, Union\n\nimport numpy as np\n\nfrom tracr.craft import bases\nfrom tracr.craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n\n  @property\n  def num_heads(self) -> int:\n    return 1\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return MultiAttentionHead([self])\n\n\n@dataclasses.dataclass\nclass MultiAttentionHead(Block):\n  \"\"\"Applies attention heads in parallel.\"\"\"\n  sub_blocks: List[Union[AttentionHead, \"MultiAttentionHead\"]]\n\n  def __post_init__(self):\n    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    # each element is seq_len x embedding\n    outs = [block.apply(x) for block in self.sub_blocks]\n    return bases.VectorInBasis.sum(outs)  # seq_len x embedding\n\n  @property\n  def num_heads(self) -> int:\n    return sum(sub_block.num_heads for sub_block in self.sub_blocks)\n\n  def heads(self) -> Iterable[AttentionHead]:\n    for sub_block in self.sub_blocks:\n      if isinstance(sub_block, AttentionHead):\n        yield sub_block\n      elif isinstance(sub_block, MultiAttentionHead):\n        yield from sub_block.heads()\n      else:\n        raise NotImplementedError()\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return self\n\n\n@dataclasses.dataclass\nclass MLP(Block):\n  \"\"\"A transformer MLP block.\"\"\"\n  fst: vectorspace_fns.Linear\n  snd: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n\n  def __post_init__(self):\n    \"\"\"Typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.fst.input_space,\n                                                     self.snd.output_space)\n\n    assert self.fst.output_space == self.snd.input_space\n    assert self.fst.input_space.issubspace(self.residual_space)\n    assert self.snd.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n\n    x = project(self.residual_space, self.fst.input_space)(x)\n    hidden = self.fst(x)\n    hidden = relu(hidden)\n    out = self.snd(hidden)\n    return project(self.snd.output_space, self.residual_space)(out)\n\n  @classmethod\n  def combine_in_parallel(cls, mlps: Sequence[\"MLP\"]) -> \"MLP\":", "metadata": {"task_id": "deepmind_tracr/116", "ground_truth": "    fst = vectorspace_fns.Linear.combine_in_parallel(\n        [block.fst for block in mlps])\n    snd = vectorspace_fns.Linear.combine_in_parallel(\n        [block.snd for block in mlps])\n    return cls(fst=fst, snd=snd, residual_space=None)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 0, "lineno": 170, "function_name": "combine_in_parallel", "line_no": 170}}
{"_id": "deepmind_tracr/117", "text": ".craft import vectorspace_fns\n\nproject = vectorspace_fns.project\n\n\ndef _np_softmax(x, axis=-1):\n  x_max = np.max(x, axis=axis, keepdims=True)\n  return np.exp(x - x_max) / np.sum(np.exp(x - x_max), axis=axis, keepdims=True)\n\n\ndef _np_relu(x):\n  return np.where(x > 0, x, 0)\n\n\ndef relu(x: bases.VectorInBasis) -> bases.VectorInBasis:\n  return bases.VectorInBasis(x.basis_directions, _np_relu(x.magnitudes))\n\n\nclass Block(abc.ABC):\n  \"\"\"Transformer block, acting on a sequence of vector space elements.\n\n  Attributes:\n    residual_space: Vector space that contains all subspaces the Block interacts\n      with. This can be either the full residual space of a model or a subspace.\n  \"\"\"\n  residual_space: bases.VectorSpaceWithBasis\n\n  @abc.abstractmethod\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Applies self to an input.\"\"\"\n\n\n@dataclasses.dataclass\nclass AttentionHead(Block):\n  \"\"\"A transformer attention head.\"\"\"\n  w_qk: vectorspace_fns.ScalarBilinear\n  w_ov: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n  causal: bool = False\n\n  def __post_init__(self):\n    \"\"\"Infer residual stream and typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.w_qk.left_space,\n                                                     self.w_qk.right_space,\n                                                     self.w_ov.input_space,\n                                                     self.w_ov.output_space)\n\n    assert self.w_qk.left_space.issubspace(self.residual_space)\n    assert self.w_qk.right_space.issubspace(self.residual_space)\n    assert self.w_ov.input_space.issubspace(self.residual_space)\n    assert self.w_ov.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n    # seq_len x query_space\n    queries = x.project(self.w_qk.left_space)\n    # seq_len x key_space\n    keys = x.project(self.w_qk.right_space)\n\n    attn_matrix = queries.magnitudes @ self.w_qk.matrix @ keys.magnitudes.T\n\n    if self.causal:\n      # The 1 gives us the matrix above the diagonal.\n      mask = np.triu(np.full_like(attn_matrix, -np.inf), 1)\n      attn_matrix = attn_matrix + mask\n\n    attn_weights = _np_softmax(attn_matrix)  # seq_len_from, seq_len_to\n    values = self.w_ov_residual(x).magnitudes  # seq_len_to, d_model\n\n    magnitudes = attn_weights @ values  # seq_len_from, d_model\n    return bases.VectorInBasis(sorted(self.residual_space.basis), magnitudes)\n\n  def w_ov_residual(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    \"\"\"Wov but acting on the residual space.\"\"\"\n    x = project(self.residual_space, self.w_ov.input_space)(x)\n    out = self.w_ov(x)\n    return project(self.w_ov.output_space, self.residual_space)(out)\n\n  @property\n  def num_heads(self) -> int:\n    return 1\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return MultiAttentionHead([self])\n\n\n@dataclasses.dataclass\nclass MultiAttentionHead(Block):\n  \"\"\"Applies attention heads in parallel.\"\"\"\n  sub_blocks: List[Union[AttentionHead, \"MultiAttentionHead\"]]\n\n  def __post_init__(self):\n    spaces = [block.residual_space for block in self.sub_blocks]\n    self.residual_space, *others = spaces\n    assert all(s == self.residual_space for s in others)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    # each element is seq_len x embedding\n    outs = [block.apply(x) for block in self.sub_blocks]\n    return bases.VectorInBasis.sum(outs)  # seq_len x embedding\n\n  @property\n  def num_heads(self) -> int:\n    return sum(sub_block.num_heads for sub_block in self.sub_blocks)\n\n  def heads(self) -> Iterable[AttentionHead]:\n    for sub_block in self.sub_blocks:\n      if isinstance(sub_block, AttentionHead):\n        yield sub_block\n      elif isinstance(sub_block, MultiAttentionHead):\n        yield from sub_block.heads()\n      else:\n        raise NotImplementedError()\n\n  def as_multi(self) -> \"MultiAttentionHead\":\n    return self\n\n\n@dataclasses.dataclass\nclass MLP(Block):\n  \"\"\"A transformer MLP block.\"\"\"\n  fst: vectorspace_fns.Linear\n  snd: vectorspace_fns.Linear\n  residual_space: Optional[bases.VectorSpaceWithBasis] = None\n\n  def __post_init__(self):\n    \"\"\"Typecheck subspaces.\"\"\"\n    if self.residual_space is None:\n      self.residual_space = bases.join_vector_spaces(self.fst.input_space,\n                                                     self.snd.output_space)\n\n    assert self.fst.output_space == self.snd.input_space\n    assert self.fst.input_space.issubspace(self.residual_space)\n    assert self.snd.output_space.issubspace(self.residual_space)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:\n    assert x in self.residual_space\n\n    x = project(self.residual_space, self.fst.input_space)(x)\n    hidden = self.fst(x)\n    hidden = relu(hidden)\n    out = self.snd(hidden)\n    return project(self.snd.output_space, self.residual_space)(out)\n\n  @classmethod\n  def combine_in_parallel(cls, mlps: Sequence[\"MLP\"]) -> \"MLP\":\n    fst = vectorspace_fns.Linear.combine_in_parallel(\n        [block.fst for block in mlps])\n    snd = vectorspace_fns.Linear.combine_in_parallel(\n        [block.snd for block in mlps])\n    return cls(fst=fst, snd=snd, residual_space=None)\n\n\n# Block that fits into a half-layer, without residual connections.\nHalfLayerBlock = Union[MLP, AttentionHead, MultiAttentionHead]\n\n\n@dataclasses.dataclass\nclass SeriesWithResiduals(Block):\n  \"\"\"A series of blocks with residual connections.\"\"\"\n  blocks: List[HalfLayerBlock]\n\n  def __post_init__(self):\n    spaces = [block.residual_space for block in self.blocks]\n    self.residual_space = bases.join_vector_spaces(*spaces)\n\n  def apply(self, x: bases.VectorInBasis) -> bases.VectorInBasis:", "metadata": {"task_id": "deepmind_tracr/117", "ground_truth": "    x = x.project(self.residual_space)\n    for block in self.blocks:\n      x_in = x.project(block.residual_space)\n      x_out = block.apply(x_in).project(self.residual_space)\n      x = x + x_out\n    return x\n", "fpath_tuple": ["deepmind_tracr", "tracr", "craft", "transformers.py"], "context_start_lineno": 23, "lineno": 191, "function_name": "apply", "line_no": 191}}
{"_id": "deepmind_tracr/118", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Helpers for handling errors in user-provided functions.\"\"\"\n\nimport functools\nimport logging\nfrom typing import Any, Callable\n\n\ndef ignoring_arithmetic_errors(fun: Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Makes fun return None instead of raising ArithmeticError.\"\"\"\n\n  @functools.wraps(fun)", "metadata": {"task_id": "deepmind_tracr/118", "ground_truth": "  def fun_wrapped(*args):\n    try:\n      return fun(*args)\n    except ArithmeticError:\n      logging.warning(\n          \"Encountered arithmetic error in function: for value %s. \"\n          \"Assuming this input will never occur.\", str(args))\n      return None\n\n  return fun_wrapped\n", "fpath_tuple": ["deepmind_tracr", "tracr", "utils", "errors.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "ignoring_arithmetic_errors", "line_no": 25}}
{"_id": "deepmind_tracr/119", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Helpers for handling errors in user-provided functions.\"\"\"\n\nimport functools\nimport logging\nfrom typing import Any, Callable\n\n\ndef ignoring_arithmetic_errors(fun: Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Makes fun return None instead of raising ArithmeticError.\"\"\"\n\n  @functools.wraps(fun)\n  def fun_wrapped(*args):", "metadata": {"task_id": "deepmind_tracr/119", "ground_truth": "    try:\n      return fun(*args)\n    except ArithmeticError:\n      logging.warning(\n          \"Encountered arithmetic error in function: for value %s. \"\n          \"Assuming this input will never occur.\", str(args))\n      return None\n", "fpath_tuple": ["deepmind_tracr", "tracr", "utils", "errors.py"], "context_start_lineno": 0, "lineno": 26, "function_name": "fun_wrapped", "line_no": 26}}
{"_id": "deepmind_tracr/120", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import compressed_model\nfrom tracr.transformer import model\n\n\nclass CompressedTransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"", "metadata": {"task_id": "deepmind_tracr/120", "ground_truth": "    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 33, "function_name": "_check_layer_naming", "line_no": 33}}
{"_id": "deepmind_tracr/121", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import compressed_model\nfrom tracr.transformer import model\n\n\nclass CompressedTransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):", "metadata": {"task_id": "deepmind_tracr/121", "ground_truth": "    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 50, "function_name": "_zero_mlps", "line_no": 50}}
{"_id": "deepmind_tracr/122", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import compressed_model\nfrom tracr.transformer import model\n\n\nclass CompressedTransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/122", "ground_truth": "      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 65, "function_name": "forward", "line_no": 65}}
{"_id": "deepmind_tracr/123", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import compressed_model\nfrom tracr.transformer import model\n\n\nclass CompressedTransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/123", "ground_truth": "      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 97, "function_name": "forward", "line_no": 97}}
{"_id": "deepmind_tracr/124", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import compressed_model\nfrom tracr.transformer import model\n\n\nclass CompressedTransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):", "metadata": {"task_id": "deepmind_tracr/124", "ground_truth": "      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 135, "function_name": "forward_zero", "line_no": 135}}
{"_id": "deepmind_tracr/125", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import compressed_model\nfrom tracr.transformer import model\n\n\nclass CompressedTransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"compressed_transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"compressed_transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"compressed_transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"compressed_transformer\")\n      if len(levels) == 1:\n        self.assertEqual(list(params[key].keys()), [\"w_emb\"])\n        continue\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/125", "ground_truth": "      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 0, "lineno": 149, "function_name": "forward", "line_no": 149}}
{"_id": "deepmind_tracr/126", "text": "zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params_no_mlps = self._zero_mlps(params)\n\n    out_zero_activation = forward_zero.apply(params, next(rng), emb, mask)\n    out_no_mlps = forward.apply(params_no_mlps, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    np.testing.assert_allclose(out_zero_activation, out_no_mlps)\n    self.assertFalse(np.allclose(out_zero_activation, 0))\n\n  def test_not_setting_embedding_size_produces_same_output_as_default_model(\n      self):\n    config = model.TransformerConfig(\n        num_heads=2,\n        num_layers=2,\n        key_size=5,\n        mlp_hidden_size=64,\n        dropout_rate=0.,\n        causal=False,\n        layer_norm=False)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def forward_model(emb, mask):\n      return model.Transformer(config)(emb, mask).output\n\n    @hk.without_apply_rng\n    @hk.transform\n    def forward_superposition(emb, mask):\n      return compressed_model.CompressedTransformer(config)(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward_model.init(next(rng), emb, mask)\n    params_superposition = {\n        k.replace(\"transformer\", \"compressed_transformer\"): v\n        for k, v in params.items()\n    }\n\n    out_model = forward_model.apply(params, emb, mask)\n    out_superposition = forward_superposition.apply(params_superposition, emb,\n                                                    mask)\n\n    self._check_layer_naming(params_superposition)\n    np.testing.assert_allclose(out_model, out_superposition)\n\n  @parameterized.parameters(\n      dict(embedding_size=2, unembed_at_every_layer=True),\n      dict(embedding_size=2, unembed_at_every_layer=False),\n      dict(embedding_size=6, unembed_at_every_layer=True),\n      dict(embedding_size=6, unembed_at_every_layer=False))\n  def test_embbeding_size_produces_correct_shape_of_residuals_and_layer_outputs(\n      self, embedding_size, unembed_at_every_layer):\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/126", "ground_truth": "      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False))\n      return transformer(\n          emb,\n          mask,\n          embedding_size=embedding_size,\n          unembed_at_every_layer=unembed_at_every_layer,\n      )\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 49, "lineno": 227, "function_name": "forward", "line_no": 227}}
{"_id": "deepmind_tracr/127", "text": ").output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params_no_mlps = self._zero_mlps(params)\n\n    out_zero_activation = forward_zero.apply(params, next(rng), emb, mask)\n    out_no_mlps = forward.apply(params_no_mlps, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    np.testing.assert_allclose(out_zero_activation, out_no_mlps)\n    self.assertFalse(np.allclose(out_zero_activation, 0))\n\n  def test_not_setting_embedding_size_produces_same_output_as_default_model(\n      self):\n    config = model.TransformerConfig(\n        num_heads=2,\n        num_layers=2,\n        key_size=5,\n        mlp_hidden_size=64,\n        dropout_rate=0.,\n        causal=False,\n        layer_norm=False)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def forward_model(emb, mask):\n      return model.Transformer(config)(emb, mask).output\n\n    @hk.without_apply_rng\n    @hk.transform\n    def forward_superposition(emb, mask):\n      return compressed_model.CompressedTransformer(config)(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward_model.init(next(rng), emb, mask)\n    params_superposition = {\n        k.replace(\"transformer\", \"compressed_transformer\"): v\n        for k, v in params.items()\n    }\n\n    out_model = forward_model.apply(params, emb, mask)\n    out_superposition = forward_superposition.apply(params_superposition, emb,\n                                                    mask)\n\n    self._check_layer_naming(params_superposition)\n    np.testing.assert_allclose(out_model, out_superposition)\n\n  @parameterized.parameters(\n      dict(embedding_size=2, unembed_at_every_layer=True),\n      dict(embedding_size=2, unembed_at_every_layer=False),\n      dict(embedding_size=6, unembed_at_every_layer=True),\n      dict(embedding_size=6, unembed_at_every_layer=False))\n  def test_embbeding_size_produces_correct_shape_of_residuals_and_layer_outputs(\n      self, embedding_size, unembed_at_every_layer):\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = compressed_model.CompressedTransformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False))\n      return transformer(\n          emb,\n          mask,\n          embedding_size=embedding_size,\n          unembed_at_every_layer=unembed_at_every_layer,\n      )\n\n    seq_len = 4\n    model_size = 16\n\n    emb = np.random.random((1, seq_len, model_size))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    activations = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n\n    for residual in activations.residuals:\n      self.assertEqual(residual.shape, (1, seq_len, embedding_size))\n\n    for layer_output in activations.layer_outputs:\n      self.assertEqual(layer_output.shape, (1, seq_len, model_size))\n\n  @parameterized.parameters(\n      dict(model_size=2, unembed_at_every_layer=True),\n      dict(model_size=2, unembed_at_every_layer=False),\n      dict(model_size=6, unembed_at_every_layer=True),\n      dict(model_size=6, unembed_at_every_layer=False))\n  def test_identity_embedding_produces_same_output_as_standard_model(\n      self, model_size, unembed_at_every_layer):\n\n    config = model.TransformerConfig(\n        num_heads=2,\n        num_layers=2,\n        key_size=5,\n        mlp_hidden_size=64,\n        dropout_rate=0.,\n        causal=False,\n        layer_norm=False)\n\n    @hk.without_apply_rng\n    @hk.transform\n    def forward_model(emb, mask):\n      return model.Transformer(config)(emb, mask).output\n\n    @hk.without_apply_rng\n    @hk.transform\n    def forward_superposition(emb, mask):", "metadata": {"task_id": "deepmind_tracr/127", "ground_truth": "      return compressed_model.CompressedTransformer(config)(\n          emb,\n          mask,\n          embedding_size=model_size,\n          unembed_at_every_layer=unembed_at_every_layer).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model_test.py"], "context_start_lineno": 106, "lineno": 287, "function_name": "forward_superposition", "line_no": 287}}
{"_id": "deepmind_tracr/128", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Didactic example of an autoregressive Transformer-based language model.\n\nGlossary of shapes:\n- B: Batch size.\n- T: Sequence length.\n- D: Model embedding size.\n- H: Number of attention heads.\n- V: Vocabulary size.\n\nForked from: haiku.examples.transformer.model\n\"\"\"\n\nimport collections\nimport dataclasses\nfrom typing import Callable, List, Optional\n\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import attention\n\n# hk.Modules are not always callable: github.com/deepmind/dm-haiku/issues/52\n# Ideally, we'd want a type:\n# CallableHaikuModule = Intersection[Callable[..., jax.Array], hk.Module]\n# But Intersection does not exist (yet): github.com/python/typing/issues/213\nCallableHaikuModule = Callable[..., jax.Array]\n\n\n@chex.dataclass\nclass TransformerOutput:\n  layer_outputs: List[jax.Array]  # [B, T, D]\n  residuals: List[jax.Array]  # [B, T, D]\n  attn_logits: List[jax.Array]  # [B, H, T, T]\n  output: jax.Array  # [B, T, D]\n  input_embeddings: jax.Array  # [B, T, D]\n\n\n@dataclasses.dataclass\nclass TransformerConfig:\n  num_heads: int\n  num_layers: int\n  key_size: int\n  mlp_hidden_size: int\n  dropout_rate: float\n  activation_function: Callable[[jax.Array], jax.Array] = jax.nn.gelu\n  layer_norm: bool = True\n  causal: bool = False\n\n\n@dataclasses.dataclass\nclass Transformer(hk.Module):\n  \"\"\"A transformer stack.\"\"\"\n\n  config: TransformerConfig\n  name: Optional[str] = None\n\n  def __call__(\n      self,\n      embeddings: jax.Array,  # [B, T, D]\n      mask: jax.Array,  # [B, T]\n      *,\n      use_dropout: bool = True,\n  ) -> TransformerOutput:\n    \"\"\"Transforms input embedding sequences to output embedding sequences.\"\"\"\n\n    def layer_norm(x: jax.Array) -> jax.Array:\n      \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"", "metadata": {"task_id": "deepmind_tracr/128", "ground_truth": "      if self.config.layer_norm:\n        return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)\n      return x\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model.py"], "context_start_lineno": 0, "lineno": 83, "function_name": "layer_norm", "line_no": 83}}
{"_id": "deepmind_tracr/129", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Didactic example of an autoregressive Transformer-based language model.\n\nGlossary of shapes:\n- B: Batch size.\n- T: Sequence length.\n- D: Model embedding size.\n- H: Number of attention heads.\n- V: Vocabulary size.\n\nForked from: haiku.examples.transformer.model\n\"\"\"\n\nimport collections\nimport dataclasses\nfrom typing import Callable, List, Optional\n\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import attention\n\n# hk.Modules are not always callable: github.com/deepmind/dm-haiku/issues/52\n# Ideally, we'd want a type:\n# CallableHaikuModule = Intersection[Callable[..., jax.Array], hk.Module]\n# But Intersection does not exist (yet): github.com/python/typing/issues/213\nCallableHaikuModule = Callable[..., jax.Array]\n\n\n@chex.dataclass\nclass TransformerOutput:\n  layer_outputs: List[jax.Array]  # [B, T, D]\n  residuals: List[jax.Array]  # [B, T, D]\n  attn_logits: List[jax.Array]  # [B, H, T, T]\n  output: jax.Array  # [B, T, D]\n  input_embeddings: jax.Array  # [B, T, D]\n\n\n@dataclasses.dataclass\nclass TransformerConfig:\n  num_heads: int\n  num_layers: int\n  key_size: int\n  mlp_hidden_size: int\n  dropout_rate: float\n  activation_function: Callable[[jax.Array], jax.Array] = jax.nn.gelu\n  layer_norm: bool = True\n  causal: bool = False\n\n\n@dataclasses.dataclass\nclass Transformer(hk.Module):\n  \"\"\"A transformer stack.\"\"\"\n\n  config: TransformerConfig\n  name: Optional[str] = None\n\n  def __call__(\n      self,\n      embeddings: jax.Array,  # [B, T, D]\n      mask: jax.Array,  # [B, T]\n      *,\n      use_dropout: bool = True,\n  ) -> TransformerOutput:\n    \"\"\"Transforms input embedding sequences to output embedding sequences.\"\"\"\n\n    def layer_norm(x: jax.Array) -> jax.Array:\n      \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"\n      if self.config.layer_norm:\n        return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)\n      return x\n\n    initializer = hk.initializers.VarianceScaling(2 / self.config.num_layers)\n    dropout_rate = self.config.dropout_rate if use_dropout else 0.\n    _, seq_len, model_size = embeddings.shape\n\n    # Compute causal mask for autoregressive sequence modelling.\n    mask = mask[:, None, None, :]  # [B, H=1, T'=1, T]\n    mask = mask.repeat(seq_len, axis=2)  # [B, H=1, T, T]\n\n    if self.config.causal:\n      causal_mask = np.ones((1, 1, seq_len, seq_len))  # [B=1, H=1, T, T]\n      causal_mask = np.tril(causal_mask)\n      mask = mask * causal_mask  # [B, H=1, T, T]\n\n    # Set up activation collection.\n    collected = collections.defaultdict(list)\n\n    def collect(**kwargs):\n      for k, v in kwargs.items():\n        collected[k].append(v)\n\n    residual = embeddings\n    for layer in range(self.config.num_layers):\n      with hk.experimental.name_scope(f\"layer_{layer}\"):\n        # First the attention block.\n        attn_block = attention.MultiHeadAttention(\n            num_heads=self.config.num_heads,\n            key_size=self.config.key_size,\n            model_size=model_size,\n            w_init=initializer,\n            name=\"attn\")\n        attn_in = layer_norm(residual)\n        attn_out = attn_block(attn_in, attn_in, attn_in, mask=mask)\n        attn_out, attn_logits = attn_out.out, attn_out.logits\n        if dropout_rate > 0:\n          attn_out = hk.dropout(hk.next_rng_key(), dropout_rate, attn_out)\n        residual = residual + attn_out\n\n        collect(\n            residuals=residual, layer_outputs=attn_out, attn_logits=attn_logits)\n\n        # Then the dense block.\n        with hk.experimental.name_scope(\"mlp\"):\n          dense_block = hk.Sequential([\n              hk.Linear(\n                  self.config.mlp_hidden_size,\n                  w_init=initializer,\n                  name=\"linear_1\"),\n              self.config.activation_function,\n              hk.Linear(model_size, w_init=initializer, name=\"linear_2\"),\n          ])\n        dense_in = layer_norm(residual)\n        dense_out = dense_block(dense_in)\n        if dropout_rate > 0:\n          dense_out = hk.dropout(hk.next_rng_key(), dropout_rate, dense_out)\n        residual = residual + dense_out\n\n        collect(residuals=residual, layer_outputs=dense_out)\n\n    return TransformerOutput(\n        residuals=collected[\"residuals\"],\n        layer_outputs=collected[\"layer_outputs\"],\n        attn_logits=collected[\"attn_logits\"],\n        output=layer_norm(residual),\n        input_embeddings=embeddings,\n    )\n\n\n@chex.dataclass\nclass CompiledTransformerModelOutput:\n  transformer_output: TransformerOutput\n  unembedded_output: jax.Array  # [B, T]\n\n\n@dataclasses.dataclass\nclass CompiledTransformerModel(hk.Module):\n  \"\"\"A transformer model with one-hot embeddings.\"\"\"\n  transformer: Transformer\n  token_embed: CallableHaikuModule\n  position_embed: CallableHaikuModule\n  unembed: CallableHaikuModule\n  use_unembed_argmax: bool\n  pad_token: Optional[int] = None\n\n  def embed(self, tokens: jax.Array) -> jax.Array:", "metadata": {"task_id": "deepmind_tracr/129", "ground_truth": "    token_embeddings = self.token_embed(tokens)\n    positional_embeddings = self.position_embed(jnp.indices(tokens.shape)[-1])\n    return token_embeddings + positional_embeddings  # [B, T, D]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model.py"], "context_start_lineno": 0, "lineno": 171, "function_name": "embed", "line_no": 171}}
{"_id": "deepmind_tracr/130", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Didactic example of an autoregressive Transformer-based language model.\n\nGlossary of shapes:\n- B: Batch size.\n- T: Sequence length.\n- D: Model embedding size.\n- H: Number of attention heads.\n- V: Vocabulary size.\n\nForked from: haiku.examples.transformer.model\n\"\"\"\n\nimport collections\nimport dataclasses\nfrom typing import Callable, List, Optional\n\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import attention\n\n# hk.Modules are not always callable: github.com/deepmind/dm-haiku/issues/52\n# Ideally, we'd want a type:\n# CallableHaikuModule = Intersection[Callable[..., jax.Array], hk.Module]\n# But Intersection does not exist (yet): github.com/python/typing/issues/213\nCallableHaikuModule = Callable[..., jax.Array]\n\n\n@chex.dataclass\nclass TransformerOutput:\n  layer_outputs: List[jax.Array]  # [B, T, D]\n  residuals: List[jax.Array]  # [B, T, D]\n  attn_logits: List[jax.Array]  # [B, H, T, T]\n  output: jax.Array  # [B, T, D]\n  input_embeddings: jax.Array  # [B, T, D]\n\n\n@dataclasses.dataclass\nclass TransformerConfig:\n  num_heads: int\n  num_layers: int\n  key_size: int\n  mlp_hidden_size: int\n  dropout_rate: float\n  activation_function: Callable[[jax.Array], jax.Array] = jax.nn.gelu\n  layer_norm: bool = True\n  causal: bool = False\n\n\n@dataclasses.dataclass\nclass Transformer(hk.Module):\n  \"\"\"A transformer stack.\"\"\"\n\n  config: TransformerConfig\n  name: Optional[str] = None\n\n  def __call__(\n      self,\n      embeddings: jax.Array,  # [B, T, D]\n      mask: jax.Array,  # [B, T]\n      *,\n      use_dropout: bool = True,\n  ) -> TransformerOutput:\n    \"\"\"Transforms input embedding sequences to output embedding sequences.\"\"\"\n\n    def layer_norm(x: jax.Array) -> jax.Array:\n      \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"\n      if self.config.layer_norm:\n        return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)\n      return x\n\n    initializer = hk.initializers.VarianceScaling(2 / self.config.num_layers)\n    dropout_rate = self.config.dropout_rate if use_dropout else 0.\n    _, seq_len, model_size = embeddings.shape\n\n    # Compute causal mask for autoregressive sequence modelling.\n    mask = mask[:, None, None, :]  # [B, H=1, T'=1, T]\n    mask = mask.repeat(seq_len, axis=2)  # [B, H=1, T, T]\n\n    if self.config.causal:\n      causal_mask = np.ones((1, 1, seq_len, seq_len))  # [B=1, H=1, T, T]\n      causal_mask = np.tril(causal_mask)\n      mask = mask * causal_mask  # [B, H=1, T, T]\n\n    # Set up activation collection.\n    collected = collections.defaultdict(list)\n\n    def collect(**kwargs):\n      for k, v in kwargs.items():\n        collected[k].append(v)\n\n    residual = embeddings\n    for layer in range(self.config.num_layers):\n      with hk.experimental.name_scope(f\"layer_{layer}\"):\n        # First the attention block.\n        attn_block = attention.MultiHeadAttention(\n            num_heads=self.config.num_heads,\n            key_size=self.config.key_size,\n            model_size=model_size,\n            w_init=initializer,\n            name=\"attn\")\n        attn_in = layer_norm(residual)\n        attn_out = attn_block(attn_in, attn_in, attn_in, mask=mask)\n        attn_out, attn_logits = attn_out.out, attn_out.logits\n        if dropout_rate > 0:\n          attn_out = hk.dropout(hk.next_rng_key(), dropout_rate, attn_out)\n        residual = residual + attn_out\n\n        collect(\n            residuals=residual, layer_outputs=attn_out, attn_logits=attn_logits)\n\n        # Then the dense block.\n        with hk.experimental.name_scope(\"mlp\"):\n          dense_block = hk.Sequential([\n              hk.Linear(\n                  self.config.mlp_hidden_size,\n                  w_init=initializer,\n                  name=\"linear_1\"),\n              self.config.activation_function,\n              hk.Linear(model_size, w_init=initializer, name=\"linear_2\"),\n          ])\n        dense_in = layer_norm(residual)\n        dense_out = dense_block(dense_in)\n        if dropout_rate > 0:\n          dense_out = hk.dropout(hk.next_rng_key(), dropout_rate, dense_out)\n        residual = residual + dense_out\n\n        collect(residuals=residual, layer_outputs=dense_out)\n\n    return TransformerOutput(\n        residuals=collected[\"residuals\"],\n        layer_outputs=collected[\"layer_outputs\"],\n        attn_logits=collected[\"attn_logits\"],\n        output=layer_norm(residual),\n        input_embeddings=embeddings,\n    )\n\n\n@chex.dataclass\nclass CompiledTransformerModelOutput:\n  transformer_output: TransformerOutput\n  unembedded_output: jax.Array  # [B, T]\n\n\n@dataclasses.dataclass\nclass CompiledTransformerModel(hk.Module):\n  \"\"\"A transformer model with one-hot embeddings.\"\"\"\n  transformer: Transformer\n  token_embed: CallableHaikuModule\n  position_embed: CallableHaikuModule\n  unembed: CallableHaikuModule\n  use_unembed_argmax: bool\n  pad_token: Optional[int] = None\n\n  def embed(self, tokens: jax.Array) -> jax.Array:\n    token_embeddings = self.token_embed(tokens)\n    positional_embeddings = self.position_embed(jnp.indices(tokens.shape)[-1])\n    return token_embeddings + positional_embeddings  # [B, T, D]\n\n  def __call__(\n      self,\n      tokens: jax.Array,\n      use_dropout: bool = True,\n  ) -> CompiledTransformerModelOutput:\n    \"\"\"Embed tokens, pass through model, and unembed output.\"\"\"", "metadata": {"task_id": "deepmind_tracr/130", "ground_truth": "    if self.pad_token is None:\n      input_mask = jnp.ones_like(tokens)\n    else:\n      input_mask = (tokens != self.pad_token)\n    input_embeddings = self.embed(tokens)\n\n    transformer_output = self.transformer(\n        input_embeddings,\n        input_mask,\n        use_dropout=use_dropout,\n    )\n    return CompiledTransformerModelOutput(\n        transformer_output=transformer_output,\n        unembedded_output=self.unembed(\n            transformer_output.output,\n            use_unembed_argmax=self.use_unembed_argmax,\n        ),\n    )\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model.py"], "context_start_lineno": 0, "lineno": 181, "function_name": "__call__", "line_no": 181}}
{"_id": "deepmind_tracr/131", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Modified transformer to learn a linear compression of the residual stream.\n\nCompressedTransformer adds three arguments compared to Transformer:\n- embedding_size: the size of the compressed residual stream.\n- unembed_at_every_layer: whether to apply the unembedding before applying\n    attention and MLP layers\n- return_activations: whether to return all model activations rather than just\n    the outputs\n\"\"\"\n\nimport collections\nimport dataclasses\nfrom typing import Optional\n\nimport haiku as hk\nimport jax\nimport numpy as np\n\nfrom tracr.transformer import attention\nfrom tracr.transformer import model\n\n\n@dataclasses.dataclass\nclass CompressedTransformer(hk.Module):\n  \"\"\"A transformer stack with linearly compressed residual stream.\"\"\"\n\n  config: model.TransformerConfig\n  name: Optional[str] = None\n\n  def __call__(\n      self,\n      embeddings: jax.Array,  # [B, T, D]\n      mask: jax.Array,  # [B, T]\n      *,\n      use_dropout: bool = True,\n      embedding_size: Optional[int] = None,\n      unembed_at_every_layer: bool = False,\n  ) -> model.TransformerOutput:  # [B, T, D]\n    \"\"\"Transforms input embedding sequences to output embedding sequences.\n\n    Args:\n      embeddings: Input embeddings to pass through the model.\n      mask: Boolean mask to restrict the inputs the model uses.\n      use_dropout: Turns dropout on/off.\n      embedding_size: Dimension to compress the residual stream to.\n      unembed_at_every_layer: Whether to unembed the residual stream when\n        reading the input for every layer (keeping the layer input sizes) or to\n        only unembed before the model output (compressing the layer inputs).\n\n    Returns:\n      The outputs of the forward pass through the transformer.\n    \"\"\"\n\n    def layer_norm(x: jax.Array) -> jax.Array:\n      \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"", "metadata": {"task_id": "deepmind_tracr/131", "ground_truth": "      if self.config.layer_norm:\n        return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)\n      return x\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "compressed_model.py"], "context_start_lineno": 0, "lineno": 69, "function_name": "layer_norm", "line_no": 69}}
{"_id": "deepmind_tracr/132", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Basic encoder for inputs with a fixed vocabulary.\"\"\"\n\nimport abc\nfrom typing import Any, List, Optional, Sequence\n\nfrom tracr.craft import bases\n\n\nclass Encoder(abc.ABC):\n  \"\"\"Encodes a list of tokens into a list of inputs for a transformer model.\n\n  The abstract class does not make assumptions on the input and output types,\n  and we have different encoders for different input types.\n  \"\"\"\n\n  @abc.abstractmethod\n  def encode(self, inputs: List[Any]) -> List[Any]:\n    return list()\n\n  @abc.abstractmethod\n  def decode(self, encodings: List[Any]) -> List[Any]:\n    return list()\n\n  @property\n  def pad_token(self) -> Optional[str]:\n    return None\n\n  @property\n  def bos_token(self) -> Optional[str]:\n    return None\n\n  @property\n  def pad_encoding(self) -> Optional[int]:\n    return None\n\n  @property\n  def bos_encoding(self) -> Optional[int]:\n    return None\n\n\nclass NumericalEncoder(Encoder):\n  \"\"\"Encodes numerical variables (simply using the identity mapping).\"\"\"\n\n  def encode(self, inputs: List[float]) -> List[float]:\n    return inputs\n\n  def decode(self, encodings: List[float]) -> List[float]:\n    return encodings\n\n\nclass CategoricalEncoder(Encoder):\n  \"\"\"Encodes categorical variables with a fixed vocabulary.\"\"\"\n\n  def __init__(\n      self,\n      basis: Sequence[bases.BasisDirection],\n      enforce_bos: bool = False,\n      bos_token: Optional[str] = None,\n      pad_token: Optional[str] = None,\n      max_seq_len: Optional[int] = None,\n  ):\n    \"\"\"Initialises. If enforce_bos is set, ensures inputs start with it.\"\"\"", "metadata": {"task_id": "deepmind_tracr/132", "ground_truth": "    if enforce_bos and not bos_token:\n      raise ValueError(\"BOS token must be specified if enforcing BOS.\")\n\n    self.encoding_map = {}\n    for i, direction in enumerate(basis):\n      val = direction.value\n      self.encoding_map[val] = i\n\n    if bos_token and bos_token not in self.encoding_map:\n      raise ValueError(\"BOS token missing in encoding.\")\n\n    if pad_token and pad_token not in self.encoding_map:\n      raise ValueError(\"PAD token missing in encoding.\")\n\n    self.enforce_bos = enforce_bos\n    self._bos_token = bos_token\n    self._pad_token = pad_token\n    self._max_seq_len = max_seq_len\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "encoder.py"], "context_start_lineno": 0, "lineno": 76, "function_name": "__init__", "line_no": 76}}
{"_id": "deepmind_tracr/133", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Basic encoder for inputs with a fixed vocabulary.\"\"\"\n\nimport abc\nfrom typing import Any, List, Optional, Sequence\n\nfrom tracr.craft import bases\n\n\nclass Encoder(abc.ABC):\n  \"\"\"Encodes a list of tokens into a list of inputs for a transformer model.\n\n  The abstract class does not make assumptions on the input and output types,\n  and we have different encoders for different input types.\n  \"\"\"\n\n  @abc.abstractmethod\n  def encode(self, inputs: List[Any]) -> List[Any]:\n    return list()\n\n  @abc.abstractmethod\n  def decode(self, encodings: List[Any]) -> List[Any]:\n    return list()\n\n  @property\n  def pad_token(self) -> Optional[str]:\n    return None\n\n  @property\n  def bos_token(self) -> Optional[str]:\n    return None\n\n  @property\n  def pad_encoding(self) -> Optional[int]:\n    return None\n\n  @property\n  def bos_encoding(self) -> Optional[int]:\n    return None\n\n\nclass NumericalEncoder(Encoder):\n  \"\"\"Encodes numerical variables (simply using the identity mapping).\"\"\"\n\n  def encode(self, inputs: List[float]) -> List[float]:\n    return inputs\n\n  def decode(self, encodings: List[float]) -> List[float]:\n    return encodings\n\n\nclass CategoricalEncoder(Encoder):\n  \"\"\"Encodes categorical variables with a fixed vocabulary.\"\"\"\n\n  def __init__(\n      self,\n      basis: Sequence[bases.BasisDirection],\n      enforce_bos: bool = False,\n      bos_token: Optional[str] = None,\n      pad_token: Optional[str] = None,\n      max_seq_len: Optional[int] = None,\n  ):\n    \"\"\"Initialises. If enforce_bos is set, ensures inputs start with it.\"\"\"\n    if enforce_bos and not bos_token:\n      raise ValueError(\"BOS token must be specified if enforcing BOS.\")\n\n    self.encoding_map = {}\n    for i, direction in enumerate(basis):\n      val = direction.value\n      self.encoding_map[val] = i\n\n    if bos_token and bos_token not in self.encoding_map:\n      raise ValueError(\"BOS token missing in encoding.\")\n\n    if pad_token and pad_token not in self.encoding_map:\n      raise ValueError(\"PAD token missing in encoding.\")\n\n    self.enforce_bos = enforce_bos\n    self._bos_token = bos_token\n    self._pad_token = pad_token\n    self._max_seq_len = max_seq_len\n\n  def encode(self, inputs: List[bases.Value]) -> List[int]:", "metadata": {"task_id": "deepmind_tracr/133", "ground_truth": "    if self.enforce_bos and inputs[0] != self.bos_token:\n      raise ValueError(\"First input token must be BOS token. \"\n                       f\"Should be '{self.bos_token}', but was '{inputs[0]}'.\")\n    if missing := set(inputs) - set(self.encoding_map.keys()):\n      raise ValueError(f\"Inputs {missing} not found in encoding \",\n                       self.encoding_map.keys())\n    if self._max_seq_len is not None and len(inputs) > self._max_seq_len:\n      raise ValueError(f\"inputs={inputs} are longer than the maximum \"\n                       f\"sequence length {self._max_seq_len}\")\n\n    return [self.encoding_map[x] for x in inputs]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "encoder.py"], "context_start_lineno": 0, "lineno": 96, "function_name": "encode", "line_no": 96}}
{"_id": "deepmind_tracr/134", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Basic encoder for inputs with a fixed vocabulary.\"\"\"\n\nimport abc\nfrom typing import Any, List, Optional, Sequence\n\nfrom tracr.craft import bases\n\n\nclass Encoder(abc.ABC):\n  \"\"\"Encodes a list of tokens into a list of inputs for a transformer model.\n\n  The abstract class does not make assumptions on the input and output types,\n  and we have different encoders for different input types.\n  \"\"\"\n\n  @abc.abstractmethod\n  def encode(self, inputs: List[Any]) -> List[Any]:\n    return list()\n\n  @abc.abstractmethod\n  def decode(self, encodings: List[Any]) -> List[Any]:\n    return list()\n\n  @property\n  def pad_token(self) -> Optional[str]:\n    return None\n\n  @property\n  def bos_token(self) -> Optional[str]:\n    return None\n\n  @property\n  def pad_encoding(self) -> Optional[int]:\n    return None\n\n  @property\n  def bos_encoding(self) -> Optional[int]:\n    return None\n\n\nclass NumericalEncoder(Encoder):\n  \"\"\"Encodes numerical variables (simply using the identity mapping).\"\"\"\n\n  def encode(self, inputs: List[float]) -> List[float]:\n    return inputs\n\n  def decode(self, encodings: List[float]) -> List[float]:\n    return encodings\n\n\nclass CategoricalEncoder(Encoder):\n  \"\"\"Encodes categorical variables with a fixed vocabulary.\"\"\"\n\n  def __init__(\n      self,\n      basis: Sequence[bases.BasisDirection],\n      enforce_bos: bool = False,\n      bos_token: Optional[str] = None,\n      pad_token: Optional[str] = None,\n      max_seq_len: Optional[int] = None,\n  ):\n    \"\"\"Initialises. If enforce_bos is set, ensures inputs start with it.\"\"\"\n    if enforce_bos and not bos_token:\n      raise ValueError(\"BOS token must be specified if enforcing BOS.\")\n\n    self.encoding_map = {}\n    for i, direction in enumerate(basis):\n      val = direction.value\n      self.encoding_map[val] = i\n\n    if bos_token and bos_token not in self.encoding_map:\n      raise ValueError(\"BOS token missing in encoding.\")\n\n    if pad_token and pad_token not in self.encoding_map:\n      raise ValueError(\"PAD token missing in encoding.\")\n\n    self.enforce_bos = enforce_bos\n    self._bos_token = bos_token\n    self._pad_token = pad_token\n    self._max_seq_len = max_seq_len\n\n  def encode(self, inputs: List[bases.Value]) -> List[int]:\n    if self.enforce_bos and inputs[0] != self.bos_token:\n      raise ValueError(\"First input token must be BOS token. \"\n                       f\"Should be '{self.bos_token}', but was '{inputs[0]}'.\")\n    if missing := set(inputs) - set(self.encoding_map.keys()):\n      raise ValueError(f\"Inputs {missing} not found in encoding \",\n                       self.encoding_map.keys())\n    if self._max_seq_len is not None and len(inputs) > self._max_seq_len:\n      raise ValueError(f\"inputs={inputs} are longer than the maximum \"\n                       f\"sequence length {self._max_seq_len}\")\n\n    return [self.encoding_map[x] for x in inputs]\n\n  def decode(self, encodings: List[int]) -> List[bases.Value]:\n    \"\"\"Recover the tokens that corresponds to `ids`. Inverse of __call__.\"\"\"", "metadata": {"task_id": "deepmind_tracr/134", "ground_truth": "    decoding_map = {val: key for key, val in self.encoding_map.items()}\n    if missing := set(encodings) - set(decoding_map.keys()):\n      raise ValueError(f\"Inputs {missing} not found in decoding map \",\n                       decoding_map.keys())\n    return [decoding_map[x] for x in encodings]\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "encoder.py"], "context_start_lineno": 0, "lineno": 110, "function_name": "decode", "line_no": 110}}
{"_id": "deepmind_tracr/135", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"", "metadata": {"task_id": "deepmind_tracr/135", "ground_truth": "    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "_check_layer_naming", "line_no": 32}}
{"_id": "deepmind_tracr/136", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):", "metadata": {"task_id": "deepmind_tracr/136", "ground_truth": "    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 46, "function_name": "_zero_mlps", "line_no": 46}}
{"_id": "deepmind_tracr/137", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/137", "ground_truth": "      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 61, "function_name": "forward", "line_no": 61}}
{"_id": "deepmind_tracr/138", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/138", "ground_truth": "      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 93, "function_name": "forward", "line_no": 93}}
{"_id": "deepmind_tracr/139", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):", "metadata": {"task_id": "deepmind_tracr/139", "ground_truth": "      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 131, "function_name": "forward_zero", "line_no": 131}}
{"_id": "deepmind_tracr/140", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):", "metadata": {"task_id": "deepmind_tracr/140", "ground_truth": "      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 145, "function_name": "forward", "line_no": 145}}
{"_id": "deepmind_tracr/141", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for transformer.model.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom tracr.transformer import model\n\n\nclass TransformerTest(parameterized.TestCase):\n\n  def _check_layer_naming(self, params):\n    # Modules should be named for example\n    # For MLPs: \"transformer/layer_{i}/mlp/linear_1\"\n    # For Attention: \"transformer/layer_{i}/attn/key\"\n    # For Layer Norm: \"transformer/layer_{i}/layer_norm\"\n    for key in params.keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params_no_mlps = self._zero_mlps(params)\n\n    out_zero_activation = forward_zero.apply(params, next(rng), emb, mask)\n    out_no_mlps = forward.apply(params_no_mlps, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    np.testing.assert_allclose(out_zero_activation, out_no_mlps)\n    self.assertFalse(np.allclose(out_zero_activation, 0))\n\n\nclass CompiledTransformerModelTest(parameterized.TestCase):\n\n  def _get_one_hot_embed_unembed(self, vocab_size, max_seq_len):\n    # Embeds tokens as one-hot into the first `vocab_size` dimensions", "metadata": {"task_id": "deepmind_tracr/141", "ground_truth": "    token_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.eye(vocab_size),\n             jnp.zeros((vocab_size, max_seq_len))]))\n\n    # Embeds positions as one-hot into the last `max_seq_len` dimensions\n    position_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.zeros((max_seq_len, vocab_size)),\n             jnp.eye(max_seq_len)]))\n\n    class Unembed(hk.Module):\n\n      def __call__(self, embeddings):\n        return jnp.argmax(embeddings[:, :, :vocab_size], axis=-1)\n\n    return token_embed, position_embed, Unembed()\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 0, "lineno": 178, "function_name": "_get_one_hot_embed_unembed", "line_no": 178}}
{"_id": "deepmind_tracr/142", "text": "keys():\n      levels = key.split(\"/\")\n      self.assertEqual(levels[0], \"transformer\")\n      if levels[1].startswith(\"layer_norm\"):\n        continue  # output layer norm\n      self.assertStartsWith(levels[1], \"layer\")\n      if levels[2] == \"mlp\":\n        self.assertIn(levels[3], {\"linear_1\", \"linear_2\"})\n      elif levels[2] == \"attn\":\n        self.assertIn(levels[3], {\"key\", \"query\", \"value\", \"linear\"})\n      else:\n        self.assertStartsWith(levels[2], \"layer_norm\")\n\n  def _zero_mlps(self, params):\n    for module in params:\n      if \"mlp\" in module:\n        for param in params[module]:\n          params[module][param] = jnp.zeros_like(params[module][param])\n    return params\n\n  @parameterized.parameters(dict(layer_norm=True), dict(layer_norm=False))\n  def test_layer_norm(self, layer_norm):\n    # input = [1, 1, 1, 1]\n    # If layer norm is used, this should give all-0 output for a freshly\n    # initialized model because LN will subtract the mean after each layer.\n    # Else we expect non-zero outputs.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params_no_mlps = self._zero_mlps(params)\n\n    out_zero_activation = forward_zero.apply(params, next(rng), emb, mask)\n    out_no_mlps = forward.apply(params_no_mlps, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    np.testing.assert_allclose(out_zero_activation, out_no_mlps)\n    self.assertFalse(np.allclose(out_zero_activation, 0))\n\n\nclass CompiledTransformerModelTest(parameterized.TestCase):\n\n  def _get_one_hot_embed_unembed(self, vocab_size, max_seq_len):\n    # Embeds tokens as one-hot into the first `vocab_size` dimensions\n    token_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.eye(vocab_size),\n             jnp.zeros((vocab_size, max_seq_len))]))\n\n    # Embeds positions as one-hot into the last `max_seq_len` dimensions\n    position_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.zeros((max_seq_len, vocab_size)),\n             jnp.eye(max_seq_len)]))\n\n    class Unembed(hk.Module):\n\n      def __call__(self, embeddings):\n        return jnp.argmax(embeddings[:, :, :vocab_size], axis=-1)\n\n    return token_embed, position_embed, Unembed()\n\n  def test_embedding_gives_desired_result(self):\n    tokens = jnp.array([[1, 2, 3]])\n    vocab_size, max_seq_len, pad_token = 5, 5, 0\n\n    expected_embeddings = jnp.array([[[0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n                                      [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n                                      [0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]])\n\n    @hk.transform\n    def embed(tokens):", "metadata": {"task_id": "deepmind_tracr/142", "ground_truth": "      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      token_embed, position_embed, unembed = self._get_one_hot_embed_unembed(\n          vocab_size, max_seq_len)\n      compiled_model = model.CompiledTransformerModel(\n          transformer=transformer,\n          token_embed=token_embed,\n          position_embed=position_embed,\n          unembed=unembed,\n          use_unembed_argmax=True,\n          pad_token=pad_token)\n      return compiled_model.embed(tokens)\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 32, "lineno": 206, "function_name": "embed", "line_no": 206}}
{"_id": "deepmind_tracr/143", "text": " key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=layer_norm))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = jnp.ones((1, seq_len, 1))\n    mask = jnp.ones((1, seq_len))\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if layer_norm:\n      np.testing.assert_allclose(out, 0)\n    else:\n      self.assertFalse(np.allclose(out, 0))\n\n  @parameterized.parameters(dict(causal=True), dict(causal=False))\n  def test_causal_attention(self, causal):\n    # input = [0, random, random, random]\n    # mask = [1, 0, 1, 1]\n    # For causal attention the second token can only attend to the first one, so\n    # it should be the same. For non-causal attention all tokens should change.\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              layer_norm=False,\n              causal=causal))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    emb[:, 0, :] = 0\n    mask = np.array([[1, 0, 1, 1]])\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params = self._zero_mlps(params)\n    out = forward.apply(params, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    if causal:\n      self.assertEqual(0, out[0, 0, 0])\n      self.assertEqual(emb[0, 1, 0], out[0, 1, 0])\n    else:\n      self.assertNotEqual(0, out[0, 0, 0])\n      self.assertNotEqual(emb[0, 1, 0], out[0, 1, 0])\n    self.assertNotEqual(emb[0, 2, 0], out[0, 2, 0])\n    self.assertNotEqual(emb[0, 3, 0], out[0, 3, 0])\n\n  def test_setting_activation_function_to_zero(self):\n    # An activation function that always returns zeros should result in the\n    # same model output as setting all MLP weights to zero.\n\n    @hk.transform\n    def forward_zero(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jnp.zeros_like))\n      return transformer(emb, mask).output\n\n    @hk.transform\n    def forward(emb, mask):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      return transformer(emb, mask).output\n\n    seq_len = 4\n    emb = np.random.random((1, seq_len, 1))\n    mask = np.ones((1, seq_len))\n    emb, mask = jnp.array(emb), jnp.array(mask)\n\n    rng = hk.PRNGSequence(1)\n    params = forward.init(next(rng), emb, mask)\n    params_no_mlps = self._zero_mlps(params)\n\n    out_zero_activation = forward_zero.apply(params, next(rng), emb, mask)\n    out_no_mlps = forward.apply(params_no_mlps, next(rng), emb, mask)\n\n    self._check_layer_naming(params)\n    np.testing.assert_allclose(out_zero_activation, out_no_mlps)\n    self.assertFalse(np.allclose(out_zero_activation, 0))\n\n\nclass CompiledTransformerModelTest(parameterized.TestCase):\n\n  def _get_one_hot_embed_unembed(self, vocab_size, max_seq_len):\n    # Embeds tokens as one-hot into the first `vocab_size` dimensions\n    token_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.eye(vocab_size),\n             jnp.zeros((vocab_size, max_seq_len))]))\n\n    # Embeds positions as one-hot into the last `max_seq_len` dimensions\n    position_embed = hk.Embed(\n        embedding_matrix=jnp.block(\n            [jnp.zeros((max_seq_len, vocab_size)),\n             jnp.eye(max_seq_len)]))\n\n    class Unembed(hk.Module):\n\n      def __call__(self, embeddings):\n        return jnp.argmax(embeddings[:, :, :vocab_size], axis=-1)\n\n    return token_embed, position_embed, Unembed()\n\n  def test_embedding_gives_desired_result(self):\n    tokens = jnp.array([[1, 2, 3]])\n    vocab_size, max_seq_len, pad_token = 5, 5, 0\n\n    expected_embeddings = jnp.array([[[0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n                                      [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n                                      [0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]])\n\n    @hk.transform\n    def embed(tokens):\n      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      token_embed, position_embed, unembed = self._get_one_hot_embed_unembed(\n          vocab_size, max_seq_len)\n      compiled_model = model.CompiledTransformerModel(\n          transformer=transformer,\n          token_embed=token_embed,\n          position_embed=position_embed,\n          unembed=unembed,\n          use_unembed_argmax=True,\n          pad_token=pad_token)\n      return compiled_model.embed(tokens)\n\n    rng = hk.PRNGSequence(1)\n    params = embed.init(next(rng), tokens)\n    embeddings = embed.apply(params, next(rng), tokens)\n\n    np.testing.assert_allclose(embeddings, expected_embeddings)\n\n  def test_embedding_then_unembedding_gives_same_tokens(self):\n    tokens = jnp.array([[1, 2, 3], [4, 5, 6], [3, 2, 4]])\n    vocab_size, max_seq_len, pad_token = 10, 5, 0\n\n    @hk.transform\n    def embed_unembed(tokens):", "metadata": {"task_id": "deepmind_tracr/143", "ground_truth": "      transformer = model.Transformer(\n          model.TransformerConfig(\n              num_heads=2,\n              num_layers=2,\n              key_size=5,\n              mlp_hidden_size=64,\n              dropout_rate=0.,\n              causal=False,\n              layer_norm=False,\n              activation_function=jax.nn.gelu))\n      token_embed, position_embed, unembed = self._get_one_hot_embed_unembed(\n          vocab_size, max_seq_len)\n      compiled_model = model.CompiledTransformerModel(\n          transformer=transformer,\n          token_embed=token_embed,\n          position_embed=position_embed,\n          unembed=unembed,\n          use_unembed_argmax=True,\n          pad_token=pad_token)\n      embeddings = compiled_model.embed(tokens)\n      unembeddings = compiled_model.unembed(embeddings)\n      return embeddings, unembeddings\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "model_test.py"], "context_start_lineno": 65, "lineno": 239, "function_name": "embed_unembed", "line_no": 239}}
{"_id": "deepmind_tracr/144", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Instrumented attention layer (forked from the Haiku library implementation).\n\"\"\"\n\nfrom typing import Optional\nimport warnings\n\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\n@chex.dataclass\nclass AttentionOutput:\n  out: jax.Array  # [..., T', D']\n  logits: jax.Array  # [..., H, T', T]\n\n\nclass MultiHeadAttention(hk.Module):\n  \"\"\"Multi-headed attention (MHA) module.\n\n  This module is intended for attending over sequences of vectors.\n\n  Rough sketch:\n  - Compute keys (K), queries (Q), and values (V) as projections of inputs.\n  - Attention weights are computed as W = softmax(QK^T / sqrt(key_size)).\n  - Output is another projection of WV^T.\n\n  For more detail, see the original Transformer paper:\n    \"Attention is all you need\" https://arxiv.org/abs/1706.03762.\n\n  Glossary of shapes:\n  - T: Sequence length.\n  - D: Vector (embedding) size.\n  - H: Number of attention heads.\n  \"\"\"\n\n  def __init__(\n      self,\n      num_heads: int,\n      key_size: int,\n      # TODO(b/240019186): Remove `w_init_scale`.\n      w_init_scale: Optional[float] = None,\n      *,\n      w_init: Optional[hk.initializers.Initializer] = None,\n      value_size: Optional[int] = None,\n      model_size: Optional[int] = None,\n      name: Optional[str] = None,\n  ):\n    \"\"\"Initialises the module.\n\n    Args:\n      num_heads: Number of independent attention heads (H).\n      key_size: The size of keys (K) and queries used for attention.\n      w_init_scale: DEPRECATED. Please use w_init instead.\n      w_init: Initialiser for weights in the linear map.\n      value_size: Optional size of the value projection (V). If None, defaults\n        to the key size (K).\n      model_size: Optional size of the output embedding (D'). If None, defaults\n        to the key size multiplied by the number of heads (K * H).\n      name: Optional name for this module.\n    \"\"\"", "metadata": {"task_id": "deepmind_tracr/144", "ground_truth": "    super().__init__(name=name)\n    self.num_heads = num_heads\n    self.key_size = key_size\n    self.value_size = value_size or key_size\n    self.model_size = model_size or key_size * num_heads\n\n    # Backwards-compatibility for w_init_scale.\n    if w_init_scale is not None:\n      warnings.warn(\n          \"w_init_scale is deprecated; please pass an explicit weight \"\n          \"initialiser instead.\", DeprecationWarning)\n    if w_init and w_init_scale:\n      raise ValueError(\"Please provide only `w_init`, not `w_init_scale`.\")\n    if w_init is None and w_init_scale is None:\n      raise ValueError(\"Please provide a weight initializer: `w_init`.\")\n    if w_init is None:\n      w_init = hk.initializers.VarianceScaling(w_init_scale)\n    self.w_init = w_init\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "attention.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "__init__", "line_no": 77}}
{"_id": "deepmind_tracr/145", "text": "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Instrumented attention layer (forked from the Haiku library implementation).\n\"\"\"\n\nfrom typing import Optional\nimport warnings\n\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\n@chex.dataclass\nclass AttentionOutput:\n  out: jax.Array  # [..., T', D']\n  logits: jax.Array  # [..., H, T', T]\n\n\nclass MultiHeadAttention(hk.Module):\n  \"\"\"Multi-headed attention (MHA) module.\n\n  This module is intended for attending over sequences of vectors.\n\n  Rough sketch:\n  - Compute keys (K), queries (Q), and values (V) as projections of inputs.\n  - Attention weights are computed as W = softmax(QK^T / sqrt(key_size)).\n  - Output is another projection of WV^T.\n\n  For more detail, see the original Transformer paper:\n    \"Attention is all you need\" https://arxiv.org/abs/1706.03762.\n\n  Glossary of shapes:\n  - T: Sequence length.\n  - D: Vector (embedding) size.\n  - H: Number of attention heads.\n  \"\"\"\n\n  def __init__(\n      self,\n      num_heads: int,\n      key_size: int,\n      # TODO(b/240019186): Remove `w_init_scale`.\n      w_init_scale: Optional[float] = None,\n      *,\n      w_init: Optional[hk.initializers.Initializer] = None,\n      value_size: Optional[int] = None,\n      model_size: Optional[int] = None,\n      name: Optional[str] = None,\n  ):\n    \"\"\"Initialises the module.\n\n    Args:\n      num_heads: Number of independent attention heads (H).\n      key_size: The size of keys (K) and queries used for attention.\n      w_init_scale: DEPRECATED. Please use w_init instead.\n      w_init: Initialiser for weights in the linear map.\n      value_size: Optional size of the value projection (V). If None, defaults\n        to the key size (K).\n      model_size: Optional size of the output embedding (D'). If None, defaults\n        to the key size multiplied by the number of heads (K * H).\n      name: Optional name for this module.\n    \"\"\"\n    super().__init__(name=name)\n    self.num_heads = num_heads\n    self.key_size = key_size\n    self.value_size = value_size or key_size\n    self.model_size = model_size or key_size * num_heads\n\n    # Backwards-compatibility for w_init_scale.\n    if w_init_scale is not None:\n      warnings.warn(\n          \"w_init_scale is deprecated; please pass an explicit weight \"\n          \"initialiser instead.\", DeprecationWarning)\n    if w_init and w_init_scale:\n      raise ValueError(\"Please provide only `w_init`, not `w_init_scale`.\")\n    if w_init is None and w_init_scale is None:\n      raise ValueError(\"Please provide a weight initializer: `w_init`.\")\n    if w_init is None:\n      w_init = hk.initializers.VarianceScaling(w_init_scale)\n    self.w_init = w_init\n\n  def __call__(\n      self,\n      query: jnp.ndarray,\n      key: jnp.ndarray,\n      value: jnp.ndarray,\n      mask: Optional[jnp.ndarray] = None,\n  ) -> AttentionOutput:\n    \"\"\"Computes (optionally masked) MHA with queries, keys & values.\n\n    This module broadcasts over zero or more 'batch-like' leading dimensions.\n\n    Args:\n      query: Embeddings sequence used to compute queries; shape [..., T', D_q].\n      key: Embeddings sequence used to compute keys; shape [..., T, D_k].\n      value: Embeddings sequence used to compute values; shape [..., T, D_v].\n      mask: Optional mask applied to attention weights; shape [..., H=1, T', T].\n\n    Returns:\n      A new sequence of embeddings, consisting of a projection of the\n        attention-weighted value projections; shape [..., T', D'].\n    \"\"\"\n\n    # In shape hints below, we suppress the leading dims [...] for brevity.\n    # Hence e.g. [A, B] should be read in every case as [..., A, B].\n    *leading_dims, sequence_length, _ = query.shape\n    projection = self._linear_projection\n\n    # Compute key/query/values (overload K/Q/V to denote the respective sizes).\n    query_heads = projection(query, self.key_size, \"query\")  # [T', H, Q=K]\n    key_heads = projection(key, self.key_size, \"key\")  # [T, H, K]\n    value_heads = projection(value, self.value_size, \"value\")  # [T, H, V]\n\n    # Compute attention weights.\n    attn_logits = jnp.einsum(\"...thd,...Thd->...htT\", query_heads, key_heads)\n    attn_logits = attn_logits / np.sqrt(self.key_size).astype(key.dtype)\n    if mask is not None:\n      if mask.ndim != attn_logits.ndim:\n        raise ValueError(\n            f\"Mask dimensionality {mask.ndim} must match logits dimensionality \"\n            f\"{attn_logits.ndim}.\")\n      attn_logits = jnp.where(mask, attn_logits, -1e30)\n    attn_weights = jax.nn.softmax(attn_logits)  # [H, T', T]\n\n    # Weight the values by the attention and flatten the head vectors.\n    attn = jnp.einsum(\"...htT,...Thd->...thd\", attn_weights, value_heads)\n    attn = jnp.reshape(attn, (*leading_dims, sequence_length, -1))  # [T', H*V]\n\n    # Apply another projection to get the final embeddings.\n    final_projection = hk.Linear(self.model_size, w_init=self.w_init)\n    return AttentionOutput(\n        out=final_projection(attn),\n        logits=attn_logits,\n    )\n\n  @hk.transparent\n  def _linear_projection(\n      self,\n      x: jnp.ndarray,\n      head_size: int,\n      name: Optional[str] = None,\n  ) -> jnp.ndarray:", "metadata": {"task_id": "deepmind_tracr/145", "ground_truth": "    y = hk.Linear(self.num_heads * head_size, w_init=self.w_init, name=name)(x)\n    *leading_dims, _ = x.shape\n    return y.reshape((*leading_dims, self.num_heads, head_size))\n", "fpath_tuple": ["deepmind_tracr", "tracr", "transformer", "attention.py"], "context_start_lineno": 0, "lineno": 157, "function_name": "_linear_projection", "line_no": 157}}
