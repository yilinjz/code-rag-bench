{"_id": "google_lightweight_mmm/0", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Constant)))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Expr(Constant)Return(Call(Attribute(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))keyword(Name(Load))keyword(Name(Load)))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Constant))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/0", "ground_truth": "  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "_make_single_prediction", "line_no": 68}}
{"_id": "google_lightweight_mmm/1", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.\n\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(\n      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions\n\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Constant)))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Expr(Constant)Return(Call(Attribute(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))keyword(Name(Load))keyword(Name(Load)))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load))SubName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantLoad))))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)If(Compare(BinOp(Name(Load)ModName(Load))EqConstant)Return(BinOp(BinOp(Name(Load)FloorDivName(Load))AddConstant)))Return(BinOp(BinOp(Name(Load)FloorDivName(Load))AddConstant))))", "metadata": {"task_id": "google_lightweight_mmm/1", "ground_truth": "  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 0, "lineno": 138, "function_name": "_calculate_number_rows_plot", "line_no": 138}}
{"_id": "google_lightweight_mmm/2", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.\n\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(\n      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions\n\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n\n\ndef _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:\n  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n\n  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Constant)))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Expr(Constant)Return(Call(Attribute(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))keyword(Name(Load))keyword(Name(Load)))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load))SubName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantLoad))))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)If(Compare(BinOp(Name(Load)ModName(Load))EqConstant)Return(BinOp(BinOp(Name(Load)FloorDivName(Load))AddConstant)))Return(BinOp(BinOp(Name(Load)FloorDivName(Load))AddConstant)))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)If(UnaryOp(NotCall(Name(Load)Name(Load)Constant))Raise(Call(Attribute(Name(Load)Load)Constant)))If(Compare(Attribute(Subscript(Attribute(Name(Load)Load)ConstantLoad)Load)GtConstant)Assign(Name(Store)Constant)If(Compare(Attribute(Subscript(Attribute(Name(Load)Load)ConstantLoad)Load)EqConstant)Assign(Name(Store)Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)Subscript(Attribute(Name(Load)Load)ConstantLoad)))If(Compare(Attribute(Subscript(Attribute(Name(Load)Load)ConstantLoad)Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/2", "ground_truth": "  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 0, "lineno": 161, "function_name": "_calculate_media_contribution", "line_no": 161}}
{"_id": "google_lightweight_mmm/3", "text": " False.\n    figure_size: Size of the plot figure.\n    n_columns: Number of columns to display in the subplots grid. Modifying this\n      parameter might require to adjust figure_size accordingly for the plot\n      to still have reasonable structure.\n    marker_size: Size of the marker for the optimization annotations. Only\n      useful if optimal_allocation_per_timeunit is not None. Default is 8.\n    legend_fontsize: Legend font size for individual subplots.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    Plots of response curves.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its response \"\n        \"curves.\")\n  media = media_mix_model.media\n  media_maxes = media.max(axis=0) * (1 + percentage_add)\n  if media_mix_model._extra_features is not None:\n    extra_features = jnp.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(\n      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,\n                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal\n  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:\n    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)\n\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(\n          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"\n  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/3", "ground_truth": "  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 408, "lineno": 571, "function_name": "plot_cross_correlate", "line_no": 571}}
{"_id": "google_lightweight_mmm/4", "text": "np.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(\n      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,\n                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal\n  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:\n    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)\n\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(\n          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"\n  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/4", "ground_truth": "  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 430, "lineno": 597, "function_name": "plot_var_cost", "line_no": 597}}
{"_id": "google_lightweight_mmm/5", "text": "ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots", "metadata": {"task_id": "google_lightweight_mmm/5", "ground_truth": "  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 520, "lineno": 688, "function_name": "_call_fit_plotter", "line_no": 688}}
{"_id": "google_lightweight_mmm/6", "text": " of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/6", "ground_truth": "  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 561, "lineno": 728, "function_name": "plot_model_fit", "line_no": 728}}
{"_id": "google_lightweight_mmm/7", "text": "t.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,\n                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/7", "ground_truth": "  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,\n      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "context_start_lineno": 603, "lineno": 763, "function_name": "plot_out_of_sample_model_fit", "line_no": 763}}
{"_id": "google_lightweight_mmm/8", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for seasonality.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,\n          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",\n          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n\n    def mock_model_function(data, degrees_seasonality, frequency):\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(Name(Load)Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Load)))FunctionDef(arguments(argargargarg)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Dict)keyword(Name(Load))))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/8", "ground_truth": "      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "context_start_lineno": 0, "lineno": 96, "function_name": "mock_model_function", "line_no": 96}}
{"_id": "google_lightweight_mmm/9", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for seasonality.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,\n          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",\n          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n\n    def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        data=data,\n        degrees_seasonality=degrees_seasonality,\n        frequency=52,\n    )\n    seasonality_values = mcmc.get_samples()[\"seasonality\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_sinusoidal_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.GAMMA_SEASONALITY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    degrees_seasonality = 3\n    frequency = 365\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.sinusoidal_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        degrees_seasonality=degrees_seasonality,\n        frequency=frequency,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees\",\n          data_shape=(500, 3),\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"five_degrees\",\n          data_shape=(500, 3, 5),\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_intra_week_seasonality_produces_correct_shape(\n      self, data_shape, expected_shape):\n\n    def mock_model_function(data):\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(Name(Load)Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Load)))FunctionDef(arguments(argargargarg)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Dict)keyword(Name(Load))))))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Subscript(Call(Attribute(Name(Load)Load))ConstantLoad))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))FunctionDef(arguments(arg)Assign(Name(Store)Attribute(Name(Load)Load))Assign(Tuple(Name(Store)Name(Store)Store)Tuple(ConstantConstantLoad))Assign(Name(Store)Dict(Name(Load)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Constant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)DictComp(Name(Load)Subscript(Name(Load)ConstantLoad)comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Compare(ConstantInName(Load)))))Assign(Name(Store)Subscript(Name(Load)Name(Load)Load))If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Assign(Name(Store)Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argargarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/9", "ground_truth": "      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "context_start_lineno": 0, "lineno": 168, "function_name": "mock_model_function", "line_no": 168}}
{"_id": "google_lightweight_mmm/10", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for trend.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import trend\n\n\nclass TrendTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          coef_trend_shape=(),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          coef_trend_shape=(5,),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n  ])\n  def test_core_trend_with_exponent_produces_correct_shape(\n      self, coef_trend_shape, trend_length, expo_trend_shape):\n    coef_trend = jnp.ones(coef_trend_shape)\n    linear_trend = jnp.arange(trend_length)\n    if coef_trend.ndim == 1:  # For geo model's case\n      linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(\n        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(Subscript(Attribute(Name(Load)Load)ConstantLoad)Starred(Name(Load)Load)Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Tuple(Load))keyword(Constant)keyword(Tuple(Load)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantLoad))keyword(Constant)keyword(Tuple(Load)))Load)))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)))))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Load)))))", "metadata": {"task_id": "google_lightweight_mmm/10", "ground_truth": "      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "mock_model_function", "line_no": 67}}
{"_id": "google_lightweight_mmm/11", "text": " linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(\n        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = () if data.ndim == 2 else (data.shape[2],)\n\n    mcmc.run(rng_key, data=data)\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.COEF_TREND}\",\n          prior_name=priors.COEF_TREND,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.EXPO_TREND}\",\n          prior_name=priors.EXPO_TREND,\n      ),\n  )\n  def test_trend_with_exponent_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(trend.trend_with_exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"dynamic_trend_national_shape\",\n          number_periods=100,\n          initial_level_shape=(),\n          initial_slope_shape=(),\n          variance_level_shape=(),\n          variance_slope_shape=(),\n      ),\n      dict(\n          testcase_name=\"dynamic_trend_geo_shape\",\n          number_periods=100,\n          initial_level_shape=(2,),\n          initial_slope_shape=(2,),\n          variance_level_shape=(2,),\n          variance_slope_shape=(2,),\n      ),\n  ])\n  def test_core_dynamic_trend_produces_correct_shape(\n      self, number_periods, initial_level_shape, initial_slope_shape,\n      variance_level_shape, variance_slope_shape):\n    initial_level = jnp.ones(initial_level_shape)\n    initial_slope = jnp.ones(initial_slope_shape)\n    variance_level = jnp.ones(variance_level_shape)\n    variance_slope = jnp.ones(variance_slope_shape)\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    if initial_level.ndim == 1:  # For geo model's case\n      random_walk_level = jnp.expand_dims(random_walk_level, axis=-1)\n      random_walk_slope = jnp.expand_dims(random_walk_slope, axis=-1)\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    self.assertEqual(dynamic_trend_values.shape,\n                     (number_periods, *initial_level_shape))\n\n  def test_core_dynamic_trend_produces_correct_value(self):\n    number_periods = 5\n    initial_level = jnp.ones(())\n    initial_slope = jnp.ones(())\n    variance_level = jnp.ones(())\n    variance_slope = jnp.ones(())\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    dynamic_trend_expected_value = jnp.array([1, 3, 7, 14, 25])\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    np.testing.assert_array_equal(x=dynamic_trend_values,\n                                  y=dynamic_trend_expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_with_prediction_is_true\",\n          data_shape=(100, 3),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"geo_with_prediction_is_true\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"national_with_prediction_is_false\",\n          data_shape=(100, 3),\n          is_trend_prediction=False),\n      dict(\n          testcase_name=\"geo_with_prediction_is_false\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=False),\n  ])\n  def test_dynamic_trend_produces_correct_shape(\n      self, data_shape, is_trend_prediction):\n\n    def mock_model_function(geo_size, data_size):", "metadata": {"task_id": "google_lightweight_mmm/11", "ground_truth": "      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "context_start_lineno": 51, "lineno": 213, "function_name": "mock_model_function", "line_no": 213}}
{"_id": "google_lightweight_mmm/12", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Name(Load)Load)ConstantName(Load)Name(Load)))Attribute(Name(Load)Load)Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/12", "ground_truth": "  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "_sinusoidal_seasonality", "line_no": 48}}
{"_id": "google_lightweight_mmm/13", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Name(Load)Load)ConstantName(Load)Name(Load)))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Constant)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))If(Compare(Name(Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/13", "ground_truth": "  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "context_start_lineno": 0, "lineno": 84, "function_name": "sinusoidal_seasonality", "line_no": 84}}
{"_id": "google_lightweight_mmm/14", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n\n\ndef _intra_week_seasonality(\n    data: jnp.ndarray,\n    weekday: jnp.ndarray,\n) -> jnp.ndarray:\n  data_size = data.shape[0]\n  return weekday[jnp.arange(data_size) % 7]\n\n\ndef intra_week_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Models intra week seasonality.\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n\n  Returns:\n    The contribution of the weekday seasonality.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Name(Load)Load)ConstantName(Load)Name(Load)))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Constant)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))If(Compare(Name(Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Return(Subscript(Name(Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load))ModConstant)Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/14", "ground_truth": "  default_priors = priors.get_default_priors()\n  with numpyro.plate(name=f\"{priors.WEEKDAY}_plate\", size=7):\n    weekday = numpyro.sample(\n        name=priors.WEEKDAY,\n        fn=custom_priors.get(priors.WEEKDAY, default_priors[priors.WEEKDAY]))\n\n  weekday_series = _intra_week_seasonality(data=data, weekday=weekday)\n\n  if data.ndim == 3:  # For geo model's case\n    weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n\n  return weekday_series\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "context_start_lineno": 0, "lineno": 130, "function_name": "intra_week_seasonality", "line_no": 130}}
{"_id": "google_lightweight_mmm/15", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for trend.\"\"\"\n\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.\n    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Return(BinOp(Name(Load)MultBinOp(Name(Load)PowName(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))If(Compare(Name(Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/15", "ground_truth": "  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "context_start_lineno": 0, "lineno": 59, "function_name": "trend_with_exponent", "line_no": 59}}
{"_id": "google_lightweight_mmm/16", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for trend.\"\"\"\n\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.\n    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_periods\",))\ndef _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,\n    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:\n    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.\n  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Return(BinOp(Name(Load)MultBinOp(Name(Load)PowName(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))If(Compare(Name(Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(BinOp(Subscript(Name(Load)ConstantLoad)AddName(Load))Load))Subscript(Name(Load)Slice(Constant)Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(BinOp(Subscript(Name(Load)ConstantLoad)AddName(Load))Load))Subscript(Name(Load)Slice(Constant)Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))Assign(Name(Store)IfExp(Compare(Attribute(Name(Load)Load)EqConstant)List(Tuple(ConstantConstantLoad)Load)List(Tuple(ConstantConstantLoad)Tuple(ConstantConstantLoad)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant))Slice(BinOp(Name(Load)SubConstant))Load)Name(Load)keyword(Constant)keyword(Constant)))Return(BinOp(Name(Load)AddName(Load)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantLoad)))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/16", "ground_truth": "  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])\n  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "context_start_lineno": 0, "lineno": 107, "function_name": "_dynamic_trend", "line_no": 107}}
{"_id": "google_lightweight_mmm/17", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for modeling the intercept.\"\"\"\n\nfrom typing import Mapping\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\ndef simple_intercept(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str,\n                           dist.Distribution] = immutabledict.immutabledict(),\n) -> jnp.ndarray:\n  \"\"\"Calculates a national or geo incercept.\n  Note that this intercept is constant over time.\n\n  Args:\n    data: Media input data. Media data must have either 2 dims for national\n      model or 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. Refer to the full documentation on custom priors for\n      details.\n\n  Returns:\n    The values of the intercept.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Call(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/17", "ground_truth": "  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n\n  with numpyro.plate(name=f\"{priors.INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=priors.INTERCEPT,\n        fn=custom_priors.get(priors.INTERCEPT,\n                             default_priors[priors.INTERCEPT]),\n    )\n  return intercept\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "context_start_lineno": 0, "lineno": 45, "function_name": "simple_intercept", "line_no": 45}}
{"_id": "google_lightweight_mmm/18", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for intercept.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.baseline import intercept\n\n\nclass InterceptTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_produces_output_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/18", "ground_truth": "      numpyro.deterministic(\n          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "context_start_lineno": 0, "lineno": 44, "function_name": "mock_model_function", "line_no": 44}}
{"_id": "google_lightweight_mmm/19", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(BinOp(Name(Load)DivName(Load)))keyword(UnaryOp(USubName(Load)))))Return(Call(Attribute(Name(Load)Load)Compare(Name(Load)EqConstant)keyword(Constant)keyword(BinOp(ConstantDivBinOp(ConstantAddName(Load))))))Attribute(Name(Load)Load)Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/19", "ground_truth": "  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "context_start_lineno": 0, "lineno": 45, "function_name": "_hill", "line_no": 45}}
{"_id": "google_lightweight_mmm/20", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(BinOp(Name(Load)DivName(Load)))keyword(UnaryOp(USubName(Load)))))Return(Call(Attribute(Name(Load)Load)Compare(Name(Load)EqConstant)keyword(Constant)keyword(BinOp(ConstantDivBinOp(ConstantAddName(Load))))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/20", "ground_truth": "  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "context_start_lineno": 0, "lineno": 69, "function_name": "hill", "line_no": 69}}
{"_id": "google_lightweight_mmm/21", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef _exponent(data: jnp.ndarray, exponent_values: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies exponent to the given data.\"\"\"\n  return core_utils.apply_exponent_safe(data=data, exponent=exponent_values)\n\n\ndef exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(BinOp(Name(Load)DivName(Load)))keyword(UnaryOp(USubName(Load)))))Return(Call(Attribute(Name(Load)Load)Compare(Name(Load)EqConstant)keyword(Constant)keyword(BinOp(ConstantDivBinOp(ConstantAddName(Load))))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Return(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/21", "ground_truth": "  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.EXPONENT}_plate\", size=data.shape[1]):\n    exponent_values = numpyro.sample(\n        name=f\"{prefix}{priors.EXPONENT}\",\n        fn=custom_priors.get(priors.EXPONENT, default_priors[priors.EXPONENT]))\n\n  if data.ndim == 3:\n    exponent_values = jnp.expand_dims(exponent_values, axis=-1)\n  return _exponent(data=data, exponent_values=exponent_values)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "context_start_lineno": 0, "lineno": 119, "function_name": "exponent", "line_no": 119}}
{"_id": "google_lightweight_mmm/22", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubConstant))Name(Load)Load)))Return(BinOp(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)Name(Load)keyword(Constant))DivCall(Attribute(Name(Load)Load))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)PowBinOp(BinOp(Name(Load)SubName(Load))PowConstant)))Return(Call(Name(Load)Name(Load)Name(Load)Name(Load)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantLoad)))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/22", "ground_truth": "  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 0, "lineno": 68, "function_name": "_carryover", "line_no": 68}}
{"_id": "google_lightweight_mmm/23", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubConstant))Name(Load)Load)))Return(BinOp(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)Name(Load)keyword(Constant))DivCall(Attribute(Name(Load)Load))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)PowBinOp(BinOp(Name(Load)SubName(Load))PowConstant)))Return(Call(Name(Load)Name(Load)Name(Load)Name(Load)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/23", "ground_truth": "  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 0, "lineno": 102, "function_name": "carryover", "line_no": 102}}
{"_id": "google_lightweight_mmm/24", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n\n@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubConstant))Name(Load)Load)))Return(BinOp(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)Name(Load)keyword(Constant))DivCall(Attribute(Name(Load)Load))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)PowBinOp(BinOp(Name(Load)SubName(Load))PowConstant)))Return(Call(Name(Load)Name(Load)Name(Load)Name(Load)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))ConstantConstant)Expr(Constant)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Name(Load))Assign(Name(Store)BinOp(BinOp(Name(Load)MultName(Load))AddName(Load)))Return(Tuple(Name(Load)Name(Load)Load))Attribute(Name(Load)Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load))keyword(Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load)Load))Name(Load)Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Lambda(arguments(arg)BinOp(Name(Load)DivBinOp(ConstantDivBinOp(ConstantSubName(Load)))))Lambda(arguments(arg)Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/24", "ground_truth": "  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 0, "lineno": 146, "function_name": "_adstock", "line_no": 146}}
{"_id": "google_lightweight_mmm/25", "text": "LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n\n@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\ndef adstock(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    normalise: bool = True,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n\nAST=Module(Expr(BinOp(Name(Load)SubConstant))Expr(Constant)Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubConstant))Name(Load)Load)))Return(BinOp(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)Name(Load)keyword(Constant))DivCall(Attribute(Name(Load)Load))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)PowBinOp(BinOp(Name(Load)SubName(Load))PowConstant)))Return(Call(Name(Load)Name(Load)Name(Load)Name(Load)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))ConstantConstant)Expr(Constant)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Name(Load))Assign(Name(Store)BinOp(BinOp(Name(Load)MultName(Load))AddName(Load)))Return(Tuple(Name(Load)Name(Load)Load))Attribute(Name(Load)Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load))keyword(Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load)Load))Name(Load)Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Lambda(arguments(arg)BinOp(Name(Load)DivBinOp(ConstantDivBinOp(ConstantSubName(Load)))))Lambda(arguments(arg)Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/25", "ground_truth": "  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.LAG_WEIGHT}_plate\", size=data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=f\"{prefix}{priors.LAG_WEIGHT}\",\n        fn=custom_priors.get(priors.LAG_WEIGHT,\n                             default_priors[priors.LAG_WEIGHT]))\n\n  if data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n\n  return _adstock(data=data, lag_weight=lag_weight, normalise=normalise)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "context_start_lineno": 6, "lineno": 185, "function_name": "adstock", "line_no": 185}}
{"_id": "google_lightweight_mmm/26", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lagging.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))))FunctionDef(arguments(argarg)FunctionDef(arguments(argarg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)keyword(Name(Load))))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/26", "ground_truth": "      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "mock_model_function", "line_no": 80}}
{"_id": "google_lightweight_mmm/27", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lagging.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    number_lags = 15\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, number_lags=number_lags)\n    carryover_values = mcmc.get_samples()[\"carryover\"]\n\n    self.assertEqual(carryover_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ad_effect_retention_rate\",\n          prior_name=priors.AD_EFFECT_RETENTION_RATE,\n      ),\n      dict(\n          testcase_name=\"peak_effect_delay\",\n          prior_name=priors.PEAK_EFFECT_DELAY,\n      ),\n  )\n  def test_carryover_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    number_lags = 13\n\n    trace_handler = handlers.trace(handlers.seed(lagging.carryover, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        number_lags=number_lags,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          lag_weight_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          lag_weight_shape=(3, 1),\n      ),\n  )\n  def test_core_adstock_produces_correct_shape(self, data_shape,\n                                               lag_weight_shape):\n    data = jnp.ones(data_shape)\n    lag_weight = jnp.ones(lag_weight_shape)\n\n    output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_adstock_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, normalise):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))))FunctionDef(arguments(argarg)FunctionDef(arguments(argarg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)keyword(Name(Load))))))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)Subscript(Call(Attribute(Name(Load)Load))ConstantLoad))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(Name(Load)Starred(Attribute(Name(Load)Load)Load)Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Store)Tuple(ConstantConstantLoad))Assign(Name(Store)Dict(Name(Load)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Constant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)DictComp(Name(Load)Subscript(Name(Load)ConstantLoad)comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Compare(ConstantInName(Load)))))Assign(Name(Store)Subscript(Name(Load)Name(Load)Load))Assign(Name(Store)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)Name(Load)Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Attribute(Name(Load)Load)))Call(Name(Load)keyword(Constant)keyword(Attribute(Name(Load)Load)))))FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantConstantLoad)))))FunctionDef(arguments(argarg)FunctionDef(arguments(argarg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)keyword(Name(Load))))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/27", "ground_truth": "      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "context_start_lineno": 0, "lineno": 168, "function_name": "mock_model_function", "line_no": 168}}
{"_id": "google_lightweight_mmm/28", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n\nAST=Module(Expr(Constant)Import(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)))))Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Load)))FunctionDef(argumentsReturn(Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)))))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/28", "ground_truth": "  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "context_start_lineno": 0, "lineno": 98, "function_name": "_get_default_priors", "line_no": 98}}
{"_id": "google_lightweight_mmm/29", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n\nAST=Module(Expr(Constant)Import(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)))))Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Load)))FunctionDef(argumentsReturn(Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)))))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))FunctionDef(argumentsReturn(Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))))Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))))Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))))))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/29", "ground_truth": "  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "context_start_lineno": 0, "lineno": 113, "function_name": "_get_transform_default_priors", "line_no": 113}}
{"_id": "google_lightweight_mmm/30", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n\n\ndef transform_adstock(media_data: jnp.ndarray,\n                      custom_priors: MutableMapping[str, Prior],\n                      normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)))))Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Load)))FunctionDef(argumentsReturn(Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)))))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))FunctionDef(argumentsReturn(Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))))Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))))Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))))))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Subscript(Call(Name(Load))ConstantLoad))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Call(Attribute(Name(Load)Load)Name(Load)Subscript(Name(Load)Name(Load)Load))))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Call(Attribute(Name(Load)Load)Name(Load)Subscript(Name(Load)Name(Load)Load))))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Return(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/30", "ground_truth": "  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n\n  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "context_start_lineno": 0, "lineno": 156, "function_name": "transform_adstock", "line_no": 156}}
{"_id": "google_lightweight_mmm/31", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantCall(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))))Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantCall(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)IfExp(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Set(ConstantConstantConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Starred(ListComp(Subscript(Attribute(Name(Load)Load)Name(Load)Load)comprehension(Name(Store)BinOp(Name(Load)SubSet(Name(Load)))))Load))SubSubscript(Attribute(Name(Load)Load)Name(Load)Load)))Assign(Name(Store)DictComp(Name(Load)Subscript(Name(Load)Name(Load)Load)comprehension(Name(Store)Name(Load)Compare(Name(Load)NotInName(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)IfExp(Name(Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Constant))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Dict)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Subscript(Subscript(Name(Load)ConstantLoad)ConstantLoad)))Assign(Attribute(Name(Load)Store)ListComp(JoinedStr(ConstantFormattedValue(Name(Load)))comprehension(Name(Store)Call(Name(Load)Constant))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/31", "ground_truth": "  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "context_start_lineno": 0, "lineno": 89, "function_name": "_set_up_mock_mmm", "line_no": 89}}
{"_id": "google_lightweight_mmm/32", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantCall(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))))Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantCall(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)IfExp(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Set(ConstantConstantConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Starred(ListComp(Subscript(Attribute(Name(Load)Load)Name(Load)Load)comprehension(Name(Store)BinOp(Name(Load)SubSet(Name(Load)))))Load))SubSubscript(Attribute(Name(Load)Load)Name(Load)Load)))Assign(Name(Store)DictComp(Name(Load)Subscript(Name(Load)Name(Load)Load)comprehension(Name(Store)Name(Load)Compare(Name(Load)NotInName(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)IfExp(Name(Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Constant))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Dict)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Subscript(Subscript(Name(Load)ConstantLoad)ConstantLoad)))Assign(Attribute(Name(Load)Store)ListComp(JoinedStr(ConstantFormattedValue(Name(Load)))comprehension(Name(Store)Call(Name(Load)Constant))))Return(Name(Load))Attribute(Name(Load)Load))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Name(Load))))", "metadata": {"task_id": "google_lightweight_mmm/32", "ground_truth": "    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "context_start_lineno": 0, "lineno": 118, "function_name": "setUpClass", "line_no": 118}}
{"_id": "google_lightweight_mmm/33", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n\n  def setUp(self):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantCall(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))))Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantCall(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))Call(Attribute(Name(Load)Load)List(ConstantConstantLoad))))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)IfExp(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Set(ConstantConstantConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Starred(ListComp(Subscript(Attribute(Name(Load)Load)Name(Load)Load)comprehension(Name(Store)BinOp(Name(Load)SubSet(Name(Load)))))Load))SubSubscript(Attribute(Name(Load)Load)Name(Load)Load)))Assign(Name(Store)DictComp(Name(Load)Subscript(Name(Load)Name(Load)Load)comprehension(Name(Store)Name(Load)Compare(Name(Load)NotInName(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)IfExp(Name(Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Constant))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Dict)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Subscript(Subscript(Name(Load)ConstantLoad)ConstantLoad)))Assign(Attribute(Name(Load)Store)ListComp(JoinedStr(ConstantFormattedValue(Name(Load)))comprehension(Name(Store)Call(Name(Load)Constant))))Return(Name(Load))Attribute(Name(Load)Load))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Name(Load))FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constantkeyword(Constant))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Constantkeyword(Constant))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constantkeyword(Constant))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constantkeyword(Constant))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Constantkeyword(Constant))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Constantkeyword(Constant))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constantkeyword(Constant)))))))", "metadata": {"task_id": "google_lightweight_mmm/33", "ground_truth": "    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))\n    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "context_start_lineno": 0, "lineno": 138, "function_name": "setUp", "line_no": 138}}
{"_id": "google_lightweight_mmm/34", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)If(BoolOp(AndCall(Name(Load)Name(Load)Constant)Compare(Attribute(Name(Load)Load)GtConstant))Assign(Name(Store)BinOp(Name(Load)MultCall(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)DivSubscript(Name(Load)ConstantLoad))keyword(Subscript(Name(Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load)))Load)keyword(Constant)))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantConstantLoad)))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/34", "ground_truth": "  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "context_start_lineno": 0, "lineno": 66, "function_name": "_objective_function", "line_no": 66}}
{"_id": "google_lightweight_mmm/35", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,\n                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.\n  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)If(BoolOp(AndCall(Name(Load)Name(Load)Constant)Compare(Attribute(Name(Load)Load)GtConstant))Assign(Name(Store)BinOp(Name(Load)MultCall(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)DivSubscript(Name(Load)ConstantLoad))keyword(Subscript(Name(Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load)))Load)keyword(Constant)))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(UnaryOp(USubConstant)Call(Name(Load)Name(Load))Load)))Return(BinOp(Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load)))SubName(Load)))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))Constant)Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultBinOp(ConstantSubName(Load)))Constant))Assign(Name(Store)BinOp(Name(Load)MultBinOp(ConstantAddName(Load))))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant)))))Return(Call(Attribute(Name(Load)Load)keyword(BinOp(Name(Load)MultName(Load)))keyword(BinOp(Name(Load)MultName(Load)))))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/35", "ground_truth": "  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "context_start_lineno": 0, "lineno": 126, "function_name": "_get_lower_and_upper_bounds", "line_no": 126}}
{"_id": "google_lightweight_mmm/36", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,\n                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.\n  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n\n\ndef _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last\n  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)If(BoolOp(AndCall(Name(Load)Name(Load)Constant)Compare(Attribute(Name(Load)Load)GtConstant))Assign(Name(Store)BinOp(Name(Load)MultCall(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)DivSubscript(Name(Load)ConstantLoad))keyword(Subscript(Name(Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load)))Load)keyword(Constant)))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(UnaryOp(USubConstant)Call(Name(Load)Name(Load))Load)))Return(BinOp(Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load)))SubName(Load)))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))Constant)Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultBinOp(ConstantSubName(Load)))Constant))Assign(Name(Store)BinOp(Name(Load)MultBinOp(ConstantAddName(Load))))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant)))))Return(Call(Attribute(Name(Load)Load)keyword(BinOp(Name(Load)MultName(Load)))keyword(BinOp(Name(Load)MultName(Load)))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)keyword(Constant))MultName(Load)))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant)))))Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Name(Store)BinOp(Name(Load)DivCall(Attribute(Name(Load)Load))))Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Name(Store)BinOp(Name(Load)DivName(Load)))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/36", "ground_truth": "  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n\n  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "context_start_lineno": 0, "lineno": 171, "function_name": "_generate_starting_values", "line_no": 171}}
{"_id": "google_lightweight_mmm/37", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for preprocessing dataset for training LightweightMMM.\"\"\"\n\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n\n  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any\n  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.\n    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)Pass)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Subscript(Name(Load)Subscript(Name(Load)Tuple(List(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(List(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load)Load))ConstantConstantConstantConstant)Expr(Constant)If(Call(Name(Load)List(Compare(Name(Load)IsConstant)Compare(Name(Load)IsConstant)Compare(Name(Load)IsConstant)Compare(Name(Load)IsConstant)Load))Raise(Call(Name(Load)Constant)))If(BoolOp(AndCompare(Name(Load)IsConstant)Compare(Name(Load)IsConstant))Raise(Call(Name(Load)Constant))If(Compare(Name(Load)IsNotConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))))If(BoolOp(AndCompare(Name(Load)IsConstant)Compare(Name(Load)IsConstant))Raise(Call(Name(Load)Constant))If(Compare(Name(Load)IsNotConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))))Constant)FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Expr(Constant)If(Call(Name(Load)Name(Load)Constant)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Constant)keyword(Name(Load))))If(BoolOp(OrCall(Name(Load)Attribute(Name(Load)Load)Name(Load))Call(Name(Load)Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)BinOp(Attribute(Name(Load)Load)MultCall(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)Slice(Constant)Load))))))If(Call(Name(Load)Name(Load)Constant)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Constant)keyword(Name(Load))))If(BoolOp(OrCall(Name(Load)Attribute(Name(Load)Load)Name(Load))Call(Name(Load)Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)BinOp(Attribute(Name(Load)Load)MultCall(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)Slice(Constant)Load))))))Constant)))", "metadata": {"task_id": "google_lightweight_mmm/37", "ground_truth": "    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 0, "lineno": 110, "function_name": "fit", "line_no": 110}}
{"_id": "google_lightweight_mmm/38", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for preprocessing dataset for training LightweightMMM.\"\"\"\n\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n\n  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any\n  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.\n    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)Pass)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Subscript(Name(Load)Subscript(Name(Load)Tuple(List(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(List(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load)Load))ConstantConstantConstantConstant)Expr(Constant)If(Call(Name(Load)List(Compare(Name(Load)IsConstant)Compare(Name(Load)IsConstant)Compare(Name(Load)IsConstant)Compare(Name(Load)IsConstant)Load))Raise(Call(Name(Load)Constant)))If(BoolOp(AndCompare(Name(Load)IsConstant)Compare(Name(Load)IsConstant))Raise(Call(Name(Load)Constant))If(Compare(Name(Load)IsNotConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))))If(BoolOp(AndCompare(Name(Load)IsConstant)Compare(Name(Load)IsConstant))Raise(Call(Name(Load)Constant))If(Compare(Name(Load)IsNotConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))))Constant)FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Expr(Constant)If(Call(Name(Load)Name(Load)Constant)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Constant)keyword(Name(Load))))If(BoolOp(OrCall(Name(Load)Attribute(Name(Load)Load)Name(Load))Call(Name(Load)Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)BinOp(Attribute(Name(Load)Load)MultCall(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)Slice(Constant)Load))))))If(Call(Name(Load)Name(Load)Constant)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Constant)keyword(Name(Load))))If(BoolOp(OrCall(Name(Load)Attribute(Name(Load)Load)Name(Load))Call(Name(Load)Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)BinOp(Attribute(Name(Load)Load)MultCall(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)Slice(Constant)Load))))))Constant)FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Expr(Constant)If(BoolOp(OrUnaryOp(NotCall(Name(Load)Name(Load)Constant))UnaryOp(NotCall(Name(Load)Name(Load)Constant)))Raise(Call(Name(Load)Constant)))Return(BinOp(BinOp(Attribute(Name(Load)Load)MultName(Load))DivAttribute(Name(Load)Load)))Attribute(Name(Load)Load))))", "metadata": {"task_id": "google_lightweight_mmm/38", "ground_truth": "    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 0, "lineno": 133, "function_name": "transform", "line_no": 133}}
{"_id": "google_lightweight_mmm/39", "text": " data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/39", "ground_truth": "  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 60, "lineno": 244, "function_name": "_compute_variances", "line_no": 244}}
{"_id": "google_lightweight_mmm/40", "text": "ide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/40", "ground_truth": "  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 92, "lineno": 282, "function_name": "_compute_spend_fractions", "line_no": 282}}
{"_id": "google_lightweight_mmm/41", "text": ", axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n\n\ndef _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:\n  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/41", "ground_truth": "  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")\n\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)\n  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "context_start_lineno": 117, "lineno": 313, "function_name": "_compute_variance_inflation_factors", "line_no": 313}}
{"_id": "google_lightweight_mmm/42", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of utilities for LightweighMMM package.\"\"\"\nimport pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nfrom scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,\n    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Constant)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load)))))For(Name(Store)Call(Name(Load)Name(Load))If(Call(Attribute(Name(Load)Load)Constant)Continue)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Expr(Call(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))))Return(Name(Load))Name(Load)))", "metadata": {"task_id": "google_lightweight_mmm/42", "ground_truth": "  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 0, "lineno": 56, "function_name": "load_model", "line_no": 56}}
{"_id": "google_lightweight_mmm/43", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of utilities for LightweighMMM package.\"\"\"\nimport pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nfrom scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,\n    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n\n\ndef get_time_seed() -> int:\n  \"\"\"Generates an integer using the last decimals of time.time().\n\n  Returns:\n    Integer to be used as seed.\n  \"\"\"\n  # time.time() has the following format: 1645174953.0429401\n  return int(str(time.time()).split(\".\")[1])\n\n\ndef simulate_dummy_data(\n    data_size: int,\n    n_media_channels: int,\n    n_extra_features: int,\n    geos: int = 1,\n    seed: int = 5\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Simulates dummy data needed for media mix modelling.\n\n  This function's goal is to be super simple and not have many parameters,\n  although it does not generate a fully realistic dataset is only meant to be\n  used for demos/tutorial purposes. Uses carryover for lagging but has no\n  saturation and no trend.\n\n  The data simulated includes the media data, extra features, a target/KPI and\n  costs.\n\n  Args:\n    data_size: Number of rows to generate.\n    n_media_channels: Number of media channels to generate.\n    n_extra_features: Number of extra features to generate.\n    geos: Number of geos for geo level data (default = 1 for national).\n    seed: Random seed.\n\n  Returns:\n    The simulated media, extra features, target and costs.\n  \"\"\"\n  if data_size < 1 or n_media_channels < 1 or n_extra_features < 1:\n    raise ValueError(\n        \"Data size, n_media_channels and n_extra_features must be greater than\"\n        \" 0. Please check the values introduced are greater than zero.\")\n  data_offset = int(data_size * 0.2)\n  data_size += data_offset\n  key = random.PRNGKey(seed)\n  sub_keys = random.split(key=key, num=7)\n  media_data = random.normal(key=sub_keys[0],\n                             shape=(data_size, n_media_channels)) * 1.5 + 20\n\n  extra_features = random.normal(key=sub_keys[1],\n                                 shape=(data_size, n_extra_features)) + 5\n  # Reduce the costs to make ROI realistic.\n  costs = media_data[data_offset:].sum(axis=0) * .1\n\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=2,\n      frequency=52,\n      gamma_seasonality=1)\n  target_noise = random.normal(key=sub_keys[2], shape=(data_size,)) + 3\n\n  # media_data_transformed = media_transforms.adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1\n  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]\n      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Constant)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load)))))For(Name(Store)Call(Name(Load)Name(Load))If(Call(Attribute(Name(Load)Load)Constant)Continue)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Expr(Call(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))))Return(Name(Load))Name(Load))FunctionDef(argumentsExpr(Constant)Return(Call(Name(Load)Subscript(Call(Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load)Constant)ConstantLoad)))Name(Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)If(BoolOp(OrCompare(Name(Load)LtConstant)Compare(Name(Load)LtConstant)Compare(Name(Load)LtConstant))Raise(Call(Name(Load)Constant)))Assign(Name(Store)Call(Name(Load)BinOp(Name(Load)MultConstant)))AugAssign(Name(Store)AddName(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)BinOp(BinOp(Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(Name(Load)Name(Load)Load)))MultConstant)AddConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(Name(Load)Name(Load)Load)))AddConstant))Assign(Name(Store)BinOp(Call(Attribute(Subscript(Name(Load)Slice(Name(Load))Load)Load)keyword(Constant))MultConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(Name(Load)Load)))AddConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Call(Attribute(Name(Load)Load)Tuple(Name(Load)Load)keyword(Constant)))keyword(Call(Attribute(Name(Load)Load)Tuple(Name(Load)Load)keyword(Constant)))))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(Name(Load)Load)))AddConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(Name(Load)Load))))Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(ConstantAddName(Load))AddCall(Attribute(Name(Load)Load)Name(Load)))AddCall(Attribute(Name(Load)Load)Name(Load)))AddName(Load)))Expr(Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)ListComp(Subscript(Call(Attribute(Name(Load)Load)Subscript(Name(Load)Slice(Name(Load))Load)Subscript(Name(Load)Tuple(Slice(Name(Load))Name(Load)Load)Load))Tuple(ConstantConstantLoad)Load)comprehension(Name(Store)Call(Name(Load)Name(Load))))))Expr(Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)ListComp(BinOp(Call(Name(Load)BinOp(Subscript(Name(Load)Tuple(Slice(Name(Load))Name(Load)Load)Load)MultSubscript(Name(Load)Name(Load)Load)))DivSubscript(Name(Load)Name(Load)Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))If(Compare(Name(Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(ConstantName(Load)Load))))AugAssign(Name(Store)DivCall(Name(Load)Name(Load)))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)keyword(Subscript(Name(Load)ConstantLoad))keyword(Tuple(Name(Load)Name(Load)Load)))MultConstant))Assign(Name(Store)BinOp(Call(Attribute(Subscript(Name(Load)Tuple(SliceAttribute(Name(Load)Load)Load)Load)Load)Name(Load))AddName(Load)))Assign(Name(Store)Call(Attribute(Subscript(Name(Load)Tuple(SliceSliceAttribute(Name(Load)Load)Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Subscript(Name(Load)Tuple(SliceSliceAttribute(Name(Load)Load)Load)Load)Load)Name(Load))))Return(Tuple(Subscript(Name(Load)Slice(Name(Load))Load)Subscript(Name(Load)Slice(Name(Load))Load)Subscript(Name(Load)Slice(Name(Load))Load)Name(Load)Load))Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Subscript(Name(Load)Name(Load)Load)Load)))Assign(Name(Store)ListComp(Attribute(Attribute(Subscript(Attribute(Name(Load)Load)Tuple(Compare(Subscript(Name(Load)Name(Load)Load)EqName(Load))Name(Load)Load)Load)Load)Load)comprehension(Name(Store)Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant))))Return(Name(Load))Subscript(Name(Load)Attribute(Name(Load)Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/43", "ground_truth": "  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 0, "lineno": 183, "function_name": "_split_array_into_list", "line_no": 183}}
{"_id": "google_lightweight_mmm/44", "text": "adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1\n  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]\n      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.", "metadata": {"task_id": "google_lightweight_mmm/44", "ground_truth": "  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 129, "lineno": 301, "function_name": "get_beta_params_from_mu_sigma", "line_no": 301}}
{"_id": "google_lightweight_mmm/45", "text": "_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/45", "ground_truth": "  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 145, "lineno": 333, "function_name": "_pmf", "line_no": 333}}
{"_id": "google_lightweight_mmm/46", "text": "], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/46", "ground_truth": "  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 164, "lineno": 357, "function_name": "distance_pior_posterior", "line_no": 357}}
{"_id": "google_lightweight_mmm/47", "text": ": Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"\n\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n\n\ndef interpolate_outliers(x: jnp.ndarray,\n                         outlier_idx: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Overwrites outliers in x with interpolated values.\n\n  Args:\n    x: The original univariate variable with outliers.\n    outlier_idx: Indices of the outliers in x.\n\n  Returns:\n    A cleaned x with outliers overwritten.\n\n  \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/47", "ground_truth": "  time_idx = jnp.arange(len(x))\n  inverse_idx = jnp.array([i for i in range(len(x)) if i not in outlier_idx])\n  interp_func = interpolate.interp1d(\n      time_idx[inverse_idx], x[inverse_idx], kind=\"linear\")\n  x = x.at[outlier_idx].set(interp_func(time_idx[outlier_idx]))\n  return x\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "context_start_lineno": 210, "lineno": 391, "function_name": "interpolate_outliers", "line_no": 391}}
{"_id": "google_lightweight_mmm/48", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/48", "ground_truth": "      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "context_start_lineno": 0, "lineno": 39, "function_name": "mock_model_function", "line_no": 39}}
{"_id": "google_lightweight_mmm/49", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Subscript(Call(Attribute(Name(Load)Load))ConstantLoad)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/49", "ground_truth": "      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "context_start_lineno": 0, "lineno": 64, "function_name": "mock_model_function", "line_no": 64}}
{"_id": "google_lightweight_mmm/50", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_carryover_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Subscript(Call(Attribute(Name(Load)Load))ConstantLoad)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Subscript(Call(Attribute(Name(Load)Load))ConstantLoad)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))))", "metadata": {"task_id": "google_lightweight_mmm/50", "ground_truth": "      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "context_start_lineno": 0, "lineno": 89, "function_name": "mock_model_function", "line_no": 89}}
{"_id": "google_lightweight_mmm/51", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for optimize_media.\"\"\"\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\nfrom lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Name(Load))))", "metadata": {"task_id": "google_lightweight_mmm/51", "ground_truth": "    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "setUpClass", "line_no": 32}}
{"_id": "google_lightweight_mmm/52", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for optimize_media.\"\"\"\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\nfrom lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n\n  def setUp(self):\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Name(Load))FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Constantkeyword(Constant)))))))", "metadata": {"task_id": "google_lightweight_mmm/52", "ground_truth": "    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "context_start_lineno": 0, "lineno": 51, "function_name": "setUp", "line_no": 51}}
{"_id": "google_lightweight_mmm/53", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantBinOp(Name(Load)AddConstant)))Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Call(Attribute(BinOp(Name(Load)MultName(Load))Load)keyword(Constant))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(List(ConstantConstantLoad)))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/53", "ground_truth": "  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "calculate_seasonality", "line_no": 48}}
{"_id": "google_lightweight_mmm/54", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantBinOp(Name(Load)AddConstant)))Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Call(Attribute(BinOp(Name(Load)MultName(Load))Load)keyword(Constant))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(List(ConstantConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load))Name(Load))Assign(Name(Store)BinOp(BinOp(Name(Load)MultName(Load))AddName(Load)))Return(Tuple(Name(Load)Name(Load)Load))Attribute(Name(Load)Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load))keyword(Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load)Load))Name(Load)Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Lambda(arguments(arg)BinOp(Name(Load)DivBinOp(ConstantDivBinOp(ConstantSubName(Load)))))Lambda(arguments(arg)Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/54", "ground_truth": "  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "adstock", "line_no": 80}}
{"_id": "google_lightweight_mmm/55", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantBinOp(Name(Load)AddConstant)))Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Call(Attribute(BinOp(Name(Load)MultName(Load))Load)keyword(Constant))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(List(ConstantConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load))Name(Load))Assign(Name(Store)BinOp(BinOp(Name(Load)MultName(Load))AddName(Load)))Return(Tuple(Name(Load)Name(Load)Load))Attribute(Name(Load)Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load))keyword(Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load)Load))Name(Load)Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Lambda(arguments(arg)BinOp(Name(Load)DivBinOp(ConstantDivBinOp(ConstantSubName(Load)))))Lambda(arguments(arg)Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Name(Load)keyword(BinOp(Name(Load)DivName(Load)))keyword(UnaryOp(USubName(Load)))))Return(Call(Attribute(Name(Load)Load)Compare(Name(Load)EqConstant)keyword(Constant)keyword(BinOp(ConstantDivBinOp(ConstantAddName(Load))))))Attribute(Name(Load)Load)Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/55", "ground_truth": "  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 112, "function_name": "hill", "line_no": 112}}
{"_id": "google_lightweight_mmm/56", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray,\n                        weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantBinOp(Name(Load)AddConstant)))Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(Name(Load)MultConstant)MultAttribute(Name(Load)Load))MultName(Load))DivName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(UnaryOp(USubConstant)))Load)keyword(UnaryOp(USubConstant))))Return(Call(Attribute(Call(Attribute(BinOp(Name(Load)MultName(Load))Load)keyword(Constant))Load)keyword(Constant)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(List(ConstantConstantLoad)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load))Name(Load))Assign(Name(Store)BinOp(BinOp(Name(Load)MultName(Load))AddName(Load)))Return(Tuple(Name(Load)Name(Load)Load))Attribute(Name(Load)Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load))keyword(Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)List(Subscript(Name(Load)Tuple(ConstantConstantLoad)Load)Load))Name(Load)Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Lambda(arguments(arg)BinOp(Name(Load)DivBinOp(ConstantDivBinOp(ConstantSubName(Load)))))Lambda(arguments(arg)Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Name(Load)keyword(BinOp(Name(Load)DivName(Load)))keyword(UnaryOp(USubName(Load)))))Return(Call(Attribute(Name(Load)Load)Compare(Name(Load)EqConstant)keyword(Constant)keyword(BinOp(ConstantDivBinOp(ConstantAddName(Load))))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubConstant))Name(Load)Load)))Return(BinOp(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)Name(Load)keyword(Constant))DivCall(Attribute(Name(Load)Load))))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant))))Assign(Name(Store)BinOp(Name(Load)PowBinOp(BinOp(Name(Load)SubName(Load))PowConstant)))Return(Call(Name(Load)Name(Load)Name(Load)Name(Load)))Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Tuple(ConstantLoad)))Attribute(Name(Load)Load)))", "metadata": {"task_id": "google_lightweight_mmm/56", "ground_truth": "  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "context_start_lineno": 0, "lineno": 158, "function_name": "carryover", "line_no": 158}}
{"_id": "google_lightweight_mmm/57", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lightweight_mmm.\"\"\"\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\n\n\nclass LightweightMmmTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Constant)keyword(Constant)keyword(Constant)))Name(Load))))", "metadata": {"task_id": "google_lightweight_mmm/57", "ground_truth": "    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "context_start_lineno": 0, "lineno": 32, "function_name": "setUpClass", "line_no": 32}}
{"_id": "google_lightweight_mmm/58", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantAttribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load))))Assign(Name(Store)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)If(Compare(Call(Name(Load)Name(Load))NotEqCall(Name(Load)Name(Load)))Assign(Name(Store)Constant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Compare(Name(Load)EqName(Load)))If(Call(Name(Load)Name(Load)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)Load))If(BoolOp(AndCall(Name(Load)GeneratorExp(Call(Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load))))Call(Name(Load)GeneratorExp(Call(Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load)))))Assign(Name(Store)Compare(Name(Load)EqName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Constant))))If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)ListComp(Call(Name(Load)Subscript(Name(Load)Name(Load)Load)Subscript(Name(Load)Name(Load)Load))comprehension(Name(Store)BinOp(Call(Attribute(Name(Load)Load))BitOrCall(Attribute(Name(Load)Load)))))))Assign(Name(Store)Compare(Name(Load)EqName(Load)))))))Return(Name(Load))Name(Load)))", "metadata": {"task_id": "google_lightweight_mmm/58", "ground_truth": "  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "_compare_equality_for_lmmm", "line_no": 94}}
{"_id": "google_lightweight_mmm/59", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jnp.DeviceArray] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantAttribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load))))Assign(Name(Store)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)If(Compare(Call(Name(Load)Name(Load))NotEqCall(Name(Load)Name(Load)))Assign(Name(Store)Constant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Compare(Name(Load)EqName(Load)))If(Call(Name(Load)Name(Load)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)Load))If(BoolOp(AndCall(Name(Load)GeneratorExp(Call(Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load))))Call(Name(Load)GeneratorExp(Call(Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load)))))Assign(Name(Store)Compare(Name(Load)EqName(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Constant))))If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)ListComp(Call(Name(Load)Subscript(Name(Load)Name(Load)Load)Subscript(Name(Load)Name(Load)Load))comprehension(Name(Store)BinOp(Call(Attribute(Name(Load)Load))BitOrCall(Attribute(Name(Load)Load)))))))Assign(Name(Store)Compare(Name(Load)EqName(Load)))))))Return(Name(Load))Name(Load))ClassDef(Name(Load)Pass)ClassDef(Expr(Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Subscript(Name(Load)Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)NotInName(Load))Raise(Call(Name(Load)Constant)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Subscript(Name(Load)Attribute(Name(Load)Load)Load))Assign(Attribute(Name(Load)Store)Call(Attribute(Attribute(Name(Load)Load)Load)Subscript(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load))))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))))", "metadata": {"task_id": "google_lightweight_mmm/59", "ground_truth": "    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 0, "lineno": 167, "function_name": "__post_init__", "line_no": 167}}
{"_id": "google_lightweight_mmm/60", "text": ",\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jnp.DeviceArray] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n\n  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/60", "ground_truth": "    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n\n    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 9, "lineno": 192, "function_name": "__eq__", "line_no": 192}}
{"_id": "google_lightweight_mmm/61", "text": "\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jnp.DeviceArray] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jnp.DeviceArray = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n\n  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:", "metadata": {"task_id": "google_lightweight_mmm/61", "ground_truth": "      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 14, "lineno": 197, "function_name": "_create_list_of_attributes_to_compare", "line_no": 197}}
{"_id": "google_lightweight_mmm/62", "text": ".\n\n    For detailed information on the selected model please refer to its\n    respective function in the models.py file.\n\n    Args:\n      media: Media input data. Media data must have either 2 dims for national\n        model or 3 for geo models.\n      media_prior: Costs of each media channel. The number of cost values must\n        be equal to the number of media channels.\n      target: Target KPI to use, like for example sales.\n      extra_features: Other variables to add to the model.\n      degrees_seasonality: Number of degrees to use for seasonality. Default is\n        2.\n      seasonality_frequency: Frequency of the time period used. Default is 52 as\n        in 52 weeks per year.\n      weekday_seasonality: In case of daily data, also estimate seven weekday\n        parameters.\n      media_names: Names of the media channels passed.\n      number_warmup: Number of warm up samples. Default is 1000.\n      number_samples: Number of samples during sampling. Default is 1000.\n      number_chains: Number of chains to sample. Default is 2.\n      target_accept_prob: Target acceptance probability for step size in the\n        NUTS sampler. Default is .85.\n      init_strategy: Initialization function for numpyro NUTS. The available\n        options can be found in\n        https://num.pyro.ai/en/stable/utilities.html#initialization-strategies.\n        Default is numpyro.infer.init_to_median.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n      seed: Seed to use for PRNGKey during training. For better replicability\n        run all different trainings with the same seed.\n    \"\"\"\n    if media.ndim not in (2, 3):\n      raise ValueError(\n          \"Media data must have either 2 dims for national model or 3 for geo \"\n          \"models.\")\n    if media.ndim == 3 and media_prior.ndim == 1:\n      media_prior = jnp.expand_dims(media_prior, axis=-1)\n\n    if media.shape[1] != len(media_prior):\n      raise ValueError(\"The number of data channels provided must match the \"\n                       \"number of cost values.\")\n    if media.min() < 0:\n      raise ValueError(\"Media values must be greater or equal to zero.\")\n\n    if custom_priors:\n      not_used_custom_priors = set(custom_priors.keys()).difference(\n          self._prior_names)\n      if not_used_custom_priors:\n        raise ValueError(\n            \"The following passed custom priors dont have a match in the model.\"\n            \" Please double check the names have been written correctly: %s\" %\n            not_used_custom_priors)\n      custom_priors = self._preprocess_custom_priors(\n          custom_priors=custom_priors)\n      geo_custom_priors = set(custom_priors.keys()).intersection(\n          models.GEO_ONLY_PRIORS)\n      if media.ndim == 2 and geo_custom_priors:\n        raise ValueError(\n            \"The given data is for national models but custom_prior contains \"\n            \"priors for the geo version of the model. Please either remove geo \"\n            \"priors for national model or pass media data with geo dimension.\")\n    else:\n      custom_priors = {}\n\n    if weekday_seasonality and seasonality_frequency == 52:\n      logging.warn(\"You have chosen daily seasonality and frequency 52 \"\n                   \"(weekly), please check you made the right seasonality \"\n                   \"choices.\")\n\n    if extra_features is not None:\n      extra_features = jnp.array(extra_features)\n\n    if seed is None:\n      seed = utils.get_time_seed()\n\n    train_media_size = media.shape[0]\n    kernel = numpyro.infer.NUTS(\n        model=self._model_function,\n        target_accept_prob=target_accept_prob,\n        init_strategy=init_strategy)\n\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel,\n        num_warmup=number_warmup,\n        num_samples=number_samples,\n        num_chains=number_chains)\n    mcmc.run(\n        rng_key=jax.random.PRNGKey(seed),\n        media_data=jnp.array(media),\n        extra_features=extra_features,\n        target_data=jnp.array(target),\n        media_prior=jnp.array(media_prior),\n        degrees_seasonality=degrees_seasonality,\n        frequency=seasonality_frequency,\n        transform_function=self._model_transform_function,\n        weekday_seasonality=weekday_seasonality,\n        custom_priors=custom_priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:\n      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains\n    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n\n  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",\n                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,\n      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.\n      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/62", "ground_truth": "    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 272, "lineno": 441, "function_name": "_predict", "line_no": 441}}
{"_id": "google_lightweight_mmm/63", "text": "priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:\n      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains\n    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n\n  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",\n                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,\n      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.\n      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n\n  def predict(\n      self,\n      media: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      media_gap: Optional[jnp.ndarray] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None,\n      seed: Optional[int] = None\n  ) -> jnp.ndarray:\n    \"\"\"Runs the model to obtain predictions for the given input data.\n\n    Predictions returned are distributions, if point estimates are desired one\n    can calculate those based on the given distribution.\n\n    Args:\n      media: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_gap: Media data gap between the end of training data and the start\n        of the out of sample media given. Eg. if 100 weeks of data were used for\n        training and prediction starts 2 months after training data finished we\n        need to provide the 8 weeks missing between the training data and the\n        prediction data so data transformations (adstock, carryover, ...) can\n        take place correctly.\n      target_scaler: Scaler that was used to scale the target before training.\n      seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n    Returns:\n      Predictions for the given media and extra features at a given date index.\n\n    Raises:\n      NotFittedModelError: When the model has not been fitted before running\n        predict.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\"Need to fit the model before running \"\n                                \"predictions.\")\n    if media_gap is not None:\n      if media.ndim != media_gap.ndim:\n        raise ValueError(\"Original media data and media gap must have the same \"\n                         \"number of dimensions.\")\n      if media.ndim > 1 and media.shape[1] != media_gap.shape[1]:\n        raise ValueError(\"Media gap must have the same numer of media channels\"\n                         \"as the original media data.\")\n      previous_media = jnp.concatenate(arrays=[self.media, media_gap], axis=0)\n      if extra_features is not None:\n        previous_extra_features = jnp.concatenate(\n            arrays=[\n                self._extra_features,\n                jnp.zeros((media_gap.shape[0], *self._extra_features.shape[1:]))\n            ],\n            axis=0)\n    else:\n      previous_media = self.media\n      previous_extra_features = self._extra_features\n\n    full_media = jnp.concatenate(arrays=[previous_media, media], axis=0)\n    if extra_features is not None:\n      full_extra_features = jnp.concatenate(\n          arrays=[previous_extra_features, extra_features], axis=0)\n    else:\n      full_extra_features = None\n    if seed is None:\n      seed = utils.get_time_seed()\n    prediction = self._predict(\n        rng_key=jax.random.PRNGKey(seed=seed),\n        media_data=full_media,\n        extra_features=full_extra_features,\n        media_prior=jnp.array(self._media_prior),\n        degrees_seasonality=self._degrees_seasonality,\n        frequency=self._seasonality_frequency,\n        weekday_seasonality=self._weekday_seasonality,\n        transform_function=self._model_transform_function,\n        model=self._model_function,\n        custom_priors=self.custom_priors,\n        posterior_samples=self.trace)[\"mu\"][:, previous_media.shape[0]:]\n    if target_scaler:\n      prediction = target_scaler.inverse_transform(prediction)\n\n    return prediction\n\n  def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n\n    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"", "metadata": {"task_id": "google_lightweight_mmm/63", "ground_truth": "    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]\n    logging.info(\"Reduction is complete\")\n", "fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "context_start_lineno": 371, "lineno": 548, "function_name": "reduce_trace", "line_no": 548}}
