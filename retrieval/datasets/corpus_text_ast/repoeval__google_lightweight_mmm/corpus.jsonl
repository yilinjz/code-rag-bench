{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_0-25", "title": "google_lightweight_mmm-setup.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Setup for lightweight_mmm value package.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef _get_readme():", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_0-35", "title": "google_lightweight_mmm-setup.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Setup for lightweight_mmm value package.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef _get_readme():\n  try:\n    readme = open(\n        os.path.join(_CURRENT_DIR, \"README.md\"), encoding=\"utf-8\").read()\n  except OSError:\n    readme = \"\"\n  return readme\n\n\ndef _get_version():\n  with open(os.path.join(_CURRENT_DIR, \"lightweight_mmm\", \"__init__.py\")) as fp:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_0-45", "title": "google_lightweight_mmm-setup.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Setup for lightweight_mmm value package.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef _get_readme():\n  try:\n    readme = open(\n        os.path.join(_CURRENT_DIR, \"README.md\"), encoding=\"utf-8\").read()\n  except OSError:\n    readme = \"\"\n  return readme\n\n\ndef _get_version():\n  with open(os.path.join(_CURRENT_DIR, \"lightweight_mmm\", \"__init__.py\")) as fp:\n    for line in fp:\n      if line.startswith(\"__version__\") and \"=\" in line:\n        version = line[line.find(\"=\") + 1:].strip(\" '\\\"\\n\")\n        if version:\n          return version\n    raise ValueError(\n        \"`__version__` not defined in `lightweight_mmm/__init__.py`\")\n\n\ndef _parse_requirements(path):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_5-55", "title": "google_lightweight_mmm-setup.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Setup for lightweight_mmm value package.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef _get_readme():\n  try:\n    readme = open(\n        os.path.join(_CURRENT_DIR, \"README.md\"), encoding=\"utf-8\").read()\n  except OSError:\n    readme = \"\"\n  return readme\n\n\ndef _get_version():\n  with open(os.path.join(_CURRENT_DIR, \"lightweight_mmm\", \"__init__.py\")) as fp:\n    for line in fp:\n      if line.startswith(\"__version__\") and \"=\" in line:\n        version = line[line.find(\"=\") + 1:].strip(\" '\\\"\\n\")\n        if version:\n          return version\n    raise ValueError(\n        \"`__version__` not defined in `lightweight_mmm/__init__.py`\")\n\n\ndef _parse_requirements(path):\n\n  with open(os.path.join(_CURRENT_DIR, path)) as f:\n    return [\n        line.rstrip()\n        for line in f\n        if not (line.isspace() or line.startswith(\"#\"))\n    ]\n\n_VERSION = _get_version()\n_README = _get_readme()\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load))))FunctionDef(argumentsTry(Assign(Name(Store)Call(Attribute(Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Constant)keyword(Constant))Load)))ExceptHandler(Name(Load)Assign(Name(Store)Constant)))Return(Name(Load)))FunctionDef(argumentsWith(withitem(Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)ConstantConstant))Name(Store))For(Name(Store)Name(Load)If(BoolOp(AndCall(Attribute(Name(Load)Load)Constant)Compare(ConstantInName(Load)))Assign(Name(Store)Call(Attribute(Subscript(Name(Load)Slice(BinOp(Call(Attribute(Name(Load)Load)Constant)AddConstant))Load)Load)Constant))If(Name(Load)Return(Name(Load)))))Raise(Call(Name(Load)Constant))))FunctionDef(arguments(arg)With(withitem(Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Name(Load)))Name(Store))Return(ListComp(Call(Attribute(Name(Load)Load))comprehension(Name(Store)Name(Load)UnaryOp(NotBoolOp(OrCall(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load)Constant))))))))Assign(Name(Store)Call(Name(Load)))Assign(Name(Store)Call(Name(Load))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_15-65", "title": "google_lightweight_mmm-setup.py", "text": "\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef _get_readme():\n  try:\n    readme = open(\n        os.path.join(_CURRENT_DIR, \"README.md\"), encoding=\"utf-8\").read()\n  except OSError:\n    readme = \"\"\n  return readme\n\n\ndef _get_version():\n  with open(os.path.join(_CURRENT_DIR, \"lightweight_mmm\", \"__init__.py\")) as fp:\n    for line in fp:\n      if line.startswith(\"__version__\") and \"=\" in line:\n        version = line[line.find(\"=\") + 1:].strip(\" '\\\"\\n\")\n        if version:\n          return version\n    raise ValueError(\n        \"`__version__` not defined in `lightweight_mmm/__init__.py`\")\n\n\ndef _parse_requirements(path):\n\n  with open(os.path.join(_CURRENT_DIR, path)) as f:\n    return [\n        line.rstrip()\n        for line in f\n        if not (line.isspace() or line.startswith(\"#\"))\n    ]\n\n_VERSION = _get_version()\n_README = _get_readme()\n_INSTALL_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements.txt\"))\n_TEST_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements_tests.txt\"))\n\nsetup(\n    name=\"lightweight_mmm\",\n    version=_VERSION,\n    description=\"Package for Media-Mix-Modelling\",\n    long_description=\"\\n\".join([_README]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_25-75", "title": "google_lightweight_mmm-setup.py", "text": "  try:\n    readme = open(\n        os.path.join(_CURRENT_DIR, \"README.md\"), encoding=\"utf-8\").read()\n  except OSError:\n    readme = \"\"\n  return readme\n\n\ndef _get_version():\n  with open(os.path.join(_CURRENT_DIR, \"lightweight_mmm\", \"__init__.py\")) as fp:\n    for line in fp:\n      if line.startswith(\"__version__\") and \"=\" in line:\n        version = line[line.find(\"=\") + 1:].strip(\" '\\\"\\n\")\n        if version:\n          return version\n    raise ValueError(\n        \"`__version__` not defined in `lightweight_mmm/__init__.py`\")\n\n\ndef _parse_requirements(path):\n\n  with open(os.path.join(_CURRENT_DIR, path)) as f:\n    return [\n        line.rstrip()\n        for line in f\n        if not (line.isspace() or line.startswith(\"#\"))\n    ]\n\n_VERSION = _get_version()\n_README = _get_readme()\n_INSTALL_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements.txt\"))\n_TEST_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements_tests.txt\"))\n\nsetup(\n    name=\"lightweight_mmm\",\n    version=_VERSION,\n    description=\"Package for Media-Mix-Modelling\",\n    long_description=\"\\n\".join([_README]),\n    long_description_content_type=\"text/markdown\",\n    author=\"Google LLC\",\n    author_email=\"no-reply@google.com\",\n    license=\"Apache 2.0\",\n    packages=find_packages(),\n    install_requires=_INSTALL_REQUIREMENTS,\n    tests_require=_TEST_REQUIREMENTS,\n    url=\"https://github.com/google/lightweight_mmm\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_35-85", "title": "google_lightweight_mmm-setup.py", "text": "    for line in fp:\n      if line.startswith(\"__version__\") and \"=\" in line:\n        version = line[line.find(\"=\") + 1:].strip(\" '\\\"\\n\")\n        if version:\n          return version\n    raise ValueError(\n        \"`__version__` not defined in `lightweight_mmm/__init__.py`\")\n\n\ndef _parse_requirements(path):\n\n  with open(os.path.join(_CURRENT_DIR, path)) as f:\n    return [\n        line.rstrip()\n        for line in f\n        if not (line.isspace() or line.startswith(\"#\"))\n    ]\n\n_VERSION = _get_version()\n_README = _get_readme()\n_INSTALL_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements.txt\"))\n_TEST_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements_tests.txt\"))\n\nsetup(\n    name=\"lightweight_mmm\",\n    version=_VERSION,\n    description=\"Package for Media-Mix-Modelling\",\n    long_description=\"\\n\".join([_README]),\n    long_description_content_type=\"text/markdown\",\n    author=\"Google LLC\",\n    author_email=\"no-reply@google.com\",\n    license=\"Apache 2.0\",\n    packages=find_packages(),\n    install_requires=_INSTALL_REQUIREMENTS,\n    tests_require=_TEST_REQUIREMENTS,\n    url=\"https://github.com/google/lightweight_mmm\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n\n    ],\n)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_45-85", "title": "google_lightweight_mmm-setup.py", "text": "\n  with open(os.path.join(_CURRENT_DIR, path)) as f:\n    return [\n        line.rstrip()\n        for line in f\n        if not (line.isspace() or line.startswith(\"#\"))\n    ]\n\n_VERSION = _get_version()\n_README = _get_readme()\n_INSTALL_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements.txt\"))\n_TEST_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements_tests.txt\"))\n\nsetup(\n    name=\"lightweight_mmm\",\n    version=_VERSION,\n    description=\"Package for Media-Mix-Modelling\",\n    long_description=\"\\n\".join([_README]),\n    long_description_content_type=\"text/markdown\",\n    author=\"Google LLC\",\n    author_email=\"no-reply@google.com\",\n    license=\"Apache 2.0\",\n    packages=find_packages(),\n    install_requires=_INSTALL_REQUIREMENTS,\n    tests_require=_TEST_REQUIREMENTS,\n    url=\"https://github.com/google/lightweight_mmm\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n\n    ],\n)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-setup.py_55-85", "title": "google_lightweight_mmm-setup.py", "text": "_INSTALL_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements.txt\"))\n_TEST_REQUIREMENTS = _parse_requirements(os.path.join(\n    _CURRENT_DIR, \"requirements\", \"requirements_tests.txt\"))\n\nsetup(\n    name=\"lightweight_mmm\",\n    version=_VERSION,\n    description=\"Package for Media-Mix-Modelling\",\n    long_description=\"\\n\".join([_README]),\n    long_description_content_type=\"text/markdown\",\n    author=\"Google LLC\",\n    author_email=\"no-reply@google.com\",\n    license=\"Apache 2.0\",\n    packages=find_packages(),\n    install_requires=_INSTALL_REQUIREMENTS,\n    tests_require=_TEST_REQUIREMENTS,\n    url=\"https://github.com/google/lightweight_mmm\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n\n    ],\n)\n\nAST=Module(Assign(Name(Store)Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)ConstantConstant)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)ConstantConstant)))Expr(Call(Name(Load)keyword(Constant)keyword(Name(Load))keyword(Constant)keyword(Call(Attribute(ConstantLoad)List(Name(Load)Load)))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Call(Name(Load)))keyword(Name(Load))keyword(Name(Load))keyword(Constant)keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantLoad)))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "setup.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_0-25", "title": "google_lightweight_mmm-docs-conf.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\n\n\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n\nAST=Module(Expr(Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_0-35", "title": "google_lightweight_mmm-docs-conf.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\n\n\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)ConstantCall(Attribute(Attribute(Name(Load)Load)Load)Constant))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_0-45", "title": "google_lightweight_mmm-docs-conf.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\n\n\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'LightweightMMM'\ncopyright = '2022, The LightweightMMM authors'\nauthor = 'The LightweightMMM authors'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)ConstantCall(Attribute(Attribute(Name(Load)Load)Load)Constant)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_5-55", "title": "google_lightweight_mmm-docs-conf.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\n\n\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'LightweightMMM'\ncopyright = '2022, The LightweightMMM authors'\nauthor = 'The LightweightMMM authors'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.autosectionlabel',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_15-65", "title": "google_lightweight_mmm-docs-conf.py", "text": "\n\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'LightweightMMM'\ncopyright = '2022, The LightweightMMM authors'\nauthor = 'The LightweightMMM authors'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.autosectionlabel',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'nbsphinx',\n    'myst_nb',  # This is used for the .ipynb notebooks\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n\nAST=Module(Import(alias)Import(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)ConstantCall(Attribute(Attribute(Name(Load)Load)Load)Constant)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))Assign(Name(Store)List(ConstantLoad)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_25-75", "title": "google_lightweight_mmm-docs-conf.py", "text": "# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('..'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'LightweightMMM'\ncopyright = '2022, The LightweightMMM authors'\nauthor = 'The LightweightMMM authors'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.autosectionlabel',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'nbsphinx',\n    'myst_nb',  # This is used for the .ipynb notebooks\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\nsource_suffix = ['.rst', '.md', '.ipynb']\n\nautosummary_generate = True\n\nmaster_doc = 'index'\n\n# -- Options for HTML output -------------------------------------------------\n\nAST=Module(Import(alias)Import(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)ConstantCall(Attribute(Attribute(Name(Load)Load)Load)Constant)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))Assign(Name(Store)List(ConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_35-85", "title": "google_lightweight_mmm-docs-conf.py", "text": "# -- Project information -----------------------------------------------------\n\nproject = 'LightweightMMM'\ncopyright = '2022, The LightweightMMM authors'\nauthor = 'The LightweightMMM authors'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.autosectionlabel',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'nbsphinx',\n    'myst_nb',  # This is used for the .ipynb notebooks\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\nsource_suffix = ['.rst', '.md', '.ipynb']\n\nautosummary_generate = True\n\nmaster_doc = 'index'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = []\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n\nAST=Module(Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))Assign(Name(Store)List(ConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)List(Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_45-95", "title": "google_lightweight_mmm-docs-conf.py", "text": "# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.autosectionlabel',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'nbsphinx',\n    'myst_nb',  # This is used for the .ipynb notebooks\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\nsource_suffix = ['.rst', '.md', '.ipynb']\n\nautosummary_generate = True\n\nmaster_doc = 'index'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = []\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\nnbsphinx_codecell_lexer = 'ipython3'\nnbsphinx_execute = 'never'\njupyter_execute_notebooks = 'off'\n\n# -- Extension configuration -------------------------------------------------\n\n# Tell sphinx-autodoc-typehints to generate stub parameter annotations including\n# types, even if the parameters aren't explicitly documented.\n\nAST=Module(Assign(Name(Store)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))Assign(Name(Store)List(ConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)List(Load))Assign(Name(Store)List(ConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_55-96", "title": "google_lightweight_mmm-docs-conf.py", "text": "    'sphinx.ext.viewcode',\n    'nbsphinx',\n    'myst_nb',  # This is used for the .ipynb notebooks\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\nsource_suffix = ['.rst', '.md', '.ipynb']\n\nautosummary_generate = True\n\nmaster_doc = 'index'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = []\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\nnbsphinx_codecell_lexer = 'ipython3'\nnbsphinx_execute = 'never'\njupyter_execute_notebooks = 'off'\n\n# -- Extension configuration -------------------------------------------------\n\n# Tell sphinx-autodoc-typehints to generate stub parameter annotations including\n# types, even if the parameters aren't explicitly documented.\nalways_document_param_types = True", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 96, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-docs-conf.py_65-96", "title": "google_lightweight_mmm-docs-conf.py", "text": "# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\nsource_suffix = ['.rst', '.md', '.ipynb']\n\nautosummary_generate = True\n\nmaster_doc = 'index'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = []\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\nnbsphinx_codecell_lexer = 'ipython3'\nnbsphinx_execute = 'never'\njupyter_execute_notebooks = 'off'\n\n# -- Extension configuration -------------------------------------------------\n\n# Tell sphinx-autodoc-typehints to generate stub parameter annotations including\n# types, even if the parameters aren't explicitly documented.\nalways_document_param_types = True\n\nAST=Module(Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)List(ConstantConstantConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)List(Load))Assign(Name(Store)List(ConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "docs", "conf.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 96, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-conftest.py_0-21", "title": "google_lightweight_mmm-lightweight_mmm-conftest.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Parse absl flags for when it is run by github actions by pytest.\"\"\"\n\nfrom absl import flags\n\n\ndef pytest_configure(config):\n  flags.FLAGS.mark_as_parsed()\n\nAST=Module(Expr(Constant)ImportFrom(alias)FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "conftest.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 21, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "conftest.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 21, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "conftest.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 21, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\n\nAST=Module(Expr(Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A simple and lightweight library for Media Mix Modelling.\n\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "\nSimple usage of this class goes as following:\n\n```\nmmm = lightweight_mmm.LightweightMMM()\nmmm.fit(media=media_data,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_samples=1000,\n        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        number_chains=2)\n\n# For obtaining media contribution percentage and ROI\npredictions, media_contribution_hat_pct, roi_hat = mmm.get_posterior_metrics()\n\n# For running predictions on unseen data\nmmm.predict(media=media_data_test, extra_features=extra_features_test)\n```\n\"\"\"\n\nimport collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "import collections\nimport dataclasses\nimport functools\nimport itertools\nimport logging\nimport numbers\nfrom typing import Any, Callable, Dict, Mapping, MutableMapping, Optional, Sequence, Tuple, Union\n\nfrom absl import logging\nimport immutabledict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import infer\n\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n_NAMES_TO_MODEL_TRANSFORMS = immutabledict.immutabledict({\n    \"hill_adstock\": models.transform_hill_adstock,\n    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    \"adstock\": models.transform_adstock,\n    \"carryover\": models.transform_carryover\n})\n_MODEL_FUNCTION = models.media_mix_model\n\n\ndef _compare_equality_for_lmmm(item_1: Any, item_2: Any) -> bool:\n  \"\"\"Compares two items for equality.\n\n  Helper function for the __eq__ method of LightweightmMM. First checks if items\n  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "  are strings or lists of strings (it's okay if empty lists compare True), then\n  uses jnp.array_equal if the items are jax.numpy.DeviceArray or other related\n  sequences, and uses items' __eq__ otherwise.\n\n  Note: this implementation does not cover every possible data structure, but\n  it does cover all the data structures seen in attributes used by\n  LightweightMMM. Sometimes the DeviceArray is hidden in the value of a\n  MutableMapping, hence the recursion.\n\n  Args:\n    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    item_1: First item to be compared.\n    item_2: Second item to be compared.\n\n  Returns:\n    Boolean for whether item_1 equals item_2.\n  \"\"\"\n\n  # This is pretty strict but LMMM classes don't need to compare equal unless\n  # they are exact copies.\n  if type(item_1) != type(item_2):\n    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    is_equal = False\n  elif isinstance(item_1, str):\n    is_equal = item_1 == item_2\n  elif isinstance(item_1, (jax.Array, np.ndarray, Sequence)):\n    if all(isinstance(x, str) for x in item_1) and all(\n        isinstance(x, str) for x in item_2):\n      is_equal = item_1 == item_2\n    else:\n      is_equal = np.array_equal(item_1, item_2, equal_nan=True)\n  elif isinstance(item_1, MutableMapping):\n    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    is_equal = all([\n        _compare_equality_for_lmmm(item_1[x], item_2[x])\n        for x in item_1.keys() | item_2.keys()\n    ])\n  else:\n    is_equal = item_1 == item_2\n\n  return is_equal\n\n\nclass NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jax.Array] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "class NotFittedModelError(Exception):\n  pass\n\n\n@dataclasses.dataclass(unsafe_hash=True, eq=False)\nclass LightweightMMM:\n  \"\"\"Lightweight Media Mix Modelling wrapper for bayesian models.\n\n  The currently available models are the following:\n   - hill_adstock\n   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jax.Array] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\nAST=Module(ClassDef(Name(Load)Pass)ClassDef(Expr(Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Subscript(Name(Load)Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))AnnAssign(Name(Store)Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "   - adstock\n   - carryover\n\n  It also offers the necessary utilities for calculating media contribution and\n  media ROI based on models' results.\n\n  Attributes:\n    trace: Sampling trace of the bayesian model once fitted.\n    n_media_channels: Number of media channels the model was trained with.\n    n_geos: Number of geos for geo models or 1 for national models.\n    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jax.Array] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    model_name: Name of the model.\n    media: The media data the model is trained on. Usefull for a variety of\n      insights post model fitting.\n    media_names: Names of the media channels passed at fitting time.\n    custom_priors: The set of custom priors the model was trained with. An empty\n      dictionary if none were passed.\n  \"\"\"\n  model_name: str = \"hill_adstock\"\n  n_media_channels: int = dataclasses.field(init=False, repr=False)\n  n_geos: int = dataclasses.field(init=False, repr=False)\n  media: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jax.Array] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n\n  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "  media: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  media_names: Sequence[str] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  trace: Dict[str, jax.Array] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n  custom_priors: MutableMapping[str, Prior] = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _degrees_seasonality: int = dataclasses.field(init=False, repr=False)\n  _weekday_seasonality: bool = dataclasses.field(init=False, repr=False)\n  _media_prior: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n\n  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "  _media_prior: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _extra_features: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _target: jax.Array = dataclasses.field(\n      init=False, repr=False, hash=False, compare=True)\n  _train_media_size: int = dataclasses.field(\n      init=False, repr=False, hash=True, compare=False)\n  _mcmc: numpyro.infer.MCMC = dataclasses.field(\n      init=False, repr=False, hash=False, compare=False)\n\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n\n  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "\n  def __post_init__(self):\n    if self.model_name not in _NAMES_TO_MODEL_TRANSFORMS:\n      raise ValueError(\"Model name passed not valid. Please use any of the\"\n                       \"following: 'hill_adstock', 'adstock', 'carryover'.\")\n    self._model_function = _MODEL_FUNCTION\n    self._model_transform_function = _NAMES_TO_MODEL_TRANSFORMS[self.model_name]\n    self._prior_names = models.MODEL_PRIORS_NAMES.union(\n        models.TRANSFORM_PRIORS_NAMES[self.model_name])\n\n  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n\n    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n\n  def _preprocess_custom_priors(\n      self,\n      custom_priors: Dict[str, Prior]) -> MutableMapping[str, Prior]:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "  def __eq__(self, other: Any) -> bool:\n    \"\"\"Equality method for LightweightMMMM.\n\n    We need a special method here to handle a couple of issues. First, some of\n    the attributes for LightweightMMM are arrays, which contain multiple values\n    and cannot be evaluated with the default __eq__ method. Second, some\n    attributes are initially undefined and only get values after fitting a\n    model. The latter is dealt with within this function, and the former within\n    the helper function _compare_equality_for_lmmm().\n\n    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n\n    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n\n  def _preprocess_custom_priors(\n      self,\n      custom_priors: Dict[str, Prior]) -> MutableMapping[str, Prior]:\n    \"\"\"Preprocesses the user input custom priors to Numpyro distributions.\n\n    If numpyro distributions are given they remains untouched, however if any\n    other option is passed, it is passed to the default distribution to alter\n    its constructor values.\n\n    Args:\n      custom_priors: Mapping of the name of the prior to its custom value.\n\n    Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    Args:\n      other: Dataclass to compare against.\n\n    Returns:\n      Boolean for whether self == other; NotImplemented if other is not a\n      LightweightMMM.\n    \"\"\"\n    if not isinstance(other, LightweightMMM):\n      return NotImplemented\n\n    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n\n    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n\n  def _preprocess_custom_priors(\n      self,\n      custom_priors: Dict[str, Prior]) -> MutableMapping[str, Prior]:\n    \"\"\"Preprocesses the user input custom priors to Numpyro distributions.\n\n    If numpyro distributions are given they remains untouched, however if any\n    other option is passed, it is passed to the default distribution to alter\n    its constructor values.\n\n    Args:\n      custom_priors: Mapping of the name of the prior to its custom value.\n\n    Returns:\n      A mapping of names to numpyro distributions based on user input and\n        default values.\n    \"\"\"\n    default_priors = {\n        **models._get_default_priors(),\n        **models._get_transform_default_priors()[self.model_name]\n    }\n    # Checking that the key is contained in custom_priors has already been done\n    # at this point in the fit function.\n    for prior_name in custom_priors:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    def _create_list_of_attributes_to_compare(\n        mmm_instance: Any) -> Sequence[str]:\n      all_attributes_that_can_be_compared = sorted(\n          [x.name for x in dataclasses.fields(mmm_instance) if x.compare])\n      attributes_which_have_been_instantiated = [\n          x for x in all_attributes_that_can_be_compared\n          if hasattr(mmm_instance, x)\n      ]\n      return attributes_which_have_been_instantiated\n\n    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n\n  def _preprocess_custom_priors(\n      self,\n      custom_priors: Dict[str, Prior]) -> MutableMapping[str, Prior]:\n    \"\"\"Preprocesses the user input custom priors to Numpyro distributions.\n\n    If numpyro distributions are given they remains untouched, however if any\n    other option is passed, it is passed to the default distribution to alter\n    its constructor values.\n\n    Args:\n      custom_priors: Mapping of the name of the prior to its custom value.\n\n    Returns:\n      A mapping of names to numpyro distributions based on user input and\n        default values.\n    \"\"\"\n    default_priors = {\n        **models._get_default_priors(),\n        **models._get_transform_default_priors()[self.model_name]\n    }\n    # Checking that the key is contained in custom_priors has already been done\n    # at this point in the fit function.\n    for prior_name in custom_priors:\n      if isinstance(custom_priors[prior_name], numbers.Number):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            custom_priors[prior_name])\n      elif (isinstance(custom_priors[prior_name], collections.abc.Sequence) and\n            not isinstance(custom_priors[prior_name], str)):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            *custom_priors[prior_name])\n      elif isinstance(custom_priors[prior_name], dict):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            **custom_priors[prior_name])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    self_attributes = _create_list_of_attributes_to_compare(self)\n    other_attributes = _create_list_of_attributes_to_compare(other)\n\n    return all(\n        _compare_equality_for_lmmm(getattr(self, a1), getattr(other, a2))\n        for a1, a2 in itertools.zip_longest(self_attributes, other_attributes))\n\n  def _preprocess_custom_priors(\n      self,\n      custom_priors: Dict[str, Prior]) -> MutableMapping[str, Prior]:\n    \"\"\"Preprocesses the user input custom priors to Numpyro distributions.\n\n    If numpyro distributions are given they remains untouched, however if any\n    other option is passed, it is passed to the default distribution to alter\n    its constructor values.\n\n    Args:\n      custom_priors: Mapping of the name of the prior to its custom value.\n\n    Returns:\n      A mapping of names to numpyro distributions based on user input and\n        default values.\n    \"\"\"\n    default_priors = {\n        **models._get_default_priors(),\n        **models._get_transform_default_priors()[self.model_name]\n    }\n    # Checking that the key is contained in custom_priors has already been done\n    # at this point in the fit function.\n    for prior_name in custom_priors:\n      if isinstance(custom_priors[prior_name], numbers.Number):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            custom_priors[prior_name])\n      elif (isinstance(custom_priors[prior_name], collections.abc.Sequence) and\n            not isinstance(custom_priors[prior_name], str)):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            *custom_priors[prior_name])\n      elif isinstance(custom_priors[prior_name], dict):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            **custom_priors[prior_name])\n      elif not isinstance(custom_priors[prior_name], dist.Distribution):\n        raise ValueError(\n            \"Priors given must be a Numpyro distribution or one of the \"\n            \"following to fit in the constructor of our default Numpyro \"\n            \"distribution. It could be given as args or kwargs as long as it \"\n            \"is the correct format for such object. Please refer to our \"\n            \"documentation on custom priors to know more.\")\n    return custom_priors\n\n  def fit(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    \"\"\"Preprocesses the user input custom priors to Numpyro distributions.\n\n    If numpyro distributions are given they remains untouched, however if any\n    other option is passed, it is passed to the default distribution to alter\n    its constructor values.\n\n    Args:\n      custom_priors: Mapping of the name of the prior to its custom value.\n\n    Returns:\n      A mapping of names to numpyro distributions based on user input and\n        default values.\n    \"\"\"\n    default_priors = {\n        **models._get_default_priors(),\n        **models._get_transform_default_priors()[self.model_name]\n    }\n    # Checking that the key is contained in custom_priors has already been done\n    # at this point in the fit function.\n    for prior_name in custom_priors:\n      if isinstance(custom_priors[prior_name], numbers.Number):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            custom_priors[prior_name])\n      elif (isinstance(custom_priors[prior_name], collections.abc.Sequence) and\n            not isinstance(custom_priors[prior_name], str)):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            *custom_priors[prior_name])\n      elif isinstance(custom_priors[prior_name], dict):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            **custom_priors[prior_name])\n      elif not isinstance(custom_priors[prior_name], dist.Distribution):\n        raise ValueError(\n            \"Priors given must be a Numpyro distribution or one of the \"\n            \"following to fit in the constructor of our default Numpyro \"\n            \"distribution. It could be given as args or kwargs as long as it \"\n            \"is the correct format for such object. Please refer to our \"\n            \"documentation on custom priors to know more.\")\n    return custom_priors\n\n  def fit(\n      self,\n      media: jnp.ndarray,\n      media_prior: jnp.ndarray,\n      target: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      degrees_seasonality: int = 2,\n      seasonality_frequency: int = 52,\n      weekday_seasonality: bool = False,\n      media_names: Optional[Sequence[str]] = None,\n      number_warmup: int = 1000,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      A mapping of names to numpyro distributions based on user input and\n        default values.\n    \"\"\"\n    default_priors = {\n        **models._get_default_priors(),\n        **models._get_transform_default_priors()[self.model_name]\n    }\n    # Checking that the key is contained in custom_priors has already been done\n    # at this point in the fit function.\n    for prior_name in custom_priors:\n      if isinstance(custom_priors[prior_name], numbers.Number):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            custom_priors[prior_name])\n      elif (isinstance(custom_priors[prior_name], collections.abc.Sequence) and\n            not isinstance(custom_priors[prior_name], str)):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            *custom_priors[prior_name])\n      elif isinstance(custom_priors[prior_name], dict):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            **custom_priors[prior_name])\n      elif not isinstance(custom_priors[prior_name], dist.Distribution):\n        raise ValueError(\n            \"Priors given must be a Numpyro distribution or one of the \"\n            \"following to fit in the constructor of our default Numpyro \"\n            \"distribution. It could be given as args or kwargs as long as it \"\n            \"is the correct format for such object. Please refer to our \"\n            \"documentation on custom priors to know more.\")\n    return custom_priors\n\n  def fit(\n      self,\n      media: jnp.ndarray,\n      media_prior: jnp.ndarray,\n      target: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      degrees_seasonality: int = 2,\n      seasonality_frequency: int = 52,\n      weekday_seasonality: bool = False,\n      media_names: Optional[Sequence[str]] = None,\n      number_warmup: int = 1000,\n      number_samples: int = 1000,\n      number_chains: int = 2,\n      target_accept_prob: float = .85,\n      init_strategy: Callable[[Mapping[Any, Any], Any],\n                              jnp.ndarray] = numpyro.infer.init_to_median,\n      custom_priors: Optional[Dict[str, Prior]] = None,\n      seed: Optional[int] = None) -> None:\n    \"\"\"Fits MMM given the media data, extra features, costs and sales/KPI.\n\n    For detailed information on the selected model please refer to its", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      if isinstance(custom_priors[prior_name], numbers.Number):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            custom_priors[prior_name])\n      elif (isinstance(custom_priors[prior_name], collections.abc.Sequence) and\n            not isinstance(custom_priors[prior_name], str)):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            *custom_priors[prior_name])\n      elif isinstance(custom_priors[prior_name], dict):\n        custom_priors[prior_name] = default_priors[prior_name].__class__(\n            **custom_priors[prior_name])\n      elif not isinstance(custom_priors[prior_name], dist.Distribution):\n        raise ValueError(\n            \"Priors given must be a Numpyro distribution or one of the \"\n            \"following to fit in the constructor of our default Numpyro \"\n            \"distribution. It could be given as args or kwargs as long as it \"\n            \"is the correct format for such object. Please refer to our \"\n            \"documentation on custom priors to know more.\")\n    return custom_priors\n\n  def fit(\n      self,\n      media: jnp.ndarray,\n      media_prior: jnp.ndarray,\n      target: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      degrees_seasonality: int = 2,\n      seasonality_frequency: int = 52,\n      weekday_seasonality: bool = False,\n      media_names: Optional[Sequence[str]] = None,\n      number_warmup: int = 1000,\n      number_samples: int = 1000,\n      number_chains: int = 2,\n      target_accept_prob: float = .85,\n      init_strategy: Callable[[Mapping[Any, Any], Any],\n                              jnp.ndarray] = numpyro.infer.init_to_median,\n      custom_priors: Optional[Dict[str, Prior]] = None,\n      seed: Optional[int] = None) -> None:\n    \"\"\"Fits MMM given the media data, extra features, costs and sales/KPI.\n\n    For detailed information on the selected model please refer to its\n    respective function in the models.py file.\n\n    Args:\n      media: Media input data. Media data must have either 2 dims for national\n        model or 3 for geo models.\n      media_prior: Costs of each media channel. The number of cost values must\n        be equal to the number of media channels.\n      target: Target KPI to use, like for example sales.\n      extra_features: Other variables to add to the model.\n      degrees_seasonality: Number of degrees to use for seasonality. Default is", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      elif not isinstance(custom_priors[prior_name], dist.Distribution):\n        raise ValueError(\n            \"Priors given must be a Numpyro distribution or one of the \"\n            \"following to fit in the constructor of our default Numpyro \"\n            \"distribution. It could be given as args or kwargs as long as it \"\n            \"is the correct format for such object. Please refer to our \"\n            \"documentation on custom priors to know more.\")\n    return custom_priors\n\n  def fit(\n      self,\n      media: jnp.ndarray,\n      media_prior: jnp.ndarray,\n      target: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      degrees_seasonality: int = 2,\n      seasonality_frequency: int = 52,\n      weekday_seasonality: bool = False,\n      media_names: Optional[Sequence[str]] = None,\n      number_warmup: int = 1000,\n      number_samples: int = 1000,\n      number_chains: int = 2,\n      target_accept_prob: float = .85,\n      init_strategy: Callable[[Mapping[Any, Any], Any],\n                              jnp.ndarray] = numpyro.infer.init_to_median,\n      custom_priors: Optional[Dict[str, Prior]] = None,\n      seed: Optional[int] = None) -> None:\n    \"\"\"Fits MMM given the media data, extra features, costs and sales/KPI.\n\n    For detailed information on the selected model please refer to its\n    respective function in the models.py file.\n\n    Args:\n      media: Media input data. Media data must have either 2 dims for national\n        model or 3 for geo models.\n      media_prior: Costs of each media channel. The number of cost values must\n        be equal to the number of media channels.\n      target: Target KPI to use, like for example sales.\n      extra_features: Other variables to add to the model.\n      degrees_seasonality: Number of degrees to use for seasonality. Default is\n        2.\n      seasonality_frequency: Frequency of the time period used. Default is 52 as\n        in 52 weeks per year.\n      weekday_seasonality: In case of daily data, also estimate seven weekday\n        parameters.\n      media_names: Names of the media channels passed.\n      number_warmup: Number of warm up samples. Default is 1000.\n      number_samples: Number of samples during sampling. Default is 1000.\n      number_chains: Number of chains to sample. Default is 2.\n      target_accept_prob: Target acceptance probability for step size in the", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      self,\n      media: jnp.ndarray,\n      media_prior: jnp.ndarray,\n      target: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      degrees_seasonality: int = 2,\n      seasonality_frequency: int = 52,\n      weekday_seasonality: bool = False,\n      media_names: Optional[Sequence[str]] = None,\n      number_warmup: int = 1000,\n      number_samples: int = 1000,\n      number_chains: int = 2,\n      target_accept_prob: float = .85,\n      init_strategy: Callable[[Mapping[Any, Any], Any],\n                              jnp.ndarray] = numpyro.infer.init_to_median,\n      custom_priors: Optional[Dict[str, Prior]] = None,\n      seed: Optional[int] = None) -> None:\n    \"\"\"Fits MMM given the media data, extra features, costs and sales/KPI.\n\n    For detailed information on the selected model please refer to its\n    respective function in the models.py file.\n\n    Args:\n      media: Media input data. Media data must have either 2 dims for national\n        model or 3 for geo models.\n      media_prior: Costs of each media channel. The number of cost values must\n        be equal to the number of media channels.\n      target: Target KPI to use, like for example sales.\n      extra_features: Other variables to add to the model.\n      degrees_seasonality: Number of degrees to use for seasonality. Default is\n        2.\n      seasonality_frequency: Frequency of the time period used. Default is 52 as\n        in 52 weeks per year.\n      weekday_seasonality: In case of daily data, also estimate seven weekday\n        parameters.\n      media_names: Names of the media channels passed.\n      number_warmup: Number of warm up samples. Default is 1000.\n      number_samples: Number of samples during sampling. Default is 1000.\n      number_chains: Number of chains to sample. Default is 2.\n      target_accept_prob: Target acceptance probability for step size in the\n        NUTS sampler. Default is .85.\n      init_strategy: Initialization function for numpyro NUTS. The available\n        options can be found in\n        https://num.pyro.ai/en/stable/utilities.html#initialization-strategies.\n        Default is numpyro.infer.init_to_median.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n      seed: Seed to use for PRNGKey during training. For better replicability\n        run all different trainings with the same seed.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      number_samples: int = 1000,\n      number_chains: int = 2,\n      target_accept_prob: float = .85,\n      init_strategy: Callable[[Mapping[Any, Any], Any],\n                              jnp.ndarray] = numpyro.infer.init_to_median,\n      custom_priors: Optional[Dict[str, Prior]] = None,\n      seed: Optional[int] = None) -> None:\n    \"\"\"Fits MMM given the media data, extra features, costs and sales/KPI.\n\n    For detailed information on the selected model please refer to its\n    respective function in the models.py file.\n\n    Args:\n      media: Media input data. Media data must have either 2 dims for national\n        model or 3 for geo models.\n      media_prior: Costs of each media channel. The number of cost values must\n        be equal to the number of media channels.\n      target: Target KPI to use, like for example sales.\n      extra_features: Other variables to add to the model.\n      degrees_seasonality: Number of degrees to use for seasonality. Default is\n        2.\n      seasonality_frequency: Frequency of the time period used. Default is 52 as\n        in 52 weeks per year.\n      weekday_seasonality: In case of daily data, also estimate seven weekday\n        parameters.\n      media_names: Names of the media channels passed.\n      number_warmup: Number of warm up samples. Default is 1000.\n      number_samples: Number of samples during sampling. Default is 1000.\n      number_chains: Number of chains to sample. Default is 2.\n      target_accept_prob: Target acceptance probability for step size in the\n        NUTS sampler. Default is .85.\n      init_strategy: Initialization function for numpyro NUTS. The available\n        options can be found in\n        https://num.pyro.ai/en/stable/utilities.html#initialization-strategies.\n        Default is numpyro.infer.init_to_median.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n      seed: Seed to use for PRNGKey during training. For better replicability\n        run all different trainings with the same seed.\n    \"\"\"\n    if media.ndim not in (2, 3):\n      raise ValueError(\n          \"Media data must have either 2 dims for national model or 3 for geo \"\n          \"models.\")\n    if media.ndim == 3 and media_prior.ndim == 1:\n      media_prior = jnp.expand_dims(media_prior, axis=-1)\n\n    if media.shape[1] != len(media_prior):\n      raise ValueError(\"The number of data channels provided must match the \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    respective function in the models.py file.\n\n    Args:\n      media: Media input data. Media data must have either 2 dims for national\n        model or 3 for geo models.\n      media_prior: Costs of each media channel. The number of cost values must\n        be equal to the number of media channels.\n      target: Target KPI to use, like for example sales.\n      extra_features: Other variables to add to the model.\n      degrees_seasonality: Number of degrees to use for seasonality. Default is\n        2.\n      seasonality_frequency: Frequency of the time period used. Default is 52 as\n        in 52 weeks per year.\n      weekday_seasonality: In case of daily data, also estimate seven weekday\n        parameters.\n      media_names: Names of the media channels passed.\n      number_warmup: Number of warm up samples. Default is 1000.\n      number_samples: Number of samples during sampling. Default is 1000.\n      number_chains: Number of chains to sample. Default is 2.\n      target_accept_prob: Target acceptance probability for step size in the\n        NUTS sampler. Default is .85.\n      init_strategy: Initialization function for numpyro NUTS. The available\n        options can be found in\n        https://num.pyro.ai/en/stable/utilities.html#initialization-strategies.\n        Default is numpyro.infer.init_to_median.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n      seed: Seed to use for PRNGKey during training. For better replicability\n        run all different trainings with the same seed.\n    \"\"\"\n    if media.ndim not in (2, 3):\n      raise ValueError(\n          \"Media data must have either 2 dims for national model or 3 for geo \"\n          \"models.\")\n    if media.ndim == 3 and media_prior.ndim == 1:\n      media_prior = jnp.expand_dims(media_prior, axis=-1)\n\n    if media.shape[1] != len(media_prior):\n      raise ValueError(\"The number of data channels provided must match the \"\n                       \"number of cost values.\")\n    if media.min() < 0:\n      raise ValueError(\"Media values must be greater or equal to zero.\")\n\n    if custom_priors:\n      not_used_custom_priors = set(custom_priors.keys()).difference(\n          self._prior_names)\n      if not_used_custom_priors:\n        raise ValueError(\n            \"The following passed custom priors dont have a match in the model.\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        2.\n      seasonality_frequency: Frequency of the time period used. Default is 52 as\n        in 52 weeks per year.\n      weekday_seasonality: In case of daily data, also estimate seven weekday\n        parameters.\n      media_names: Names of the media channels passed.\n      number_warmup: Number of warm up samples. Default is 1000.\n      number_samples: Number of samples during sampling. Default is 1000.\n      number_chains: Number of chains to sample. Default is 2.\n      target_accept_prob: Target acceptance probability for step size in the\n        NUTS sampler. Default is .85.\n      init_strategy: Initialization function for numpyro NUTS. The available\n        options can be found in\n        https://num.pyro.ai/en/stable/utilities.html#initialization-strategies.\n        Default is numpyro.infer.init_to_median.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n      seed: Seed to use for PRNGKey during training. For better replicability\n        run all different trainings with the same seed.\n    \"\"\"\n    if media.ndim not in (2, 3):\n      raise ValueError(\n          \"Media data must have either 2 dims for national model or 3 for geo \"\n          \"models.\")\n    if media.ndim == 3 and media_prior.ndim == 1:\n      media_prior = jnp.expand_dims(media_prior, axis=-1)\n\n    if media.shape[1] != len(media_prior):\n      raise ValueError(\"The number of data channels provided must match the \"\n                       \"number of cost values.\")\n    if media.min() < 0:\n      raise ValueError(\"Media values must be greater or equal to zero.\")\n\n    if custom_priors:\n      not_used_custom_priors = set(custom_priors.keys()).difference(\n          self._prior_names)\n      if not_used_custom_priors:\n        raise ValueError(\n            \"The following passed custom priors dont have a match in the model.\"\n            \" Please double check the names have been written correctly: %s\" %\n            not_used_custom_priors)\n      custom_priors = self._preprocess_custom_priors(\n          custom_priors=custom_priors)\n      geo_custom_priors = set(custom_priors.keys()).intersection(\n          models.GEO_ONLY_PRIORS)\n      if media.ndim == 2 and geo_custom_priors:\n        raise ValueError(\n            \"The given data is for national models but custom_prior contains \"\n            \"priors for the geo version of the model. Please either remove geo \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        NUTS sampler. Default is .85.\n      init_strategy: Initialization function for numpyro NUTS. The available\n        options can be found in\n        https://num.pyro.ai/en/stable/utilities.html#initialization-strategies.\n        Default is numpyro.infer.init_to_median.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n      seed: Seed to use for PRNGKey during training. For better replicability\n        run all different trainings with the same seed.\n    \"\"\"\n    if media.ndim not in (2, 3):\n      raise ValueError(\n          \"Media data must have either 2 dims for national model or 3 for geo \"\n          \"models.\")\n    if media.ndim == 3 and media_prior.ndim == 1:\n      media_prior = jnp.expand_dims(media_prior, axis=-1)\n\n    if media.shape[1] != len(media_prior):\n      raise ValueError(\"The number of data channels provided must match the \"\n                       \"number of cost values.\")\n    if media.min() < 0:\n      raise ValueError(\"Media values must be greater or equal to zero.\")\n\n    if custom_priors:\n      not_used_custom_priors = set(custom_priors.keys()).difference(\n          self._prior_names)\n      if not_used_custom_priors:\n        raise ValueError(\n            \"The following passed custom priors dont have a match in the model.\"\n            \" Please double check the names have been written correctly: %s\" %\n            not_used_custom_priors)\n      custom_priors = self._preprocess_custom_priors(\n          custom_priors=custom_priors)\n      geo_custom_priors = set(custom_priors.keys()).intersection(\n          models.GEO_ONLY_PRIORS)\n      if media.ndim == 2 and geo_custom_priors:\n        raise ValueError(\n            \"The given data is for national models but custom_prior contains \"\n            \"priors for the geo version of the model. Please either remove geo \"\n            \"priors for national model or pass media data with geo dimension.\")\n    else:\n      custom_priors = {}\n\n    if weekday_seasonality and seasonality_frequency == 52:\n      logging.warn(\"You have chosen daily seasonality and frequency 52 \"\n                   \"(weekly), please check you made the right seasonality \"\n                   \"choices.\")\n\n    if extra_features is not None:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    \"\"\"\n    if media.ndim not in (2, 3):\n      raise ValueError(\n          \"Media data must have either 2 dims for national model or 3 for geo \"\n          \"models.\")\n    if media.ndim == 3 and media_prior.ndim == 1:\n      media_prior = jnp.expand_dims(media_prior, axis=-1)\n\n    if media.shape[1] != len(media_prior):\n      raise ValueError(\"The number of data channels provided must match the \"\n                       \"number of cost values.\")\n    if media.min() < 0:\n      raise ValueError(\"Media values must be greater or equal to zero.\")\n\n    if custom_priors:\n      not_used_custom_priors = set(custom_priors.keys()).difference(\n          self._prior_names)\n      if not_used_custom_priors:\n        raise ValueError(\n            \"The following passed custom priors dont have a match in the model.\"\n            \" Please double check the names have been written correctly: %s\" %\n            not_used_custom_priors)\n      custom_priors = self._preprocess_custom_priors(\n          custom_priors=custom_priors)\n      geo_custom_priors = set(custom_priors.keys()).intersection(\n          models.GEO_ONLY_PRIORS)\n      if media.ndim == 2 and geo_custom_priors:\n        raise ValueError(\n            \"The given data is for national models but custom_prior contains \"\n            \"priors for the geo version of the model. Please either remove geo \"\n            \"priors for national model or pass media data with geo dimension.\")\n    else:\n      custom_priors = {}\n\n    if weekday_seasonality and seasonality_frequency == 52:\n      logging.warn(\"You have chosen daily seasonality and frequency 52 \"\n                   \"(weekly), please check you made the right seasonality \"\n                   \"choices.\")\n\n    if extra_features is not None:\n      extra_features = jnp.array(extra_features)\n\n    if seed is None:\n      seed = utils.get_time_seed()\n\n    train_media_size = media.shape[0]\n    kernel = numpyro.infer.NUTS(\n        model=self._model_function,\n        target_accept_prob=target_accept_prob,\n        init_strategy=init_strategy)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "                       \"number of cost values.\")\n    if media.min() < 0:\n      raise ValueError(\"Media values must be greater or equal to zero.\")\n\n    if custom_priors:\n      not_used_custom_priors = set(custom_priors.keys()).difference(\n          self._prior_names)\n      if not_used_custom_priors:\n        raise ValueError(\n            \"The following passed custom priors dont have a match in the model.\"\n            \" Please double check the names have been written correctly: %s\" %\n            not_used_custom_priors)\n      custom_priors = self._preprocess_custom_priors(\n          custom_priors=custom_priors)\n      geo_custom_priors = set(custom_priors.keys()).intersection(\n          models.GEO_ONLY_PRIORS)\n      if media.ndim == 2 and geo_custom_priors:\n        raise ValueError(\n            \"The given data is for national models but custom_prior contains \"\n            \"priors for the geo version of the model. Please either remove geo \"\n            \"priors for national model or pass media data with geo dimension.\")\n    else:\n      custom_priors = {}\n\n    if weekday_seasonality and seasonality_frequency == 52:\n      logging.warn(\"You have chosen daily seasonality and frequency 52 \"\n                   \"(weekly), please check you made the right seasonality \"\n                   \"choices.\")\n\n    if extra_features is not None:\n      extra_features = jnp.array(extra_features)\n\n    if seed is None:\n      seed = utils.get_time_seed()\n\n    train_media_size = media.shape[0]\n    kernel = numpyro.infer.NUTS(\n        model=self._model_function,\n        target_accept_prob=target_accept_prob,\n        init_strategy=init_strategy)\n\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel,\n        num_warmup=number_warmup,\n        num_samples=number_samples,\n        num_chains=number_chains)\n    mcmc.run(\n        rng_key=jax.random.PRNGKey(seed),\n        media_data=jnp.array(media),\n        extra_features=extra_features,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "            \" Please double check the names have been written correctly: %s\" %\n            not_used_custom_priors)\n      custom_priors = self._preprocess_custom_priors(\n          custom_priors=custom_priors)\n      geo_custom_priors = set(custom_priors.keys()).intersection(\n          models.GEO_ONLY_PRIORS)\n      if media.ndim == 2 and geo_custom_priors:\n        raise ValueError(\n            \"The given data is for national models but custom_prior contains \"\n            \"priors for the geo version of the model. Please either remove geo \"\n            \"priors for national model or pass media data with geo dimension.\")\n    else:\n      custom_priors = {}\n\n    if weekday_seasonality and seasonality_frequency == 52:\n      logging.warn(\"You have chosen daily seasonality and frequency 52 \"\n                   \"(weekly), please check you made the right seasonality \"\n                   \"choices.\")\n\n    if extra_features is not None:\n      extra_features = jnp.array(extra_features)\n\n    if seed is None:\n      seed = utils.get_time_seed()\n\n    train_media_size = media.shape[0]\n    kernel = numpyro.infer.NUTS(\n        model=self._model_function,\n        target_accept_prob=target_accept_prob,\n        init_strategy=init_strategy)\n\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel,\n        num_warmup=number_warmup,\n        num_samples=number_samples,\n        num_chains=number_chains)\n    mcmc.run(\n        rng_key=jax.random.PRNGKey(seed),\n        media_data=jnp.array(media),\n        extra_features=extra_features,\n        target_data=jnp.array(target),\n        media_prior=jnp.array(media_prior),\n        degrees_seasonality=degrees_seasonality,\n        frequency=seasonality_frequency,\n        transform_function=self._model_transform_function,\n        weekday_seasonality=weekday_seasonality,\n        custom_priors=custom_priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "            \"priors for national model or pass media data with geo dimension.\")\n    else:\n      custom_priors = {}\n\n    if weekday_seasonality and seasonality_frequency == 52:\n      logging.warn(\"You have chosen daily seasonality and frequency 52 \"\n                   \"(weekly), please check you made the right seasonality \"\n                   \"choices.\")\n\n    if extra_features is not None:\n      extra_features = jnp.array(extra_features)\n\n    if seed is None:\n      seed = utils.get_time_seed()\n\n    train_media_size = media.shape[0]\n    kernel = numpyro.infer.NUTS(\n        model=self._model_function,\n        target_accept_prob=target_accept_prob,\n        init_strategy=init_strategy)\n\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel,\n        num_warmup=number_warmup,\n        num_samples=number_samples,\n        num_chains=number_chains)\n    mcmc.run(\n        rng_key=jax.random.PRNGKey(seed),\n        media_data=jnp.array(media),\n        extra_features=extra_features,\n        target_data=jnp.array(target),\n        media_prior=jnp.array(media_prior),\n        degrees_seasonality=degrees_seasonality,\n        frequency=seasonality_frequency,\n        transform_function=self._model_transform_function,\n        weekday_seasonality=weekday_seasonality,\n        custom_priors=custom_priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:\n      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      extra_features = jnp.array(extra_features)\n\n    if seed is None:\n      seed = utils.get_time_seed()\n\n    train_media_size = media.shape[0]\n    kernel = numpyro.infer.NUTS(\n        model=self._model_function,\n        target_accept_prob=target_accept_prob,\n        init_strategy=init_strategy)\n\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel,\n        num_warmup=number_warmup,\n        num_samples=number_samples,\n        num_chains=number_chains)\n    mcmc.run(\n        rng_key=jax.random.PRNGKey(seed),\n        media_data=jnp.array(media),\n        extra_features=extra_features,\n        target_data=jnp.array(target),\n        media_prior=jnp.array(media_prior),\n        degrees_seasonality=degrees_seasonality,\n        frequency=seasonality_frequency,\n        transform_function=self._model_transform_function,\n        weekday_seasonality=weekday_seasonality,\n        custom_priors=custom_priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:\n      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains\n    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel,\n        num_warmup=number_warmup,\n        num_samples=number_samples,\n        num_chains=number_chains)\n    mcmc.run(\n        rng_key=jax.random.PRNGKey(seed),\n        media_data=jnp.array(media),\n        extra_features=extra_features,\n        target_data=jnp.array(target),\n        media_prior=jnp.array(media_prior),\n        degrees_seasonality=degrees_seasonality,\n        frequency=seasonality_frequency,\n        transform_function=self._model_transform_function,\n        weekday_seasonality=weekday_seasonality,\n        custom_priors=custom_priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:\n      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains\n    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n\n  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        target_data=jnp.array(target),\n        media_prior=jnp.array(media_prior),\n        degrees_seasonality=degrees_seasonality,\n        frequency=seasonality_frequency,\n        transform_function=self._model_transform_function,\n        weekday_seasonality=weekday_seasonality,\n        custom_priors=custom_priors)\n\n    self.custom_priors = custom_priors\n    if media_names is not None:\n      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains\n    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n\n  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",\n                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      self.media_names = media_names\n    else:\n      self.media_names = [f\"channel_{i}\" for i in range(media.shape[1])]\n    self.n_media_channels = media.shape[1]\n    self.n_geos = media.shape[2] if media.ndim == 3 else 1\n    self._media_prior = media_prior\n    self.trace = mcmc.get_samples()\n    self._number_warmup = number_warmup\n    self._number_samples = number_samples\n    self._number_chains = number_chains\n    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n\n  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",\n                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,\n      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    self._target = target\n    self._train_media_size = train_media_size\n    self._degrees_seasonality = degrees_seasonality\n    self._seasonality_frequency = seasonality_frequency\n    self._weekday_seasonality = weekday_seasonality\n    self.media = media\n    self._extra_features = extra_features# jax-devicearray\n    self._mcmc = mcmc\n    logging.info(\"Model has been fitted\")\n\n  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",\n                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,\n      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.\n      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "  def print_summary(self) -> None:\n    \"\"\"Calls print_summary function from numpyro to print parameters summary.\n    \"\"\"\n    # TODO(): add name selection for print.\n    self._mcmc.print_summary()\n\n  @functools.partial(\n      jax.jit,\n      static_argnums=(0,),\n      static_argnames=(\"degrees_seasonality\", \"weekday_seasonality\",\n                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,\n      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.\n      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_405-455", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "                       \"transform_function\", \"model\"))\n  def _predict(\n      self,\n      rng_key: jnp.ndarray,\n      media_data: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray],\n      media_prior: jnp.ndarray,\n      degrees_seasonality: int, frequency: int,\n      transform_function: Callable[[Any], jnp.ndarray],\n      weekday_seasonality: bool,\n      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.\n      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n\n  def predict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_415-465", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      model: Callable[[Any], None],\n      posterior_samples: Dict[str, jnp.ndarray],\n      custom_priors: Dict[str, Prior]\n      ) -> Dict[str, jnp.ndarray]:\n    \"\"\"Encapsulates the numpyro.infer.Predictive function for predict method.\n\n    It serves as a helper jitted function for running predictions.\n\n    Args:\n      rng_key: A jax.random.PRNGKey.\n      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n\n  def predict(\n      self,\n      media: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      media_gap: Optional[jnp.ndarray] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None,\n      seed: Optional[int] = None\n  ) -> jnp.ndarray:\n    \"\"\"Runs the model to obtain predictions for the given input data.\n\n    Predictions returned are distributions, if point estimates are desired one", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_425-475", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      media_data: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_prior: Cost prior used for training the model.\n      degrees_seasonality: Number of degrees for the seasonality.\n      frequency: Frequency of the seasonality.\n      transform_function: Media transform function to use within the model.\n      weekday_seasonality: Allow daily weekday estimation.\n      model: Numpyro model to use for numpyro.infer.Predictive.\n      posterior_samples: Mapping of the posterior samples.\n      custom_priors: The custom priors we want the model to take instead of the\n        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n\n  def predict(\n      self,\n      media: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      media_gap: Optional[jnp.ndarray] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None,\n      seed: Optional[int] = None\n  ) -> jnp.ndarray:\n    \"\"\"Runs the model to obtain predictions for the given input data.\n\n    Predictions returned are distributions, if point estimates are desired one\n    can calculate those based on the given distribution.\n\n    Args:\n      media: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_gap: Media data gap between the end of training data and the start\n        of the out of sample media given. Eg. if 100 weeks of data were used for\n        training and prediction starts 2 months after training data finished we\n        need to provide the 8 weeks missing between the training data and the\n        prediction data so data transformations (adstock, carryover, ...) can", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_435-485", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        default ones. Refer to the full documentation on custom priors for\n        details.\n\n    Returns:\n      The predictions for the given data.\n    \"\"\"\n    return infer.Predictive(\n        model=model, posterior_samples=posterior_samples)(\n            rng_key=rng_key,\n            media_data=media_data,\n            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n\n  def predict(\n      self,\n      media: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      media_gap: Optional[jnp.ndarray] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None,\n      seed: Optional[int] = None\n  ) -> jnp.ndarray:\n    \"\"\"Runs the model to obtain predictions for the given input data.\n\n    Predictions returned are distributions, if point estimates are desired one\n    can calculate those based on the given distribution.\n\n    Args:\n      media: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_gap: Media data gap between the end of training data and the start\n        of the out of sample media given. Eg. if 100 weeks of data were used for\n        training and prediction starts 2 months after training data finished we\n        need to provide the 8 weeks missing between the training data and the\n        prediction data so data transformations (adstock, carryover, ...) can\n        take place correctly.\n      target_scaler: Scaler that was used to scale the target before training.\n      seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n    Returns:\n      Predictions for the given media and extra features at a given date index.\n\n    Raises:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_445-495", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "            extra_features=extra_features,\n            media_prior=media_prior,\n            target_data=None,\n            degrees_seasonality=degrees_seasonality,\n            frequency=frequency,\n            transform_function=transform_function,\n            custom_priors=custom_priors,\n            weekday_seasonality=weekday_seasonality)\n\n  def predict(\n      self,\n      media: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      media_gap: Optional[jnp.ndarray] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None,\n      seed: Optional[int] = None\n  ) -> jnp.ndarray:\n    \"\"\"Runs the model to obtain predictions for the given input data.\n\n    Predictions returned are distributions, if point estimates are desired one\n    can calculate those based on the given distribution.\n\n    Args:\n      media: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_gap: Media data gap between the end of training data and the start\n        of the out of sample media given. Eg. if 100 weeks of data were used for\n        training and prediction starts 2 months after training data finished we\n        need to provide the 8 weeks missing between the training data and the\n        prediction data so data transformations (adstock, carryover, ...) can\n        take place correctly.\n      target_scaler: Scaler that was used to scale the target before training.\n      seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n    Returns:\n      Predictions for the given media and extra features at a given date index.\n\n    Raises:\n      NotFittedModelError: When the model has not been fitted before running\n        predict.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\"Need to fit the model before running \"\n                                \"predictions.\")\n    if media_gap is not None:\n      if media.ndim != media_gap.ndim:\n        raise ValueError(\"Original media data and media gap must have the same \"\n                         \"number of dimensions.\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_455-505", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      self,\n      media: jnp.ndarray,\n      extra_features: Optional[jnp.ndarray] = None,\n      media_gap: Optional[jnp.ndarray] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None,\n      seed: Optional[int] = None\n  ) -> jnp.ndarray:\n    \"\"\"Runs the model to obtain predictions for the given input data.\n\n    Predictions returned are distributions, if point estimates are desired one\n    can calculate those based on the given distribution.\n\n    Args:\n      media: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_gap: Media data gap between the end of training data and the start\n        of the out of sample media given. Eg. if 100 weeks of data were used for\n        training and prediction starts 2 months after training data finished we\n        need to provide the 8 weeks missing between the training data and the\n        prediction data so data transformations (adstock, carryover, ...) can\n        take place correctly.\n      target_scaler: Scaler that was used to scale the target before training.\n      seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n    Returns:\n      Predictions for the given media and extra features at a given date index.\n\n    Raises:\n      NotFittedModelError: When the model has not been fitted before running\n        predict.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\"Need to fit the model before running \"\n                                \"predictions.\")\n    if media_gap is not None:\n      if media.ndim != media_gap.ndim:\n        raise ValueError(\"Original media data and media gap must have the same \"\n                         \"number of dimensions.\")\n      if media.ndim > 1 and media.shape[1] != media_gap.shape[1]:\n        raise ValueError(\"Media gap must have the same numer of media channels\"\n                         \"as the original media data.\")\n      previous_media = jnp.concatenate(arrays=[self.media, media_gap], axis=0)\n      if extra_features is not None:\n        previous_extra_features = jnp.concatenate(\n            arrays=[\n                self._extra_features,\n                jnp.zeros((media_gap.shape[0], *self._extra_features.shape[1:]))\n            ],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_465-515", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    can calculate those based on the given distribution.\n\n    Args:\n      media: Media array for needed for the model to run predictions.\n      extra_features: Extra features for needed for the model to run.\n      media_gap: Media data gap between the end of training data and the start\n        of the out of sample media given. Eg. if 100 weeks of data were used for\n        training and prediction starts 2 months after training data finished we\n        need to provide the 8 weeks missing between the training data and the\n        prediction data so data transformations (adstock, carryover, ...) can\n        take place correctly.\n      target_scaler: Scaler that was used to scale the target before training.\n      seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n    Returns:\n      Predictions for the given media and extra features at a given date index.\n\n    Raises:\n      NotFittedModelError: When the model has not been fitted before running\n        predict.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\"Need to fit the model before running \"\n                                \"predictions.\")\n    if media_gap is not None:\n      if media.ndim != media_gap.ndim:\n        raise ValueError(\"Original media data and media gap must have the same \"\n                         \"number of dimensions.\")\n      if media.ndim > 1 and media.shape[1] != media_gap.shape[1]:\n        raise ValueError(\"Media gap must have the same numer of media channels\"\n                         \"as the original media data.\")\n      previous_media = jnp.concatenate(arrays=[self.media, media_gap], axis=0)\n      if extra_features is not None:\n        previous_extra_features = jnp.concatenate(\n            arrays=[\n                self._extra_features,\n                jnp.zeros((media_gap.shape[0], *self._extra_features.shape[1:]))\n            ],\n            axis=0)\n    else:\n      previous_media = self.media\n      previous_extra_features = self._extra_features\n\n    full_media = jnp.concatenate(arrays=[previous_media, media], axis=0)\n    if extra_features is not None:\n      full_extra_features = jnp.concatenate(\n          arrays=[previous_extra_features, extra_features], axis=0)\n    else:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_475-525", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        take place correctly.\n      target_scaler: Scaler that was used to scale the target before training.\n      seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n    Returns:\n      Predictions for the given media and extra features at a given date index.\n\n    Raises:\n      NotFittedModelError: When the model has not been fitted before running\n        predict.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\"Need to fit the model before running \"\n                                \"predictions.\")\n    if media_gap is not None:\n      if media.ndim != media_gap.ndim:\n        raise ValueError(\"Original media data and media gap must have the same \"\n                         \"number of dimensions.\")\n      if media.ndim > 1 and media.shape[1] != media_gap.shape[1]:\n        raise ValueError(\"Media gap must have the same numer of media channels\"\n                         \"as the original media data.\")\n      previous_media = jnp.concatenate(arrays=[self.media, media_gap], axis=0)\n      if extra_features is not None:\n        previous_extra_features = jnp.concatenate(\n            arrays=[\n                self._extra_features,\n                jnp.zeros((media_gap.shape[0], *self._extra_features.shape[1:]))\n            ],\n            axis=0)\n    else:\n      previous_media = self.media\n      previous_extra_features = self._extra_features\n\n    full_media = jnp.concatenate(arrays=[previous_media, media], axis=0)\n    if extra_features is not None:\n      full_extra_features = jnp.concatenate(\n          arrays=[previous_extra_features, extra_features], axis=0)\n    else:\n      full_extra_features = None\n    if seed is None:\n      seed = utils.get_time_seed()\n    prediction = self._predict(\n        rng_key=jax.random.PRNGKey(seed=seed),\n        media_data=full_media,\n        extra_features=full_extra_features,\n        media_prior=jnp.array(self._media_prior),\n        degrees_seasonality=self._degrees_seasonality,\n        frequency=self._seasonality_frequency,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_485-535", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      NotFittedModelError: When the model has not been fitted before running\n        predict.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\"Need to fit the model before running \"\n                                \"predictions.\")\n    if media_gap is not None:\n      if media.ndim != media_gap.ndim:\n        raise ValueError(\"Original media data and media gap must have the same \"\n                         \"number of dimensions.\")\n      if media.ndim > 1 and media.shape[1] != media_gap.shape[1]:\n        raise ValueError(\"Media gap must have the same numer of media channels\"\n                         \"as the original media data.\")\n      previous_media = jnp.concatenate(arrays=[self.media, media_gap], axis=0)\n      if extra_features is not None:\n        previous_extra_features = jnp.concatenate(\n            arrays=[\n                self._extra_features,\n                jnp.zeros((media_gap.shape[0], *self._extra_features.shape[1:]))\n            ],\n            axis=0)\n    else:\n      previous_media = self.media\n      previous_extra_features = self._extra_features\n\n    full_media = jnp.concatenate(arrays=[previous_media, media], axis=0)\n    if extra_features is not None:\n      full_extra_features = jnp.concatenate(\n          arrays=[previous_extra_features, extra_features], axis=0)\n    else:\n      full_extra_features = None\n    if seed is None:\n      seed = utils.get_time_seed()\n    prediction = self._predict(\n        rng_key=jax.random.PRNGKey(seed=seed),\n        media_data=full_media,\n        extra_features=full_extra_features,\n        media_prior=jnp.array(self._media_prior),\n        degrees_seasonality=self._degrees_seasonality,\n        frequency=self._seasonality_frequency,\n        weekday_seasonality=self._weekday_seasonality,\n        transform_function=self._model_transform_function,\n        model=self._model_function,\n        custom_priors=self.custom_priors,\n        posterior_samples=self.trace)[\"mu\"][:, previous_media.shape[0]:]\n    if target_scaler:\n      prediction = target_scaler.inverse_transform(prediction)\n\n    return prediction\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_495-545", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      if media.ndim > 1 and media.shape[1] != media_gap.shape[1]:\n        raise ValueError(\"Media gap must have the same numer of media channels\"\n                         \"as the original media data.\")\n      previous_media = jnp.concatenate(arrays=[self.media, media_gap], axis=0)\n      if extra_features is not None:\n        previous_extra_features = jnp.concatenate(\n            arrays=[\n                self._extra_features,\n                jnp.zeros((media_gap.shape[0], *self._extra_features.shape[1:]))\n            ],\n            axis=0)\n    else:\n      previous_media = self.media\n      previous_extra_features = self._extra_features\n\n    full_media = jnp.concatenate(arrays=[previous_media, media], axis=0)\n    if extra_features is not None:\n      full_extra_features = jnp.concatenate(\n          arrays=[previous_extra_features, extra_features], axis=0)\n    else:\n      full_extra_features = None\n    if seed is None:\n      seed = utils.get_time_seed()\n    prediction = self._predict(\n        rng_key=jax.random.PRNGKey(seed=seed),\n        media_data=full_media,\n        extra_features=full_extra_features,\n        media_prior=jnp.array(self._media_prior),\n        degrees_seasonality=self._degrees_seasonality,\n        frequency=self._seasonality_frequency,\n        weekday_seasonality=self._weekday_seasonality,\n        transform_function=self._model_transform_function,\n        model=self._model_function,\n        custom_priors=self.custom_priors,\n        posterior_samples=self.trace)[\"mu\"][:, previous_media.shape[0]:]\n    if target_scaler:\n      prediction = target_scaler.inverse_transform(prediction)\n\n    return prediction\n\n  def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_505-555", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "            axis=0)\n    else:\n      previous_media = self.media\n      previous_extra_features = self._extra_features\n\n    full_media = jnp.concatenate(arrays=[previous_media, media], axis=0)\n    if extra_features is not None:\n      full_extra_features = jnp.concatenate(\n          arrays=[previous_extra_features, extra_features], axis=0)\n    else:\n      full_extra_features = None\n    if seed is None:\n      seed = utils.get_time_seed()\n    prediction = self._predict(\n        rng_key=jax.random.PRNGKey(seed=seed),\n        media_data=full_media,\n        extra_features=full_extra_features,\n        media_prior=jnp.array(self._media_prior),\n        degrees_seasonality=self._degrees_seasonality,\n        frequency=self._seasonality_frequency,\n        weekday_seasonality=self._weekday_seasonality,\n        transform_function=self._model_transform_function,\n        model=self._model_function,\n        custom_priors=self.custom_priors,\n        posterior_samples=self.trace)[\"mu\"][:, previous_media.shape[0]:]\n    if target_scaler:\n      prediction = target_scaler.inverse_transform(prediction)\n\n    return prediction\n\n  def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n\n    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"\n    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_515-565", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      full_extra_features = None\n    if seed is None:\n      seed = utils.get_time_seed()\n    prediction = self._predict(\n        rng_key=jax.random.PRNGKey(seed=seed),\n        media_data=full_media,\n        extra_features=full_extra_features,\n        media_prior=jnp.array(self._media_prior),\n        degrees_seasonality=self._degrees_seasonality,\n        frequency=self._seasonality_frequency,\n        weekday_seasonality=self._weekday_seasonality,\n        transform_function=self._model_transform_function,\n        model=self._model_function,\n        custom_priors=self.custom_priors,\n        posterior_samples=self.trace)[\"mu\"][:, previous_media.shape[0]:]\n    if target_scaler:\n      prediction = target_scaler.inverse_transform(prediction)\n\n    return prediction\n\n  def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n\n    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"\n    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]\n    logging.info(\"Reduction is complete\")\n\n  def get_posterior_metrics(\n      self,\n      unscaled_costs: Optional[jnp.ndarray] = None,\n      cost_scaler: Optional[preprocessing.CustomScaler] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None\n  ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"It estimates the media contribution percentage and ROI of each channel.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_525-575", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        weekday_seasonality=self._weekday_seasonality,\n        transform_function=self._model_transform_function,\n        model=self._model_function,\n        custom_priors=self.custom_priors,\n        posterior_samples=self.trace)[\"mu\"][:, previous_media.shape[0]:]\n    if target_scaler:\n      prediction = target_scaler.inverse_transform(prediction)\n\n    return prediction\n\n  def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n\n    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"\n    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]\n    logging.info(\"Reduction is complete\")\n\n  def get_posterior_metrics(\n      self,\n      unscaled_costs: Optional[jnp.ndarray] = None,\n      cost_scaler: Optional[preprocessing.CustomScaler] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None\n  ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"It estimates the media contribution percentage and ROI of each channel.\n\n    If data was scaled prior to training then the target and costs scalers need\n    to be passed to this function to correctly calculate media contribution\n    percentage and ROI in the unscaled space.\n\n    Args:\n      unscaled_costs: Optionally you can pass new costs to get these set of\n        metrics. If None, the costs used for training will be used for\n        calculating ROI.\n      cost_scaler: Scaler that was used to scale the cost data before training.\n        It is ignored if 'unscaled_costs' is provided.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_535-585", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "  def reduce_trace(self, nsample: int = 100, seed: int = 0) -> None:\n    \"\"\"Reduces the samples in `trace` to speed up `predict` and optimize.\n\n    Please note this step is not reversible. Only do this after you have\n    investigated convergence of the model.\n\n    Args:\n      nsample: Target number of samples.\n      seed: Random seed for down sampling.\n\n    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"\n    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]\n    logging.info(\"Reduction is complete\")\n\n  def get_posterior_metrics(\n      self,\n      unscaled_costs: Optional[jnp.ndarray] = None,\n      cost_scaler: Optional[preprocessing.CustomScaler] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None\n  ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"It estimates the media contribution percentage and ROI of each channel.\n\n    If data was scaled prior to training then the target and costs scalers need\n    to be passed to this function to correctly calculate media contribution\n    percentage and ROI in the unscaled space.\n\n    Args:\n      unscaled_costs: Optionally you can pass new costs to get these set of\n        metrics. If None, the costs used for training will be used for\n        calculating ROI.\n      cost_scaler: Scaler that was used to scale the cost data before training.\n        It is ignored if 'unscaled_costs' is provided.\n      target_scaler: Scaler that was used to scale the target before training.\n\n    Returns:\n      media_contribution_hat_pct: The average media contribution percentage for\n      each channel.\n      roi_hat: The return on investment of each channel calculated as its\n      contribution divided by the cost.\n\n    Raises:\n      NotFittedModelError: When the this method is called without the model", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_545-595", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    Raises:\n      ValueError: if `nsample` is too big.\n    \"\"\"\n    ntrace = len(self.trace[\"sigma\"])\n    if ntrace < nsample:\n      raise ValueError(\"nsample is bigger than the actual posterior samples\")\n    key = jax.random.PRNGKey(seed)\n    samples = jax.random.choice(key, ntrace, (nsample,), replace=False)\n    for name in self.trace.keys():\n      self.trace[name] = self.trace[name][samples]\n    logging.info(\"Reduction is complete\")\n\n  def get_posterior_metrics(\n      self,\n      unscaled_costs: Optional[jnp.ndarray] = None,\n      cost_scaler: Optional[preprocessing.CustomScaler] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None\n  ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"It estimates the media contribution percentage and ROI of each channel.\n\n    If data was scaled prior to training then the target and costs scalers need\n    to be passed to this function to correctly calculate media contribution\n    percentage and ROI in the unscaled space.\n\n    Args:\n      unscaled_costs: Optionally you can pass new costs to get these set of\n        metrics. If None, the costs used for training will be used for\n        calculating ROI.\n      cost_scaler: Scaler that was used to scale the cost data before training.\n        It is ignored if 'unscaled_costs' is provided.\n      target_scaler: Scaler that was used to scale the target before training.\n\n    Returns:\n      media_contribution_hat_pct: The average media contribution percentage for\n      each channel.\n      roi_hat: The return on investment of each channel calculated as its\n      contribution divided by the cost.\n\n    Raises:\n      NotFittedModelError: When the this method is called without the model\n        being trained previously.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\n          \"LightweightMMM has not been fitted and cannot run estimations. \"\n          \"Please first fit the model.\")\n    if unscaled_costs is None and not cost_scaler:\n      logging.warning(\n          \"Unscaled cost data or cost scaler were not given and  \"\n          \"therefore unscaling wont be applied to calculcate contribution\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_555-605", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    logging.info(\"Reduction is complete\")\n\n  def get_posterior_metrics(\n      self,\n      unscaled_costs: Optional[jnp.ndarray] = None,\n      cost_scaler: Optional[preprocessing.CustomScaler] = None,\n      target_scaler: Optional[preprocessing.CustomScaler] = None\n  ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"It estimates the media contribution percentage and ROI of each channel.\n\n    If data was scaled prior to training then the target and costs scalers need\n    to be passed to this function to correctly calculate media contribution\n    percentage and ROI in the unscaled space.\n\n    Args:\n      unscaled_costs: Optionally you can pass new costs to get these set of\n        metrics. If None, the costs used for training will be used for\n        calculating ROI.\n      cost_scaler: Scaler that was used to scale the cost data before training.\n        It is ignored if 'unscaled_costs' is provided.\n      target_scaler: Scaler that was used to scale the target before training.\n\n    Returns:\n      media_contribution_hat_pct: The average media contribution percentage for\n      each channel.\n      roi_hat: The return on investment of each channel calculated as its\n      contribution divided by the cost.\n\n    Raises:\n      NotFittedModelError: When the this method is called without the model\n        being trained previously.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\n          \"LightweightMMM has not been fitted and cannot run estimations. \"\n          \"Please first fit the model.\")\n    if unscaled_costs is None and not cost_scaler:\n      logging.warning(\n          \"Unscaled cost data or cost scaler were not given and  \"\n          \"therefore unscaling wont be applied to calculcate contribution\"\n          \" and ROI. If data was not scaled prior to training \"\n          \"please ignore this warning.\")\n    if not target_scaler:\n      logging.warning(\"Target scaler was not given and unscaling of the target \"\n                      \"will not occur. If your target was not scaled prior to \"\n                      \"training you can ignore this warning.\")\n    if unscaled_costs is None:\n      if cost_scaler:\n        unscaled_costs = cost_scaler.inverse_transform(self._media_prior)\n      else:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_565-615", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    If data was scaled prior to training then the target and costs scalers need\n    to be passed to this function to correctly calculate media contribution\n    percentage and ROI in the unscaled space.\n\n    Args:\n      unscaled_costs: Optionally you can pass new costs to get these set of\n        metrics. If None, the costs used for training will be used for\n        calculating ROI.\n      cost_scaler: Scaler that was used to scale the cost data before training.\n        It is ignored if 'unscaled_costs' is provided.\n      target_scaler: Scaler that was used to scale the target before training.\n\n    Returns:\n      media_contribution_hat_pct: The average media contribution percentage for\n      each channel.\n      roi_hat: The return on investment of each channel calculated as its\n      contribution divided by the cost.\n\n    Raises:\n      NotFittedModelError: When the this method is called without the model\n        being trained previously.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\n          \"LightweightMMM has not been fitted and cannot run estimations. \"\n          \"Please first fit the model.\")\n    if unscaled_costs is None and not cost_scaler:\n      logging.warning(\n          \"Unscaled cost data or cost scaler were not given and  \"\n          \"therefore unscaling wont be applied to calculcate contribution\"\n          \" and ROI. If data was not scaled prior to training \"\n          \"please ignore this warning.\")\n    if not target_scaler:\n      logging.warning(\"Target scaler was not given and unscaling of the target \"\n                      \"will not occur. If your target was not scaled prior to \"\n                      \"training you can ignore this warning.\")\n    if unscaled_costs is None:\n      if cost_scaler:\n        unscaled_costs = cost_scaler.inverse_transform(self._media_prior)\n      else:\n        unscaled_costs = self._media_prior\n\n    if self.media.ndim == 3:\n      # cost shape (channel, geo) -> add a new axis to (channel, geo, sample)\n      unscaled_costs = unscaled_costs = unscaled_costs[:, :, jnp.newaxis]\n      # reshape cost to (sample, channel, geo)\n      unscaled_costs = jnp.einsum(\"cgs->scg\", unscaled_costs)\n\n    # get the scaled posterior prediction\n    posterior_pred = self.trace[\"mu\"]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_575-625", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      target_scaler: Scaler that was used to scale the target before training.\n\n    Returns:\n      media_contribution_hat_pct: The average media contribution percentage for\n      each channel.\n      roi_hat: The return on investment of each channel calculated as its\n      contribution divided by the cost.\n\n    Raises:\n      NotFittedModelError: When the this method is called without the model\n        being trained previously.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\n          \"LightweightMMM has not been fitted and cannot run estimations. \"\n          \"Please first fit the model.\")\n    if unscaled_costs is None and not cost_scaler:\n      logging.warning(\n          \"Unscaled cost data or cost scaler were not given and  \"\n          \"therefore unscaling wont be applied to calculcate contribution\"\n          \" and ROI. If data was not scaled prior to training \"\n          \"please ignore this warning.\")\n    if not target_scaler:\n      logging.warning(\"Target scaler was not given and unscaling of the target \"\n                      \"will not occur. If your target was not scaled prior to \"\n                      \"training you can ignore this warning.\")\n    if unscaled_costs is None:\n      if cost_scaler:\n        unscaled_costs = cost_scaler.inverse_transform(self._media_prior)\n      else:\n        unscaled_costs = self._media_prior\n\n    if self.media.ndim == 3:\n      # cost shape (channel, geo) -> add a new axis to (channel, geo, sample)\n      unscaled_costs = unscaled_costs = unscaled_costs[:, :, jnp.newaxis]\n      # reshape cost to (sample, channel, geo)\n      unscaled_costs = jnp.einsum(\"cgs->scg\", unscaled_costs)\n\n    # get the scaled posterior prediction\n    posterior_pred = self.trace[\"mu\"]\n    if target_scaler:\n      unscaled_posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    else:\n      unscaled_posterior_pred = posterior_pred\n\n    if self.media.ndim == 2:\n      # s for samples, t for time, c for media channels\n      einsum_str = \"stc, sc -> sc\"\n    elif self.media.ndim == 3:\n      # s for samples, t for time, c for media channels, g for geo", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_585-635", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        being trained previously.\n    \"\"\"\n    if not hasattr(self, \"trace\"):\n      raise NotFittedModelError(\n          \"LightweightMMM has not been fitted and cannot run estimations. \"\n          \"Please first fit the model.\")\n    if unscaled_costs is None and not cost_scaler:\n      logging.warning(\n          \"Unscaled cost data or cost scaler were not given and  \"\n          \"therefore unscaling wont be applied to calculcate contribution\"\n          \" and ROI. If data was not scaled prior to training \"\n          \"please ignore this warning.\")\n    if not target_scaler:\n      logging.warning(\"Target scaler was not given and unscaling of the target \"\n                      \"will not occur. If your target was not scaled prior to \"\n                      \"training you can ignore this warning.\")\n    if unscaled_costs is None:\n      if cost_scaler:\n        unscaled_costs = cost_scaler.inverse_transform(self._media_prior)\n      else:\n        unscaled_costs = self._media_prior\n\n    if self.media.ndim == 3:\n      # cost shape (channel, geo) -> add a new axis to (channel, geo, sample)\n      unscaled_costs = unscaled_costs = unscaled_costs[:, :, jnp.newaxis]\n      # reshape cost to (sample, channel, geo)\n      unscaled_costs = jnp.einsum(\"cgs->scg\", unscaled_costs)\n\n    # get the scaled posterior prediction\n    posterior_pred = self.trace[\"mu\"]\n    if target_scaler:\n      unscaled_posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    else:\n      unscaled_posterior_pred = posterior_pred\n\n    if self.media.ndim == 2:\n      # s for samples, t for time, c for media channels\n      einsum_str = \"stc, sc -> sc\"\n    elif self.media.ndim == 3:\n      # s for samples, t for time, c for media channels, g for geo\n      einsum_str = \"stcg, scg -> scg\"\n\n    media_contribution = jnp.einsum(einsum_str, self.trace[\"media_transformed\"],\n                                    jnp.squeeze(self.trace[\"coef_media\"]))\n\n    # aggregate posterior_pred across time:\n    sum_scaled_prediction = jnp.sum(posterior_pred, axis=1)\n    # aggregate unscaled_posterior_pred across time:\n    sum_unscaled_prediction = jnp.sum(unscaled_posterior_pred, axis=1)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_595-645", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "          \" and ROI. If data was not scaled prior to training \"\n          \"please ignore this warning.\")\n    if not target_scaler:\n      logging.warning(\"Target scaler was not given and unscaling of the target \"\n                      \"will not occur. If your target was not scaled prior to \"\n                      \"training you can ignore this warning.\")\n    if unscaled_costs is None:\n      if cost_scaler:\n        unscaled_costs = cost_scaler.inverse_transform(self._media_prior)\n      else:\n        unscaled_costs = self._media_prior\n\n    if self.media.ndim == 3:\n      # cost shape (channel, geo) -> add a new axis to (channel, geo, sample)\n      unscaled_costs = unscaled_costs = unscaled_costs[:, :, jnp.newaxis]\n      # reshape cost to (sample, channel, geo)\n      unscaled_costs = jnp.einsum(\"cgs->scg\", unscaled_costs)\n\n    # get the scaled posterior prediction\n    posterior_pred = self.trace[\"mu\"]\n    if target_scaler:\n      unscaled_posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    else:\n      unscaled_posterior_pred = posterior_pred\n\n    if self.media.ndim == 2:\n      # s for samples, t for time, c for media channels\n      einsum_str = \"stc, sc -> sc\"\n    elif self.media.ndim == 3:\n      # s for samples, t for time, c for media channels, g for geo\n      einsum_str = \"stcg, scg -> scg\"\n\n    media_contribution = jnp.einsum(einsum_str, self.trace[\"media_transformed\"],\n                                    jnp.squeeze(self.trace[\"coef_media\"]))\n\n    # aggregate posterior_pred across time:\n    sum_scaled_prediction = jnp.sum(posterior_pred, axis=1)\n    # aggregate unscaled_posterior_pred across time:\n    sum_unscaled_prediction = jnp.sum(unscaled_posterior_pred, axis=1)\n\n    if self.media.ndim == 2:\n      # add a new axis to represent channel:(sample,) -> (sample,channel)\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis]\n      sum_unscaled_prediction = sum_unscaled_prediction[:, jnp.newaxis]\n\n    elif self.media.ndim == 3:\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis, :]\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_605-655", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "        unscaled_costs = self._media_prior\n\n    if self.media.ndim == 3:\n      # cost shape (channel, geo) -> add a new axis to (channel, geo, sample)\n      unscaled_costs = unscaled_costs = unscaled_costs[:, :, jnp.newaxis]\n      # reshape cost to (sample, channel, geo)\n      unscaled_costs = jnp.einsum(\"cgs->scg\", unscaled_costs)\n\n    # get the scaled posterior prediction\n    posterior_pred = self.trace[\"mu\"]\n    if target_scaler:\n      unscaled_posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    else:\n      unscaled_posterior_pred = posterior_pred\n\n    if self.media.ndim == 2:\n      # s for samples, t for time, c for media channels\n      einsum_str = \"stc, sc -> sc\"\n    elif self.media.ndim == 3:\n      # s for samples, t for time, c for media channels, g for geo\n      einsum_str = \"stcg, scg -> scg\"\n\n    media_contribution = jnp.einsum(einsum_str, self.trace[\"media_transformed\"],\n                                    jnp.squeeze(self.trace[\"coef_media\"]))\n\n    # aggregate posterior_pred across time:\n    sum_scaled_prediction = jnp.sum(posterior_pred, axis=1)\n    # aggregate unscaled_posterior_pred across time:\n    sum_unscaled_prediction = jnp.sum(unscaled_posterior_pred, axis=1)\n\n    if self.media.ndim == 2:\n      # add a new axis to represent channel:(sample,) -> (sample,channel)\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis]\n      sum_unscaled_prediction = sum_unscaled_prediction[:, jnp.newaxis]\n\n    elif self.media.ndim == 3:\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis, :]\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_unscaled_prediction = sum_unscaled_prediction[:, :, jnp.newaxis]\n      # reshape the array (sample,geo,channel) -> (sample,channel,geo)\n      sum_unscaled_prediction = jnp.einsum(\"sgc->scg\", sum_unscaled_prediction)\n\n    # media contribution pct = media contribution / prediction\n    # for geo level model:\n    # media_contribution shape (sample, channel, geo)\n    # sum_scaled_prediction shape (sample, channel, geo)\n    # -> media_contribution_hat shape (sample, channel, geo)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_615-665", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    if target_scaler:\n      unscaled_posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    else:\n      unscaled_posterior_pred = posterior_pred\n\n    if self.media.ndim == 2:\n      # s for samples, t for time, c for media channels\n      einsum_str = \"stc, sc -> sc\"\n    elif self.media.ndim == 3:\n      # s for samples, t for time, c for media channels, g for geo\n      einsum_str = \"stcg, scg -> scg\"\n\n    media_contribution = jnp.einsum(einsum_str, self.trace[\"media_transformed\"],\n                                    jnp.squeeze(self.trace[\"coef_media\"]))\n\n    # aggregate posterior_pred across time:\n    sum_scaled_prediction = jnp.sum(posterior_pred, axis=1)\n    # aggregate unscaled_posterior_pred across time:\n    sum_unscaled_prediction = jnp.sum(unscaled_posterior_pred, axis=1)\n\n    if self.media.ndim == 2:\n      # add a new axis to represent channel:(sample,) -> (sample,channel)\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis]\n      sum_unscaled_prediction = sum_unscaled_prediction[:, jnp.newaxis]\n\n    elif self.media.ndim == 3:\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis, :]\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_unscaled_prediction = sum_unscaled_prediction[:, :, jnp.newaxis]\n      # reshape the array (sample,geo,channel) -> (sample,channel,geo)\n      sum_unscaled_prediction = jnp.einsum(\"sgc->scg\", sum_unscaled_prediction)\n\n    # media contribution pct = media contribution / prediction\n    # for geo level model:\n    # media_contribution shape (sample, channel, geo)\n    # sum_scaled_prediction shape (sample, channel, geo)\n    # -> media_contribution_hat shape (sample, channel, geo)\n    media_contribution_hat = media_contribution / sum_scaled_prediction\n\n    # media roi = unscaled prediction * media contribution pct / unscaled costs\n    # for geo leve model:\n    # sum_unscaled_prediction shape (sample, channel, geo)\n    # media_contribution_hat shape (sample, channel, geo)\n    # unscaled_costs shape (sample, channel, geo)\n    # -> roi_hat shape (sample, channel, geo)\n    roi_hat = sum_unscaled_prediction * media_contribution_hat / unscaled_costs\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_625-666", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "      einsum_str = \"stcg, scg -> scg\"\n\n    media_contribution = jnp.einsum(einsum_str, self.trace[\"media_transformed\"],\n                                    jnp.squeeze(self.trace[\"coef_media\"]))\n\n    # aggregate posterior_pred across time:\n    sum_scaled_prediction = jnp.sum(posterior_pred, axis=1)\n    # aggregate unscaled_posterior_pred across time:\n    sum_unscaled_prediction = jnp.sum(unscaled_posterior_pred, axis=1)\n\n    if self.media.ndim == 2:\n      # add a new axis to represent channel:(sample,) -> (sample,channel)\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis]\n      sum_unscaled_prediction = sum_unscaled_prediction[:, jnp.newaxis]\n\n    elif self.media.ndim == 3:\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis, :]\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_unscaled_prediction = sum_unscaled_prediction[:, :, jnp.newaxis]\n      # reshape the array (sample,geo,channel) -> (sample,channel,geo)\n      sum_unscaled_prediction = jnp.einsum(\"sgc->scg\", sum_unscaled_prediction)\n\n    # media contribution pct = media contribution / prediction\n    # for geo level model:\n    # media_contribution shape (sample, channel, geo)\n    # sum_scaled_prediction shape (sample, channel, geo)\n    # -> media_contribution_hat shape (sample, channel, geo)\n    media_contribution_hat = media_contribution / sum_scaled_prediction\n\n    # media roi = unscaled prediction * media contribution pct / unscaled costs\n    # for geo leve model:\n    # sum_unscaled_prediction shape (sample, channel, geo)\n    # media_contribution_hat shape (sample, channel, geo)\n    # unscaled_costs shape (sample, channel, geo)\n    # -> roi_hat shape (sample, channel, geo)\n    roi_hat = sum_unscaled_prediction * media_contribution_hat / unscaled_costs\n\n    return media_contribution_hat, roi_hat", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 666, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py_635-666", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm.py", "text": "    if self.media.ndim == 2:\n      # add a new axis to represent channel:(sample,) -> (sample,channel)\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis]\n      sum_unscaled_prediction = sum_unscaled_prediction[:, jnp.newaxis]\n\n    elif self.media.ndim == 3:\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_scaled_prediction = sum_scaled_prediction[:, jnp.newaxis, :]\n      # add a new axis to represent channel:(sample,geo) -> (sample,geo,channel)\n      # note: the total prediction value stays the same for all channels\n      sum_unscaled_prediction = sum_unscaled_prediction[:, :, jnp.newaxis]\n      # reshape the array (sample,geo,channel) -> (sample,channel,geo)\n      sum_unscaled_prediction = jnp.einsum(\"sgc->scg\", sum_unscaled_prediction)\n\n    # media contribution pct = media contribution / prediction\n    # for geo level model:\n    # media_contribution shape (sample, channel, geo)\n    # sum_scaled_prediction shape (sample, channel, geo)\n    # -> media_contribution_hat shape (sample, channel, geo)\n    media_contribution_hat = media_contribution / sum_scaled_prediction\n\n    # media roi = unscaled prediction * media contribution pct / unscaled costs\n    # for geo leve model:\n    # sum_unscaled_prediction shape (sample, channel, geo)\n    # media_contribution_hat shape (sample, channel, geo)\n    # unscaled_costs shape (sample, channel, geo)\n    # -> roi_hat shape (sample, channel, geo)\n    roi_hat = sum_unscaled_prediction * media_contribution_hat / unscaled_costs\n\n    return media_contribution_hat, roi_hat", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 666, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lightweight_mmm.\"\"\"\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm import lightweight_mmm\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lightweight_mmm.\"\"\"\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\n\n\nclass LightweightMmmTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lightweight_mmm.\"\"\"\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\n\n\nclass LightweightMmmTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lightweight_mmm.\"\"\"\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\n\n\nclass LightweightMmmTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\n\n\nclass LightweightMmmTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_trans_func=models.transform_hill_adstock),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_trans_func=models.transform_adstock),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_trans_func=models.transform_carryover),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "from lightweight_mmm import models\n\n\nclass LightweightMmmTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(LightweightMmmTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_trans_func=models.transform_hill_adstock),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_trans_func=models.transform_adstock),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_trans_func=models.transform_carryover),\n  ])\n  def test_instantiate_model_correctly_from_available_models(\n      self, model_name, expected_trans_func):\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    self.assertEqual(mmm_object._model_transform_function, expected_trans_func)\n\n  def test_instantiate_model_wrong_raises_valueerror(self):\n    with self.assertRaises(ValueError):\n      lightweight_mmm.LightweightMMM(model_name=\"non_existing_model\")\n\nAST=Module(ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Constant)keyword(Constant)keyword(Constant)))Name(Load))FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Attribute(Name(Load)Load)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Attribute(Name(Load)Load)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Attribute(Name(Load)Load)))Load)))FunctionDef(arguments(arg)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)keyword(Constant)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_trans_func=models.transform_hill_adstock),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_trans_func=models.transform_adstock),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_trans_func=models.transform_carryover),\n  ])\n  def test_instantiate_model_correctly_from_available_models(\n      self, model_name, expected_trans_func):\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    self.assertEqual(mmm_object._model_transform_function, expected_trans_func)\n\n  def test_instantiate_model_wrong_raises_valueerror(self):\n    with self.assertRaises(ValueError):\n      lightweight_mmm.LightweightMMM(model_name=\"non_existing_model\")\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_values_national\",\n          media=-np.ones((20, 3)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"negative_values_geo\",\n          media=-np.ones((20, 3, 3)),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        extra_features=jnp.ones((50, 2, 3)),\n        number_warmup=2,\n        number_samples=4,\n        number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_trans_func=models.transform_hill_adstock),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_trans_func=models.transform_adstock),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_trans_func=models.transform_carryover),\n  ])\n  def test_instantiate_model_correctly_from_available_models(\n      self, model_name, expected_trans_func):\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    self.assertEqual(mmm_object._model_transform_function, expected_trans_func)\n\n  def test_instantiate_model_wrong_raises_valueerror(self):\n    with self.assertRaises(ValueError):\n      lightweight_mmm.LightweightMMM(model_name=\"non_existing_model\")\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_values_national\",\n          media=-np.ones((20, 3)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"negative_values_geo\",\n          media=-np.ones((20, 3, 3)),\n          target_shape=(20, 3),\n          total_costs_shape=(3, 1)),\n      dict(\n          testcase_name=\"wrong_media_shape\",\n          media=-np.ones((20, 2)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"extra_dims\",\n          media=np.ones((20, 2, 4, 5)),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          model_name=\"hill_adstock\",\n          expected_trans_func=models.transform_hill_adstock),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_trans_func=models.transform_adstock),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_trans_func=models.transform_carryover),\n  ])\n  def test_instantiate_model_correctly_from_available_models(\n      self, model_name, expected_trans_func):\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    self.assertEqual(mmm_object._model_transform_function, expected_trans_func)\n\n  def test_instantiate_model_wrong_raises_valueerror(self):\n    with self.assertRaises(ValueError):\n      lightweight_mmm.LightweightMMM(model_name=\"non_existing_model\")\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_values_national\",\n          media=-np.ones((20, 3)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"negative_values_geo\",\n          media=-np.ones((20, 3, 3)),\n          target_shape=(20, 3),\n          total_costs_shape=(3, 1)),\n      dict(\n          testcase_name=\"wrong_media_shape\",\n          media=-np.ones((20, 2)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"extra_dims\",\n          media=np.ones((20, 2, 4, 5)),\n          target_shape=(20,),\n          total_costs_shape=(3,))\n  ])\n  def test_fit_wrong_inputs_raises_value_error(\n      self, media, target_shape, total_costs_shape):\n    media = jnp.array(media)\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(total_costs_shape)\n    target = jnp.ones(target_shape)\n    mmm_object = lightweight_mmm.LightweightMMM()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "  ])\n  def test_instantiate_model_correctly_from_available_models(\n      self, model_name, expected_trans_func):\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    self.assertEqual(mmm_object._model_transform_function, expected_trans_func)\n\n  def test_instantiate_model_wrong_raises_valueerror(self):\n    with self.assertRaises(ValueError):\n      lightweight_mmm.LightweightMMM(model_name=\"non_existing_model\")\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_values_national\",\n          media=-np.ones((20, 3)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"negative_values_geo\",\n          media=-np.ones((20, 3, 3)),\n          target_shape=(20, 3),\n          total_costs_shape=(3, 1)),\n      dict(\n          testcase_name=\"wrong_media_shape\",\n          media=-np.ones((20, 2)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"extra_dims\",\n          media=np.ones((20, 2, 4, 5)),\n          target_shape=(20,),\n          total_costs_shape=(3,))\n  ])\n  def test_fit_wrong_inputs_raises_value_error(\n      self, media, target_shape, total_costs_shape):\n    media = jnp.array(media)\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(total_costs_shape)\n    target = jnp.ones(target_shape)\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(ValueError):\n      mmm_object.fit(\n          media=media,\n          extra_features=extra_features,\n          media_prior=costs,\n          target=target,\n          number_warmup=5,\n          number_samples=5,\n          number_chains=1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_values_national\",\n          media=-np.ones((20, 3)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"negative_values_geo\",\n          media=-np.ones((20, 3, 3)),\n          target_shape=(20, 3),\n          total_costs_shape=(3, 1)),\n      dict(\n          testcase_name=\"wrong_media_shape\",\n          media=-np.ones((20, 2)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"extra_dims\",\n          media=np.ones((20, 2, 4, 5)),\n          target_shape=(20,),\n          total_costs_shape=(3,))\n  ])\n  def test_fit_wrong_inputs_raises_value_error(\n      self, media, target_shape, total_costs_shape):\n    media = jnp.array(media)\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(total_costs_shape)\n    target = jnp.ones(target_shape)\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(ValueError):\n      mmm_object.fit(\n          media=media,\n          extra_features=extra_features,\n          media_prior=costs,\n          target=target,\n          number_warmup=5,\n          number_samples=5,\n          number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          custom_priors={models._EXPONENT: 3.}),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          custom_priors={models._HALF_MAX_EFFECTIVE_CONCENTRATION: 6.}),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          target_shape=(20, 3),\n          total_costs_shape=(3, 1)),\n      dict(\n          testcase_name=\"wrong_media_shape\",\n          media=-np.ones((20, 2)),\n          target_shape=(20,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"extra_dims\",\n          media=np.ones((20, 2, 4, 5)),\n          target_shape=(20,),\n          total_costs_shape=(3,))\n  ])\n  def test_fit_wrong_inputs_raises_value_error(\n      self, media, target_shape, total_costs_shape):\n    media = jnp.array(media)\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(total_costs_shape)\n    target = jnp.ones(target_shape)\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(ValueError):\n      mmm_object.fit(\n          media=media,\n          extra_features=extra_features,\n          media_prior=costs,\n          target=target,\n          number_warmup=5,\n          number_samples=5,\n          number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          custom_priors={models._EXPONENT: 3.}),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          custom_priors={models._HALF_MAX_EFFECTIVE_CONCENTRATION: 6.}),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          custom_priors={models._AD_EFFECT_RETENTION_RATE: 5.})\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_not_used_priors(\n      self, model_name, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          target_shape=(20,),\n          total_costs_shape=(3,))\n  ])\n  def test_fit_wrong_inputs_raises_value_error(\n      self, media, target_shape, total_costs_shape):\n    media = jnp.array(media)\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(total_costs_shape)\n    target = jnp.ones(target_shape)\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(ValueError):\n      mmm_object.fit(\n          media=media,\n          extra_features=extra_features,\n          media_prior=costs,\n          target=target,\n          number_warmup=5,\n          number_samples=5,\n          number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          custom_priors={models._EXPONENT: 3.}),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          custom_priors={models._HALF_MAX_EFFECTIVE_CONCENTRATION: 6.}),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          custom_priors={models._AD_EFFECT_RETENTION_RATE: 5.})\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_not_used_priors(\n      self, model_name, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The following passed custom priors dont have a match in the model.\"):\n      mmm_object.fit(\n          media=media,\n          target=target,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "\n    with self.assertRaises(ValueError):\n      mmm_object.fit(\n          media=media,\n          extra_features=extra_features,\n          media_prior=costs,\n          target=target,\n          number_warmup=5,\n          number_samples=5,\n          number_chains=1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          custom_priors={models._EXPONENT: 3.}),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          custom_priors={models._HALF_MAX_EFFECTIVE_CONCENTRATION: 6.}),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          custom_priors={models._AD_EFFECT_RETENTION_RATE: 5.})\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_not_used_priors(\n      self, model_name, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The following passed custom priors dont have a match in the model.\"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_geo_custom_prior_raises_valueerror_if_national_data(self):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          custom_priors={models._EXPONENT: 3.}),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          custom_priors={models._HALF_MAX_EFFECTIVE_CONCENTRATION: 6.}),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          custom_priors={models._AD_EFFECT_RETENTION_RATE: 5.})\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_not_used_priors(\n      self, model_name, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The following passed custom priors dont have a match in the model.\"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_geo_custom_prior_raises_valueerror_if_national_data(self):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The given data is for national models but custom_prior contains \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          custom_priors={models._AD_EFFECT_RETENTION_RATE: 5.})\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_not_used_priors(\n      self, model_name, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The following passed custom priors dont have a match in the model.\"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_geo_custom_prior_raises_valueerror_if_national_data(self):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The given data is for national models but custom_prior contains \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors={models._COEF_SEASONALITY: dist.HalfNormal(3)})\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"too_many_args\",\n          custom_priors={models._INTERCEPT: (3., 4.)}),\n      dict(\n          testcase_name=\"dict_wrong_keys\",\n          custom_priors={models._INTERCEPT: {\"foo\": 6.}})\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The following passed custom priors dont have a match in the model.\"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_geo_custom_prior_raises_valueerror_if_national_data(self):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The given data is for national models but custom_prior contains \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors={models._COEF_SEASONALITY: dist.HalfNormal(3)})\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"too_many_args\",\n          custom_priors={models._INTERCEPT: (3., 4.)}),\n      dict(\n          testcase_name=\"dict_wrong_keys\",\n          custom_priors={models._INTERCEPT: {\"foo\": 6.}})\n  ])\n  def test_fit_with_custom_prior_raises_numpyro_error_if_wrong_args(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(TypeError):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_geo_custom_prior_raises_valueerror_if_national_data(self):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The given data is for national models but custom_prior contains \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors={models._COEF_SEASONALITY: dist.HalfNormal(3)})\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"too_many_args\",\n          custom_priors={models._INTERCEPT: (3., 4.)}),\n      dict(\n          testcase_name=\"dict_wrong_keys\",\n          custom_priors={models._INTERCEPT: {\"foo\": 6.}})\n  ])\n  def test_fit_with_custom_prior_raises_numpyro_error_if_wrong_args(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(TypeError):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"wrong_type1\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"The given data is for national models but custom_prior contains \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors={models._COEF_SEASONALITY: dist.HalfNormal(3)})\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"too_many_args\",\n          custom_priors={models._INTERCEPT: (3., 4.)}),\n      dict(\n          testcase_name=\"dict_wrong_keys\",\n          custom_priors={models._INTERCEPT: {\"foo\": 6.}})\n  ])\n  def test_fit_with_custom_prior_raises_numpyro_error_if_wrong_args(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(TypeError):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"wrong_type1\",\n          custom_priors={models._INTERCEPT: \"hello\"}),\n      dict(\n          testcase_name=\"wrong_type2\",\n          custom_priors={models._INTERCEPT: lightweight_mmm.LightweightMMM()}),\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_if_wrong_format(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          custom_priors={models._COEF_SEASONALITY: dist.HalfNormal(3)})\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"too_many_args\",\n          custom_priors={models._INTERCEPT: (3., 4.)}),\n      dict(\n          testcase_name=\"dict_wrong_keys\",\n          custom_priors={models._INTERCEPT: {\"foo\": 6.}})\n  ])\n  def test_fit_with_custom_prior_raises_numpyro_error_if_wrong_args(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(TypeError):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"wrong_type1\",\n          custom_priors={models._INTERCEPT: \"hello\"}),\n      dict(\n          testcase_name=\"wrong_type2\",\n          custom_priors={models._INTERCEPT: lightweight_mmm.LightweightMMM()}),\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_if_wrong_format(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Priors given must be a Numpyro distribution or one of the \"):\n      mmm_object.fit(\n          media=media,\n          target=target,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "  def test_fit_with_custom_prior_raises_numpyro_error_if_wrong_args(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaises(TypeError):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"wrong_type1\",\n          custom_priors={models._INTERCEPT: \"hello\"}),\n      dict(\n          testcase_name=\"wrong_type2\",\n          custom_priors={models._INTERCEPT: lightweight_mmm.LightweightMMM()}),\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_if_wrong_format(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Priors given must be a Numpyro distribution or one of the \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_custom_priors_uses_correct_given_priors(self):\n    pass\n\n  def test_daily_data_returns_weekday_parameter(self):\n    n = 50\n    media = jnp.arange(2 * n).reshape((n, 2)).astype(jnp.float32)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"wrong_type1\",\n          custom_priors={models._INTERCEPT: \"hello\"}),\n      dict(\n          testcase_name=\"wrong_type2\",\n          custom_priors={models._INTERCEPT: lightweight_mmm.LightweightMMM()}),\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_if_wrong_format(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Priors given must be a Numpyro distribution or one of the \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_custom_priors_uses_correct_given_priors(self):\n    pass\n\n  def test_daily_data_returns_weekday_parameter(self):\n    n = 50\n    media = jnp.arange(2 * n).reshape((n, 2)).astype(jnp.float32)\n    target = 1 + 1 * (jnp.arange(n) % 7 == 1) + media[:, 1]\n    costs = jnp.array([1, 2])\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(media=media, media_prior=costs, target=target,\n                   weekday_seasonality=True, number_warmup=5, number_samples=5,\n                   number_chains=1)\n    self.assertEqual(mmm_object.trace[\"weekday\"].shape, (5, 7))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace\", attribute_name=\"trace\"),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          custom_priors={models._INTERCEPT: \"hello\"}),\n      dict(\n          testcase_name=\"wrong_type2\",\n          custom_priors={models._INTERCEPT: lightweight_mmm.LightweightMMM()}),\n  ])\n  def test_fit_with_custom_prior_raises_valueerror_if_wrong_format(\n      self, custom_priors):\n    media = jnp.ones((20, 3))\n    target = jnp.ones((20,))\n    extra_features = jnp.ones((20, 3))\n    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Priors given must be a Numpyro distribution or one of the \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_custom_priors_uses_correct_given_priors(self):\n    pass\n\n  def test_daily_data_returns_weekday_parameter(self):\n    n = 50\n    media = jnp.arange(2 * n).reshape((n, 2)).astype(jnp.float32)\n    target = 1 + 1 * (jnp.arange(n) % 7 == 1) + media[:, 1]\n    costs = jnp.array([1, 2])\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(media=media, media_prior=costs, target=target,\n                   weekday_seasonality=True, number_warmup=5, number_samples=5,\n                   number_chains=1)\n    self.assertEqual(mmm_object.trace[\"weekday\"].shape, (5, 7))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace\", attribute_name=\"trace\"),\n      dict(testcase_name=\"n_media_channels\", attribute_name=\"n_media_channels\"),\n      dict(testcase_name=\"n_geos\", attribute_name=\"n_geos\"),\n      dict(testcase_name=\"_number_warmup\", attribute_name=\"_number_warmup\"),\n      dict(testcase_name=\"_number_samples\", attribute_name=\"_number_samples\"),\n      dict(testcase_name=\"_number_chains\", attribute_name=\"_number_chains\"),\n      dict(testcase_name=\"_target\", attribute_name=\"_target\"),\n      dict(\n          testcase_name=\"_train_media_size\",\n          attribute_name=\"_train_media_size\"),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    costs = jnp.ones(3)\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Priors given must be a Numpyro distribution or one of the \"):\n      mmm_object.fit(\n          media=media,\n          target=target,\n          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_custom_priors_uses_correct_given_priors(self):\n    pass\n\n  def test_daily_data_returns_weekday_parameter(self):\n    n = 50\n    media = jnp.arange(2 * n).reshape((n, 2)).astype(jnp.float32)\n    target = 1 + 1 * (jnp.arange(n) % 7 == 1) + media[:, 1]\n    costs = jnp.array([1, 2])\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(media=media, media_prior=costs, target=target,\n                   weekday_seasonality=True, number_warmup=5, number_samples=5,\n                   number_chains=1)\n    self.assertEqual(mmm_object.trace[\"weekday\"].shape, (5, 7))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace\", attribute_name=\"trace\"),\n      dict(testcase_name=\"n_media_channels\", attribute_name=\"n_media_channels\"),\n      dict(testcase_name=\"n_geos\", attribute_name=\"n_geos\"),\n      dict(testcase_name=\"_number_warmup\", attribute_name=\"_number_warmup\"),\n      dict(testcase_name=\"_number_samples\", attribute_name=\"_number_samples\"),\n      dict(testcase_name=\"_number_chains\", attribute_name=\"_number_chains\"),\n      dict(testcase_name=\"_target\", attribute_name=\"_target\"),\n      dict(\n          testcase_name=\"_train_media_size\",\n          attribute_name=\"_train_media_size\"),\n      dict(\n          testcase_name=\"_degrees_seasonality\",\n          attribute_name=\"_degrees_seasonality\"),\n      dict(\n          testcase_name=\"_seasonality_frequency\",\n          attribute_name=\"_seasonality_frequency\"),\n      dict(\n          testcase_name=\"_weekday_seasonality\",\n          attribute_name=\"_weekday_seasonality\"),\n      dict(testcase_name=\"extra_features\", attribute_name=\"extra_features\"),\n      dict(testcase_name=\"media\", attribute_name=\"media\"),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          extra_features=extra_features,\n          media_prior=costs,\n          custom_priors=custom_priors)\n\n  def test_fit_with_custom_priors_uses_correct_given_priors(self):\n    pass\n\n  def test_daily_data_returns_weekday_parameter(self):\n    n = 50\n    media = jnp.arange(2 * n).reshape((n, 2)).astype(jnp.float32)\n    target = 1 + 1 * (jnp.arange(n) % 7 == 1) + media[:, 1]\n    costs = jnp.array([1, 2])\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(media=media, media_prior=costs, target=target,\n                   weekday_seasonality=True, number_warmup=5, number_samples=5,\n                   number_chains=1)\n    self.assertEqual(mmm_object.trace[\"weekday\"].shape, (5, 7))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace\", attribute_name=\"trace\"),\n      dict(testcase_name=\"n_media_channels\", attribute_name=\"n_media_channels\"),\n      dict(testcase_name=\"n_geos\", attribute_name=\"n_geos\"),\n      dict(testcase_name=\"_number_warmup\", attribute_name=\"_number_warmup\"),\n      dict(testcase_name=\"_number_samples\", attribute_name=\"_number_samples\"),\n      dict(testcase_name=\"_number_chains\", attribute_name=\"_number_chains\"),\n      dict(testcase_name=\"_target\", attribute_name=\"_target\"),\n      dict(\n          testcase_name=\"_train_media_size\",\n          attribute_name=\"_train_media_size\"),\n      dict(\n          testcase_name=\"_degrees_seasonality\",\n          attribute_name=\"_degrees_seasonality\"),\n      dict(\n          testcase_name=\"_seasonality_frequency\",\n          attribute_name=\"_seasonality_frequency\"),\n      dict(\n          testcase_name=\"_weekday_seasonality\",\n          attribute_name=\"_weekday_seasonality\"),\n      dict(testcase_name=\"extra_features\", attribute_name=\"extra_features\"),\n      dict(testcase_name=\"media\", attribute_name=\"media\"),\n      dict(testcase_name=\"custom_priors\", attribute_name=\"custom_priors\"),\n  ])\n  def test_fitting_attributes_do_not_exist_before_fitting(self, attribute_name):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    self.assertFalse(hasattr(mmm_object, attribute_name))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    target = 1 + 1 * (jnp.arange(n) % 7 == 1) + media[:, 1]\n    costs = jnp.array([1, 2])\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(media=media, media_prior=costs, target=target,\n                   weekday_seasonality=True, number_warmup=5, number_samples=5,\n                   number_chains=1)\n    self.assertEqual(mmm_object.trace[\"weekday\"].shape, (5, 7))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace\", attribute_name=\"trace\"),\n      dict(testcase_name=\"n_media_channels\", attribute_name=\"n_media_channels\"),\n      dict(testcase_name=\"n_geos\", attribute_name=\"n_geos\"),\n      dict(testcase_name=\"_number_warmup\", attribute_name=\"_number_warmup\"),\n      dict(testcase_name=\"_number_samples\", attribute_name=\"_number_samples\"),\n      dict(testcase_name=\"_number_chains\", attribute_name=\"_number_chains\"),\n      dict(testcase_name=\"_target\", attribute_name=\"_target\"),\n      dict(\n          testcase_name=\"_train_media_size\",\n          attribute_name=\"_train_media_size\"),\n      dict(\n          testcase_name=\"_degrees_seasonality\",\n          attribute_name=\"_degrees_seasonality\"),\n      dict(\n          testcase_name=\"_seasonality_frequency\",\n          attribute_name=\"_seasonality_frequency\"),\n      dict(\n          testcase_name=\"_weekday_seasonality\",\n          attribute_name=\"_weekday_seasonality\"),\n      dict(testcase_name=\"extra_features\", attribute_name=\"extra_features\"),\n      dict(testcase_name=\"media\", attribute_name=\"media\"),\n      dict(testcase_name=\"custom_priors\", attribute_name=\"custom_priors\"),\n  ])\n  def test_fitting_attributes_do_not_exist_before_fitting(self, attribute_name):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    self.assertFalse(hasattr(mmm_object, attribute_name))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\")\n  ])\n  def test_predict_fit_sets_correct_attributes(self, media_mix_model):\n    expected_attributes = (\"n_media_channels\", \"_media_prior\", \"trace\",\n                           \"_number_warmup\", \"_number_samples\",\n                           \"_number_chains\", \"_target\", \"_train_media_size\",\n                           \"_degrees_seasonality\", \"_seasonality_frequency\",\n                           \"_weekday_seasonality\", \"media\", \"_extra_features\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      dict(testcase_name=\"n_media_channels\", attribute_name=\"n_media_channels\"),\n      dict(testcase_name=\"n_geos\", attribute_name=\"n_geos\"),\n      dict(testcase_name=\"_number_warmup\", attribute_name=\"_number_warmup\"),\n      dict(testcase_name=\"_number_samples\", attribute_name=\"_number_samples\"),\n      dict(testcase_name=\"_number_chains\", attribute_name=\"_number_chains\"),\n      dict(testcase_name=\"_target\", attribute_name=\"_target\"),\n      dict(\n          testcase_name=\"_train_media_size\",\n          attribute_name=\"_train_media_size\"),\n      dict(\n          testcase_name=\"_degrees_seasonality\",\n          attribute_name=\"_degrees_seasonality\"),\n      dict(\n          testcase_name=\"_seasonality_frequency\",\n          attribute_name=\"_seasonality_frequency\"),\n      dict(\n          testcase_name=\"_weekday_seasonality\",\n          attribute_name=\"_weekday_seasonality\"),\n      dict(testcase_name=\"extra_features\", attribute_name=\"extra_features\"),\n      dict(testcase_name=\"media\", attribute_name=\"media\"),\n      dict(testcase_name=\"custom_priors\", attribute_name=\"custom_priors\"),\n  ])\n  def test_fitting_attributes_do_not_exist_before_fitting(self, attribute_name):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    self.assertFalse(hasattr(mmm_object, attribute_name))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\")\n  ])\n  def test_predict_fit_sets_correct_attributes(self, media_mix_model):\n    expected_attributes = (\"n_media_channels\", \"_media_prior\", \"trace\",\n                           \"_number_warmup\", \"_number_samples\",\n                           \"_number_chains\", \"_target\", \"_train_media_size\",\n                           \"_degrees_seasonality\", \"_seasonality_frequency\",\n                           \"_weekday_seasonality\", \"media\", \"_extra_features\",\n                           \"_mcmc\", \"media_names\", \"custom_priors\")\n\n    mmm_object = getattr(self, media_mix_model)\n\n    for attribute in expected_attributes:\n      self.assertTrue(hasattr(mmm_object, attribute))\n\n  # TODO(): Add testing for more scaled/unscaled options.\n  def test_get_posterior_metrics_produces_without_scaling_expected_output(self):\n    mmm_object = lightweight_mmm.LightweightMMM()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          testcase_name=\"_degrees_seasonality\",\n          attribute_name=\"_degrees_seasonality\"),\n      dict(\n          testcase_name=\"_seasonality_frequency\",\n          attribute_name=\"_seasonality_frequency\"),\n      dict(\n          testcase_name=\"_weekday_seasonality\",\n          attribute_name=\"_weekday_seasonality\"),\n      dict(testcase_name=\"extra_features\", attribute_name=\"extra_features\"),\n      dict(testcase_name=\"media\", attribute_name=\"media\"),\n      dict(testcase_name=\"custom_priors\", attribute_name=\"custom_priors\"),\n  ])\n  def test_fitting_attributes_do_not_exist_before_fitting(self, attribute_name):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    self.assertFalse(hasattr(mmm_object, attribute_name))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\")\n  ])\n  def test_predict_fit_sets_correct_attributes(self, media_mix_model):\n    expected_attributes = (\"n_media_channels\", \"_media_prior\", \"trace\",\n                           \"_number_warmup\", \"_number_samples\",\n                           \"_number_chains\", \"_target\", \"_train_media_size\",\n                           \"_degrees_seasonality\", \"_seasonality_frequency\",\n                           \"_weekday_seasonality\", \"media\", \"_extra_features\",\n                           \"_mcmc\", \"media_names\", \"custom_priors\")\n\n    mmm_object = getattr(self, media_mix_model)\n\n    for attribute in expected_attributes:\n      self.assertTrue(hasattr(mmm_object, attribute))\n\n  # TODO(): Add testing for more scaled/unscaled options.\n  def test_get_posterior_metrics_produces_without_scaling_expected_output(self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((140, 3))\n    mmm_object._media_prior = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((140, 1)) * 5\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 140, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 140)),\n        \"coef_media\": jnp.ones((500, 3)) * 6\n    }\n    contribution, roi = mmm_object.get_posterior_metrics()\n    np.testing.assert_array_almost_equal(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      dict(testcase_name=\"custom_priors\", attribute_name=\"custom_priors\"),\n  ])\n  def test_fitting_attributes_do_not_exist_before_fitting(self, attribute_name):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    self.assertFalse(hasattr(mmm_object, attribute_name))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\")\n  ])\n  def test_predict_fit_sets_correct_attributes(self, media_mix_model):\n    expected_attributes = (\"n_media_channels\", \"_media_prior\", \"trace\",\n                           \"_number_warmup\", \"_number_samples\",\n                           \"_number_chains\", \"_target\", \"_train_media_size\",\n                           \"_degrees_seasonality\", \"_seasonality_frequency\",\n                           \"_weekday_seasonality\", \"media\", \"_extra_features\",\n                           \"_mcmc\", \"media_names\", \"custom_priors\")\n\n    mmm_object = getattr(self, media_mix_model)\n\n    for attribute in expected_attributes:\n      self.assertTrue(hasattr(mmm_object, attribute))\n\n  # TODO(): Add testing for more scaled/unscaled options.\n  def test_get_posterior_metrics_produces_without_scaling_expected_output(self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((140, 3))\n    mmm_object._media_prior = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((140, 1)) * 5\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 140, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 140)),\n        \"coef_media\": jnp.ones((500, 3)) * 6\n    }\n    contribution, roi = mmm_object.get_posterior_metrics()\n    np.testing.assert_array_almost_equal(\n        contribution.mean(axis=0), jnp.array([6., 12., 18.]), decimal=3)\n    np.testing.assert_array_almost_equal(\n        roi.mean(axis=0), jnp.array([28., 112., 56.]), decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\")\n  ])\n  def test_predict_fit_sets_correct_attributes(self, media_mix_model):\n    expected_attributes = (\"n_media_channels\", \"_media_prior\", \"trace\",\n                           \"_number_warmup\", \"_number_samples\",\n                           \"_number_chains\", \"_target\", \"_train_media_size\",\n                           \"_degrees_seasonality\", \"_seasonality_frequency\",\n                           \"_weekday_seasonality\", \"media\", \"_extra_features\",\n                           \"_mcmc\", \"media_names\", \"custom_priors\")\n\n    mmm_object = getattr(self, media_mix_model)\n\n    for attribute in expected_attributes:\n      self.assertTrue(hasattr(mmm_object, attribute))\n\n  # TODO(): Add testing for more scaled/unscaled options.\n  def test_get_posterior_metrics_produces_without_scaling_expected_output(self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((140, 3))\n    mmm_object._media_prior = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((140, 1)) * 5\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 140, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 140)),\n        \"coef_media\": jnp.ones((500, 3)) * 6\n    }\n    contribution, roi = mmm_object.get_posterior_metrics()\n    np.testing.assert_array_almost_equal(\n        contribution.mean(axis=0), jnp.array([6., 12., 18.]), decimal=3)\n    np.testing.assert_array_almost_equal(\n        roi.mean(axis=0), jnp.array([28., 112., 56.]), decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_get_posterior_metrics_produces_correct_shapes(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    contribution, roi = mmm_object.get_posterior_metrics()\n\n    self.assertEqual(contribution.shape, (4, *mmm_object.media.shape[1:]))\n    self.assertEqual(roi.shape, (4, *mmm_object.media.shape[1:]))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "                           \"_mcmc\", \"media_names\", \"custom_priors\")\n\n    mmm_object = getattr(self, media_mix_model)\n\n    for attribute in expected_attributes:\n      self.assertTrue(hasattr(mmm_object, attribute))\n\n  # TODO(): Add testing for more scaled/unscaled options.\n  def test_get_posterior_metrics_produces_without_scaling_expected_output(self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((140, 3))\n    mmm_object._media_prior = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((140, 1)) * 5\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 140, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 140)),\n        \"coef_media\": jnp.ones((500, 3)) * 6\n    }\n    contribution, roi = mmm_object.get_posterior_metrics()\n    np.testing.assert_array_almost_equal(\n        contribution.mean(axis=0), jnp.array([6., 12., 18.]), decimal=3)\n    np.testing.assert_array_almost_equal(\n        roi.mean(axis=0), jnp.array([28., 112., 56.]), decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_get_posterior_metrics_produces_correct_shapes(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    contribution, roi = mmm_object.get_posterior_metrics()\n\n    self.assertEqual(contribution.shape, (4, *mmm_object.media.shape[1:]))\n    self.assertEqual(roi.shape, (4, *mmm_object.media.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_without_gap(\n      self, media_mix_model):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    mmm_object.media = jnp.ones((140, 3))\n    mmm_object._media_prior = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((140, 1)) * 5\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 140, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 140)),\n        \"coef_media\": jnp.ones((500, 3)) * 6\n    }\n    contribution, roi = mmm_object.get_posterior_metrics()\n    np.testing.assert_array_almost_equal(\n        contribution.mean(axis=0), jnp.array([6., 12., 18.]), decimal=3)\n    np.testing.assert_array_almost_equal(\n        roi.mean(axis=0), jnp.array([28., 112., 56.]), decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_get_posterior_metrics_produces_correct_shapes(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    contribution, roi = mmm_object.get_posterior_metrics()\n\n    self.assertEqual(contribution.shape, (4, *mmm_object.media.shape[1:]))\n    self.assertEqual(roi.shape, (4, *mmm_object.media.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_without_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media, extra_features=mmm_object._extra_features)\n\n    self.assertEqual(predictions.shape, (4, *mmm_object._target.shape))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "        contribution.mean(axis=0), jnp.array([6., 12., 18.]), decimal=3)\n    np.testing.assert_array_almost_equal(\n        roi.mean(axis=0), jnp.array([28., 112., 56.]), decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_get_posterior_metrics_produces_correct_shapes(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    contribution, roi = mmm_object.get_posterior_metrics()\n\n    self.assertEqual(contribution.shape, (4, *mmm_object.media.shape[1:]))\n    self.assertEqual(roi.shape, (4, *mmm_object.media.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_without_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media, extra_features=mmm_object._extra_features)\n\n    self.assertEqual(predictions.shape, (4, *mmm_object._target.shape))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_with_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          media_mix_model=\"geo_mmm\")\n  ])\n  def test_get_posterior_metrics_produces_correct_shapes(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    contribution, roi = mmm_object.get_posterior_metrics()\n\n    self.assertEqual(contribution.shape, (4, *mmm_object.media.shape[1:]))\n    self.assertEqual(roi.shape, (4, *mmm_object.media.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_without_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media, extra_features=mmm_object._extra_features)\n\n    self.assertEqual(predictions.shape, (4, *mmm_object._target.shape))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_with_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media[-10:],\n        extra_features=mmm_object._extra_features[-10:],\n        media_gap=mmm_object.media[-10:])\n\n    self.assertEqual(predictions.shape, (4, 10, *mmm_object._target.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_without_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media, extra_features=mmm_object._extra_features)\n\n    self.assertEqual(predictions.shape, (4, *mmm_object._target.shape))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_with_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media[-10:],\n        extra_features=mmm_object._extra_features[-10:],\n        media_gap=mmm_object.media[-10:])\n\n    self.assertEqual(predictions.shape, (4, 10, *mmm_object._target.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_matches_nsample(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    mmm_object.reduce_trace(2)\n\n    self.assertLen(mmm_object.trace[\"sigma\"], 2)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media, extra_features=mmm_object._extra_features)\n\n    self.assertEqual(predictions.shape, (4, *mmm_object._target.shape))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_with_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media[-10:],\n        extra_features=mmm_object._extra_features[-10:],\n        media_gap=mmm_object.media[-10:])\n\n    self.assertEqual(predictions.shape, (4, 10, *mmm_object._target.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_matches_nsample(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    mmm_object.reduce_trace(2)\n\n    self.assertLen(mmm_object.trace[\"sigma\"], 2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_raise_error_with_wrong_nsample(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_predict_produces_output_with_expected_shape_with_gap(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    predictions = mmm_object.predict(\n        media=mmm_object.media[-10:],\n        extra_features=mmm_object._extra_features[-10:],\n        media_gap=mmm_object.media[-10:])\n\n    self.assertEqual(predictions.shape, (4, 10, *mmm_object._target.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_matches_nsample(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    mmm_object.reduce_trace(2)\n\n    self.assertLen(mmm_object.trace[\"sigma\"], 2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_raise_error_with_wrong_nsample(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    with self.assertRaises(ValueError):\n      mmm_object.reduce_trace(200)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "        media=mmm_object.media[-10:],\n        extra_features=mmm_object._extra_features[-10:],\n        media_gap=mmm_object.media[-10:])\n\n    self.assertEqual(predictions.shape, (4, 10, *mmm_object._target.shape[1:]))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_matches_nsample(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    mmm_object.reduce_trace(2)\n\n    self.assertLen(mmm_object.trace[\"sigma\"], 2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_raise_error_with_wrong_nsample(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    with self.assertRaises(ValueError):\n      mmm_object.reduce_trace(200)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_equality_method_lmmm_instance_equals_itself(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n    self.assertEqual(mmm_object, mmm_object)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_matches_nsample(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    mmm_object.reduce_trace(2)\n\n    self.assertLen(mmm_object.trace[\"sigma\"], 2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_raise_error_with_wrong_nsample(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    with self.assertRaises(ValueError):\n      mmm_object.reduce_trace(200)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_equality_method_lmmm_instance_equals_itself(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n    self.assertEqual(mmm_object, mmm_object)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other(self, media_mix_model_1,\n                                               media_mix_model_2):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_trace_after_fit_raise_error_with_wrong_nsample(\n      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    with self.assertRaises(ValueError):\n      mmm_object.reduce_trace(200)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_equality_method_lmmm_instance_equals_itself(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n    self.assertEqual(mmm_object, mmm_object)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other(self, media_mix_model_1,\n                                               media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.copy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n\n    with self.assertRaises(ValueError):\n      mmm_object.reduce_trace(200)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_equality_method_lmmm_instance_equals_itself(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n    self.assertEqual(mmm_object, mmm_object)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other(self, media_mix_model_1,\n                                               media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.copy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other_deepcopy(self, media_mix_model_1,\n                                                        media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.deepcopy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_equality_method_lmmm_instance_equals_itself(self, media_mix_model):\n    mmm_object = getattr(self, media_mix_model)\n    self.assertEqual(mmm_object, mmm_object)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other(self, media_mix_model_1,\n                                               media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.copy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other_deepcopy(self, media_mix_model_1,\n                                                        media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.deepcopy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  def test_different_lmmms_are_not_equal(self):\n    self.assertNotEqual(self.national_mmm, self.geo_mmm)\n\n  def test_default_mmm_instances_equal_each_other(self):\n    self.assertEqual(lightweight_mmm.LightweightMMM(),\n                     lightweight_mmm.LightweightMMM())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other(self, media_mix_model_1,\n                                               media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.copy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other_deepcopy(self, media_mix_model_1,\n                                                        media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.deepcopy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  def test_different_lmmms_are_not_equal(self):\n    self.assertNotEqual(self.national_mmm, self.geo_mmm)\n\n  def test_default_mmm_instances_equal_each_other(self):\n    self.assertEqual(lightweight_mmm.LightweightMMM(),\n                     lightweight_mmm.LightweightMMM())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_equal=True),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_405-455", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.copy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model_1=\"national_mmm\",\n          media_mix_model_2=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other_deepcopy(self, media_mix_model_1,\n                                                        media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.deepcopy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  def test_different_lmmms_are_not_equal(self):\n    self.assertNotEqual(self.national_mmm, self.geo_mmm)\n\n  def test_default_mmm_instances_equal_each_other(self):\n    self.assertEqual(lightweight_mmm.LightweightMMM(),\n                     lightweight_mmm.LightweightMMM())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_equal=True),\n  ])\n  def test_default_carryover_mmm_instance_only_equals_carryover_mmms(\n      self, model_name, expected_equal):\n    carryover_mmm = lightweight_mmm.LightweightMMM(model_name=\"carryover\")\n    other_mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    if expected_equal:\n      self.assertEqual(carryover_mmm, other_mmm)\n    else:\n      self.assertNotEqual(carryover_mmm, other_mmm)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_415-465", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          testcase_name=\"geo_mmm\",\n          media_mix_model_1=\"geo_mmm\",\n          media_mix_model_2=\"geo_mmm\"),\n  ])\n  def test_two_lmmm_instances_equal_each_other_deepcopy(self, media_mix_model_1,\n                                                        media_mix_model_2):\n    mmm_object_1 = getattr(self, media_mix_model_1)\n    mmm_object_2 = copy.deepcopy(getattr(self, media_mix_model_2))\n    self.assertEqual(mmm_object_1, mmm_object_2)\n\n  def test_different_lmmms_are_not_equal(self):\n    self.assertNotEqual(self.national_mmm, self.geo_mmm)\n\n  def test_default_mmm_instances_equal_each_other(self):\n    self.assertEqual(lightweight_mmm.LightweightMMM(),\n                     lightweight_mmm.LightweightMMM())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_equal=True),\n  ])\n  def test_default_carryover_mmm_instance_only_equals_carryover_mmms(\n      self, model_name, expected_equal):\n    carryover_mmm = lightweight_mmm.LightweightMMM(model_name=\"carryover\")\n    other_mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    if expected_equal:\n      self.assertEqual(carryover_mmm, other_mmm)\n    else:\n      self.assertNotEqual(carryover_mmm, other_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_fitted_mmm_does_not_equal_default_mmm(self, media_mix_model):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_425-471", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "  def test_different_lmmms_are_not_equal(self):\n    self.assertNotEqual(self.national_mmm, self.geo_mmm)\n\n  def test_default_mmm_instances_equal_each_other(self):\n    self.assertEqual(lightweight_mmm.LightweightMMM(),\n                     lightweight_mmm.LightweightMMM())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"hill_adstock\",\n          model_name=\"hill_adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_equal=True),\n  ])\n  def test_default_carryover_mmm_instance_only_equals_carryover_mmms(\n      self, model_name, expected_equal):\n    carryover_mmm = lightweight_mmm.LightweightMMM(model_name=\"carryover\")\n    other_mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    if expected_equal:\n      self.assertEqual(carryover_mmm, other_mmm)\n    else:\n      self.assertNotEqual(carryover_mmm, other_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_fitted_mmm_does_not_equal_default_mmm(self, media_mix_model):\n    default_mmm_object = lightweight_mmm.LightweightMMM()\n    fitted_mmm_object = getattr(self, media_mix_model)\n    self.assertNotEqual(default_mmm_object, fitted_mmm_object)\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 471, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_435-471", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "          model_name=\"hill_adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"adstock\",\n          model_name=\"adstock\",\n          expected_equal=False),\n      dict(\n          testcase_name=\"carryover\",\n          model_name=\"carryover\",\n          expected_equal=True),\n  ])\n  def test_default_carryover_mmm_instance_only_equals_carryover_mmms(\n      self, model_name, expected_equal):\n    carryover_mmm = lightweight_mmm.LightweightMMM(model_name=\"carryover\")\n    other_mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    if expected_equal:\n      self.assertEqual(carryover_mmm, other_mmm)\n    else:\n      self.assertNotEqual(carryover_mmm, other_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_fitted_mmm_does_not_equal_default_mmm(self, media_mix_model):\n    default_mmm_object = lightweight_mmm.LightweightMMM()\n    fitted_mmm_object = getattr(self, media_mix_model)\n    self.assertNotEqual(default_mmm_object, fitted_mmm_object)\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 471, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py_445-471", "title": "google_lightweight_mmm-lightweight_mmm-lightweight_mmm_test.py", "text": "  ])\n  def test_default_carryover_mmm_instance_only_equals_carryover_mmms(\n      self, model_name, expected_equal):\n    carryover_mmm = lightweight_mmm.LightweightMMM(model_name=\"carryover\")\n    other_mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n\n    if expected_equal:\n      self.assertEqual(carryover_mmm, other_mmm)\n    else:\n      self.assertNotEqual(carryover_mmm, other_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_mmm\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_mmm\",\n          media_mix_model=\"geo_mmm\"),\n  ])\n  def test_fitted_mmm_does_not_equal_default_mmm(self, media_mix_model):\n    default_mmm_object = lightweight_mmm.LightweightMMM()\n    fitted_mmm_object = getattr(self, media_mix_model)\n    self.assertNotEqual(default_mmm_object, fitted_mmm_object)\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "lightweight_mmm_test.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 471, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Media transformations for accounting for lagging or media effects.\"\"\"\n\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "\nimport functools\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\n\n@functools.partial(jax.jit, static_argnums=[0, 1])\ndef calculate_seasonality(\n    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "    number_periods: int,\n    degrees: int,\n    gamma_seasonality: Union[int, float, jnp.ndarray],\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality using Fourier terms.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "  Args:\n    number_periods: Number of seasonal periods in the data. Eg. for 1 year of\n      seasonal data it will be 52, for 3 years of the same kind 156.\n    degrees: Number of degrees to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frequency of the seasonality being computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "    An array with the seasonality values.\n  \"\"\"\n\n  seasonality_range = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_range = jnp.arange(1, degrees+1)\n  inner_value = seasonality_range * 2 * jnp.pi * degrees_range / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return (season_matrix * gamma_seasonality).sum(axis=2).sum(axis=1)\n\n\n@jax.jit\ndef adstock(data: jnp.ndarray,\n            lag_weight: float = .9,\n            normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(prev_adstock: jnp.ndarray,\n                       data: jnp.ndarray,\n                       lag_weight: float = lag_weight) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray,\n                        weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray,\n                        weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "\n@jax.jit\ndef hill(data: jnp.ndarray, half_max_effective_concentration: jnp.ndarray,\n         slope: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray,\n                        weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray,\n                        weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray,\n                        weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n@jax.jit\ndef apply_exponent_safe(\n    data: jnp.ndarray,\n    exponent: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef carryover(data: jnp.ndarray,\n              ad_effect_retention_rate: jnp.ndarray,\n              peak_effect_delay: jnp.ndarray,\n              number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n@jax.jit\ndef apply_exponent_safe(\n    data: jnp.ndarray,\n    exponent: jnp.ndarray,\n    ) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:\n  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf\n\n  Args:\n    data: Input data to use.\n    exponent: Exponent required for the operations.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_145-190", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n@jax.jit\ndef apply_exponent_safe(\n    data: jnp.ndarray,\n    exponent: jnp.ndarray,\n    ) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:\n  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf\n\n  Args:\n    data: Input data to use.\n    exponent: Exponent required for the operations.\n\n  Returns:\n    The result of the exponent operation with the inputs provided.\n  \"\"\"\n  exponent_safe = jnp.where(condition=(data == 0), x=1, y=data) ** exponent\n  return jnp.where(condition=(data == 0), x=0, y=exponent_safe)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 190, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms.py_155-190", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms.py", "text": "  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(jnp.arange(number_lags, dtype=jnp.float32),\n                                axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n@jax.jit\ndef apply_exponent_safe(\n    data: jnp.ndarray,\n    exponent: jnp.ndarray,\n    ) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:\n  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf\n\n  Args:\n    data: Input data to use.\n    exponent: Exponent required for the operations.\n\n  Returns:\n    The result of the exponent operation with the inputs provided.\n  \"\"\"\n  exponent_safe = jnp.where(condition=(data == 0), x=1, y=data) ** exponent\n  return jnp.where(condition=(data == 0), x=0, y=exponent_safe)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 190, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for media_transforms.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import media_transforms\n\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for media_transforms.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import media_transforms\n\n\nclass MediaTransformsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_four_channels\",\n          data=np.ones((100, 4)),\n          ad_effect_retention_rate=np.array([0.9, 0.8, 0.7, 1]),\n          peak_effect_delay=np.array([0.9, 0.8, 0.7, 1]),\n          number_lags=5),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for media_transforms.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import media_transforms\n\n\nclass MediaTransformsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_four_channels\",\n          data=np.ones((100, 4)),\n          ad_effect_retention_rate=np.array([0.9, 0.8, 0.7, 1]),\n          peak_effect_delay=np.array([0.9, 0.8, 0.7, 1]),\n          number_lags=5),\n      dict(\n          testcase_name=\"2d_one_channel\",\n          data=np.ones((300, 1)),\n          ad_effect_retention_rate=np.array([0.2]),\n          peak_effect_delay=np.array([1]),\n          number_lags=10),\n      dict(\n          testcase_name=\"3d_10channels_10geos\",\n          data=np.ones((100, 10, 10)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for media_transforms.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import media_transforms\n\n\nclass MediaTransformsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_four_channels\",\n          data=np.ones((100, 4)),\n          ad_effect_retention_rate=np.array([0.9, 0.8, 0.7, 1]),\n          peak_effect_delay=np.array([0.9, 0.8, 0.7, 1]),\n          number_lags=5),\n      dict(\n          testcase_name=\"2d_one_channel\",\n          data=np.ones((300, 1)),\n          ad_effect_retention_rate=np.array([0.2]),\n          peak_effect_delay=np.array([1]),\n          number_lags=10),\n      dict(\n          testcase_name=\"3d_10channels_10geos\",\n          data=np.ones((100, 10, 10)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n      dict(\n          testcase_name=\"3d_10channels_8geos\",\n          data=np.ones((100, 10, 8)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n  ])\n  def test_carryover_produces_correct_shape(self, data,\n                                            ad_effect_retention_rate,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import media_transforms\n\n\nclass MediaTransformsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_four_channels\",\n          data=np.ones((100, 4)),\n          ad_effect_retention_rate=np.array([0.9, 0.8, 0.7, 1]),\n          peak_effect_delay=np.array([0.9, 0.8, 0.7, 1]),\n          number_lags=5),\n      dict(\n          testcase_name=\"2d_one_channel\",\n          data=np.ones((300, 1)),\n          ad_effect_retention_rate=np.array([0.2]),\n          peak_effect_delay=np.array([1]),\n          number_lags=10),\n      dict(\n          testcase_name=\"3d_10channels_10geos\",\n          data=np.ones((100, 10, 10)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n      dict(\n          testcase_name=\"3d_10channels_8geos\",\n          data=np.ones((100, 10, 8)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n  ])\n  def test_carryover_produces_correct_shape(self, data,\n                                            ad_effect_retention_rate,\n                                            peak_effect_delay, number_lags):\n\n    generated_output = media_transforms.carryover(data,\n                                                  ad_effect_retention_rate,\n                                                  peak_effect_delay,\n                                                  number_lags)\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "class MediaTransformsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_four_channels\",\n          data=np.ones((100, 4)),\n          ad_effect_retention_rate=np.array([0.9, 0.8, 0.7, 1]),\n          peak_effect_delay=np.array([0.9, 0.8, 0.7, 1]),\n          number_lags=5),\n      dict(\n          testcase_name=\"2d_one_channel\",\n          data=np.ones((300, 1)),\n          ad_effect_retention_rate=np.array([0.2]),\n          peak_effect_delay=np.array([1]),\n          number_lags=10),\n      dict(\n          testcase_name=\"3d_10channels_10geos\",\n          data=np.ones((100, 10, 10)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n      dict(\n          testcase_name=\"3d_10channels_8geos\",\n          data=np.ones((100, 10, 8)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n  ])\n  def test_carryover_produces_correct_shape(self, data,\n                                            ad_effect_retention_rate,\n                                            peak_effect_delay, number_lags):\n\n    generated_output = media_transforms.carryover(data,\n                                                  ad_effect_retention_rate,\n                                                  peak_effect_delay,\n                                                  number_lags)\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_three_channels\",\n          data=np.ones((100, 3)),\n          half_max_effective_concentration=np.array([0.9, 0.8, 0.7]),\n          slope=np.array([2, 2, 1])),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          half_max_effective_concentration=np.array([0.9]),\n          slope=np.array([5])),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "          testcase_name=\"2d_one_channel\",\n          data=np.ones((300, 1)),\n          ad_effect_retention_rate=np.array([0.2]),\n          peak_effect_delay=np.array([1]),\n          number_lags=10),\n      dict(\n          testcase_name=\"3d_10channels_10geos\",\n          data=np.ones((100, 10, 10)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n      dict(\n          testcase_name=\"3d_10channels_8geos\",\n          data=np.ones((100, 10, 8)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n  ])\n  def test_carryover_produces_correct_shape(self, data,\n                                            ad_effect_retention_rate,\n                                            peak_effect_delay, number_lags):\n\n    generated_output = media_transforms.carryover(data,\n                                                  ad_effect_retention_rate,\n                                                  peak_effect_delay,\n                                                  number_lags)\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_three_channels\",\n          data=np.ones((100, 3)),\n          half_max_effective_concentration=np.array([0.9, 0.8, 0.7]),\n          slope=np.array([2, 2, 1])),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          half_max_effective_concentration=np.array([0.9]),\n          slope=np.array([5])),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          half_max_effective_concentration=np.expand_dims(np.ones(10), axis=-1),\n          slope=np.expand_dims(np.ones(10), axis=-1)),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          half_max_effective_concentration=np.expand_dims(np.ones(8), axis=-1),\n          slope=np.expand_dims(np.ones(8), axis=-1)),\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "          number_lags=13),\n      dict(\n          testcase_name=\"3d_10channels_8geos\",\n          data=np.ones((100, 10, 8)),\n          ad_effect_retention_rate=np.ones(10),\n          peak_effect_delay=np.ones(10),\n          number_lags=13),\n  ])\n  def test_carryover_produces_correct_shape(self, data,\n                                            ad_effect_retention_rate,\n                                            peak_effect_delay, number_lags):\n\n    generated_output = media_transforms.carryover(data,\n                                                  ad_effect_retention_rate,\n                                                  peak_effect_delay,\n                                                  number_lags)\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_three_channels\",\n          data=np.ones((100, 3)),\n          half_max_effective_concentration=np.array([0.9, 0.8, 0.7]),\n          slope=np.array([2, 2, 1])),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          half_max_effective_concentration=np.array([0.9]),\n          slope=np.array([5])),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          half_max_effective_concentration=np.expand_dims(np.ones(10), axis=-1),\n          slope=np.expand_dims(np.ones(10), axis=-1)),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          half_max_effective_concentration=np.expand_dims(np.ones(8), axis=-1),\n          slope=np.expand_dims(np.ones(8), axis=-1)),\n  ])\n  def test_hill_produces_correct_shape(self, data,\n                                       half_max_effective_concentration, slope):\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "                                            peak_effect_delay, number_lags):\n\n    generated_output = media_transforms.carryover(data,\n                                                  ad_effect_retention_rate,\n                                                  peak_effect_delay,\n                                                  number_lags)\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_three_channels\",\n          data=np.ones((100, 3)),\n          half_max_effective_concentration=np.array([0.9, 0.8, 0.7]),\n          slope=np.array([2, 2, 1])),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          half_max_effective_concentration=np.array([0.9]),\n          slope=np.array([5])),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          half_max_effective_concentration=np.expand_dims(np.ones(10), axis=-1),\n          slope=np.expand_dims(np.ones(10), axis=-1)),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          half_max_effective_concentration=np.expand_dims(np.ones(8), axis=-1),\n          slope=np.expand_dims(np.ones(8), axis=-1)),\n  ])\n  def test_hill_produces_correct_shape(self, data,\n                                       half_max_effective_concentration, slope):\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_five_channels\",\n          data=np.ones((100, 5)),\n          lag_weight=np.array([0.2, 0.3, 0.8, 0.2, 0.1]),\n          normalise=True),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          lag_weight=np.array([0.4]),\n          normalise=False),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "          testcase_name=\"2d_three_channels\",\n          data=np.ones((100, 3)),\n          half_max_effective_concentration=np.array([0.9, 0.8, 0.7]),\n          slope=np.array([2, 2, 1])),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          half_max_effective_concentration=np.array([0.9]),\n          slope=np.array([5])),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          half_max_effective_concentration=np.expand_dims(np.ones(10), axis=-1),\n          slope=np.expand_dims(np.ones(10), axis=-1)),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          half_max_effective_concentration=np.expand_dims(np.ones(8), axis=-1),\n          slope=np.expand_dims(np.ones(8), axis=-1)),\n  ])\n  def test_hill_produces_correct_shape(self, data,\n                                       half_max_effective_concentration, slope):\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_five_channels\",\n          data=np.ones((100, 5)),\n          lag_weight=np.array([0.2, 0.3, 0.8, 0.2, 0.1]),\n          normalise=True),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          lag_weight=np.array([0.4]),\n          normalise=False),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          lag_weight=np.expand_dims(np.ones(10), axis=-1),\n          normalise=True),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          lag_weight=np.expand_dims(np.ones(8), axis=-1),\n          normalise=True),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          half_max_effective_concentration=np.expand_dims(np.ones(10), axis=-1),\n          slope=np.expand_dims(np.ones(10), axis=-1)),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          half_max_effective_concentration=np.expand_dims(np.ones(8), axis=-1),\n          slope=np.expand_dims(np.ones(8), axis=-1)),\n  ])\n  def test_hill_produces_correct_shape(self, data,\n                                       half_max_effective_concentration, slope):\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_five_channels\",\n          data=np.ones((100, 5)),\n          lag_weight=np.array([0.2, 0.3, 0.8, 0.2, 0.1]),\n          normalise=True),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          lag_weight=np.array([0.4]),\n          normalise=False),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          lag_weight=np.expand_dims(np.ones(10), axis=-1),\n          normalise=True),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          lag_weight=np.expand_dims(np.ones(8), axis=-1),\n          normalise=True),\n  ])\n  def test_adstock_produces_correct_shape(self, data, lag_weight, normalise):\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight, normalise=normalise)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "  def test_hill_produces_correct_shape(self, data,\n                                       half_max_effective_concentration, slope):\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2d_five_channels\",\n          data=np.ones((100, 5)),\n          lag_weight=np.array([0.2, 0.3, 0.8, 0.2, 0.1]),\n          normalise=True),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          lag_weight=np.array([0.4]),\n          normalise=False),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          lag_weight=np.expand_dims(np.ones(10), axis=-1),\n          normalise=True),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          lag_weight=np.expand_dims(np.ones(8), axis=-1),\n          normalise=True),\n  ])\n  def test_adstock_produces_correct_shape(self, data, lag_weight, normalise):\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight, normalise=normalise)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "      dict(\n          testcase_name=\"2d_five_channels\",\n          data=np.ones((100, 5)),\n          lag_weight=np.array([0.2, 0.3, 0.8, 0.2, 0.1]),\n          normalise=True),\n      dict(\n          testcase_name=\"2d_one_channels\",\n          data=np.ones((100, 1)),\n          lag_weight=np.array([0.4]),\n          normalise=False),\n      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          lag_weight=np.expand_dims(np.ones(10), axis=-1),\n          normalise=True),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          lag_weight=np.expand_dims(np.ones(8), axis=-1),\n          normalise=True),\n  ])\n  def test_adstock_produces_correct_shape(self, data, lag_weight, normalise):\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight, normalise=normalise)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n    def f_safe(data, exponent):\n      x = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "      dict(\n          testcase_name=\"3d_10channels_5geos\",\n          data=np.ones((100, 10, 5)),\n          lag_weight=np.expand_dims(np.ones(10), axis=-1),\n          normalise=True),\n      dict(\n          testcase_name=\"3d_8channels_10geos\",\n          data=np.ones((100, 8, 10)),\n          lag_weight=np.expand_dims(np.ones(8), axis=-1),\n          normalise=True),\n  ])\n  def test_adstock_produces_correct_shape(self, data, lag_weight, normalise):\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight, normalise=normalise)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n    def f_safe(data, exponent):\n      x = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "  ])\n  def test_adstock_produces_correct_shape(self, data, lag_weight, normalise):\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight, normalise=normalise)\n\n    self.assertEqual(generated_output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n    def f_safe(data, exponent):\n      x = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n    def f_safe(data, exponent):\n      x = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n    def f_safe(data, exponent):\n      x = media_transforms.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\n@parameterized.parameters(range(1, 5))\ndef test_calculate_seasonality_produces_correct_standard_deviation(\n    self, degrees):\n  # It's not very obvious that this is the expected standard deviation, but it\n  # seems to be true mathematically and this makes a very convenient unit test.\n  expected_standard_deviation = jnp.sqrt(degrees)\n\n  seasonal_curve = media_transforms.calculate_seasonality(\n      number_periods=1,\n      degrees=degrees,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "    generated_output = media_transforms.adstock(\n        data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\n@parameterized.parameters(range(1, 5))\ndef test_calculate_seasonality_produces_correct_standard_deviation(\n    self, degrees):\n  # It's not very obvious that this is the expected standard deviation, but it\n  # seems to be true mathematically and this makes a very convenient unit test.\n  expected_standard_deviation = jnp.sqrt(degrees)\n\n  seasonal_curve = media_transforms.calculate_seasonality(\n      number_periods=1,\n      degrees=degrees,\n      gamma_seasonality=1,\n      frequency=1200,\n  )\n  observed_standard_deviation = jnp.std(seasonal_curve)\n\n  self.assertAlmostEqual(\n      observed_standard_deviation, expected_standard_deviation, delta=0.01)\n\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_165-206", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "    generated_output = media_transforms.hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\n@parameterized.parameters(range(1, 5))\ndef test_calculate_seasonality_produces_correct_standard_deviation(\n    self, degrees):\n  # It's not very obvious that this is the expected standard deviation, but it\n  # seems to be true mathematically and this makes a very convenient unit test.\n  expected_standard_deviation = jnp.sqrt(degrees)\n\n  seasonal_curve = media_transforms.calculate_seasonality(\n      number_periods=1,\n      degrees=degrees,\n      gamma_seasonality=1,\n      frequency=1200,\n  )\n  observed_standard_deviation = jnp.std(seasonal_curve)\n\n  self.assertAlmostEqual(\n      observed_standard_deviation, expected_standard_deviation, delta=0.01)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 206, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-media_transforms_test.py_175-206", "title": "google_lightweight_mmm-lightweight_mmm-media_transforms_test.py", "text": "    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = media_transforms.carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\n@parameterized.parameters(range(1, 5))\ndef test_calculate_seasonality_produces_correct_standard_deviation(\n    self, degrees):\n  # It's not very obvious that this is the expected standard deviation, but it\n  # seems to be true mathematically and this makes a very convenient unit test.\n  expected_standard_deviation = jnp.sqrt(degrees)\n\n  seasonal_curve = media_transforms.calculate_seasonality(\n      number_periods=1,\n      degrees=degrees,\n      gamma_seasonality=1,\n      frequency=1200,\n  )\n  observed_standard_deviation = jnp.std(seasonal_curve)\n\n  self.assertAlmostEqual(\n      observed_standard_deviation, expected_standard_deviation, delta=0.01)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "media_transforms_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 206, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\n\nAST=Module(Expr(Constant)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\n\nAST=Module(Expr(Constant)Import(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\nAST=Module(Expr(Constant)Import(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module containing the different models available in the lightweightMMM lib.\n\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\nAST=Module(Expr(Constant)Import(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\nCurrently this file contains a main model with three possible options for\nprocessing the media data. Which essentially grants the possibility of building\nthree different models.\n  - Adstock\n  - Hill-Adstock\n  - Carryover\n\"\"\"\nimport sys\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "if sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\nfrom typing import Any, Dict, Mapping, MutableMapping, Optional, Sequence, Union\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\nAST=Module(If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ImportFrom(aliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Load))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "from numpyro import distributions as dist\n\nfrom lightweight_mmm import media_transforms\n\nPrior = Union[\n    dist.Distribution,\n    Dict[str, float],\n    Sequence[float],\n    float\n]\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      media_data: jnp.ndarray,\n      custom_priors: MutableMapping[str, Prior],\n      **kwargs: Any) -> jnp.ndarray:\n    ...\n\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Load))Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)))))Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Load))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n\n_INTERCEPT = \"intercept\"\n_COEF_TREND = \"coef_trend\"\n_EXPO_TREND = \"expo_trend\"\n_SIGMA = \"sigma\"\n_GAMMA_SEASONALITY = \"gamma_seasonality\"\n_WEEKDAY = \"weekday\"\n_COEF_EXTRA_FEATURES = \"coef_extra_features\"\n_COEF_SEASONALITY = \"coef_seasonality\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\nMODEL_PRIORS_NAMES = frozenset((\n    _INTERCEPT,\n    _COEF_TREND,\n    _EXPO_TREND,\n    _SIGMA,\n    _GAMMA_SEASONALITY,\n    _WEEKDAY,\n    _COEF_EXTRA_FEATURES,\n    _COEF_SEASONALITY))\n\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n_EXPONENT = \"exponent\"\n_LAG_WEIGHT = \"lag_weight\"\n_HALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\n_SLOPE = \"slope\"\n_AD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\n_PEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\nTRANSFORM_PRIORS_NAMES = immutabledict.immutabledict({\n    \"carryover\":\n        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "        frozenset((_AD_EFFECT_RETENTION_RATE, _PEAK_EFFECT_DELAY, _EXPONENT)),\n    \"adstock\":\n        frozenset((_EXPONENT, _LAG_WEIGHT)),\n    \"hill_adstock\":\n        frozenset((_LAG_WEIGHT, _HALF_MAX_EFFECTIVE_CONCENTRATION, _SLOPE))\n})\n\nGEO_ONLY_PRIORS = frozenset((_COEF_SEASONALITY,))\n\n\ndef _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "def _get_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      _INTERCEPT: dist.HalfNormal(scale=2.),\n      _COEF_TREND: dist.Normal(loc=0., scale=1.),\n      _EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      _SIGMA: dist.Gamma(concentration=1., rate=1.),\n      _GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      _WEEKDAY: dist.Normal(loc=0., scale=.5),\n      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n\n\ndef transform_adstock(media_data: jnp.ndarray,\n                      custom_priors: MutableMapping[str, Prior],\n                      normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "      _COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      _COEF_SEASONALITY: dist.HalfNormal(scale=.5)\n  })\n\n\ndef _get_transform_default_priors() -> Mapping[str, Prior]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      \"carryover\":\n          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n\n\ndef transform_adstock(media_data: jnp.ndarray,\n                      custom_priors: MutableMapping[str, Prior],\n                      normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "          immutabledict.immutabledict({\n              _AD_EFFECT_RETENTION_RATE:\n                  dist.Beta(concentration1=1., concentration0=1.),\n              _PEAK_EFFECT_DELAY:\n                  dist.HalfNormal(scale=2.),\n              _EXPONENT:\n                  dist.Beta(concentration1=9., concentration0=1.)\n          }),\n      \"adstock\":\n          immutabledict.immutabledict({\n              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n\n\ndef transform_adstock(media_data: jnp.ndarray,\n                      custom_priors: MutableMapping[str, Prior],\n                      normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "              _EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n              _LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.)\n          }),\n      \"hill_adstock\":\n          immutabledict.immutabledict({\n              _LAG_WEIGHT:\n                  dist.Beta(concentration1=2., concentration0=1.),\n              _HALF_MAX_EFFECTIVE_CONCENTRATION:\n                  dist.Gamma(concentration=1., rate=1.),\n              _SLOPE:\n                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n\n\ndef transform_adstock(media_data: jnp.ndarray,\n                      custom_priors: MutableMapping[str, Prior],\n                      normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "                  dist.Gamma(concentration=1., rate=1.)\n          })\n  })\n\n\ndef transform_adstock(media_data: jnp.ndarray,\n                      custom_priors: MutableMapping[str, Prior],\n                      normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n\n  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n\n\ndef transform_hill_adstock(media_data: jnp.ndarray,\n                           custom_priors: MutableMapping[str, Prior],\n                           normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n\n  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n\n\ndef transform_hill_adstock(media_data: jnp.ndarray,\n                           custom_priors: MutableMapping[str, Prior],\n                           normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    normalise: Whether to normalise the output values.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n\n  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n\n\ndef transform_hill_adstock(media_data: jnp.ndarray,\n                           custom_priors: MutableMapping[str, Prior],\n                           normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"hill_adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    exponent = jnp.expand_dims(exponent, axis=-1)\n\n  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n\n\ndef transform_hill_adstock(media_data: jnp.ndarray,\n                           custom_priors: MutableMapping[str, Prior],\n                           normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"hill_adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n                     size=media_data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=_HALF_MAX_EFFECTIVE_CONCENTRATION,\n        fn=custom_priors.get(\n            _HALF_MAX_EFFECTIVE_CONCENTRATION,\n            transform_default_priors[_HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{_SLOPE}_plate\",\n                     size=media_data.shape[1]):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  adstock = media_transforms.adstock(\n      data=media_data, lag_weight=lag_weight, normalise=normalise)\n\n  return media_transforms.apply_exponent_safe(data=adstock, exponent=exponent)\n\n\ndef transform_hill_adstock(media_data: jnp.ndarray,\n                           custom_priors: MutableMapping[str, Prior],\n                           normalise: bool = True) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"hill_adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n                     size=media_data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=_HALF_MAX_EFFECTIVE_CONCENTRATION,\n        fn=custom_priors.get(\n            _HALF_MAX_EFFECTIVE_CONCENTRATION,\n            transform_default_priors[_HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{_SLOPE}_plate\",\n                     size=media_data.shape[1]):\n    slope = numpyro.sample(\n        name=_SLOPE,\n        fn=custom_priors.get(_SLOPE, transform_default_priors[_SLOPE]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    normalise: Whether to normalise the output values.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"hill_adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n                     size=media_data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=_HALF_MAX_EFFECTIVE_CONCENTRATION,\n        fn=custom_priors.get(\n            _HALF_MAX_EFFECTIVE_CONCENTRATION,\n            transform_default_priors[_HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{_SLOPE}_plate\",\n                     size=media_data.shape[1]):\n    slope = numpyro.sample(\n        name=_SLOPE,\n        fn=custom_priors.get(_SLOPE, transform_default_priors[_SLOPE]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return media_transforms.hill(\n      data=media_transforms.adstock(\n          data=media_data, lag_weight=lag_weight, normalise=normalise),\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef transform_carryover(media_data: jnp.ndarray,\n                        custom_priors: MutableMapping[str, Prior],\n                        number_lags: int = 13) -> jnp.ndarray:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"hill_adstock\"]\n  with numpyro.plate(name=f\"{_LAG_WEIGHT}_plate\",\n                     size=media_data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=_LAG_WEIGHT,\n        fn=custom_priors.get(_LAG_WEIGHT,\n                             transform_default_priors[_LAG_WEIGHT]))\n\n  with numpyro.plate(name=f\"{_HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n                     size=media_data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=_HALF_MAX_EFFECTIVE_CONCENTRATION,\n        fn=custom_priors.get(\n            _HALF_MAX_EFFECTIVE_CONCENTRATION,\n            transform_default_priors[_HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{_SLOPE}_plate\",\n                     size=media_data.shape[1]):\n    slope = numpyro.sample(\n        name=_SLOPE,\n        fn=custom_priors.get(_SLOPE, transform_default_priors[_SLOPE]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return media_transforms.hill(\n      data=media_transforms.adstock(\n          data=media_data, lag_weight=lag_weight, normalise=normalise),\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef transform_carryover(media_data: jnp.ndarray,\n                        custom_priors: MutableMapping[str, Prior],\n                        number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for carryover and exponent\n      are \"ad_effect_retention_rate_plate\", \"peak_effect_delay_plate\" and\n      \"exponent\".\n    number_lags: Number of lags for the carryover function.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  with numpyro.plate(name=f\"{_HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n                     size=media_data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=_HALF_MAX_EFFECTIVE_CONCENTRATION,\n        fn=custom_priors.get(\n            _HALF_MAX_EFFECTIVE_CONCENTRATION,\n            transform_default_priors[_HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{_SLOPE}_plate\",\n                     size=media_data.shape[1]):\n    slope = numpyro.sample(\n        name=_SLOPE,\n        fn=custom_priors.get(_SLOPE, transform_default_priors[_SLOPE]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return media_transforms.hill(\n      data=media_transforms.adstock(\n          data=media_data, lag_weight=lag_weight, normalise=normalise),\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef transform_carryover(media_data: jnp.ndarray,\n                        custom_priors: MutableMapping[str, Prior],\n                        number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for carryover and exponent\n      are \"ad_effect_retention_rate_plate\", \"peak_effect_delay_plate\" and\n      \"exponent\".\n    number_lags: Number of lags for the carryover function.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"carryover\"]\n  with numpyro.plate(name=f\"{_AD_EFFECT_RETENTION_RATE}_plate\",\n                     size=media_data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=_AD_EFFECT_RETENTION_RATE,\n        fn=custom_priors.get(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "    slope = numpyro.sample(\n        name=_SLOPE,\n        fn=custom_priors.get(_SLOPE, transform_default_priors[_SLOPE]))\n\n  if media_data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return media_transforms.hill(\n      data=media_transforms.adstock(\n          data=media_data, lag_weight=lag_weight, normalise=normalise),\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef transform_carryover(media_data: jnp.ndarray,\n                        custom_priors: MutableMapping[str, Prior],\n                        number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for carryover and exponent\n      are \"ad_effect_retention_rate_plate\", \"peak_effect_delay_plate\" and\n      \"exponent\".\n    number_lags: Number of lags for the carryover function.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"carryover\"]\n  with numpyro.plate(name=f\"{_AD_EFFECT_RETENTION_RATE}_plate\",\n                     size=media_data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=_AD_EFFECT_RETENTION_RATE,\n        fn=custom_priors.get(\n            _AD_EFFECT_RETENTION_RATE,\n            transform_default_priors[_AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(name=f\"{_PEAK_EFFECT_DELAY}_plate\",\n                     size=media_data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=_PEAK_EFFECT_DELAY,\n        fn=custom_priors.get(\n            _PEAK_EFFECT_DELAY, transform_default_priors[_PEAK_EFFECT_DELAY]))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  return media_transforms.hill(\n      data=media_transforms.adstock(\n          data=media_data, lag_weight=lag_weight, normalise=normalise),\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef transform_carryover(media_data: jnp.ndarray,\n                        custom_priors: MutableMapping[str, Prior],\n                        number_lags: int = 13) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for carryover and exponent\n      are \"ad_effect_retention_rate_plate\", \"peak_effect_delay_plate\" and\n      \"exponent\".\n    number_lags: Number of lags for the carryover function.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"carryover\"]\n  with numpyro.plate(name=f\"{_AD_EFFECT_RETENTION_RATE}_plate\",\n                     size=media_data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=_AD_EFFECT_RETENTION_RATE,\n        fn=custom_priors.get(\n            _AD_EFFECT_RETENTION_RATE,\n            transform_default_priors[_AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(name=f\"{_PEAK_EFFECT_DELAY}_plate\",\n                     size=media_data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=_PEAK_EFFECT_DELAY,\n        fn=custom_priors.get(\n            _PEAK_EFFECT_DELAY, transform_default_priors[_PEAK_EFFECT_DELAY]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n  carryover = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    media_data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for carryover and exponent\n      are \"ad_effect_retention_rate_plate\", \"peak_effect_delay_plate\" and\n      \"exponent\".\n    number_lags: Number of lags for the carryover function.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"carryover\"]\n  with numpyro.plate(name=f\"{_AD_EFFECT_RETENTION_RATE}_plate\",\n                     size=media_data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=_AD_EFFECT_RETENTION_RATE,\n        fn=custom_priors.get(\n            _AD_EFFECT_RETENTION_RATE,\n            transform_default_priors[_AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(name=f\"{_PEAK_EFFECT_DELAY}_plate\",\n                     size=media_data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=_PEAK_EFFECT_DELAY,\n        fn=custom_priors.get(\n            _PEAK_EFFECT_DELAY, transform_default_priors[_PEAK_EFFECT_DELAY]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n  carryover = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n  if media_data.ndim == 3:\n    exponent = jnp.expand_dims(exponent, axis=-1)\n  return media_transforms.apply_exponent_safe(data=carryover, exponent=exponent)\n\n\ndef media_mix_model(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n  Returns:\n    The transformed media data.\n  \"\"\"\n  transform_default_priors = _get_transform_default_priors()[\"carryover\"]\n  with numpyro.plate(name=f\"{_AD_EFFECT_RETENTION_RATE}_plate\",\n                     size=media_data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=_AD_EFFECT_RETENTION_RATE,\n        fn=custom_priors.get(\n            _AD_EFFECT_RETENTION_RATE,\n            transform_default_priors[_AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(name=f\"{_PEAK_EFFECT_DELAY}_plate\",\n                     size=media_data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=_PEAK_EFFECT_DELAY,\n        fn=custom_priors.get(\n            _PEAK_EFFECT_DELAY, transform_default_priors[_PEAK_EFFECT_DELAY]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n  carryover = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n  if media_data.ndim == 3:\n    exponent = jnp.expand_dims(exponent, axis=-1)\n  return media_transforms.apply_exponent_safe(data=carryover, exponent=exponent)\n\n\ndef media_mix_model(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    media_prior: jnp.ndarray,\n    degrees_seasonality: int,\n    frequency: int,\n    transform_function: TransformFunction,\n    custom_priors: MutableMapping[str, Prior],\n    transform_kwargs: Optional[MutableMapping[str, Any]] = None,\n    weekday_seasonality: bool = False,\n    extra_features: Optional[jnp.array] = None\n    ) -> None:\n  \"\"\"Media mix model.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "            _AD_EFFECT_RETENTION_RATE,\n            transform_default_priors[_AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(name=f\"{_PEAK_EFFECT_DELAY}_plate\",\n                     size=media_data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=_PEAK_EFFECT_DELAY,\n        fn=custom_priors.get(\n            _PEAK_EFFECT_DELAY, transform_default_priors[_PEAK_EFFECT_DELAY]))\n\n  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n  carryover = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n  if media_data.ndim == 3:\n    exponent = jnp.expand_dims(exponent, axis=-1)\n  return media_transforms.apply_exponent_safe(data=carryover, exponent=exponent)\n\n\ndef media_mix_model(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    media_prior: jnp.ndarray,\n    degrees_seasonality: int,\n    frequency: int,\n    transform_function: TransformFunction,\n    custom_priors: MutableMapping[str, Prior],\n    transform_kwargs: Optional[MutableMapping[str, Any]] = None,\n    weekday_seasonality: bool = False,\n    extra_features: Optional[jnp.array] = None\n    ) -> None:\n  \"\"\"Media mix model.\n\n  Args:\n    media_data: Media data to be be used in the model.\n    target_data: Target data for the model.\n    media_prior: Cost prior for each of the media channels.\n    degrees_seasonality: Number of degrees of seasonality to use.\n    frequency: Frequency of the time span which was used to aggregate the data.\n      Eg. if weekly data then frequency is 52.\n    transform_function: Function to use to transform the media data in the\n      model. Currently the following are supported: 'transform_adstock',", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  with numpyro.plate(name=f\"{_EXPONENT}_plate\",\n                     size=media_data.shape[1]):\n    exponent = numpyro.sample(\n        name=_EXPONENT,\n        fn=custom_priors.get(_EXPONENT,\n                             transform_default_priors[_EXPONENT]))\n  carryover = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n  if media_data.ndim == 3:\n    exponent = jnp.expand_dims(exponent, axis=-1)\n  return media_transforms.apply_exponent_safe(data=carryover, exponent=exponent)\n\n\ndef media_mix_model(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    media_prior: jnp.ndarray,\n    degrees_seasonality: int,\n    frequency: int,\n    transform_function: TransformFunction,\n    custom_priors: MutableMapping[str, Prior],\n    transform_kwargs: Optional[MutableMapping[str, Any]] = None,\n    weekday_seasonality: bool = False,\n    extra_features: Optional[jnp.array] = None\n    ) -> None:\n  \"\"\"Media mix model.\n\n  Args:\n    media_data: Media data to be be used in the model.\n    target_data: Target data for the model.\n    media_prior: Cost prior for each of the media channels.\n    degrees_seasonality: Number of degrees of seasonality to use.\n    frequency: Frequency of the time span which was used to aggregate the data.\n      Eg. if weekly data then frequency is 52.\n    transform_function: Function to use to transform the media data in the\n      model. Currently the following are supported: 'transform_adstock',\n        'transform_carryover' and 'transform_hill_adstock'.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n    transform_kwargs: Any extra keyword arguments to pass to the transform\n      function. For example the adstock function can take a boolean to noramlise\n      output or not.\n    weekday_seasonality: In case of daily data you can estimate a weekday (7)\n      parameter.\n    extra_features: Extra features data to include in the model.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "      number_lags=number_lags)\n\n  if media_data.ndim == 3:\n    exponent = jnp.expand_dims(exponent, axis=-1)\n  return media_transforms.apply_exponent_safe(data=carryover, exponent=exponent)\n\n\ndef media_mix_model(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    media_prior: jnp.ndarray,\n    degrees_seasonality: int,\n    frequency: int,\n    transform_function: TransformFunction,\n    custom_priors: MutableMapping[str, Prior],\n    transform_kwargs: Optional[MutableMapping[str, Any]] = None,\n    weekday_seasonality: bool = False,\n    extra_features: Optional[jnp.array] = None\n    ) -> None:\n  \"\"\"Media mix model.\n\n  Args:\n    media_data: Media data to be be used in the model.\n    target_data: Target data for the model.\n    media_prior: Cost prior for each of the media channels.\n    degrees_seasonality: Number of degrees of seasonality to use.\n    frequency: Frequency of the time span which was used to aggregate the data.\n      Eg. if weekly data then frequency is 52.\n    transform_function: Function to use to transform the media data in the\n      model. Currently the following are supported: 'transform_adstock',\n        'transform_carryover' and 'transform_hill_adstock'.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n    transform_kwargs: Any extra keyword arguments to pass to the transform\n      function. For example the adstock function can take a boolean to noramlise\n      output or not.\n    weekday_seasonality: In case of daily data you can estimate a weekday (7)\n      parameter.\n    extra_features: Extra features data to include in the model.\n  \"\"\"\n  default_priors = _get_default_priors()\n  data_size = media_data.shape[0]\n  n_channels = media_data.shape[1]\n  geo_shape = (media_data.shape[2],) if media_data.ndim == 3 else ()\n  n_geos = media_data.shape[2] if media_data.ndim == 3 else 1\n\n  with numpyro.plate(name=f\"{_INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=_INTERCEPT,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "    media_prior: jnp.ndarray,\n    degrees_seasonality: int,\n    frequency: int,\n    transform_function: TransformFunction,\n    custom_priors: MutableMapping[str, Prior],\n    transform_kwargs: Optional[MutableMapping[str, Any]] = None,\n    weekday_seasonality: bool = False,\n    extra_features: Optional[jnp.array] = None\n    ) -> None:\n  \"\"\"Media mix model.\n\n  Args:\n    media_data: Media data to be be used in the model.\n    target_data: Target data for the model.\n    media_prior: Cost prior for each of the media channels.\n    degrees_seasonality: Number of degrees of seasonality to use.\n    frequency: Frequency of the time span which was used to aggregate the data.\n      Eg. if weekly data then frequency is 52.\n    transform_function: Function to use to transform the media data in the\n      model. Currently the following are supported: 'transform_adstock',\n        'transform_carryover' and 'transform_hill_adstock'.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n    transform_kwargs: Any extra keyword arguments to pass to the transform\n      function. For example the adstock function can take a boolean to noramlise\n      output or not.\n    weekday_seasonality: In case of daily data you can estimate a weekday (7)\n      parameter.\n    extra_features: Extra features data to include in the model.\n  \"\"\"\n  default_priors = _get_default_priors()\n  data_size = media_data.shape[0]\n  n_channels = media_data.shape[1]\n  geo_shape = (media_data.shape[2],) if media_data.ndim == 3 else ()\n  n_geos = media_data.shape[2] if media_data.ndim == 3 else 1\n\n  with numpyro.plate(name=f\"{_INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=_INTERCEPT,\n        fn=custom_priors.get(_INTERCEPT, default_priors[_INTERCEPT]))\n\n  with numpyro.plate(name=f\"{_SIGMA}_plate\", size=n_geos):\n    sigma = numpyro.sample(\n        name=_SIGMA,\n        fn=custom_priors.get(_SIGMA, default_priors[_SIGMA]))\n\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{_COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n  Args:\n    media_data: Media data to be be used in the model.\n    target_data: Target data for the model.\n    media_prior: Cost prior for each of the media channels.\n    degrees_seasonality: Number of degrees of seasonality to use.\n    frequency: Frequency of the time span which was used to aggregate the data.\n      Eg. if weekly data then frequency is 52.\n    transform_function: Function to use to transform the media data in the\n      model. Currently the following are supported: 'transform_adstock',\n        'transform_carryover' and 'transform_hill_adstock'.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n    transform_kwargs: Any extra keyword arguments to pass to the transform\n      function. For example the adstock function can take a boolean to noramlise\n      output or not.\n    weekday_seasonality: In case of daily data you can estimate a weekday (7)\n      parameter.\n    extra_features: Extra features data to include in the model.\n  \"\"\"\n  default_priors = _get_default_priors()\n  data_size = media_data.shape[0]\n  n_channels = media_data.shape[1]\n  geo_shape = (media_data.shape[2],) if media_data.ndim == 3 else ()\n  n_geos = media_data.shape[2] if media_data.ndim == 3 else 1\n\n  with numpyro.plate(name=f\"{_INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=_INTERCEPT,\n        fn=custom_priors.get(_INTERCEPT, default_priors[_INTERCEPT]))\n\n  with numpyro.plate(name=f\"{_SIGMA}_plate\", size=n_geos):\n    sigma = numpyro.sample(\n        name=_SIGMA,\n        fn=custom_priors.get(_SIGMA, default_priors[_SIGMA]))\n\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{_COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=_COEF_TREND,\n        fn=custom_priors.get(_COEF_TREND, default_priors[_COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=_EXPO_TREND,\n      fn=custom_priors.get(\n          _EXPO_TREND, default_priors[_EXPO_TREND]))\n\n  with numpyro.plate(\n      name=\"channel_media_plate\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "        'transform_carryover' and 'transform_hill_adstock'.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n    transform_kwargs: Any extra keyword arguments to pass to the transform\n      function. For example the adstock function can take a boolean to noramlise\n      output or not.\n    weekday_seasonality: In case of daily data you can estimate a weekday (7)\n      parameter.\n    extra_features: Extra features data to include in the model.\n  \"\"\"\n  default_priors = _get_default_priors()\n  data_size = media_data.shape[0]\n  n_channels = media_data.shape[1]\n  geo_shape = (media_data.shape[2],) if media_data.ndim == 3 else ()\n  n_geos = media_data.shape[2] if media_data.ndim == 3 else 1\n\n  with numpyro.plate(name=f\"{_INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=_INTERCEPT,\n        fn=custom_priors.get(_INTERCEPT, default_priors[_INTERCEPT]))\n\n  with numpyro.plate(name=f\"{_SIGMA}_plate\", size=n_geos):\n    sigma = numpyro.sample(\n        name=_SIGMA,\n        fn=custom_priors.get(_SIGMA, default_priors[_SIGMA]))\n\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{_COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=_COEF_TREND,\n        fn=custom_priors.get(_COEF_TREND, default_priors[_COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=_EXPO_TREND,\n      fn=custom_priors.get(\n          _EXPO_TREND, default_priors[_EXPO_TREND]))\n\n  with numpyro.plate(\n      name=\"channel_media_plate\",\n      size=n_channels,\n      dim=-2 if media_data.ndim == 3 else -1):\n    coef_media = numpyro.sample(\n        name=\"channel_coef_media\" if media_data.ndim == 3 else \"coef_media\",\n        fn=dist.HalfNormal(scale=media_prior))\n    if media_data.ndim == 3:\n      with numpyro.plate(\n          name=\"geo_media_plate\",\n          size=n_geos,\n          dim=-1):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  \"\"\"\n  default_priors = _get_default_priors()\n  data_size = media_data.shape[0]\n  n_channels = media_data.shape[1]\n  geo_shape = (media_data.shape[2],) if media_data.ndim == 3 else ()\n  n_geos = media_data.shape[2] if media_data.ndim == 3 else 1\n\n  with numpyro.plate(name=f\"{_INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=_INTERCEPT,\n        fn=custom_priors.get(_INTERCEPT, default_priors[_INTERCEPT]))\n\n  with numpyro.plate(name=f\"{_SIGMA}_plate\", size=n_geos):\n    sigma = numpyro.sample(\n        name=_SIGMA,\n        fn=custom_priors.get(_SIGMA, default_priors[_SIGMA]))\n\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{_COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=_COEF_TREND,\n        fn=custom_priors.get(_COEF_TREND, default_priors[_COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=_EXPO_TREND,\n      fn=custom_priors.get(\n          _EXPO_TREND, default_priors[_EXPO_TREND]))\n\n  with numpyro.plate(\n      name=\"channel_media_plate\",\n      size=n_channels,\n      dim=-2 if media_data.ndim == 3 else -1):\n    coef_media = numpyro.sample(\n        name=\"channel_coef_media\" if media_data.ndim == 3 else \"coef_media\",\n        fn=dist.HalfNormal(scale=media_prior))\n    if media_data.ndim == 3:\n      with numpyro.plate(\n          name=\"geo_media_plate\",\n          size=n_geos,\n          dim=-1):\n        coef_media = numpyro.sample(\n            name=\"coef_media\", fn=dist.HalfNormal(scale=coef_media))\n\n  with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_plate\",\n                       size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=_GAMMA_SEASONALITY,\n          fn=custom_priors.get(\n              _GAMMA_SEASONALITY, default_priors[_GAMMA_SEASONALITY]))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "        fn=custom_priors.get(_INTERCEPT, default_priors[_INTERCEPT]))\n\n  with numpyro.plate(name=f\"{_SIGMA}_plate\", size=n_geos):\n    sigma = numpyro.sample(\n        name=_SIGMA,\n        fn=custom_priors.get(_SIGMA, default_priors[_SIGMA]))\n\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{_COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=_COEF_TREND,\n        fn=custom_priors.get(_COEF_TREND, default_priors[_COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=_EXPO_TREND,\n      fn=custom_priors.get(\n          _EXPO_TREND, default_priors[_EXPO_TREND]))\n\n  with numpyro.plate(\n      name=\"channel_media_plate\",\n      size=n_channels,\n      dim=-2 if media_data.ndim == 3 else -1):\n    coef_media = numpyro.sample(\n        name=\"channel_coef_media\" if media_data.ndim == 3 else \"coef_media\",\n        fn=dist.HalfNormal(scale=media_prior))\n    if media_data.ndim == 3:\n      with numpyro.plate(\n          name=\"geo_media_plate\",\n          size=n_geos,\n          dim=-1):\n        coef_media = numpyro.sample(\n            name=\"coef_media\", fn=dist.HalfNormal(scale=coef_media))\n\n  with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_plate\",\n                       size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=_GAMMA_SEASONALITY,\n          fn=custom_priors.get(\n              _GAMMA_SEASONALITY, default_priors[_GAMMA_SEASONALITY]))\n\n  if weekday_seasonality:\n    with numpyro.plate(name=f\"{_WEEKDAY}_plate\", size=7):\n      weekday = numpyro.sample(\n          name=_WEEKDAY,\n          fn=custom_priors.get(_WEEKDAY, default_priors[_WEEKDAY]))\n    weekday_series = weekday[jnp.arange(data_size) % 7]\n    # In case of daily data, number of lags should be 13*7.\n    if transform_function == \"carryover\" and transform_kwargs and \"number_lags\" not in transform_kwargs:\n      transform_kwargs[\"number_lags\"] = 13 * 7", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "        name=_COEF_TREND,\n        fn=custom_priors.get(_COEF_TREND, default_priors[_COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=_EXPO_TREND,\n      fn=custom_priors.get(\n          _EXPO_TREND, default_priors[_EXPO_TREND]))\n\n  with numpyro.plate(\n      name=\"channel_media_plate\",\n      size=n_channels,\n      dim=-2 if media_data.ndim == 3 else -1):\n    coef_media = numpyro.sample(\n        name=\"channel_coef_media\" if media_data.ndim == 3 else \"coef_media\",\n        fn=dist.HalfNormal(scale=media_prior))\n    if media_data.ndim == 3:\n      with numpyro.plate(\n          name=\"geo_media_plate\",\n          size=n_geos,\n          dim=-1):\n        coef_media = numpyro.sample(\n            name=\"coef_media\", fn=dist.HalfNormal(scale=coef_media))\n\n  with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_plate\",\n                       size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=_GAMMA_SEASONALITY,\n          fn=custom_priors.get(\n              _GAMMA_SEASONALITY, default_priors[_GAMMA_SEASONALITY]))\n\n  if weekday_seasonality:\n    with numpyro.plate(name=f\"{_WEEKDAY}_plate\", size=7):\n      weekday = numpyro.sample(\n          name=_WEEKDAY,\n          fn=custom_priors.get(_WEEKDAY, default_priors[_WEEKDAY]))\n    weekday_series = weekday[jnp.arange(data_size) % 7]\n    # In case of daily data, number of lags should be 13*7.\n    if transform_function == \"carryover\" and transform_kwargs and \"number_lags\" not in transform_kwargs:\n      transform_kwargs[\"number_lags\"] = 13 * 7\n    elif transform_function == \"carryover\" and not transform_kwargs:\n      transform_kwargs = {\"number_lags\": 13 * 7}\n\n  media_transformed = numpyro.deterministic(\n      name=\"media_transformed\",\n      value=transform_function(media_data,\n                               custom_priors=custom_priors,\n                               **transform_kwargs if transform_kwargs else {}))\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "      size=n_channels,\n      dim=-2 if media_data.ndim == 3 else -1):\n    coef_media = numpyro.sample(\n        name=\"channel_coef_media\" if media_data.ndim == 3 else \"coef_media\",\n        fn=dist.HalfNormal(scale=media_prior))\n    if media_data.ndim == 3:\n      with numpyro.plate(\n          name=\"geo_media_plate\",\n          size=n_geos,\n          dim=-1):\n        coef_media = numpyro.sample(\n            name=\"coef_media\", fn=dist.HalfNormal(scale=coef_media))\n\n  with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_plate\",\n                       size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=_GAMMA_SEASONALITY,\n          fn=custom_priors.get(\n              _GAMMA_SEASONALITY, default_priors[_GAMMA_SEASONALITY]))\n\n  if weekday_seasonality:\n    with numpyro.plate(name=f\"{_WEEKDAY}_plate\", size=7):\n      weekday = numpyro.sample(\n          name=_WEEKDAY,\n          fn=custom_priors.get(_WEEKDAY, default_priors[_WEEKDAY]))\n    weekday_series = weekday[jnp.arange(data_size) % 7]\n    # In case of daily data, number of lags should be 13*7.\n    if transform_function == \"carryover\" and transform_kwargs and \"number_lags\" not in transform_kwargs:\n      transform_kwargs[\"number_lags\"] = 13 * 7\n    elif transform_function == \"carryover\" and not transform_kwargs:\n      transform_kwargs = {\"number_lags\": 13 * 7}\n\n  media_transformed = numpyro.deterministic(\n      name=\"media_transformed\",\n      value=transform_function(media_data,\n                               custom_priors=custom_priors,\n                               **transform_kwargs if transform_kwargs else {}))\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=degrees_seasonality,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality)\n  # For national model's case\n  trend = jnp.arange(data_size)\n  media_einsum = \"tc, c -> t\"  # t = time, c = channel\n  coef_seasonality = 1\n\n  # TODO(): Add conversion of prior for HalfNormal distribution.\n  if media_data.ndim == 3:  # For geo model's case", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "        coef_media = numpyro.sample(\n            name=\"coef_media\", fn=dist.HalfNormal(scale=coef_media))\n\n  with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(name=f\"{_GAMMA_SEASONALITY}_plate\",\n                       size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=_GAMMA_SEASONALITY,\n          fn=custom_priors.get(\n              _GAMMA_SEASONALITY, default_priors[_GAMMA_SEASONALITY]))\n\n  if weekday_seasonality:\n    with numpyro.plate(name=f\"{_WEEKDAY}_plate\", size=7):\n      weekday = numpyro.sample(\n          name=_WEEKDAY,\n          fn=custom_priors.get(_WEEKDAY, default_priors[_WEEKDAY]))\n    weekday_series = weekday[jnp.arange(data_size) % 7]\n    # In case of daily data, number of lags should be 13*7.\n    if transform_function == \"carryover\" and transform_kwargs and \"number_lags\" not in transform_kwargs:\n      transform_kwargs[\"number_lags\"] = 13 * 7\n    elif transform_function == \"carryover\" and not transform_kwargs:\n      transform_kwargs = {\"number_lags\": 13 * 7}\n\n  media_transformed = numpyro.deterministic(\n      name=\"media_transformed\",\n      value=transform_function(media_data,\n                               custom_priors=custom_priors,\n                               **transform_kwargs if transform_kwargs else {}))\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=degrees_seasonality,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality)\n  # For national model's case\n  trend = jnp.arange(data_size)\n  media_einsum = \"tc, c -> t\"  # t = time, c = channel\n  coef_seasonality = 1\n\n  # TODO(): Add conversion of prior for HalfNormal distribution.\n  if media_data.ndim == 3:  # For geo model's case\n    trend = jnp.expand_dims(trend, axis=-1)\n    seasonality = jnp.expand_dims(seasonality, axis=-1)\n    media_einsum = \"tcg, cg -> tg\"  # t = time, c = channel, g = geo\n    if weekday_seasonality:\n      weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n    with numpyro.plate(name=\"seasonality_plate\", size=n_geos):\n      coef_seasonality = numpyro.sample(\n          name=_COEF_SEASONALITY,\n          fn=custom_priors.get(\n              _COEF_SEASONALITY, default_priors[_COEF_SEASONALITY]))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "\n  if weekday_seasonality:\n    with numpyro.plate(name=f\"{_WEEKDAY}_plate\", size=7):\n      weekday = numpyro.sample(\n          name=_WEEKDAY,\n          fn=custom_priors.get(_WEEKDAY, default_priors[_WEEKDAY]))\n    weekday_series = weekday[jnp.arange(data_size) % 7]\n    # In case of daily data, number of lags should be 13*7.\n    if transform_function == \"carryover\" and transform_kwargs and \"number_lags\" not in transform_kwargs:\n      transform_kwargs[\"number_lags\"] = 13 * 7\n    elif transform_function == \"carryover\" and not transform_kwargs:\n      transform_kwargs = {\"number_lags\": 13 * 7}\n\n  media_transformed = numpyro.deterministic(\n      name=\"media_transformed\",\n      value=transform_function(media_data,\n                               custom_priors=custom_priors,\n                               **transform_kwargs if transform_kwargs else {}))\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=degrees_seasonality,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality)\n  # For national model's case\n  trend = jnp.arange(data_size)\n  media_einsum = \"tc, c -> t\"  # t = time, c = channel\n  coef_seasonality = 1\n\n  # TODO(): Add conversion of prior for HalfNormal distribution.\n  if media_data.ndim == 3:  # For geo model's case\n    trend = jnp.expand_dims(trend, axis=-1)\n    seasonality = jnp.expand_dims(seasonality, axis=-1)\n    media_einsum = \"tcg, cg -> tg\"  # t = time, c = channel, g = geo\n    if weekday_seasonality:\n      weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n    with numpyro.plate(name=\"seasonality_plate\", size=n_geos):\n      coef_seasonality = numpyro.sample(\n          name=_COEF_SEASONALITY,\n          fn=custom_priors.get(\n              _COEF_SEASONALITY, default_priors[_COEF_SEASONALITY]))\n  # expo_trend is B(1, 1) so that the exponent on time is in [.5, 1.5].\n  prediction = (\n      intercept + coef_trend * trend ** expo_trend +\n      seasonality * coef_seasonality +\n      jnp.einsum(media_einsum, media_transformed, coef_media))\n  if extra_features is not None:\n    plate_prefixes = (\"extra_feature\",)\n    extra_features_einsum = \"tf, f -> t\"  # t = time, f = feature\n    extra_features_plates_shape = (extra_features.shape[1],)\n    if extra_features.ndim == 3:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "    elif transform_function == \"carryover\" and not transform_kwargs:\n      transform_kwargs = {\"number_lags\": 13 * 7}\n\n  media_transformed = numpyro.deterministic(\n      name=\"media_transformed\",\n      value=transform_function(media_data,\n                               custom_priors=custom_priors,\n                               **transform_kwargs if transform_kwargs else {}))\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=degrees_seasonality,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality)\n  # For national model's case\n  trend = jnp.arange(data_size)\n  media_einsum = \"tc, c -> t\"  # t = time, c = channel\n  coef_seasonality = 1\n\n  # TODO(): Add conversion of prior for HalfNormal distribution.\n  if media_data.ndim == 3:  # For geo model's case\n    trend = jnp.expand_dims(trend, axis=-1)\n    seasonality = jnp.expand_dims(seasonality, axis=-1)\n    media_einsum = \"tcg, cg -> tg\"  # t = time, c = channel, g = geo\n    if weekday_seasonality:\n      weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n    with numpyro.plate(name=\"seasonality_plate\", size=n_geos):\n      coef_seasonality = numpyro.sample(\n          name=_COEF_SEASONALITY,\n          fn=custom_priors.get(\n              _COEF_SEASONALITY, default_priors[_COEF_SEASONALITY]))\n  # expo_trend is B(1, 1) so that the exponent on time is in [.5, 1.5].\n  prediction = (\n      intercept + coef_trend * trend ** expo_trend +\n      seasonality * coef_seasonality +\n      jnp.einsum(media_einsum, media_transformed, coef_media))\n  if extra_features is not None:\n    plate_prefixes = (\"extra_feature\",)\n    extra_features_einsum = \"tf, f -> t\"  # t = time, f = feature\n    extra_features_plates_shape = (extra_features.shape[1],)\n    if extra_features.ndim == 3:\n      plate_prefixes = (\"extra_feature\", \"geo\")\n      extra_features_einsum = \"tfg, fg -> tg\"  # t = time, f = feature, g = geo\n      extra_features_plates_shape = (extra_features.shape[1], *geo_shape)\n    with numpyro.plate_stack(plate_prefixes,\n                             sizes=extra_features_plates_shape):\n      coef_extra_features = numpyro.sample(\n          name=_COEF_EXTRA_FEATURES,\n          fn=custom_priors.get(\n              _COEF_EXTRA_FEATURES, default_priors[_COEF_EXTRA_FEATURES]))\n    extra_features_effect = jnp.einsum(extra_features_einsum,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "      degrees=degrees_seasonality,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality)\n  # For national model's case\n  trend = jnp.arange(data_size)\n  media_einsum = \"tc, c -> t\"  # t = time, c = channel\n  coef_seasonality = 1\n\n  # TODO(): Add conversion of prior for HalfNormal distribution.\n  if media_data.ndim == 3:  # For geo model's case\n    trend = jnp.expand_dims(trend, axis=-1)\n    seasonality = jnp.expand_dims(seasonality, axis=-1)\n    media_einsum = \"tcg, cg -> tg\"  # t = time, c = channel, g = geo\n    if weekday_seasonality:\n      weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n    with numpyro.plate(name=\"seasonality_plate\", size=n_geos):\n      coef_seasonality = numpyro.sample(\n          name=_COEF_SEASONALITY,\n          fn=custom_priors.get(\n              _COEF_SEASONALITY, default_priors[_COEF_SEASONALITY]))\n  # expo_trend is B(1, 1) so that the exponent on time is in [.5, 1.5].\n  prediction = (\n      intercept + coef_trend * trend ** expo_trend +\n      seasonality * coef_seasonality +\n      jnp.einsum(media_einsum, media_transformed, coef_media))\n  if extra_features is not None:\n    plate_prefixes = (\"extra_feature\",)\n    extra_features_einsum = \"tf, f -> t\"  # t = time, f = feature\n    extra_features_plates_shape = (extra_features.shape[1],)\n    if extra_features.ndim == 3:\n      plate_prefixes = (\"extra_feature\", \"geo\")\n      extra_features_einsum = \"tfg, fg -> tg\"  # t = time, f = feature, g = geo\n      extra_features_plates_shape = (extra_features.shape[1], *geo_shape)\n    with numpyro.plate_stack(plate_prefixes,\n                             sizes=extra_features_plates_shape):\n      coef_extra_features = numpyro.sample(\n          name=_COEF_EXTRA_FEATURES,\n          fn=custom_priors.get(\n              _COEF_EXTRA_FEATURES, default_priors[_COEF_EXTRA_FEATURES]))\n    extra_features_effect = jnp.einsum(extra_features_einsum,\n                                       extra_features,\n                                       coef_extra_features)\n    prediction += extra_features_effect\n\n  if weekday_seasonality:\n    prediction += weekday_series\n  mu = numpyro.deterministic(name=\"mu\", value=prediction)\n\n  numpyro.sample(\n      name=\"target\", fn=dist.Normal(loc=mu, scale=sigma), obs=target_data)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_395-435", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "    trend = jnp.expand_dims(trend, axis=-1)\n    seasonality = jnp.expand_dims(seasonality, axis=-1)\n    media_einsum = \"tcg, cg -> tg\"  # t = time, c = channel, g = geo\n    if weekday_seasonality:\n      weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n    with numpyro.plate(name=\"seasonality_plate\", size=n_geos):\n      coef_seasonality = numpyro.sample(\n          name=_COEF_SEASONALITY,\n          fn=custom_priors.get(\n              _COEF_SEASONALITY, default_priors[_COEF_SEASONALITY]))\n  # expo_trend is B(1, 1) so that the exponent on time is in [.5, 1.5].\n  prediction = (\n      intercept + coef_trend * trend ** expo_trend +\n      seasonality * coef_seasonality +\n      jnp.einsum(media_einsum, media_transformed, coef_media))\n  if extra_features is not None:\n    plate_prefixes = (\"extra_feature\",)\n    extra_features_einsum = \"tf, f -> t\"  # t = time, f = feature\n    extra_features_plates_shape = (extra_features.shape[1],)\n    if extra_features.ndim == 3:\n      plate_prefixes = (\"extra_feature\", \"geo\")\n      extra_features_einsum = \"tfg, fg -> tg\"  # t = time, f = feature, g = geo\n      extra_features_plates_shape = (extra_features.shape[1], *geo_shape)\n    with numpyro.plate_stack(plate_prefixes,\n                             sizes=extra_features_plates_shape):\n      coef_extra_features = numpyro.sample(\n          name=_COEF_EXTRA_FEATURES,\n          fn=custom_priors.get(\n              _COEF_EXTRA_FEATURES, default_priors[_COEF_EXTRA_FEATURES]))\n    extra_features_effect = jnp.einsum(extra_features_einsum,\n                                       extra_features,\n                                       coef_extra_features)\n    prediction += extra_features_effect\n\n  if weekday_seasonality:\n    prediction += weekday_series\n  mu = numpyro.deterministic(name=\"mu\", value=prediction)\n\n  numpyro.sample(\n      name=\"target\", fn=dist.Normal(loc=mu, scale=sigma), obs=target_data)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models.py_405-435", "title": "google_lightweight_mmm-lightweight_mmm-models.py", "text": "  # expo_trend is B(1, 1) so that the exponent on time is in [.5, 1.5].\n  prediction = (\n      intercept + coef_trend * trend ** expo_trend +\n      seasonality * coef_seasonality +\n      jnp.einsum(media_einsum, media_transformed, coef_media))\n  if extra_features is not None:\n    plate_prefixes = (\"extra_feature\",)\n    extra_features_einsum = \"tf, f -> t\"  # t = time, f = feature\n    extra_features_plates_shape = (extra_features.shape[1],)\n    if extra_features.ndim == 3:\n      plate_prefixes = (\"extra_feature\", \"geo\")\n      extra_features_einsum = \"tfg, fg -> tg\"  # t = time, f = feature, g = geo\n      extra_features_plates_shape = (extra_features.shape[1], *geo_shape)\n    with numpyro.plate_stack(plate_prefixes,\n                             sizes=extra_features_plates_shape):\n      coef_extra_features = numpyro.sample(\n          name=_COEF_EXTRA_FEATURES,\n          fn=custom_priors.get(\n              _COEF_EXTRA_FEATURES, default_priors[_COEF_EXTRA_FEATURES]))\n    extra_features_effect = jnp.einsum(extra_features_einsum,\n                                       extra_features,\n                                       coef_extra_features)\n    prediction += extra_features_effect\n\n  if weekday_seasonality:\n    prediction += weekday_series\n  mu = numpyro.deterministic(name=\"mu\", value=prediction)\n\n  numpyro.sample(\n      name=\"target\", fn=dist.Normal(loc=mu, scale=sigma), obs=target_data)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for models.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm import models\n\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "\n\nclass ModelsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Subscript(Call(Attribute(Name(Load)Load))ConstantLoad)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)Name(Load)keyword(Dict)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_carryover_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_hill_adstock_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_carryover_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          \"transformed_media\",\n          models.transform_hill_adstock(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_carryover_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national_no_extra\",\n          media_shape=(10, 3),\n          extra_features_shape=(),\n          target_shape=(10,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"national_extra\",\n          media_shape=(10, 5),\n          extra_features_shape=(10, 2),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"one_channel\", shape=(10, 1)),\n      dict(testcase_name=\"five_channel\", shape=(10, 5)),\n      dict(testcase_name=\"same_channels_as_rows\", shape=(10, 10)),\n      dict(testcase_name=\"geo_shape_1\", shape=(10, 10, 5)),\n      dict(testcase_name=\"geo_shape_2\", shape=(10, 5, 2)),\n      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_carryover_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national_no_extra\",\n          media_shape=(10, 3),\n          extra_features_shape=(),\n          target_shape=(10,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"national_extra\",\n          media_shape=(10, 5),\n          extra_features_shape=(10, 2),\n          target_shape=(10,),\n          total_costs_shape=(5,)),\n      dict(\n          testcase_name=\"geo_extra_3d\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(10, 8, 5),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)),\n      dict(\n          testcase_name=\"geo_no_extra\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "      dict(testcase_name=\"one_channel_one_row\", shape=(1, 1)))\n  def test_transform_carryover_produces_correct_output_shape(self, shape):\n\n    def mock_model_function(media_data):\n      numpyro.deterministic(\n          \"transformed_media\",\n          models.transform_carryover(media_data, custom_priors={}))\n\n    media = jnp.ones(shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national_no_extra\",\n          media_shape=(10, 3),\n          extra_features_shape=(),\n          target_shape=(10,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"national_extra\",\n          media_shape=(10, 5),\n          extra_features_shape=(10, 2),\n          target_shape=(10,),\n          total_costs_shape=(5,)),\n      dict(\n          testcase_name=\"geo_extra_3d\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(10, 8, 5),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)),\n      dict(\n          testcase_name=\"geo_no_extra\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)))\n  def test_media_mix_model_parameters_have_correct_shapes(\n      self, media_shape, extra_features_shape, target_shape, total_costs_shape):\n    media = jnp.ones(media_shape)\n    extra_features = None if not extra_features_shape else jnp.ones(\n        extra_features_shape)\n    costs_prior = jnp.ones(total_costs_shape)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, media_data=media)\n    transformed_media = mcmc.get_samples()[\"transformed_media\"].mean(axis=0)\n\n    self.assertEqual(media.shape, transformed_media.shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national_no_extra\",\n          media_shape=(10, 3),\n          extra_features_shape=(),\n          target_shape=(10,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"national_extra\",\n          media_shape=(10, 5),\n          extra_features_shape=(10, 2),\n          target_shape=(10,),\n          total_costs_shape=(5,)),\n      dict(\n          testcase_name=\"geo_extra_3d\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(10, 8, 5),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)),\n      dict(\n          testcase_name=\"geo_no_extra\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)))\n  def test_media_mix_model_parameters_have_correct_shapes(\n      self, media_shape, extra_features_shape, target_shape, total_costs_shape):\n    media = jnp.ones(media_shape)\n    extra_features = None if not extra_features_shape else jnp.ones(\n        extra_features_shape)\n    costs_prior = jnp.ones(total_costs_shape)\n    degrees = 2\n    target = jnp.ones(target_shape)\n    kernel = numpyro.infer.NUTS(model=models.media_mix_model)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        media_data=media,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "      dict(\n          testcase_name=\"national_no_extra\",\n          media_shape=(10, 3),\n          extra_features_shape=(),\n          target_shape=(10,),\n          total_costs_shape=(3,)),\n      dict(\n          testcase_name=\"national_extra\",\n          media_shape=(10, 5),\n          extra_features_shape=(10, 2),\n          target_shape=(10,),\n          total_costs_shape=(5,)),\n      dict(\n          testcase_name=\"geo_extra_3d\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(10, 8, 5),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)),\n      dict(\n          testcase_name=\"geo_no_extra\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)))\n  def test_media_mix_model_parameters_have_correct_shapes(\n      self, media_shape, extra_features_shape, target_shape, total_costs_shape):\n    media = jnp.ones(media_shape)\n    extra_features = None if not extra_features_shape else jnp.ones(\n        extra_features_shape)\n    costs_prior = jnp.ones(total_costs_shape)\n    degrees = 2\n    target = jnp.ones(target_shape)\n    kernel = numpyro.infer.NUTS(model=models.media_mix_model)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        degrees_seasonality=degrees,\n        custom_priors={},\n        frequency=52,\n        transform_function=models.transform_carryover)\n    trace = mcmc.get_samples()\n\n    self.assertEqual(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          target_shape=(10,),\n          total_costs_shape=(5,)),\n      dict(\n          testcase_name=\"geo_extra_3d\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(10, 8, 5),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)),\n      dict(\n          testcase_name=\"geo_no_extra\",\n          media_shape=(10, 7, 5),\n          extra_features_shape=(),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)))\n  def test_media_mix_model_parameters_have_correct_shapes(\n      self, media_shape, extra_features_shape, target_shape, total_costs_shape):\n    media = jnp.ones(media_shape)\n    extra_features = None if not extra_features_shape else jnp.ones(\n        extra_features_shape)\n    costs_prior = jnp.ones(total_costs_shape)\n    degrees = 2\n    target = jnp.ones(target_shape)\n    kernel = numpyro.infer.NUTS(model=models.media_mix_model)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        degrees_seasonality=degrees,\n        custom_priors={},\n        frequency=52,\n        transform_function=models.transform_carryover)\n    trace = mcmc.get_samples()\n\n    self.assertEqual(\n        jnp.squeeze(trace[\"intercept\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"sigma\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"expo_trend\"].mean(axis=0)).shape, ())\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_trend\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_media\"].mean(axis=0)).shape, media_shape[1:])\n    if extra_features_shape:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          media_shape=(10, 7, 5),\n          extra_features_shape=(),\n          target_shape=(10, 5),\n          total_costs_shape=(7, 1)))\n  def test_media_mix_model_parameters_have_correct_shapes(\n      self, media_shape, extra_features_shape, target_shape, total_costs_shape):\n    media = jnp.ones(media_shape)\n    extra_features = None if not extra_features_shape else jnp.ones(\n        extra_features_shape)\n    costs_prior = jnp.ones(total_costs_shape)\n    degrees = 2\n    target = jnp.ones(target_shape)\n    kernel = numpyro.infer.NUTS(model=models.media_mix_model)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        degrees_seasonality=degrees,\n        custom_priors={},\n        frequency=52,\n        transform_function=models.transform_carryover)\n    trace = mcmc.get_samples()\n\n    self.assertEqual(\n        jnp.squeeze(trace[\"intercept\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"sigma\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"expo_trend\"].mean(axis=0)).shape, ())\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_trend\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_media\"].mean(axis=0)).shape, media_shape[1:])\n    if extra_features_shape:\n      self.assertEqual(trace[\"coef_extra_features\"].mean(axis=0).shape,\n                       extra_features.shape[1:])\n    self.assertEqual(trace[\"gamma_seasonality\"].mean(axis=0).shape,\n                     (degrees, 2))\n    self.assertEqual(trace[\"mu\"].mean(axis=0).shape, target_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{models._INTERCEPT}\",\n          prior_name=models._INTERCEPT,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "    degrees = 2\n    target = jnp.ones(target_shape)\n    kernel = numpyro.infer.NUTS(model=models.media_mix_model)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=10, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        degrees_seasonality=degrees,\n        custom_priors={},\n        frequency=52,\n        transform_function=models.transform_carryover)\n    trace = mcmc.get_samples()\n\n    self.assertEqual(\n        jnp.squeeze(trace[\"intercept\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"sigma\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"expo_trend\"].mean(axis=0)).shape, ())\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_trend\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_media\"].mean(axis=0)).shape, media_shape[1:])\n    if extra_features_shape:\n      self.assertEqual(trace[\"coef_extra_features\"].mean(axis=0).shape,\n                       extra_features.shape[1:])\n    self.assertEqual(trace[\"gamma_seasonality\"].mean(axis=0).shape,\n                     (degrees, 2))\n    self.assertEqual(trace[\"mu\"].mean(axis=0).shape, target_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{models._INTERCEPT}\",\n          prior_name=models._INTERCEPT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_TREND}\",\n          prior_name=models._COEF_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._EXPO_TREND}\",\n          prior_name=models._EXPO_TREND,\n          transform_function=models.transform_carryover),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        degrees_seasonality=degrees,\n        custom_priors={},\n        frequency=52,\n        transform_function=models.transform_carryover)\n    trace = mcmc.get_samples()\n\n    self.assertEqual(\n        jnp.squeeze(trace[\"intercept\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"sigma\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"expo_trend\"].mean(axis=0)).shape, ())\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_trend\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_media\"].mean(axis=0)).shape, media_shape[1:])\n    if extra_features_shape:\n      self.assertEqual(trace[\"coef_extra_features\"].mean(axis=0).shape,\n                       extra_features.shape[1:])\n    self.assertEqual(trace[\"gamma_seasonality\"].mean(axis=0).shape,\n                     (degrees, 2))\n    self.assertEqual(trace[\"mu\"].mean(axis=0).shape, target_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{models._INTERCEPT}\",\n          prior_name=models._INTERCEPT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_TREND}\",\n          prior_name=models._COEF_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._EXPO_TREND}\",\n          prior_name=models._EXPO_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._SIGMA}\",\n          prior_name=models._SIGMA,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._GAMMA_SEASONALITY}\",\n          prior_name=models._GAMMA_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._WEEKDAY}\",\n          prior_name=models._WEEKDAY,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "        jnp.squeeze(trace[\"intercept\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"sigma\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"expo_trend\"].mean(axis=0)).shape, ())\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_trend\"].mean(axis=0)).shape, target_shape[1:])\n    self.assertEqual(\n        jnp.squeeze(trace[\"coef_media\"].mean(axis=0)).shape, media_shape[1:])\n    if extra_features_shape:\n      self.assertEqual(trace[\"coef_extra_features\"].mean(axis=0).shape,\n                       extra_features.shape[1:])\n    self.assertEqual(trace[\"gamma_seasonality\"].mean(axis=0).shape,\n                     (degrees, 2))\n    self.assertEqual(trace[\"mu\"].mean(axis=0).shape, target_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{models._INTERCEPT}\",\n          prior_name=models._INTERCEPT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_TREND}\",\n          prior_name=models._COEF_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._EXPO_TREND}\",\n          prior_name=models._EXPO_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._SIGMA}\",\n          prior_name=models._SIGMA,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._GAMMA_SEASONALITY}\",\n          prior_name=models._GAMMA_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._WEEKDAY}\",\n          prior_name=models._WEEKDAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_EXTRA_FEATURES}\",\n          prior_name=models._COEF_EXTRA_FEATURES,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_SEASONALITY}\",\n          prior_name=models._COEF_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "      self.assertEqual(trace[\"coef_extra_features\"].mean(axis=0).shape,\n                       extra_features.shape[1:])\n    self.assertEqual(trace[\"gamma_seasonality\"].mean(axis=0).shape,\n                     (degrees, 2))\n    self.assertEqual(trace[\"mu\"].mean(axis=0).shape, target_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{models._INTERCEPT}\",\n          prior_name=models._INTERCEPT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_TREND}\",\n          prior_name=models._COEF_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._EXPO_TREND}\",\n          prior_name=models._EXPO_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._SIGMA}\",\n          prior_name=models._SIGMA,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._GAMMA_SEASONALITY}\",\n          prior_name=models._GAMMA_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._WEEKDAY}\",\n          prior_name=models._WEEKDAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_EXTRA_FEATURES}\",\n          prior_name=models._COEF_EXTRA_FEATURES,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_SEASONALITY}\",\n          prior_name=models._COEF_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._AD_EFFECT_RETENTION_RATE}\",\n          prior_name=models._AD_EFFECT_RETENTION_RATE,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._PEAK_EFFECT_DELAY}\",\n          prior_name=models._PEAK_EFFECT_DELAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_TREND}\",\n          prior_name=models._COEF_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._EXPO_TREND}\",\n          prior_name=models._EXPO_TREND,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._SIGMA}\",\n          prior_name=models._SIGMA,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._GAMMA_SEASONALITY}\",\n          prior_name=models._GAMMA_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._WEEKDAY}\",\n          prior_name=models._WEEKDAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_EXTRA_FEATURES}\",\n          prior_name=models._COEF_EXTRA_FEATURES,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_SEASONALITY}\",\n          prior_name=models._COEF_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._AD_EFFECT_RETENTION_RATE}\",\n          prior_name=models._AD_EFFECT_RETENTION_RATE,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._PEAK_EFFECT_DELAY}\",\n          prior_name=models._PEAK_EFFECT_DELAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"adstock_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"adstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_adstock),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          testcase_name=f\"model_{models._SIGMA}\",\n          prior_name=models._SIGMA,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._GAMMA_SEASONALITY}\",\n          prior_name=models._GAMMA_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._WEEKDAY}\",\n          prior_name=models._WEEKDAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_EXTRA_FEATURES}\",\n          prior_name=models._COEF_EXTRA_FEATURES,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_SEASONALITY}\",\n          prior_name=models._COEF_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._AD_EFFECT_RETENTION_RATE}\",\n          prior_name=models._AD_EFFECT_RETENTION_RATE,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._PEAK_EFFECT_DELAY}\",\n          prior_name=models._PEAK_EFFECT_DELAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"adstock_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"adstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n          prior_name=models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._SLOPE}\",\n          prior_name=models._SLOPE,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_EXTRA_FEATURES}\",\n          prior_name=models._COEF_EXTRA_FEATURES,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"model_{models._COEF_SEASONALITY}\",\n          prior_name=models._COEF_SEASONALITY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._AD_EFFECT_RETENTION_RATE}\",\n          prior_name=models._AD_EFFECT_RETENTION_RATE,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._PEAK_EFFECT_DELAY}\",\n          prior_name=models._PEAK_EFFECT_DELAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"adstock_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"adstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n          prior_name=models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._SLOPE}\",\n          prior_name=models._SLOPE,\n          transform_function=models.transform_hill_adstock),\n  )\n  def test_media_mix_model_custom_priors_are_taken_correctly(\n      self, prior_name, transform_function):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name: dist.Kumaraswamy(\n            concentration1=expected_value1, concentration0=expected_value2)}\n    media = jnp.ones((10, 5, 5))\n    extra_features = jnp.ones((10, 3, 5))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          testcase_name=f\"carryover_{models._AD_EFFECT_RETENTION_RATE}\",\n          prior_name=models._AD_EFFECT_RETENTION_RATE,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._PEAK_EFFECT_DELAY}\",\n          prior_name=models._PEAK_EFFECT_DELAY,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"carryover_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"adstock_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"adstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n          prior_name=models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._SLOPE}\",\n          prior_name=models._SLOPE,\n          transform_function=models.transform_hill_adstock),\n  )\n  def test_media_mix_model_custom_priors_are_taken_correctly(\n      self, prior_name, transform_function):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name: dist.Kumaraswamy(\n            concentration1=expected_value1, concentration0=expected_value2)}\n    media = jnp.ones((10, 5, 5))\n    extra_features = jnp.ones((10, 3, 5))\n    costs_prior = jnp.ones((5, 1))\n    target = jnp.ones((10, 5))\n\n    trace_handler = handlers.trace(handlers.seed(\n        models.media_mix_model, rng_seed=0))\n    trace = trace_handler.get_trace(\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          transform_function=models.transform_carryover),\n      dict(\n          testcase_name=f\"adstock_{models._EXPONENT}\",\n          prior_name=models._EXPONENT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"adstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n          prior_name=models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._SLOPE}\",\n          prior_name=models._SLOPE,\n          transform_function=models.transform_hill_adstock),\n  )\n  def test_media_mix_model_custom_priors_are_taken_correctly(\n      self, prior_name, transform_function):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name: dist.Kumaraswamy(\n            concentration1=expected_value1, concentration0=expected_value2)}\n    media = jnp.ones((10, 5, 5))\n    extra_features = jnp.ones((10, 3, 5))\n    costs_prior = jnp.ones((5, 1))\n    target = jnp.ones((10, 5))\n\n    trace_handler = handlers.trace(handlers.seed(\n        models.media_mix_model, rng_seed=0))\n    trace = trace_handler.get_trace(\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        custom_priors=custom_priors,\n        degrees_seasonality=2,\n        frequency=52,\n        transform_function=transform_function,\n        weekday_seasonality=True\n    )\n    values_and_dists = {\n        name: site[\"fn\"]\n        for name, site in trace.items() if \"fn\" in site\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          testcase_name=f\"hilladstock_{models._LAG_WEIGHT}\",\n          prior_name=models._LAG_WEIGHT,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n          prior_name=models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n          transform_function=models.transform_hill_adstock),\n      dict(\n          testcase_name=f\"hilladstock_{models._SLOPE}\",\n          prior_name=models._SLOPE,\n          transform_function=models.transform_hill_adstock),\n  )\n  def test_media_mix_model_custom_priors_are_taken_correctly(\n      self, prior_name, transform_function):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name: dist.Kumaraswamy(\n            concentration1=expected_value1, concentration0=expected_value2)}\n    media = jnp.ones((10, 5, 5))\n    extra_features = jnp.ones((10, 3, 5))\n    costs_prior = jnp.ones((5, 1))\n    target = jnp.ones((10, 5))\n\n    trace_handler = handlers.trace(handlers.seed(\n        models.media_mix_model, rng_seed=0))\n    trace = trace_handler.get_trace(\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        custom_priors=custom_priors,\n        degrees_seasonality=2,\n        frequency=52,\n        transform_function=transform_function,\n        weekday_seasonality=True\n    )\n    values_and_dists = {\n        name: site[\"fn\"]\n        for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_235-276", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "          transform_function=models.transform_hill_adstock),\n  )\n  def test_media_mix_model_custom_priors_are_taken_correctly(\n      self, prior_name, transform_function):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name: dist.Kumaraswamy(\n            concentration1=expected_value1, concentration0=expected_value2)}\n    media = jnp.ones((10, 5, 5))\n    extra_features = jnp.ones((10, 3, 5))\n    costs_prior = jnp.ones((5, 1))\n    target = jnp.ones((10, 5))\n\n    trace_handler = handlers.trace(handlers.seed(\n        models.media_mix_model, rng_seed=0))\n    trace = trace_handler.get_trace(\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        custom_priors=custom_priors,\n        degrees_seasonality=2,\n        frequency=52,\n        transform_function=transform_function,\n        weekday_seasonality=True\n    )\n    values_and_dists = {\n        name: site[\"fn\"]\n        for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 276, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-models_test.py_245-276", "title": "google_lightweight_mmm-lightweight_mmm-models_test.py", "text": "    costs_prior = jnp.ones((5, 1))\n    target = jnp.ones((10, 5))\n\n    trace_handler = handlers.trace(handlers.seed(\n        models.media_mix_model, rng_seed=0))\n    trace = trace_handler.get_trace(\n        media_data=media,\n        extra_features=extra_features,\n        target_data=target,\n        media_prior=costs_prior,\n        custom_priors=custom_priors,\n        degrees_seasonality=2,\n        frequency=52,\n        transform_function=transform_function,\n        weekday_seasonality=True\n    )\n    values_and_dists = {\n        name: site[\"fn\"]\n        for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "models_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 276, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for optimizing your media based on media mix models.\"\"\"\nimport functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "import functools\nfrom typing import Optional, Tuple, Union\nfrom absl import logging\nimport jax\nimport jax.numpy as jnp\nfrom scipy import optimize\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import preprocessing\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"media_input_shape\", \"target_scaler\",\n                     \"media_scaler\"))\ndef _objective_function(extra_features: jnp.ndarray,\n                        media_mix_model: lightweight_mmm.LightweightMMM,\n                        media_input_shape: Tuple[int,\n                                                 int], media_gap: Optional[int],\n                        target_scaler: Optional[preprocessing.CustomScaler],\n                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "                        media_scaler: preprocessing.CustomScaler,\n                        geo_ratio: jnp.array,\n                        seed: Optional[int],\n                        media_values: jnp.ndarray) -> jnp.float64:\n  \"\"\"Objective function to calculate the sum of all predictions of the model.\n\n  Args:\n    extra_features: Extra features the model requires for prediction.\n    media_mix_model: Media mix model to use. Must have a predict method to be\n      used.\n    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    media_input_shape: Input shape of the data required by the model to get\n      predictions. This is needed since optimization might flatten some arrays\n      and they need to be reshaped before running new predictions.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 2 months after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,\n                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    media_scaler: Scaler that was used to scale the media data before training.\n    geo_ratio: The ratio to split channel media across geo. Should sum up to 1\n      for each channel and should have shape (c, g).\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n    media_values: Media values required by the model to run predictions.\n\n  Returns:\n    The negative value of the sum of all predictions.\n  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,\n                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.\n  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "  \"\"\"\n  if hasattr(media_mix_model, \"n_geos\") and media_mix_model.n_geos > 1:\n    media_values = geo_ratio * jnp.expand_dims(media_values, axis=-1)\n  media_values = jnp.tile(\n      media_values / media_input_shape[0], reps=media_input_shape[0])\n  # Distribute budget of each channels across time.\n  media_values = jnp.reshape(a=media_values, newshape=media_input_shape)\n  media_values = media_scaler.transform(media_values)\n  return -jnp.sum(\n      media_mix_model.predict(\n          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,\n                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.\n  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "          media=media_values.reshape(media_input_shape),\n          extra_features=extra_features,\n          media_gap=media_gap,\n          target_scaler=target_scaler,\n          seed=seed).mean(axis=0))\n\n\n@jax.jit\ndef _budget_constraint(media: jnp.ndarray,\n                       prices: jnp.ndarray,\n                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.\n  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "                       budget: jnp.ndarray) -> jnp.float64:\n  \"\"\"Calculates optimization constraint to keep spend equal to the budget.\n\n  Args:\n    media: Array with the values of the media for this iteration.\n    prices: Prices of each media channel at any given time.\n    budget: Total budget of the optimization.\n\n  Returns:\n    The result from substracting the total spending and the budget.\n  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "  \"\"\"\n  media = media.reshape((-1, len(prices)))\n  return jnp.sum(media * prices) - budget\n\n\ndef _get_lower_and_upper_bounds(\n    media: jnp.ndarray,\n    n_time_periods: int,\n    lower_pct: jnp.ndarray,\n    upper_pct: jnp.ndarray,\n    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    media_scaler: Optional[preprocessing.CustomScaler] = None\n) -> optimize.Bounds:\n  \"\"\"Gets the lower and upper bounds for optimisation based on historic data.\n\n  It creates an upper bound based on a percentage above the mean value on\n  each channel and a lower bound based on a relative decrease of the mean\n  value.\n\n  Args:\n    media: Media data to get historic mean.\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n\n\ndef _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    lower_pct: Relative percentage decrease from the mean value to consider as\n      new lower bound.\n    upper_pct: Relative percentage increase from the mean value to consider as\n      new upper bound.\n    media_scaler: Scaler that was used to scale the media data before training.\n\n  Returns:\n    A list of tuples with the lower and upper bound for each media channel.\n  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n\n\ndef _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last\n  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "  \"\"\"\n  if media.ndim == 3:\n    lower_pct = jnp.expand_dims(lower_pct, axis=-1)\n    upper_pct = jnp.expand_dims(upper_pct, axis=-1)\n\n  mean_data = media.mean(axis=0)\n  lower_bounds = jnp.maximum(mean_data * (1 - lower_pct), 0)\n  upper_bounds = mean_data * (1 + upper_pct)\n\n  if media_scaler:\n    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n\n\ndef _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last\n  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    lower_bounds = media_scaler.inverse_transform(lower_bounds)\n    upper_bounds = media_scaler.inverse_transform(upper_bounds)\n\n  if media.ndim == 3:\n    lower_bounds = lower_bounds.sum(axis=-1)\n    upper_bounds = upper_bounds.sum(axis=-1)\n\n  return optimize.Bounds(lb=lower_bounds * n_time_periods,\n                         ub=upper_bounds * n_time_periods)\n\n\ndef _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last\n  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n\n  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "\ndef _generate_starting_values(\n    n_time_periods: int, media: jnp.ndarray,\n    media_scaler: preprocessing.CustomScaler,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Generates starting values based on historic allocation and budget.\n\n  In order to make a comparison we can take the allocation of the last\n  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n\n  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n\n\ndef find_optimal_budgets(\n    n_time_periods: int,\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray] = None,\n    media_gap: Optional[jnp.ndarray] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    bounds_lower_pct: Union[float, jnp.ndarray] = .2,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "  `n_time_periods` and scale it based on the given budget. Given this, one can\n  compare how this initial values (based on average historic allocation) compare\n  to the output of the optimisation in terms of sales/KPI.\n\n  Args:\n    n_time_periods: Number of time periods the optimization will be done with.\n    media: Historic media data the model was trained with.\n    media_scaler: Scaler that was used to scale the media data before training.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n\n  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n\n\ndef find_optimal_budgets(\n    n_time_periods: int,\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray] = None,\n    media_gap: Optional[jnp.ndarray] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    bounds_lower_pct: Union[float, jnp.ndarray] = .2,\n    bounds_upper_pct: Union[float, jnp.ndarray] = .2,\n    max_iterations: int = 200,\n    solver_func_tolerance: float = 1e-06,\n    solver_step_size: float = 1.4901161193847656e-08,\n    seed: Optional[int] = None) -> optimize.OptimizeResult:\n  \"\"\"Finds the best media allocation based on MMM model, prices and a budget.\n\n  Args:\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "      channel unit.\n\n  Returns:\n    An array with the starting value for each media channel for the\n      optimization.\n  \"\"\"\n  previous_allocation = media.mean(axis=0) * n_time_periods\n  if media_scaler:  # Scale before sum as geo scaler has shape (c, g).\n    previous_allocation = media_scaler.inverse_transform(previous_allocation)\n\n  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n\n\ndef find_optimal_budgets(\n    n_time_periods: int,\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray] = None,\n    media_gap: Optional[jnp.ndarray] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    bounds_lower_pct: Union[float, jnp.ndarray] = .2,\n    bounds_upper_pct: Union[float, jnp.ndarray] = .2,\n    max_iterations: int = 200,\n    solver_func_tolerance: float = 1e-06,\n    solver_step_size: float = 1.4901161193847656e-08,\n    seed: Optional[int] = None) -> optimize.OptimizeResult:\n  \"\"\"Finds the best media allocation based on MMM model, prices and a budget.\n\n  Args:\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    media_mix_model: Media mix model to use for the optimization.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n    extra_features: Extra features needed for the model to predict.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 8 weeks after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "  if media.ndim == 3:\n    previous_allocation = previous_allocation.sum(axis=-1)\n\n  avg_spend_per_channel = previous_allocation * prices\n  pct_spend_per_channel = avg_spend_per_channel / avg_spend_per_channel.sum()\n  budget_per_channel = budget * pct_spend_per_channel\n  media_unit_per_channel = budget_per_channel / prices\n  return media_unit_per_channel\n\n\ndef find_optimal_budgets(\n    n_time_periods: int,\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray] = None,\n    media_gap: Optional[jnp.ndarray] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    bounds_lower_pct: Union[float, jnp.ndarray] = .2,\n    bounds_upper_pct: Union[float, jnp.ndarray] = .2,\n    max_iterations: int = 200,\n    solver_func_tolerance: float = 1e-06,\n    solver_step_size: float = 1.4901161193847656e-08,\n    seed: Optional[int] = None) -> optimize.OptimizeResult:\n  \"\"\"Finds the best media allocation based on MMM model, prices and a budget.\n\n  Args:\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    media_mix_model: Media mix model to use for the optimization.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n    extra_features: Extra features needed for the model to predict.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 8 weeks after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    bounds_lower_pct: Relative percentage decrease from the mean value to\n      consider as new lower bound.\n    bounds_upper_pct: Relative percentage increase from the mean value to\n      consider as new upper bound.\n    max_iterations: Number of max iterations to use for the SLSQP scipy\n      optimizer. Default is 200.\n    solver_func_tolerance: Precision goal for the value of the prediction in", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "def find_optimal_budgets(\n    n_time_periods: int,\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    budget: Union[float, int],\n    prices: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray] = None,\n    media_gap: Optional[jnp.ndarray] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    bounds_lower_pct: Union[float, jnp.ndarray] = .2,\n    bounds_upper_pct: Union[float, jnp.ndarray] = .2,\n    max_iterations: int = 200,\n    solver_func_tolerance: float = 1e-06,\n    solver_step_size: float = 1.4901161193847656e-08,\n    seed: Optional[int] = None) -> optimize.OptimizeResult:\n  \"\"\"Finds the best media allocation based on MMM model, prices and a budget.\n\n  Args:\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    media_mix_model: Media mix model to use for the optimization.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n    extra_features: Extra features needed for the model to predict.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 8 weeks after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    bounds_lower_pct: Relative percentage decrease from the mean value to\n      consider as new lower bound.\n    bounds_upper_pct: Relative percentage increase from the mean value to\n      consider as new upper bound.\n    max_iterations: Number of max iterations to use for the SLSQP scipy\n      optimizer. Default is 200.\n    solver_func_tolerance: Precision goal for the value of the prediction in\n      the stopping criterion. Maps directly to scipy's `ftol`. Intended only\n      for advanced users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    solver_step_size: Step size used for numerical approximation of the\n      Jacobian. Maps directly to scipy's `eps`. Intended only for advanced\n      users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    bounds_upper_pct: Union[float, jnp.ndarray] = .2,\n    max_iterations: int = 200,\n    solver_func_tolerance: float = 1e-06,\n    solver_step_size: float = 1.4901161193847656e-08,\n    seed: Optional[int] = None) -> optimize.OptimizeResult:\n  \"\"\"Finds the best media allocation based on MMM model, prices and a budget.\n\n  Args:\n    n_time_periods: Number of time periods to optimize for. If model is built on\n      weekly data, this would be the number of weeks ahead to optimize.\n    media_mix_model: Media mix model to use for the optimization.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n    extra_features: Extra features needed for the model to predict.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 8 weeks after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    bounds_lower_pct: Relative percentage decrease from the mean value to\n      consider as new lower bound.\n    bounds_upper_pct: Relative percentage increase from the mean value to\n      consider as new upper bound.\n    max_iterations: Number of max iterations to use for the SLSQP scipy\n      optimizer. Default is 200.\n    solver_func_tolerance: Precision goal for the value of the prediction in\n      the stopping criterion. Maps directly to scipy's `ftol`. Intended only\n      for advanced users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    solver_step_size: Step size used for numerical approximation of the\n      Jacobian. Maps directly to scipy's `eps`. Intended only for advanced\n      users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    solution: OptimizeResult object containing the results of the optimization.\n    kpi_without_optim: Predicted target based on original allocation proportion\n    among channels from the historical data.\n    starting_values: Budget Allocation based on original allocation proportion\n    and the given total budget.\n  \"\"\"\n  if not hasattr(media_mix_model, \"media\"):\n    raise ValueError(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    media_mix_model: Media mix model to use for the optimization.\n    budget: Total budget to allocate during the optimization time.\n    prices: An array with shape (n_media_channels,) for the cost of each media\n      channel unit.\n    extra_features: Extra features needed for the model to predict.\n    media_gap: Media data gap between the end of training data and the start of\n      the out of sample media given. Eg. if 100 weeks of data were used for\n      training and prediction starts 8 weeks after training data finished we\n      need to provide the 8 weeks missing between the training data and the\n      prediction data so data transformations (adstock, carryover, ...) can take\n      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    bounds_lower_pct: Relative percentage decrease from the mean value to\n      consider as new lower bound.\n    bounds_upper_pct: Relative percentage increase from the mean value to\n      consider as new upper bound.\n    max_iterations: Number of max iterations to use for the SLSQP scipy\n      optimizer. Default is 200.\n    solver_func_tolerance: Precision goal for the value of the prediction in\n      the stopping criterion. Maps directly to scipy's `ftol`. Intended only\n      for advanced users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    solver_step_size: Step size used for numerical approximation of the\n      Jacobian. Maps directly to scipy's `eps`. Intended only for advanced\n      users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    solution: OptimizeResult object containing the results of the optimization.\n    kpi_without_optim: Predicted target based on original allocation proportion\n    among channels from the historical data.\n    starting_values: Budget Allocation based on original allocation proportion\n    and the given total budget.\n  \"\"\"\n  if not hasattr(media_mix_model, \"media\"):\n    raise ValueError(\n        \"The passed model has not been trained. Please fit the model before \"\n        \"running optimization.\")\n  jax.config.update(\"jax_enable_x64\", True)\n\n  if isinstance(bounds_lower_pct, float):\n    bounds_lower_pct = jnp.repeat(a=bounds_lower_pct, repeats=len(prices))\n  if isinstance(bounds_upper_pct, float):\n    bounds_upper_pct = jnp.repeat(a=bounds_upper_pct, repeats=len(prices))\n\n  bounds = _get_lower_and_upper_bounds(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "      place correctly.\n    target_scaler: Scaler that was used to scale the target before training.\n    media_scaler: Scaler that was used to scale the media data before training.\n    bounds_lower_pct: Relative percentage decrease from the mean value to\n      consider as new lower bound.\n    bounds_upper_pct: Relative percentage increase from the mean value to\n      consider as new upper bound.\n    max_iterations: Number of max iterations to use for the SLSQP scipy\n      optimizer. Default is 200.\n    solver_func_tolerance: Precision goal for the value of the prediction in\n      the stopping criterion. Maps directly to scipy's `ftol`. Intended only\n      for advanced users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    solver_step_size: Step size used for numerical approximation of the\n      Jacobian. Maps directly to scipy's `eps`. Intended only for advanced\n      users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    solution: OptimizeResult object containing the results of the optimization.\n    kpi_without_optim: Predicted target based on original allocation proportion\n    among channels from the historical data.\n    starting_values: Budget Allocation based on original allocation proportion\n    and the given total budget.\n  \"\"\"\n  if not hasattr(media_mix_model, \"media\"):\n    raise ValueError(\n        \"The passed model has not been trained. Please fit the model before \"\n        \"running optimization.\")\n  jax.config.update(\"jax_enable_x64\", True)\n\n  if isinstance(bounds_lower_pct, float):\n    bounds_lower_pct = jnp.repeat(a=bounds_lower_pct, repeats=len(prices))\n  if isinstance(bounds_upper_pct, float):\n    bounds_upper_pct = jnp.repeat(a=bounds_upper_pct, repeats=len(prices))\n\n  bounds = _get_lower_and_upper_bounds(\n      media=media_mix_model.media,\n      n_time_periods=n_time_periods,\n      lower_pct=bounds_lower_pct,\n      upper_pct=bounds_upper_pct,\n      media_scaler=media_scaler)\n  if jnp.sum(bounds.lb * prices) > budget:\n    logging.warning(\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "      the stopping criterion. Maps directly to scipy's `ftol`. Intended only\n      for advanced users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    solver_step_size: Step size used for numerical approximation of the\n      Jacobian. Maps directly to scipy's `eps`. Intended only for advanced\n      users. For more details see:\n      https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    solution: OptimizeResult object containing the results of the optimization.\n    kpi_without_optim: Predicted target based on original allocation proportion\n    among channels from the historical data.\n    starting_values: Budget Allocation based on original allocation proportion\n    and the given total budget.\n  \"\"\"\n  if not hasattr(media_mix_model, \"media\"):\n    raise ValueError(\n        \"The passed model has not been trained. Please fit the model before \"\n        \"running optimization.\")\n  jax.config.update(\"jax_enable_x64\", True)\n\n  if isinstance(bounds_lower_pct, float):\n    bounds_lower_pct = jnp.repeat(a=bounds_lower_pct, repeats=len(prices))\n  if isinstance(bounds_upper_pct, float):\n    bounds_upper_pct = jnp.repeat(a=bounds_upper_pct, repeats=len(prices))\n\n  bounds = _get_lower_and_upper_bounds(\n      media=media_mix_model.media,\n      n_time_periods=n_time_periods,\n      lower_pct=bounds_lower_pct,\n      upper_pct=bounds_upper_pct,\n      media_scaler=media_scaler)\n  if jnp.sum(bounds.lb * prices) > budget:\n    logging.warning(\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n  if jnp.sum(bounds.ub * prices) < budget:\n    logging.warning(\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n  starting_values = _generate_starting_values(\n      n_time_periods=n_time_periods,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "\n  Returns:\n    solution: OptimizeResult object containing the results of the optimization.\n    kpi_without_optim: Predicted target based on original allocation proportion\n    among channels from the historical data.\n    starting_values: Budget Allocation based on original allocation proportion\n    and the given total budget.\n  \"\"\"\n  if not hasattr(media_mix_model, \"media\"):\n    raise ValueError(\n        \"The passed model has not been trained. Please fit the model before \"\n        \"running optimization.\")\n  jax.config.update(\"jax_enable_x64\", True)\n\n  if isinstance(bounds_lower_pct, float):\n    bounds_lower_pct = jnp.repeat(a=bounds_lower_pct, repeats=len(prices))\n  if isinstance(bounds_upper_pct, float):\n    bounds_upper_pct = jnp.repeat(a=bounds_upper_pct, repeats=len(prices))\n\n  bounds = _get_lower_and_upper_bounds(\n      media=media_mix_model.media,\n      n_time_periods=n_time_periods,\n      lower_pct=bounds_lower_pct,\n      upper_pct=bounds_upper_pct,\n      media_scaler=media_scaler)\n  if jnp.sum(bounds.lb * prices) > budget:\n    logging.warning(\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n  if jnp.sum(bounds.ub * prices) < budget:\n    logging.warning(\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n  starting_values = _generate_starting_values(\n      n_time_periods=n_time_periods,\n      media=media_mix_model.media,\n      media_scaler=media_scaler,\n      budget=budget,\n      prices=prices,\n  )\n  if not media_scaler:\n    media_scaler = preprocessing.CustomScaler(multiply_by=1, divide_by=1)\n  if media_mix_model.n_geos == 1:\n    geo_ratio = 1.0\n  else:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "        \"The passed model has not been trained. Please fit the model before \"\n        \"running optimization.\")\n  jax.config.update(\"jax_enable_x64\", True)\n\n  if isinstance(bounds_lower_pct, float):\n    bounds_lower_pct = jnp.repeat(a=bounds_lower_pct, repeats=len(prices))\n  if isinstance(bounds_upper_pct, float):\n    bounds_upper_pct = jnp.repeat(a=bounds_upper_pct, repeats=len(prices))\n\n  bounds = _get_lower_and_upper_bounds(\n      media=media_mix_model.media,\n      n_time_periods=n_time_periods,\n      lower_pct=bounds_lower_pct,\n      upper_pct=bounds_upper_pct,\n      media_scaler=media_scaler)\n  if jnp.sum(bounds.lb * prices) > budget:\n    logging.warning(\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n  if jnp.sum(bounds.ub * prices) < budget:\n    logging.warning(\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n  starting_values = _generate_starting_values(\n      n_time_periods=n_time_periods,\n      media=media_mix_model.media,\n      media_scaler=media_scaler,\n      budget=budget,\n      prices=prices,\n  )\n  if not media_scaler:\n    media_scaler = preprocessing.CustomScaler(multiply_by=1, divide_by=1)\n  if media_mix_model.n_geos == 1:\n    geo_ratio = 1.0\n  else:\n    average_per_time = media_mix_model.media.mean(axis=0)\n    geo_ratio = average_per_time / jnp.expand_dims(\n        average_per_time.sum(axis=-1), axis=-1)\n  media_input_shape = (n_time_periods, *media_mix_model.media.shape[1:])\n  partial_objective_function = functools.partial(\n      _objective_function, extra_features, media_mix_model,\n      media_input_shape, media_gap,\n      target_scaler, media_scaler, geo_ratio, seed)\n  solution = optimize.minimize(\n      fun=partial_objective_function,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "      media=media_mix_model.media,\n      n_time_periods=n_time_periods,\n      lower_pct=bounds_lower_pct,\n      upper_pct=bounds_upper_pct,\n      media_scaler=media_scaler)\n  if jnp.sum(bounds.lb * prices) > budget:\n    logging.warning(\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n  if jnp.sum(bounds.ub * prices) < budget:\n    logging.warning(\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n  starting_values = _generate_starting_values(\n      n_time_periods=n_time_periods,\n      media=media_mix_model.media,\n      media_scaler=media_scaler,\n      budget=budget,\n      prices=prices,\n  )\n  if not media_scaler:\n    media_scaler = preprocessing.CustomScaler(multiply_by=1, divide_by=1)\n  if media_mix_model.n_geos == 1:\n    geo_ratio = 1.0\n  else:\n    average_per_time = media_mix_model.media.mean(axis=0)\n    geo_ratio = average_per_time / jnp.expand_dims(\n        average_per_time.sum(axis=-1), axis=-1)\n  media_input_shape = (n_time_periods, *media_mix_model.media.shape[1:])\n  partial_objective_function = functools.partial(\n      _objective_function, extra_features, media_mix_model,\n      media_input_shape, media_gap,\n      target_scaler, media_scaler, geo_ratio, seed)\n  solution = optimize.minimize(\n      fun=partial_objective_function,\n      x0=starting_values,\n      bounds=bounds,\n      method=\"SLSQP\",\n      jac=\"3-point\",\n      options={\n          \"maxiter\": max_iterations,\n          \"disp\": True,\n          \"ftol\": solver_func_tolerance,\n          \"eps\": solver_step_size,\n      },", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n  if jnp.sum(bounds.ub * prices) < budget:\n    logging.warning(\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n  starting_values = _generate_starting_values(\n      n_time_periods=n_time_periods,\n      media=media_mix_model.media,\n      media_scaler=media_scaler,\n      budget=budget,\n      prices=prices,\n  )\n  if not media_scaler:\n    media_scaler = preprocessing.CustomScaler(multiply_by=1, divide_by=1)\n  if media_mix_model.n_geos == 1:\n    geo_ratio = 1.0\n  else:\n    average_per_time = media_mix_model.media.mean(axis=0)\n    geo_ratio = average_per_time / jnp.expand_dims(\n        average_per_time.sum(axis=-1), axis=-1)\n  media_input_shape = (n_time_periods, *media_mix_model.media.shape[1:])\n  partial_objective_function = functools.partial(\n      _objective_function, extra_features, media_mix_model,\n      media_input_shape, media_gap,\n      target_scaler, media_scaler, geo_ratio, seed)\n  solution = optimize.minimize(\n      fun=partial_objective_function,\n      x0=starting_values,\n      bounds=bounds,\n      method=\"SLSQP\",\n      jac=\"3-point\",\n      options={\n          \"maxiter\": max_iterations,\n          \"disp\": True,\n          \"ftol\": solver_func_tolerance,\n          \"eps\": solver_step_size,\n      },\n      constraints={\n          \"type\": \"eq\",\n          \"fun\": _budget_constraint,\n          \"args\": (prices, budget)\n      })\n\n  kpi_without_optim = _objective_function(extra_features=extra_features,\n                                          media_mix_model=media_mix_model,\n                                          media_input_shape=media_input_shape,\n                                          media_gap=media_gap,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "      media=media_mix_model.media,\n      media_scaler=media_scaler,\n      budget=budget,\n      prices=prices,\n  )\n  if not media_scaler:\n    media_scaler = preprocessing.CustomScaler(multiply_by=1, divide_by=1)\n  if media_mix_model.n_geos == 1:\n    geo_ratio = 1.0\n  else:\n    average_per_time = media_mix_model.media.mean(axis=0)\n    geo_ratio = average_per_time / jnp.expand_dims(\n        average_per_time.sum(axis=-1), axis=-1)\n  media_input_shape = (n_time_periods, *media_mix_model.media.shape[1:])\n  partial_objective_function = functools.partial(\n      _objective_function, extra_features, media_mix_model,\n      media_input_shape, media_gap,\n      target_scaler, media_scaler, geo_ratio, seed)\n  solution = optimize.minimize(\n      fun=partial_objective_function,\n      x0=starting_values,\n      bounds=bounds,\n      method=\"SLSQP\",\n      jac=\"3-point\",\n      options={\n          \"maxiter\": max_iterations,\n          \"disp\": True,\n          \"ftol\": solver_func_tolerance,\n          \"eps\": solver_step_size,\n      },\n      constraints={\n          \"type\": \"eq\",\n          \"fun\": _budget_constraint,\n          \"args\": (prices, budget)\n      })\n\n  kpi_without_optim = _objective_function(extra_features=extra_features,\n                                          media_mix_model=media_mix_model,\n                                          media_input_shape=media_input_shape,\n                                          media_gap=media_gap,\n                                          target_scaler=target_scaler,\n                                          media_scaler=media_scaler,\n                                          seed=seed,\n                                          geo_ratio=geo_ratio,\n                                          media_values=starting_values)\n  logging.info(\"KPI without optimization: %r\", -1 * kpi_without_optim.item())\n  logging.info(\"KPI with optimization: %r\", -1 * solution.fun)\n\n  jax.config.update(\"jax_enable_x64\", False)\n  # TODO(yukaabe): Create an object to contain the results of this function.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_285-326", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "    average_per_time = media_mix_model.media.mean(axis=0)\n    geo_ratio = average_per_time / jnp.expand_dims(\n        average_per_time.sum(axis=-1), axis=-1)\n  media_input_shape = (n_time_periods, *media_mix_model.media.shape[1:])\n  partial_objective_function = functools.partial(\n      _objective_function, extra_features, media_mix_model,\n      media_input_shape, media_gap,\n      target_scaler, media_scaler, geo_ratio, seed)\n  solution = optimize.minimize(\n      fun=partial_objective_function,\n      x0=starting_values,\n      bounds=bounds,\n      method=\"SLSQP\",\n      jac=\"3-point\",\n      options={\n          \"maxiter\": max_iterations,\n          \"disp\": True,\n          \"ftol\": solver_func_tolerance,\n          \"eps\": solver_step_size,\n      },\n      constraints={\n          \"type\": \"eq\",\n          \"fun\": _budget_constraint,\n          \"args\": (prices, budget)\n      })\n\n  kpi_without_optim = _objective_function(extra_features=extra_features,\n                                          media_mix_model=media_mix_model,\n                                          media_input_shape=media_input_shape,\n                                          media_gap=media_gap,\n                                          target_scaler=target_scaler,\n                                          media_scaler=media_scaler,\n                                          seed=seed,\n                                          geo_ratio=geo_ratio,\n                                          media_values=starting_values)\n  logging.info(\"KPI without optimization: %r\", -1 * kpi_without_optim.item())\n  logging.info(\"KPI with optimization: %r\", -1 * solution.fun)\n\n  jax.config.update(\"jax_enable_x64\", False)\n  # TODO(yukaabe): Create an object to contain the results of this function.\n  return solution, kpi_without_optim, starting_values", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 326, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media.py_295-326", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media.py", "text": "      x0=starting_values,\n      bounds=bounds,\n      method=\"SLSQP\",\n      jac=\"3-point\",\n      options={\n          \"maxiter\": max_iterations,\n          \"disp\": True,\n          \"ftol\": solver_func_tolerance,\n          \"eps\": solver_step_size,\n      },\n      constraints={\n          \"type\": \"eq\",\n          \"fun\": _budget_constraint,\n          \"args\": (prices, budget)\n      })\n\n  kpi_without_optim = _objective_function(extra_features=extra_features,\n                                          media_mix_model=media_mix_model,\n                                          media_input_shape=media_input_shape,\n                                          media_gap=media_gap,\n                                          target_scaler=target_scaler,\n                                          media_scaler=media_scaler,\n                                          seed=seed,\n                                          geo_ratio=geo_ratio,\n                                          media_values=starting_values)\n  logging.info(\"KPI without optimization: %r\", -1 * kpi_without_optim.item())\n  logging.info(\"KPI with optimization: %r\", -1 * solution.fun)\n\n  jax.config.update(\"jax_enable_x64\", False)\n  # TODO(yukaabe): Create an object to contain the results of this function.\n  return solution, kpi_without_optim, starting_values", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 326, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for optimize_media.\"\"\"\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for optimize_media.\"\"\"\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\nfrom lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for optimize_media.\"\"\"\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\nfrom lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for optimize_media.\"\"\"\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\nfrom lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n\n  def setUp(self):\n    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Constant))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantConstantLoad)))keyword(Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))keyword(BinOp(Call(Attribute(Name(Load)Load)Constant)MultConstant))keyword(Constant)keyword(Constant)keyword(Constant)))Name(Load))FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Constantkeyword(Constant)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "from unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import optimize_media\nfrom lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n\n  def setUp(self):\n    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          geo_ratio=1),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          geo_ratio=np.tile(0.33, reps=(5, 3)))\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "from lightweight_mmm import preprocessing\n\n\nclass OptimizeMediaTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(OptimizeMediaTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n\n  def setUp(self):\n    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          geo_ratio=1),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          geo_ratio=np.tile(0.33, reps=(5, 3)))\n  ])\n  def test_objective_function_generates_correct_value_type_and_sign(\n      self, model_name, geo_ratio):\n\n    mmm = getattr(self, model_name)\n    extra_features = mmm._extra_features\n    time_periods = 10\n\n    kpi_predicted = optimize_media._objective_function(\n        extra_features=extra_features,\n        media_mix_model=mmm,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n\n  def setUp(self):\n    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          geo_ratio=1),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          geo_ratio=np.tile(0.33, reps=(5, 3)))\n  ])\n  def test_objective_function_generates_correct_value_type_and_sign(\n      self, model_name, geo_ratio):\n\n    mmm = getattr(self, model_name)\n    extra_features = mmm._extra_features\n    time_periods = 10\n\n    kpi_predicted = optimize_media._objective_function(\n        extra_features=extra_features,\n        media_mix_model=mmm,\n        media_input_shape=(time_periods, *mmm.media.shape[1:]),\n        media_gap=None,\n        target_scaler=None,\n        media_scaler=preprocessing.CustomScaler(),\n        media_values=jnp.ones(mmm.n_media_channels) * time_periods,\n        geo_ratio=geo_ratio,\n        seed=10)\n\n    self.assertIsInstance(kpi_predicted, jax.Array)\n    self.assertLessEqual(kpi_predicted, 0)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n\n  def setUp(self):\n    super().setUp()\n    self.mock_minimize = self.enter_context(\n        mock.patch.object(optimize_media.optimize, \"minimize\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          geo_ratio=1),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          geo_ratio=np.tile(0.33, reps=(5, 3)))\n  ])\n  def test_objective_function_generates_correct_value_type_and_sign(\n      self, model_name, geo_ratio):\n\n    mmm = getattr(self, model_name)\n    extra_features = mmm._extra_features\n    time_periods = 10\n\n    kpi_predicted = optimize_media._objective_function(\n        extra_features=extra_features,\n        media_mix_model=mmm,\n        media_input_shape=(time_periods, *mmm.media.shape[1:]),\n        media_gap=None,\n        target_scaler=None,\n        media_scaler=preprocessing.CustomScaler(),\n        media_values=jnp.ones(mmm.n_media_channels) * time_periods,\n        geo_ratio=geo_ratio,\n        seed=10)\n\n    self.assertIsInstance(kpi_predicted, jax.Array)\n    self.assertLessEqual(kpi_predicted, 0)\n    self.assertEqual(kpi_predicted.shape, ())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"zero_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=18,\n          expected_value=0),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          geo_ratio=1),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          geo_ratio=np.tile(0.33, reps=(5, 3)))\n  ])\n  def test_objective_function_generates_correct_value_type_and_sign(\n      self, model_name, geo_ratio):\n\n    mmm = getattr(self, model_name)\n    extra_features = mmm._extra_features\n    time_periods = 10\n\n    kpi_predicted = optimize_media._objective_function(\n        extra_features=extra_features,\n        media_mix_model=mmm,\n        media_input_shape=(time_periods, *mmm.media.shape[1:]),\n        media_gap=None,\n        target_scaler=None,\n        media_scaler=preprocessing.CustomScaler(),\n        media_values=jnp.ones(mmm.n_media_channels) * time_periods,\n        geo_ratio=geo_ratio,\n        seed=10)\n\n    self.assertIsInstance(kpi_predicted, jax.Array)\n    self.assertLessEqual(kpi_predicted, 0)\n    self.assertEqual(kpi_predicted.shape, ())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"zero_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=18,\n          expected_value=0),\n      dict(\n          testcase_name=\"negative_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=20,\n          expected_value=-2),\n      dict(\n          testcase_name=\"positive_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=16,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "  def test_objective_function_generates_correct_value_type_and_sign(\n      self, model_name, geo_ratio):\n\n    mmm = getattr(self, model_name)\n    extra_features = mmm._extra_features\n    time_periods = 10\n\n    kpi_predicted = optimize_media._objective_function(\n        extra_features=extra_features,\n        media_mix_model=mmm,\n        media_input_shape=(time_periods, *mmm.media.shape[1:]),\n        media_gap=None,\n        target_scaler=None,\n        media_scaler=preprocessing.CustomScaler(),\n        media_values=jnp.ones(mmm.n_media_channels) * time_periods,\n        geo_ratio=geo_ratio,\n        seed=10)\n\n    self.assertIsInstance(kpi_predicted, jax.Array)\n    self.assertLessEqual(kpi_predicted, 0)\n    self.assertEqual(kpi_predicted.shape, ())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"zero_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=18,\n          expected_value=0),\n      dict(\n          testcase_name=\"negative_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=20,\n          expected_value=-2),\n      dict(\n          testcase_name=\"positive_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=16,\n          expected_value=2),\n      dict(\n          testcase_name=\"bigger_array\",\n          media=np.ones(18),\n          prices=np.array([2, 2, 2]),\n          budget=36,\n          expected_value=0),\n  ])\n  def test_budget_constraint(self, media, prices, budget, expected_value):\n    generated_value = optimize_media._budget_constraint(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media_input_shape=(time_periods, *mmm.media.shape[1:]),\n        media_gap=None,\n        target_scaler=None,\n        media_scaler=preprocessing.CustomScaler(),\n        media_values=jnp.ones(mmm.n_media_channels) * time_periods,\n        geo_ratio=geo_ratio,\n        seed=10)\n\n    self.assertIsInstance(kpi_predicted, jax.Array)\n    self.assertLessEqual(kpi_predicted, 0)\n    self.assertEqual(kpi_predicted.shape, ())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"zero_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=18,\n          expected_value=0),\n      dict(\n          testcase_name=\"negative_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=20,\n          expected_value=-2),\n      dict(\n          testcase_name=\"positive_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=16,\n          expected_value=2),\n      dict(\n          testcase_name=\"bigger_array\",\n          media=np.ones(18),\n          prices=np.array([2, 2, 2]),\n          budget=36,\n          expected_value=0),\n  ])\n  def test_budget_constraint(self, media, prices, budget, expected_value):\n    generated_value = optimize_media._budget_constraint(\n        media=media, prices=prices, budget=budget)\n\n    self.assertEqual(generated_value, expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_media_scaler\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_media_scaler\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "    self.assertEqual(kpi_predicted.shape, ())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"zero_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=18,\n          expected_value=0),\n      dict(\n          testcase_name=\"negative_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=20,\n          expected_value=-2),\n      dict(\n          testcase_name=\"positive_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=16,\n          expected_value=2),\n      dict(\n          testcase_name=\"bigger_array\",\n          media=np.ones(18),\n          prices=np.array([2, 2, 2]),\n          budget=36,\n          expected_value=0),\n  ])\n  def test_budget_constraint(self, media, prices, budget, expected_value):\n    generated_value = optimize_media._budget_constraint(\n        media=media, prices=prices, budget=budget)\n\n    self.assertEqual(generated_value, expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_media_scaler\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_media_scaler\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_with_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "          testcase_name=\"negative_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=20,\n          expected_value=-2),\n      dict(\n          testcase_name=\"positive_output\",\n          media=np.ones(9),\n          prices=np.array([1, 2, 3]),\n          budget=16,\n          expected_value=2),\n      dict(\n          testcase_name=\"bigger_array\",\n          media=np.ones(18),\n          prices=np.array([2, 2, 2]),\n          budget=36,\n          expected_value=0),\n  ])\n  def test_budget_constraint(self, media, prices, budget, expected_value):\n    generated_value = optimize_media._budget_constraint(\n        media=media, prices=prices, budget=budget)\n\n    self.assertEqual(generated_value, expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_media_scaler\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_media_scaler\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_with_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 12. and 18. bounds times 2 (scaler) 24. and 36.\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].lb,\n                                         np.repeat(24., repeats=5) * mmm.n_geos,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "          expected_value=2),\n      dict(\n          testcase_name=\"bigger_array\",\n          media=np.ones(18),\n          prices=np.array([2, 2, 2]),\n          budget=36,\n          expected_value=0),\n  ])\n  def test_budget_constraint(self, media, prices, budget, expected_value):\n    generated_value = optimize_media._budget_constraint(\n        media=media, prices=prices, budget=budget)\n\n    self.assertEqual(generated_value, expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_media_scaler\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_media_scaler\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_with_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 12. and 18. bounds times 2 (scaler) 24. and 36.\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].lb,\n                                         np.repeat(24., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].ub,\n                                         np.repeat(36., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    # We only added scaler with divide operation so we only expectec x2 in\n    # the divide_by parameter.\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].divide_by,\n                                         2 * jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].multiply_by,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media=media, prices=prices, budget=budget)\n\n    self.assertEqual(generated_value, expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_media_scaler\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_media_scaler\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_with_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 12. and 18. bounds times 2 (scaler) 24. and 36.\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].lb,\n                                         np.repeat(24., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].ub,\n                                         np.repeat(36., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    # We only added scaler with divide operation so we only expectec x2 in\n    # the divide_by parameter.\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].divide_by,\n                                         2 * jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].multiply_by,\n                                         jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_with_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 12. and 18. bounds times 2 (scaler) 24. and 36.\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].lb,\n                                         np.repeat(24., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].ub,\n                                         np.repeat(36., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    # We only added scaler with divide operation so we only expectec x2 in\n    # the divide_by parameter.\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].divide_by,\n                                         2 * jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].multiply_by,\n                                         jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_without_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 12. and 18. bounds times 2 (scaler) 24. and 36.\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].lb,\n                                         np.repeat(24., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].ub,\n                                         np.repeat(36., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    # We only added scaler with divide operation so we only expectec x2 in\n    # the divide_by parameter.\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].divide_by,\n                                         2 * jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].multiply_by,\n                                         jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_without_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 18. bounds\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].lb,\n        np.repeat(12., repeats=5) * mmm.n_geos,\n        decimal=3)\n    np.testing.assert_array_almost_equal(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"bounds\"].ub,\n                                         np.repeat(36., repeats=5) * mmm.n_geos,\n                                         decimal=3)\n    # We only added scaler with divide operation so we only expectec x2 in\n    # the divide_by parameter.\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].divide_by,\n                                         2 * jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n    np.testing.assert_array_almost_equal(call_kwargs[\"fun\"].args[5].multiply_by,\n                                         jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_without_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 18. bounds\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].lb,\n        np.repeat(12., repeats=5) * mmm.n_geos,\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].ub,\n        np.repeat(18., repeats=5) * mmm.n_geos,\n        decimal=3)\n\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].divide_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].multiply_by,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "                                         jnp.ones(mmm.media.shape[1:]),\n                                         decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_find_optimal_budgets_without_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 18. bounds\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].lb,\n        np.repeat(12., repeats=5) * mmm.n_geos,\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].ub,\n        np.repeat(18., repeats=5) * mmm.n_geos,\n        decimal=3)\n\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].divide_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].multiply_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "  ])\n  def test_find_optimal_budgets_without_scaler_optimize_called_with_right_params(\n      self, model_name):\n\n    mmm = getattr(self, model_name)\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 18. bounds\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].lb,\n        np.repeat(12., repeats=5) * mmm.n_geos,\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].ub,\n        np.repeat(18., repeats=5) * mmm.n_geos,\n        decimal=3)\n\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].divide_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].multiply_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_predict_called_with_right_args(self, model_name):\n    mmm = getattr(self, model_name)\n\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        target_scaler=None,\n        media_scaler=None)\n\n    _, call_kwargs = self.mock_minimize.call_args_list[0]\n    # 15 weeks at 1.2 gives us 18. bounds\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].lb,\n        np.repeat(12., repeats=5) * mmm.n_geos,\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"bounds\"].ub,\n        np.repeat(18., repeats=5) * mmm.n_geos,\n        decimal=3)\n\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].divide_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].multiply_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_predict_called_with_right_args(self, model_name):\n    mmm = getattr(self, model_name)\n\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        call_kwargs[\"bounds\"].ub,\n        np.repeat(18., repeats=5) * mmm.n_geos,\n        decimal=3)\n\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].divide_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n    np.testing.assert_array_almost_equal(\n        call_kwargs[\"fun\"].args[5].multiply_by,\n        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_predict_called_with_right_args(self, model_name):\n    mmm = getattr(self, model_name)\n\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_lower_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        jnp.ones(mmm.n_media_channels),\n        decimal=3)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_predict_called_with_right_args(self, model_name):\n    mmm = getattr(self, model_name)\n\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_lower_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=1,\n          prices=jnp.ones(mmm.n_media_channels))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "  ])\n  def test_predict_called_with_right_args(self, model_name):\n    mmm = getattr(self, model_name)\n\n    optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_lower_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=1,\n          prices=jnp.ones(mmm.n_media_channels))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_higher_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media_scaler=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_lower_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=1,\n          prices=jnp.ones(mmm.n_media_channels))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_higher_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=2000,\n          prices=jnp.ones(5))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "  def test_budget_lower_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is smaller than the lower bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"increase the budget or change the lower bound by increasing the \"\n        \"percentage decrease with the `bounds_lower_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=1,\n          prices=jnp.ones(mmm.n_media_channels))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_higher_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=2000,\n          prices=jnp.ones(5))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          expected_len=3),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=1,\n          prices=jnp.ones(mmm.n_media_channels))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_higher_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=2000,\n          prices=jnp.ones(5))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          expected_len=3),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          expected_len=3)\n  ])\n  def test_find_optimal_budgets_has_right_output_length_datatype(\n      self, model_name, expected_len):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    results = optimize_media.find_optimal_budgets(\n        n_time_periods=15,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\")\n  ])\n  def test_budget_higher_than_constraints_warns_user(self, model_name):\n    mmm = getattr(self, model_name)\n    expected_warning = (\n        \"Budget given is larger than the upper bounds of the constraints for \"\n        \"optimization. This will lead to faulty optimization. Please either \"\n        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=2000,\n          prices=jnp.ones(5))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          expected_len=3),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          expected_len=3)\n  ])\n  def test_find_optimal_budgets_has_right_output_length_datatype(\n      self, model_name, expected_len):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    results = optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n    self.assertLen(results, expected_len)\n    self.assertIsInstance(results[1], jax.Array)\n    self.assertIsInstance(results[2], jax.Array)\n\n  @parameterized.named_parameters([", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        \"reduce the budget or change the upper bound by increasing the \"\n        \"percentage increase with the `bounds_upper_pct` parameter.\")\n\n    with self.assertLogs(level=\"WARNING\") as context_manager:\n      optimize_media.find_optimal_budgets(\n          n_time_periods=5,\n          media_mix_model=mmm,\n          budget=2000,\n          prices=jnp.ones(5))\n    self.assertEqual(f\"WARNING:absl:{expected_warning}\",\n                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          expected_len=3),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          expected_len=3)\n  ])\n  def test_find_optimal_budgets_has_right_output_length_datatype(\n      self, model_name, expected_len):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    results = optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n    self.assertLen(results, expected_len)\n    self.assertIsInstance(results[1], jax.Array)\n    self.assertIsInstance(results[2], jax.Array)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_prices\",\n          model_name=\"national_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"national_ones\",\n          model_name=\"national_mmm\",\n          prices=np.ones(5),\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "                     context_manager.output[0])\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          model_name=\"national_mmm\",\n          expected_len=3),\n      dict(\n          testcase_name=\"geo\",\n          model_name=\"geo_mmm\",\n          expected_len=3)\n  ])\n  def test_find_optimal_budgets_has_right_output_length_datatype(\n      self, model_name, expected_len):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    results = optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n    self.assertLen(results, expected_len)\n    self.assertIsInstance(results[1], jax.Array)\n    self.assertIsInstance(results[2], jax.Array)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_prices\",\n          model_name=\"national_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"national_ones\",\n          model_name=\"national_mmm\",\n          prices=np.ones(5),\n      ),\n      dict(\n          testcase_name=\"geo_prices\",\n          model_name=\"geo_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"geo_ones\",\n          model_name=\"geo_mmm\",\n          prices=np.ones(5),\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "          expected_len=3)\n  ])\n  def test_find_optimal_budgets_has_right_output_length_datatype(\n      self, model_name, expected_len):\n\n    mmm = getattr(self, model_name)\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    media_scaler.fit(2 * jnp.ones((10, *mmm.media.shape[1:])))\n    results = optimize_media.find_optimal_budgets(\n        n_time_periods=15,\n        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n    self.assertLen(results, expected_len)\n    self.assertIsInstance(results[1], jax.Array)\n    self.assertIsInstance(results[2], jax.Array)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_prices\",\n          model_name=\"national_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"national_ones\",\n          model_name=\"national_mmm\",\n          prices=np.ones(5),\n      ),\n      dict(\n          testcase_name=\"geo_prices\",\n          model_name=\"geo_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"geo_ones\",\n          model_name=\"geo_mmm\",\n          prices=np.ones(5),\n      ),\n  ])\n  def test_generate_starting_values_calculates_correct_values(\n      self, model_name, prices):\n    mmm = getattr(self, model_name)\n    n_time_periods = 10\n    budget = mmm.n_media_channels * n_time_periods\n    starting_values = optimize_media._generate_starting_values(\n        n_time_periods=10,\n        media_scaler=None,\n        media=mmm.media,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "        media_mix_model=mmm,\n        budget=30,\n        prices=jnp.ones(mmm.n_media_channels),\n        target_scaler=None,\n        media_scaler=media_scaler)\n    self.assertLen(results, expected_len)\n    self.assertIsInstance(results[1], jax.Array)\n    self.assertIsInstance(results[2], jax.Array)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_prices\",\n          model_name=\"national_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"national_ones\",\n          model_name=\"national_mmm\",\n          prices=np.ones(5),\n      ),\n      dict(\n          testcase_name=\"geo_prices\",\n          model_name=\"geo_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"geo_ones\",\n          model_name=\"geo_mmm\",\n          prices=np.ones(5),\n      ),\n  ])\n  def test_generate_starting_values_calculates_correct_values(\n      self, model_name, prices):\n    mmm = getattr(self, model_name)\n    n_time_periods = 10\n    budget = mmm.n_media_channels * n_time_periods\n    starting_values = optimize_media._generate_starting_values(\n        n_time_periods=10,\n        media_scaler=None,\n        media=mmm.media,\n        budget=budget,\n        prices=prices,\n    )\n\n    # Given that data is all ones, starting values will be equal to prices.\n    np.testing.assert_array_almost_equal(\n        starting_values, jnp.repeat(n_time_periods, repeats=5))\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_295-335", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "      dict(\n          testcase_name=\"national_prices\",\n          model_name=\"national_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"national_ones\",\n          model_name=\"national_mmm\",\n          prices=np.ones(5),\n      ),\n      dict(\n          testcase_name=\"geo_prices\",\n          model_name=\"geo_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"geo_ones\",\n          model_name=\"geo_mmm\",\n          prices=np.ones(5),\n      ),\n  ])\n  def test_generate_starting_values_calculates_correct_values(\n      self, model_name, prices):\n    mmm = getattr(self, model_name)\n    n_time_periods = 10\n    budget = mmm.n_media_channels * n_time_periods\n    starting_values = optimize_media._generate_starting_values(\n        n_time_periods=10,\n        media_scaler=None,\n        media=mmm.media,\n        budget=budget,\n        prices=prices,\n    )\n\n    # Given that data is all ones, starting values will be equal to prices.\n    np.testing.assert_array_almost_equal(\n        starting_values, jnp.repeat(n_time_periods, repeats=5))\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-optimize_media_test.py_305-335", "title": "google_lightweight_mmm-lightweight_mmm-optimize_media_test.py", "text": "      dict(\n          testcase_name=\"geo_prices\",\n          model_name=\"geo_mmm\",\n          prices=np.array([1., 0.8, 1.2, 1.5, 0.5]),\n      ),\n      dict(\n          testcase_name=\"geo_ones\",\n          model_name=\"geo_mmm\",\n          prices=np.ones(5),\n      ),\n  ])\n  def test_generate_starting_values_calculates_correct_values(\n      self, model_name, prices):\n    mmm = getattr(self, model_name)\n    n_time_periods = 10\n    budget = mmm.n_media_channels * n_time_periods\n    starting_values = optimize_media._generate_starting_values(\n        n_time_periods=10,\n        media_scaler=None,\n        media=mmm.media,\n        budget=budget,\n        prices=prices,\n    )\n\n    # Given that data is all ones, starting values will be equal to prices.\n    np.testing.assert_array_almost_equal(\n        starting_values, jnp.repeat(n_time_periods, repeats=5))\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "optimize_media_test.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)Import(alias)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Plotting functions pre and post model fitting.\"\"\"\n\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\nimport functools\nimport logging\n\n# Using these types from typing instead of their generic types in the type hints\n# in order to be compatible with Python 3.7 and 3.8.\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nimport arviz\nimport jax\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "import jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "from lightweight_mmm import models\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm import utils\n\nplt.style.use(\"default\")\n\n_PALETTE = sns.color_palette(n_colors=100)\n\n\n@functools.partial(jax.jit, static_argnames=(\"media_mix_model\"))\ndef _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "def _make_single_prediction(media_mix_model: lightweight_mmm.LightweightMMM,\n                            mock_media: jnp.ndarray,\n                            extra_features: Optional[jnp.ndarray],\n                            seed: Optional[int]\n                            ) -> jnp.ndarray:\n  \"\"\"Makes a prediction of a single row.\n\n  Serves as a helper function for making predictions individually for each media\n  channel and one row at a time. It is meant to be used vmaped otherwise it can\n  be slow as it's meant to be used for plotting curve responses only. Use\n  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.\n\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  lightweight_mmm.LightweightMMM for regular predict functionality.\n\n  Args:\n    media_mix_model: Media mix model to use for getting the predictions.\n    mock_media: Mock media for this iteration of predictions.\n    extra_features: Extra features to use for predictions.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.\n\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Returns:\n    A point estimate for the given data.\n  \"\"\"\n  return media_mix_model.predict(\n      media=jnp.expand_dims(mock_media, axis=0),\n      extra_features=extra_features,\n      seed=seed).mean(axis=0)\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.\n\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    jax.jit,\n    static_argnames=(\"media_mix_model\", \"target_scaler\"))\ndef _generate_diagonal_predictions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_values: jnp.ndarray,\n    extra_features: Optional[jnp.ndarray],\n    target_scaler: Optional[preprocessing.CustomScaler],\n    prediction_offset: jnp.ndarray,\n    seed: Optional[int]):\n  \"\"\"Generates predictions for one value per channel leaving the rest to zero.\n\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(\n      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  This function does the following steps:\n    - Vmaps the single prediction function on axis=0 of the media arg.\n    - Diagonalizes the media input values so that each value is represented\n      along side zeros on for the rest of the channels.\n    - Generate predictions.\n    - Unscale prediction if target_scaler is given.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(\n      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions\n\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    media_values: Media values.\n    extra_features: Extra features values.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prediction_offset: The value of a prediction of an all zero media input.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(\n      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions\n\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n\n\ndef _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    The predictions for the given data.\n  \"\"\"\n  make_predictions = jax.vmap(fun=_make_single_prediction,\n                              in_axes=(None, 0, None, None))\n  diagonal = jnp.eye(media_values.shape[0])\n  if media_values.ndim == 2:  # Only two since we only provide one row\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    media_values = jnp.expand_dims(media_values, axis=0)\n  diag_media_values = diagonal * media_values\n  predictions = make_predictions(\n      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions\n\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n\n\ndef _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:\n  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      media_mix_model,\n      diag_media_values,\n      extra_features,\n      seed) - prediction_offset\n  predictions = jnp.squeeze(predictions)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n  if predictions.ndim == 2:\n    predictions = jnp.sum(predictions, axis=-1)\n  return predictions\n\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n\n\ndef _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:\n  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n\n  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n\ndef _calculate_number_rows_plot(n_media_channels: int, n_columns: int):\n  \"\"\"Calculates the number of rows of plots needed to fit n + 1 plots in n_cols.\n\n  Args:\n    n_media_channels: Number of media channels. The total of plots needed is\n      n_media_channels + 1.\n    n_columns: Number of columns in the plot grid.\n\n  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n\n\ndef _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:\n  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n\n  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n\nAST=Module(FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)If(Compare(BinOp(Name(Load)ModName(Load))EqConstant)Return(BinOp(BinOp(Name(Load)FloorDivName(Load))AddConstant)))Return(BinOp(BinOp(Name(Load)FloorDivName(Load))AddConstant)))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)If(UnaryOp(NotCall(Name(Load)Name(Load)Constant))Raise(Call(Attribute(Name(Load)Load)Constant)))If(Compare(Attribute(Subscript(Attribute(Name(Load)Load)ConstantLoad)Load)GtConstant)Assign(Name(Store)Constant)If(Compare(Attribute(Subscript(Attribute(Name(Load)Load)ConstantLoad)Load)EqConstant)Assign(Name(Store)Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)Subscript(Attribute(Name(Load)Load)ConstantLoad)))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Returns:\n    The number of rows of plots needed to fit n + 1 plots in n cols\n  \"\"\"\n  if n_media_channels % n_columns == 0:\n    return n_media_channels // n_columns + 1\n  return n_media_channels // n_columns + 2\n\n\ndef _calculate_media_contribution(\n    media_mix_model: lightweight_mmm.LightweightMMM) -> jnp.ndarray:\n  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n\n  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n\n\ndef create_attribution_over_spend_fractions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_spend: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  \"\"\"Computes contribution for each sample, time, channel.\n\n  Serves as a helper function for making predictions for each channel, time\n  and estimate sample. It is meant to be used in creating media baseline\n  contribution dataframe and visualize media attribution over spend proportion\n  plot.\n\n  Args:\n    media_mix_model: Media mix model.\n\n  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n\n\ndef create_attribution_over_spend_fractions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_spend: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    time_index: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for media attribution over spend.\n\n  The output dataframe will be used to create media attribution and spend\n  barplot; and attribution over spend lineplot\n\n  Args:\n    media_mix_model: Media mix model.\n    media_spend: Media spend per channel. If 1D, it needs to be pre-processed to\n      align with time index range. 2D represents media spend per time and", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Returns:\n    Estimation of contribution for each sample, time, channel.\n\n  Raises:\n    NotFittedModelError: if the model is not fitted before computation\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n\n\ndef create_attribution_over_spend_fractions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_spend: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    time_index: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for media attribution over spend.\n\n  The output dataframe will be used to create media attribution and spend\n  barplot; and attribution over spend lineplot\n\n  Args:\n    media_mix_model: Media mix model.\n    media_spend: Media spend per channel. If 1D, it needs to be pre-processed to\n      align with time index range. 2D represents media spend per time and\n      channel. Media spends need to be aggregated over geo before input.\n    channel_names: Names of media channels to be added to the output dataframe.\n    time_index: Time range index used for calculation.\n\n  Returns:\n    DataFrame containing fractions of the contribution and spend and\n    attribution over spend for each channel.\n\n  Rasies:\n    ValueError: if any of the media values are negative or any aggregated media", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # s for samples, t for time, c for media channels, g for geo\n    einsum_str = \"stcg, scg->stcg\"\n  elif media_mix_model.trace[\"media_transformed\"].ndim == 3:\n    # s for samples, t for time, c for media channels\n    einsum_str = \"stc, sc->stc\"\n\n  media_contribution = jnp.einsum(einsum_str,\n                                  media_mix_model.trace[\"media_transformed\"],\n                                  media_mix_model.trace[\"coef_media\"])\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n\n\ndef create_attribution_over_spend_fractions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_spend: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    time_index: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for media attribution over spend.\n\n  The output dataframe will be used to create media attribution and spend\n  barplot; and attribution over spend lineplot\n\n  Args:\n    media_mix_model: Media mix model.\n    media_spend: Media spend per channel. If 1D, it needs to be pre-processed to\n      align with time index range. 2D represents media spend per time and\n      channel. Media spends need to be aggregated over geo before input.\n    channel_names: Names of media channels to be added to the output dataframe.\n    time_index: Time range index used for calculation.\n\n  Returns:\n    DataFrame containing fractions of the contribution and spend and\n    attribution over spend for each channel.\n\n  Rasies:\n    ValueError: if any of the media values are negative or any aggregated media\n    spends are zero or negative.\n    NotFittedModelError: if the model is not fitted.\n  \"\"\"\n  if (media_spend < 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n  if channel_names is None:\n    try:\n      channel_names = media_mix_model.media_names\n    except AttributeError as att_error:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Aggregate media channel contribution across geos.\n    media_contribution = media_contribution.sum(axis=-1)\n  return media_contribution\n\n\ndef create_attribution_over_spend_fractions(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_spend: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    time_index: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for media attribution over spend.\n\n  The output dataframe will be used to create media attribution and spend\n  barplot; and attribution over spend lineplot\n\n  Args:\n    media_mix_model: Media mix model.\n    media_spend: Media spend per channel. If 1D, it needs to be pre-processed to\n      align with time index range. 2D represents media spend per time and\n      channel. Media spends need to be aggregated over geo before input.\n    channel_names: Names of media channels to be added to the output dataframe.\n    time_index: Time range index used for calculation.\n\n  Returns:\n    DataFrame containing fractions of the contribution and spend and\n    attribution over spend for each channel.\n\n  Rasies:\n    ValueError: if any of the media values are negative or any aggregated media\n    spends are zero or negative.\n    NotFittedModelError: if the model is not fitted.\n  \"\"\"\n  if (media_spend < 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n  if channel_names is None:\n    try:\n      channel_names = media_mix_model.media_names\n    except AttributeError as att_error:\n      raise lightweight_mmm.NotFittedModelError(\n          \"Model needs to be fit first before attempting to plot its fit.\"\n      ) from att_error\n\n  media_contribution = _calculate_media_contribution(media_mix_model)\n  if time_index is None:\n    time_index = (0, media_contribution.shape[1])\n\n  # Select time span and aggregate 2D media spend.\n  if media_spend.ndim == 1:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    time_index: Optional[Tuple[int, int]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for media attribution over spend.\n\n  The output dataframe will be used to create media attribution and spend\n  barplot; and attribution over spend lineplot\n\n  Args:\n    media_mix_model: Media mix model.\n    media_spend: Media spend per channel. If 1D, it needs to be pre-processed to\n      align with time index range. 2D represents media spend per time and\n      channel. Media spends need to be aggregated over geo before input.\n    channel_names: Names of media channels to be added to the output dataframe.\n    time_index: Time range index used for calculation.\n\n  Returns:\n    DataFrame containing fractions of the contribution and spend and\n    attribution over spend for each channel.\n\n  Rasies:\n    ValueError: if any of the media values are negative or any aggregated media\n    spends are zero or negative.\n    NotFittedModelError: if the model is not fitted.\n  \"\"\"\n  if (media_spend < 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n  if channel_names is None:\n    try:\n      channel_names = media_mix_model.media_names\n    except AttributeError as att_error:\n      raise lightweight_mmm.NotFittedModelError(\n          \"Model needs to be fit first before attempting to plot its fit.\"\n      ) from att_error\n\n  media_contribution = _calculate_media_contribution(media_mix_model)\n  if time_index is None:\n    time_index = (0, media_contribution.shape[1])\n\n  # Select time span and aggregate 2D media spend.\n  if media_spend.ndim == 1:\n    logging.warning(\"1D media spend has to align with time index range.\")\n  elif media_spend.ndim == 2:\n    media_spend = media_spend[time_index[0]:time_index[1], :]\n    media_spend = media_spend.sum(axis=0)\n\n  # Select time span and aggregate media contribution.\n  media_contribution = media_contribution[:, time_index[0]:time_index[1],]\n  media_contribution = media_contribution.sum(axis=(0, 1))\n\n  # Create media contribution and spend dataframe", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      channel. Media spends need to be aggregated over geo before input.\n    channel_names: Names of media channels to be added to the output dataframe.\n    time_index: Time range index used for calculation.\n\n  Returns:\n    DataFrame containing fractions of the contribution and spend and\n    attribution over spend for each channel.\n\n  Rasies:\n    ValueError: if any of the media values are negative or any aggregated media\n    spends are zero or negative.\n    NotFittedModelError: if the model is not fitted.\n  \"\"\"\n  if (media_spend < 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n  if channel_names is None:\n    try:\n      channel_names = media_mix_model.media_names\n    except AttributeError as att_error:\n      raise lightweight_mmm.NotFittedModelError(\n          \"Model needs to be fit first before attempting to plot its fit.\"\n      ) from att_error\n\n  media_contribution = _calculate_media_contribution(media_mix_model)\n  if time_index is None:\n    time_index = (0, media_contribution.shape[1])\n\n  # Select time span and aggregate 2D media spend.\n  if media_spend.ndim == 1:\n    logging.warning(\"1D media spend has to align with time index range.\")\n  elif media_spend.ndim == 2:\n    media_spend = media_spend[time_index[0]:time_index[1], :]\n    media_spend = media_spend.sum(axis=0)\n\n  # Select time span and aggregate media contribution.\n  media_contribution = media_contribution[:, time_index[0]:time_index[1],]\n  media_contribution = media_contribution.sum(axis=(0, 1))\n\n  # Create media contribution and spend dataframe\n  media_df = pd.DataFrame(\n      np.transpose([media_contribution, media_spend]),\n      index=channel_names,\n      columns=[\"media attribution\", \"media spend\"])\n\n  if (media_df[\"media spend\"] == 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n\n  normalized_media_df = media_df.div(media_df.sum(axis=0), axis=1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    spends are zero or negative.\n    NotFittedModelError: if the model is not fitted.\n  \"\"\"\n  if (media_spend < 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n  if channel_names is None:\n    try:\n      channel_names = media_mix_model.media_names\n    except AttributeError as att_error:\n      raise lightweight_mmm.NotFittedModelError(\n          \"Model needs to be fit first before attempting to plot its fit.\"\n      ) from att_error\n\n  media_contribution = _calculate_media_contribution(media_mix_model)\n  if time_index is None:\n    time_index = (0, media_contribution.shape[1])\n\n  # Select time span and aggregate 2D media spend.\n  if media_spend.ndim == 1:\n    logging.warning(\"1D media spend has to align with time index range.\")\n  elif media_spend.ndim == 2:\n    media_spend = media_spend[time_index[0]:time_index[1], :]\n    media_spend = media_spend.sum(axis=0)\n\n  # Select time span and aggregate media contribution.\n  media_contribution = media_contribution[:, time_index[0]:time_index[1],]\n  media_contribution = media_contribution.sum(axis=(0, 1))\n\n  # Create media contribution and spend dataframe\n  media_df = pd.DataFrame(\n      np.transpose([media_contribution, media_spend]),\n      index=channel_names,\n      columns=[\"media attribution\", \"media spend\"])\n\n  if (media_df[\"media spend\"] == 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n\n  normalized_media_df = media_df.div(media_df.sum(axis=0), axis=1)\n  normalized_media_df[\"attribution over spend\"] = normalized_media_df[\n      \"media attribution\"] / normalized_media_df[\"media spend\"]\n  return normalized_media_df\n\n\ndef create_media_baseline_contribution_df(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[str]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for weekly media channels & basline contribution.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      raise lightweight_mmm.NotFittedModelError(\n          \"Model needs to be fit first before attempting to plot its fit.\"\n      ) from att_error\n\n  media_contribution = _calculate_media_contribution(media_mix_model)\n  if time_index is None:\n    time_index = (0, media_contribution.shape[1])\n\n  # Select time span and aggregate 2D media spend.\n  if media_spend.ndim == 1:\n    logging.warning(\"1D media spend has to align with time index range.\")\n  elif media_spend.ndim == 2:\n    media_spend = media_spend[time_index[0]:time_index[1], :]\n    media_spend = media_spend.sum(axis=0)\n\n  # Select time span and aggregate media contribution.\n  media_contribution = media_contribution[:, time_index[0]:time_index[1],]\n  media_contribution = media_contribution.sum(axis=(0, 1))\n\n  # Create media contribution and spend dataframe\n  media_df = pd.DataFrame(\n      np.transpose([media_contribution, media_spend]),\n      index=channel_names,\n      columns=[\"media attribution\", \"media spend\"])\n\n  if (media_df[\"media spend\"] == 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n\n  normalized_media_df = media_df.div(media_df.sum(axis=0), axis=1)\n  normalized_media_df[\"attribution over spend\"] = normalized_media_df[\n      \"media attribution\"] / normalized_media_df[\"media spend\"]\n  return normalized_media_df\n\n\ndef create_media_baseline_contribution_df(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[str]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for weekly media channels & basline contribution.\n\n  The output dataframe will be used to create a stacked area plot to visualize\n  the contribution of each media channels & baseline.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    logging.warning(\"1D media spend has to align with time index range.\")\n  elif media_spend.ndim == 2:\n    media_spend = media_spend[time_index[0]:time_index[1], :]\n    media_spend = media_spend.sum(axis=0)\n\n  # Select time span and aggregate media contribution.\n  media_contribution = media_contribution[:, time_index[0]:time_index[1],]\n  media_contribution = media_contribution.sum(axis=(0, 1))\n\n  # Create media contribution and spend dataframe\n  media_df = pd.DataFrame(\n      np.transpose([media_contribution, media_spend]),\n      index=channel_names,\n      columns=[\"media attribution\", \"media spend\"])\n\n  if (media_df[\"media spend\"] == 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n\n  normalized_media_df = media_df.div(media_df.sum(axis=0), axis=1)\n  normalized_media_df[\"attribution over spend\"] = normalized_media_df[\n      \"media attribution\"] / normalized_media_df[\"media spend\"]\n  return normalized_media_df\n\n\ndef create_media_baseline_contribution_df(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[str]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for weekly media channels & basline contribution.\n\n  The output dataframe will be used to create a stacked area plot to visualize\n  the contribution of each media channels & baseline.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n\n  Returns:\n    contribution_df: DataFrame of weekly channels & baseline contribution\n    percentage & volume.\n  \"\"\"\n  # Create media contribution matrix.\n  scaled_media_contribution = _calculate_media_contribution(media_mix_model)\n\n  # Aggregate media channel contribution across samples.\n  sum_scaled_media_contribution_across_samples = scaled_media_contribution.sum(\n      axis=0)\n  # Aggregate media channel contribution across channels.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  media_df = pd.DataFrame(\n      np.transpose([media_contribution, media_spend]),\n      index=channel_names,\n      columns=[\"media attribution\", \"media spend\"])\n\n  if (media_df[\"media spend\"] == 0).any():\n    raise ValueError(\"Values in media must all be non-negative or values \"\n                     \"in aggregated media must be possitive.\")\n\n  normalized_media_df = media_df.div(media_df.sum(axis=0), axis=1)\n  normalized_media_df[\"attribution over spend\"] = normalized_media_df[\n      \"media attribution\"] / normalized_media_df[\"media spend\"]\n  return normalized_media_df\n\n\ndef create_media_baseline_contribution_df(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[str]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for weekly media channels & basline contribution.\n\n  The output dataframe will be used to create a stacked area plot to visualize\n  the contribution of each media channels & baseline.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n\n  Returns:\n    contribution_df: DataFrame of weekly channels & baseline contribution\n    percentage & volume.\n  \"\"\"\n  # Create media contribution matrix.\n  scaled_media_contribution = _calculate_media_contribution(media_mix_model)\n\n  # Aggregate media channel contribution across samples.\n  sum_scaled_media_contribution_across_samples = scaled_media_contribution.sum(\n      axis=0)\n  # Aggregate media channel contribution across channels.\n  sum_scaled_media_contribution_across_channels = scaled_media_contribution.sum(\n      axis=2)\n\n  # Calculate the baseline contribution.\n  # Scaled prediction - sum of scaled contribution across channels.\n  scaled_prediction = media_mix_model.trace[\"mu\"]\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Sum up the scaled prediction across all the geos.\n    scaled_prediction = scaled_prediction.sum(axis=-1)\n  baseline_contribution = scaled_prediction - sum_scaled_media_contribution_across_channels", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  normalized_media_df[\"attribution over spend\"] = normalized_media_df[\n      \"media attribution\"] / normalized_media_df[\"media spend\"]\n  return normalized_media_df\n\n\ndef create_media_baseline_contribution_df(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[str]] = None) -> pd.DataFrame:\n  \"\"\"Creates a dataframe for weekly media channels & basline contribution.\n\n  The output dataframe will be used to create a stacked area plot to visualize\n  the contribution of each media channels & baseline.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n\n  Returns:\n    contribution_df: DataFrame of weekly channels & baseline contribution\n    percentage & volume.\n  \"\"\"\n  # Create media contribution matrix.\n  scaled_media_contribution = _calculate_media_contribution(media_mix_model)\n\n  # Aggregate media channel contribution across samples.\n  sum_scaled_media_contribution_across_samples = scaled_media_contribution.sum(\n      axis=0)\n  # Aggregate media channel contribution across channels.\n  sum_scaled_media_contribution_across_channels = scaled_media_contribution.sum(\n      axis=2)\n\n  # Calculate the baseline contribution.\n  # Scaled prediction - sum of scaled contribution across channels.\n  scaled_prediction = media_mix_model.trace[\"mu\"]\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Sum up the scaled prediction across all the geos.\n    scaled_prediction = scaled_prediction.sum(axis=-1)\n  baseline_contribution = scaled_prediction - sum_scaled_media_contribution_across_channels\n\n  # Sum up the scaled media, baseline contribution and predictio across samples.\n  sum_scaled_media_contribution_across_channels_samples = sum_scaled_media_contribution_across_channels.sum(\n      axis=0)\n  sum_scaled_baseline_contribution_across_samples = baseline_contribution.sum(\n      axis=0)\n\n  # Adjust baseline contribution and prediction when there's any negative value.\n  adjusted_sum_scaled_baseline_contribution_across_samples = np.where(\n      sum_scaled_baseline_contribution_across_samples < 0, 0,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  The output dataframe will be used to create a stacked area plot to visualize\n  the contribution of each media channels & baseline.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n\n  Returns:\n    contribution_df: DataFrame of weekly channels & baseline contribution\n    percentage & volume.\n  \"\"\"\n  # Create media contribution matrix.\n  scaled_media_contribution = _calculate_media_contribution(media_mix_model)\n\n  # Aggregate media channel contribution across samples.\n  sum_scaled_media_contribution_across_samples = scaled_media_contribution.sum(\n      axis=0)\n  # Aggregate media channel contribution across channels.\n  sum_scaled_media_contribution_across_channels = scaled_media_contribution.sum(\n      axis=2)\n\n  # Calculate the baseline contribution.\n  # Scaled prediction - sum of scaled contribution across channels.\n  scaled_prediction = media_mix_model.trace[\"mu\"]\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Sum up the scaled prediction across all the geos.\n    scaled_prediction = scaled_prediction.sum(axis=-1)\n  baseline_contribution = scaled_prediction - sum_scaled_media_contribution_across_channels\n\n  # Sum up the scaled media, baseline contribution and predictio across samples.\n  sum_scaled_media_contribution_across_channels_samples = sum_scaled_media_contribution_across_channels.sum(\n      axis=0)\n  sum_scaled_baseline_contribution_across_samples = baseline_contribution.sum(\n      axis=0)\n\n  # Adjust baseline contribution and prediction when there's any negative value.\n  adjusted_sum_scaled_baseline_contribution_across_samples = np.where(\n      sum_scaled_baseline_contribution_across_samples < 0, 0,\n      sum_scaled_baseline_contribution_across_samples)\n  adjusted_sum_scaled_prediction_across_samples = adjusted_sum_scaled_baseline_contribution_across_samples + sum_scaled_media_contribution_across_channels_samples\n\n  # Calculate the media and baseline pct.\n  # Media/baseline contribution across samples/total prediction across samples.\n  media_contribution_pct_by_channel = (\n      sum_scaled_media_contribution_across_samples /\n      adjusted_sum_scaled_prediction_across_samples.reshape(-1, 1))\n  # Adjust media pct contribution if the value is nan\n  media_contribution_pct_by_channel = np.nan_to_num(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    contribution_df: DataFrame of weekly channels & baseline contribution\n    percentage & volume.\n  \"\"\"\n  # Create media contribution matrix.\n  scaled_media_contribution = _calculate_media_contribution(media_mix_model)\n\n  # Aggregate media channel contribution across samples.\n  sum_scaled_media_contribution_across_samples = scaled_media_contribution.sum(\n      axis=0)\n  # Aggregate media channel contribution across channels.\n  sum_scaled_media_contribution_across_channels = scaled_media_contribution.sum(\n      axis=2)\n\n  # Calculate the baseline contribution.\n  # Scaled prediction - sum of scaled contribution across channels.\n  scaled_prediction = media_mix_model.trace[\"mu\"]\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Sum up the scaled prediction across all the geos.\n    scaled_prediction = scaled_prediction.sum(axis=-1)\n  baseline_contribution = scaled_prediction - sum_scaled_media_contribution_across_channels\n\n  # Sum up the scaled media, baseline contribution and predictio across samples.\n  sum_scaled_media_contribution_across_channels_samples = sum_scaled_media_contribution_across_channels.sum(\n      axis=0)\n  sum_scaled_baseline_contribution_across_samples = baseline_contribution.sum(\n      axis=0)\n\n  # Adjust baseline contribution and prediction when there's any negative value.\n  adjusted_sum_scaled_baseline_contribution_across_samples = np.where(\n      sum_scaled_baseline_contribution_across_samples < 0, 0,\n      sum_scaled_baseline_contribution_across_samples)\n  adjusted_sum_scaled_prediction_across_samples = adjusted_sum_scaled_baseline_contribution_across_samples + sum_scaled_media_contribution_across_channels_samples\n\n  # Calculate the media and baseline pct.\n  # Media/baseline contribution across samples/total prediction across samples.\n  media_contribution_pct_by_channel = (\n      sum_scaled_media_contribution_across_samples /\n      adjusted_sum_scaled_prediction_across_samples.reshape(-1, 1))\n  # Adjust media pct contribution if the value is nan\n  media_contribution_pct_by_channel = np.nan_to_num(\n      media_contribution_pct_by_channel)\n\n  baseline_contribution_pct = adjusted_sum_scaled_baseline_contribution_across_samples / adjusted_sum_scaled_prediction_across_samples\n  # Adjust baseline pct contribution if the value is nan\n  baseline_contribution_pct = np.nan_to_num(\n      baseline_contribution_pct)\n\n  # If the channel_names is none, then create naming covention for the channels.\n  if channel_names is None:\n    channel_names = media_mix_model.media_names", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  sum_scaled_media_contribution_across_channels = scaled_media_contribution.sum(\n      axis=2)\n\n  # Calculate the baseline contribution.\n  # Scaled prediction - sum of scaled contribution across channels.\n  scaled_prediction = media_mix_model.trace[\"mu\"]\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    # Sum up the scaled prediction across all the geos.\n    scaled_prediction = scaled_prediction.sum(axis=-1)\n  baseline_contribution = scaled_prediction - sum_scaled_media_contribution_across_channels\n\n  # Sum up the scaled media, baseline contribution and predictio across samples.\n  sum_scaled_media_contribution_across_channels_samples = sum_scaled_media_contribution_across_channels.sum(\n      axis=0)\n  sum_scaled_baseline_contribution_across_samples = baseline_contribution.sum(\n      axis=0)\n\n  # Adjust baseline contribution and prediction when there's any negative value.\n  adjusted_sum_scaled_baseline_contribution_across_samples = np.where(\n      sum_scaled_baseline_contribution_across_samples < 0, 0,\n      sum_scaled_baseline_contribution_across_samples)\n  adjusted_sum_scaled_prediction_across_samples = adjusted_sum_scaled_baseline_contribution_across_samples + sum_scaled_media_contribution_across_channels_samples\n\n  # Calculate the media and baseline pct.\n  # Media/baseline contribution across samples/total prediction across samples.\n  media_contribution_pct_by_channel = (\n      sum_scaled_media_contribution_across_samples /\n      adjusted_sum_scaled_prediction_across_samples.reshape(-1, 1))\n  # Adjust media pct contribution if the value is nan\n  media_contribution_pct_by_channel = np.nan_to_num(\n      media_contribution_pct_by_channel)\n\n  baseline_contribution_pct = adjusted_sum_scaled_baseline_contribution_across_samples / adjusted_sum_scaled_prediction_across_samples\n  # Adjust baseline pct contribution if the value is nan\n  baseline_contribution_pct = np.nan_to_num(\n      baseline_contribution_pct)\n\n  # If the channel_names is none, then create naming covention for the channels.\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n\n  # Create media/baseline contribution pct as dataframes.\n  media_contribution_pct_by_channel_df = pd.DataFrame(\n      media_contribution_pct_by_channel, columns=channel_names)\n  baseline_contribution_pct_df = pd.DataFrame(\n      baseline_contribution_pct, columns=[\"baseline\"])\n  contribution_pct_df = pd.merge(\n      media_contribution_pct_by_channel_df,\n      baseline_contribution_pct_df,\n      left_index=True,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  # Sum up the scaled media, baseline contribution and predictio across samples.\n  sum_scaled_media_contribution_across_channels_samples = sum_scaled_media_contribution_across_channels.sum(\n      axis=0)\n  sum_scaled_baseline_contribution_across_samples = baseline_contribution.sum(\n      axis=0)\n\n  # Adjust baseline contribution and prediction when there's any negative value.\n  adjusted_sum_scaled_baseline_contribution_across_samples = np.where(\n      sum_scaled_baseline_contribution_across_samples < 0, 0,\n      sum_scaled_baseline_contribution_across_samples)\n  adjusted_sum_scaled_prediction_across_samples = adjusted_sum_scaled_baseline_contribution_across_samples + sum_scaled_media_contribution_across_channels_samples\n\n  # Calculate the media and baseline pct.\n  # Media/baseline contribution across samples/total prediction across samples.\n  media_contribution_pct_by_channel = (\n      sum_scaled_media_contribution_across_samples /\n      adjusted_sum_scaled_prediction_across_samples.reshape(-1, 1))\n  # Adjust media pct contribution if the value is nan\n  media_contribution_pct_by_channel = np.nan_to_num(\n      media_contribution_pct_by_channel)\n\n  baseline_contribution_pct = adjusted_sum_scaled_baseline_contribution_across_samples / adjusted_sum_scaled_prediction_across_samples\n  # Adjust baseline pct contribution if the value is nan\n  baseline_contribution_pct = np.nan_to_num(\n      baseline_contribution_pct)\n\n  # If the channel_names is none, then create naming covention for the channels.\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n\n  # Create media/baseline contribution pct as dataframes.\n  media_contribution_pct_by_channel_df = pd.DataFrame(\n      media_contribution_pct_by_channel, columns=channel_names)\n  baseline_contribution_pct_df = pd.DataFrame(\n      baseline_contribution_pct, columns=[\"baseline\"])\n  contribution_pct_df = pd.merge(\n      media_contribution_pct_by_channel_df,\n      baseline_contribution_pct_df,\n      left_index=True,\n      right_index=True)\n\n  # If there's target scaler then inverse transform the posterior prediction.\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n\n  # Take the sum of posterior predictions across geos.\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    posterior_pred = posterior_pred.sum(axis=-1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      sum_scaled_baseline_contribution_across_samples)\n  adjusted_sum_scaled_prediction_across_samples = adjusted_sum_scaled_baseline_contribution_across_samples + sum_scaled_media_contribution_across_channels_samples\n\n  # Calculate the media and baseline pct.\n  # Media/baseline contribution across samples/total prediction across samples.\n  media_contribution_pct_by_channel = (\n      sum_scaled_media_contribution_across_samples /\n      adjusted_sum_scaled_prediction_across_samples.reshape(-1, 1))\n  # Adjust media pct contribution if the value is nan\n  media_contribution_pct_by_channel = np.nan_to_num(\n      media_contribution_pct_by_channel)\n\n  baseline_contribution_pct = adjusted_sum_scaled_baseline_contribution_across_samples / adjusted_sum_scaled_prediction_across_samples\n  # Adjust baseline pct contribution if the value is nan\n  baseline_contribution_pct = np.nan_to_num(\n      baseline_contribution_pct)\n\n  # If the channel_names is none, then create naming covention for the channels.\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n\n  # Create media/baseline contribution pct as dataframes.\n  media_contribution_pct_by_channel_df = pd.DataFrame(\n      media_contribution_pct_by_channel, columns=channel_names)\n  baseline_contribution_pct_df = pd.DataFrame(\n      baseline_contribution_pct, columns=[\"baseline\"])\n  contribution_pct_df = pd.merge(\n      media_contribution_pct_by_channel_df,\n      baseline_contribution_pct_df,\n      left_index=True,\n      right_index=True)\n\n  # If there's target scaler then inverse transform the posterior prediction.\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n\n  # Take the sum of posterior predictions across geos.\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    posterior_pred = posterior_pred.sum(axis=-1)\n\n  # Take the average of the inverse transformed prediction across samples.\n  posterior_pred_df = pd.DataFrame(\n      posterior_pred.mean(axis=0), columns=[\"avg_prediction\"])\n\n  # Adjust prediction value when prediction is less than 0.\n  posterior_pred_df[\"avg_prediction\"] = np.where(\n      posterior_pred_df[\"avg_prediction\"] < 0, 0,\n      posterior_pred_df[\"avg_prediction\"])\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      media_contribution_pct_by_channel)\n\n  baseline_contribution_pct = adjusted_sum_scaled_baseline_contribution_across_samples / adjusted_sum_scaled_prediction_across_samples\n  # Adjust baseline pct contribution if the value is nan\n  baseline_contribution_pct = np.nan_to_num(\n      baseline_contribution_pct)\n\n  # If the channel_names is none, then create naming covention for the channels.\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n\n  # Create media/baseline contribution pct as dataframes.\n  media_contribution_pct_by_channel_df = pd.DataFrame(\n      media_contribution_pct_by_channel, columns=channel_names)\n  baseline_contribution_pct_df = pd.DataFrame(\n      baseline_contribution_pct, columns=[\"baseline\"])\n  contribution_pct_df = pd.merge(\n      media_contribution_pct_by_channel_df,\n      baseline_contribution_pct_df,\n      left_index=True,\n      right_index=True)\n\n  # If there's target scaler then inverse transform the posterior prediction.\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n\n  # Take the sum of posterior predictions across geos.\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    posterior_pred = posterior_pred.sum(axis=-1)\n\n  # Take the average of the inverse transformed prediction across samples.\n  posterior_pred_df = pd.DataFrame(\n      posterior_pred.mean(axis=0), columns=[\"avg_prediction\"])\n\n  # Adjust prediction value when prediction is less than 0.\n  posterior_pred_df[\"avg_prediction\"] = np.where(\n      posterior_pred_df[\"avg_prediction\"] < 0, 0,\n      posterior_pred_df[\"avg_prediction\"])\n\n  contribution_pct_df.columns = [\n      \"{}_percentage\".format(col) for col in contribution_pct_df.columns\n  ]\n  contribution_df = pd.merge(\n      contribution_pct_df, posterior_pred_df, left_index=True, right_index=True)\n\n  # Create contribution by multiplying average prediction by media/baseline pct.\n  for channel in channel_names:\n    channel_contribution_col_name = \"{} contribution\".format(channel)\n    channel_pct_col = \"{}_percentage\".format(channel)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  # Create media/baseline contribution pct as dataframes.\n  media_contribution_pct_by_channel_df = pd.DataFrame(\n      media_contribution_pct_by_channel, columns=channel_names)\n  baseline_contribution_pct_df = pd.DataFrame(\n      baseline_contribution_pct, columns=[\"baseline\"])\n  contribution_pct_df = pd.merge(\n      media_contribution_pct_by_channel_df,\n      baseline_contribution_pct_df,\n      left_index=True,\n      right_index=True)\n\n  # If there's target scaler then inverse transform the posterior prediction.\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n\n  # Take the sum of posterior predictions across geos.\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    posterior_pred = posterior_pred.sum(axis=-1)\n\n  # Take the average of the inverse transformed prediction across samples.\n  posterior_pred_df = pd.DataFrame(\n      posterior_pred.mean(axis=0), columns=[\"avg_prediction\"])\n\n  # Adjust prediction value when prediction is less than 0.\n  posterior_pred_df[\"avg_prediction\"] = np.where(\n      posterior_pred_df[\"avg_prediction\"] < 0, 0,\n      posterior_pred_df[\"avg_prediction\"])\n\n  contribution_pct_df.columns = [\n      \"{}_percentage\".format(col) for col in contribution_pct_df.columns\n  ]\n  contribution_df = pd.merge(\n      contribution_pct_df, posterior_pred_df, left_index=True, right_index=True)\n\n  # Create contribution by multiplying average prediction by media/baseline pct.\n  for channel in channel_names:\n    channel_contribution_col_name = \"{} contribution\".format(channel)\n    channel_pct_col = \"{}_percentage\".format(channel)\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_pct_col] * contribution_df[\"avg_prediction\"]\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_contribution_col_name].astype(\"float\")\n  contribution_df.loc[:, \"baseline contribution\"] = contribution_df[\n      \"baseline_percentage\"] * contribution_df[\"avg_prediction\"]\n\n  period = np.arange(1, contribution_df.shape[0] + 1)\n  contribution_df.loc[:, \"period\"] = period\n  return contribution_df", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      right_index=True)\n\n  # If there's target scaler then inverse transform the posterior prediction.\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n\n  # Take the sum of posterior predictions across geos.\n  if media_mix_model.trace[\"media_transformed\"].ndim > 3:\n    posterior_pred = posterior_pred.sum(axis=-1)\n\n  # Take the average of the inverse transformed prediction across samples.\n  posterior_pred_df = pd.DataFrame(\n      posterior_pred.mean(axis=0), columns=[\"avg_prediction\"])\n\n  # Adjust prediction value when prediction is less than 0.\n  posterior_pred_df[\"avg_prediction\"] = np.where(\n      posterior_pred_df[\"avg_prediction\"] < 0, 0,\n      posterior_pred_df[\"avg_prediction\"])\n\n  contribution_pct_df.columns = [\n      \"{}_percentage\".format(col) for col in contribution_pct_df.columns\n  ]\n  contribution_df = pd.merge(\n      contribution_pct_df, posterior_pred_df, left_index=True, right_index=True)\n\n  # Create contribution by multiplying average prediction by media/baseline pct.\n  for channel in channel_names:\n    channel_contribution_col_name = \"{} contribution\".format(channel)\n    channel_pct_col = \"{}_percentage\".format(channel)\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_pct_col] * contribution_df[\"avg_prediction\"]\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_contribution_col_name].astype(\"float\")\n  contribution_df.loc[:, \"baseline contribution\"] = contribution_df[\n      \"baseline_percentage\"] * contribution_df[\"avg_prediction\"]\n\n  period = np.arange(1, contribution_df.shape[0] + 1)\n  contribution_df.loc[:, \"period\"] = period\n  return contribution_df\n\n\ndef plot_response_curves(# jax-ndarray\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    prices: jnp.ndarray = None,\n    optimal_allocation_per_timeunit: Optional[jnp.ndarray] = None,\n    steps: int = 50,\n    percentage_add: float = 0.2,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  # Take the average of the inverse transformed prediction across samples.\n  posterior_pred_df = pd.DataFrame(\n      posterior_pred.mean(axis=0), columns=[\"avg_prediction\"])\n\n  # Adjust prediction value when prediction is less than 0.\n  posterior_pred_df[\"avg_prediction\"] = np.where(\n      posterior_pred_df[\"avg_prediction\"] < 0, 0,\n      posterior_pred_df[\"avg_prediction\"])\n\n  contribution_pct_df.columns = [\n      \"{}_percentage\".format(col) for col in contribution_pct_df.columns\n  ]\n  contribution_df = pd.merge(\n      contribution_pct_df, posterior_pred_df, left_index=True, right_index=True)\n\n  # Create contribution by multiplying average prediction by media/baseline pct.\n  for channel in channel_names:\n    channel_contribution_col_name = \"{} contribution\".format(channel)\n    channel_pct_col = \"{}_percentage\".format(channel)\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_pct_col] * contribution_df[\"avg_prediction\"]\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_contribution_col_name].astype(\"float\")\n  contribution_df.loc[:, \"baseline contribution\"] = contribution_df[\n      \"baseline_percentage\"] * contribution_df[\"avg_prediction\"]\n\n  period = np.arange(1, contribution_df.shape[0] + 1)\n  contribution_df.loc[:, \"period\"] = period\n  return contribution_df\n\n\ndef plot_response_curves(# jax-ndarray\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    prices: jnp.ndarray = None,\n    optimal_allocation_per_timeunit: Optional[jnp.ndarray] = None,\n    steps: int = 50,\n    percentage_add: float = 0.2,\n    apply_log_scale: bool = False,\n    figure_size: Tuple[int, int] = (8, 10),\n    n_columns: int = 3,\n    marker_size: int = 8,\n    legend_fontsize: int = 8,\n    seed: Optional[int] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the response curves of each media channel based on the model.\n\n  It plots an individual subplot for each media channel. If '\n  optimal_allocation_per_timeunit is given it uses it to add markers based on", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  contribution_pct_df.columns = [\n      \"{}_percentage\".format(col) for col in contribution_pct_df.columns\n  ]\n  contribution_df = pd.merge(\n      contribution_pct_df, posterior_pred_df, left_index=True, right_index=True)\n\n  # Create contribution by multiplying average prediction by media/baseline pct.\n  for channel in channel_names:\n    channel_contribution_col_name = \"{} contribution\".format(channel)\n    channel_pct_col = \"{}_percentage\".format(channel)\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_pct_col] * contribution_df[\"avg_prediction\"]\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_contribution_col_name].astype(\"float\")\n  contribution_df.loc[:, \"baseline contribution\"] = contribution_df[\n      \"baseline_percentage\"] * contribution_df[\"avg_prediction\"]\n\n  period = np.arange(1, contribution_df.shape[0] + 1)\n  contribution_df.loc[:, \"period\"] = period\n  return contribution_df\n\n\ndef plot_response_curves(# jax-ndarray\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    prices: jnp.ndarray = None,\n    optimal_allocation_per_timeunit: Optional[jnp.ndarray] = None,\n    steps: int = 50,\n    percentage_add: float = 0.2,\n    apply_log_scale: bool = False,\n    figure_size: Tuple[int, int] = (8, 10),\n    n_columns: int = 3,\n    marker_size: int = 8,\n    legend_fontsize: int = 8,\n    seed: Optional[int] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the response curves of each media channel based on the model.\n\n  It plots an individual subplot for each media channel. If '\n  optimal_allocation_per_timeunit is given it uses it to add markers based on\n  historic average spend and the given optimal one on each of the individual\n  subplots.\n\n  It then plots a combined plot with all the response curves which can be\n  changed to log scale if apply_log_scale is True.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_scaler: Scaler that was used to scale the media data before training.\n    target_scaler: Scaler used for scaling the target, to unscaled values and", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_pct_col] * contribution_df[\"avg_prediction\"]\n    contribution_df.loc[:, channel_contribution_col_name] = contribution_df[\n        channel_contribution_col_name].astype(\"float\")\n  contribution_df.loc[:, \"baseline contribution\"] = contribution_df[\n      \"baseline_percentage\"] * contribution_df[\"avg_prediction\"]\n\n  period = np.arange(1, contribution_df.shape[0] + 1)\n  contribution_df.loc[:, \"period\"] = period\n  return contribution_df\n\n\ndef plot_response_curves(# jax-ndarray\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    prices: jnp.ndarray = None,\n    optimal_allocation_per_timeunit: Optional[jnp.ndarray] = None,\n    steps: int = 50,\n    percentage_add: float = 0.2,\n    apply_log_scale: bool = False,\n    figure_size: Tuple[int, int] = (8, 10),\n    n_columns: int = 3,\n    marker_size: int = 8,\n    legend_fontsize: int = 8,\n    seed: Optional[int] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the response curves of each media channel based on the model.\n\n  It plots an individual subplot for each media channel. If '\n  optimal_allocation_per_timeunit is given it uses it to add markers based on\n  historic average spend and the given optimal one on each of the individual\n  subplots.\n\n  It then plots a combined plot with all the response curves which can be\n  changed to log scale if apply_log_scale is True.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_scaler: Scaler that was used to scale the media data before training.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prices: Prices to translate the media units to spend. If all your data is\n      already in spend numbers you can leave this as None. If some of your data\n      is media spend and others is media unit, leave the media spend with price\n      1 and add the price to the media unit channels.\n    optimal_allocation_per_timeunit: Optimal allocation per time unit per media\n      channel. This can be obtained by running the optimization provided by\n      LightweightMMM.\n    steps: Number of steps to simulate.\n    percentage_add: Percentage too exceed the maximum historic spend for the", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n\ndef plot_response_curves(# jax-ndarray\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    media_scaler: Optional[preprocessing.CustomScaler] = None,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    prices: jnp.ndarray = None,\n    optimal_allocation_per_timeunit: Optional[jnp.ndarray] = None,\n    steps: int = 50,\n    percentage_add: float = 0.2,\n    apply_log_scale: bool = False,\n    figure_size: Tuple[int, int] = (8, 10),\n    n_columns: int = 3,\n    marker_size: int = 8,\n    legend_fontsize: int = 8,\n    seed: Optional[int] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the response curves of each media channel based on the model.\n\n  It plots an individual subplot for each media channel. If '\n  optimal_allocation_per_timeunit is given it uses it to add markers based on\n  historic average spend and the given optimal one on each of the individual\n  subplots.\n\n  It then plots a combined plot with all the response curves which can be\n  changed to log scale if apply_log_scale is True.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_scaler: Scaler that was used to scale the media data before training.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prices: Prices to translate the media units to spend. If all your data is\n      already in spend numbers you can leave this as None. If some of your data\n      is media spend and others is media unit, leave the media spend with price\n      1 and add the price to the media unit channels.\n    optimal_allocation_per_timeunit: Optimal allocation per time unit per media\n      channel. This can be obtained by running the optimization provided by\n      LightweightMMM.\n    steps: Number of steps to simulate.\n    percentage_add: Percentage too exceed the maximum historic spend for the\n      simulation of the response curve.\n    apply_log_scale: Whether to apply the log scale to the predictions (Y axis).\n      When some media channels have very large scale compare to others it might\n      be useful to use apply_log_scale=True. Default is False.\n    figure_size: Size of the plot figure.\n    n_columns: Number of columns to display in the subplots grid. Modifying this\n      parameter might require to adjust figure_size accordingly for the plot\n      to still have reasonable structure.\n    marker_size: Size of the marker for the optimization annotations. Only\n      useful if optimal_allocation_per_timeunit is not None. Default is 8.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    apply_log_scale: bool = False,\n    figure_size: Tuple[int, int] = (8, 10),\n    n_columns: int = 3,\n    marker_size: int = 8,\n    legend_fontsize: int = 8,\n    seed: Optional[int] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the response curves of each media channel based on the model.\n\n  It plots an individual subplot for each media channel. If '\n  optimal_allocation_per_timeunit is given it uses it to add markers based on\n  historic average spend and the given optimal one on each of the individual\n  subplots.\n\n  It then plots a combined plot with all the response curves which can be\n  changed to log scale if apply_log_scale is True.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_scaler: Scaler that was used to scale the media data before training.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prices: Prices to translate the media units to spend. If all your data is\n      already in spend numbers you can leave this as None. If some of your data\n      is media spend and others is media unit, leave the media spend with price\n      1 and add the price to the media unit channels.\n    optimal_allocation_per_timeunit: Optimal allocation per time unit per media\n      channel. This can be obtained by running the optimization provided by\n      LightweightMMM.\n    steps: Number of steps to simulate.\n    percentage_add: Percentage too exceed the maximum historic spend for the\n      simulation of the response curve.\n    apply_log_scale: Whether to apply the log scale to the predictions (Y axis).\n      When some media channels have very large scale compare to others it might\n      be useful to use apply_log_scale=True. Default is False.\n    figure_size: Size of the plot figure.\n    n_columns: Number of columns to display in the subplots grid. Modifying this\n      parameter might require to adjust figure_size accordingly for the plot\n      to still have reasonable structure.\n    marker_size: Size of the marker for the optimization annotations. Only\n      useful if optimal_allocation_per_timeunit is not None. Default is 8.\n    legend_fontsize: Legend font size for individual subplots.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    Plots of response curves.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  historic average spend and the given optimal one on each of the individual\n  subplots.\n\n  It then plots a combined plot with all the response curves which can be\n  changed to log scale if apply_log_scale is True.\n\n  Args:\n    media_mix_model: Media mix model to use for plotting the response curves.\n    media_scaler: Scaler that was used to scale the media data before training.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    prices: Prices to translate the media units to spend. If all your data is\n      already in spend numbers you can leave this as None. If some of your data\n      is media spend and others is media unit, leave the media spend with price\n      1 and add the price to the media unit channels.\n    optimal_allocation_per_timeunit: Optimal allocation per time unit per media\n      channel. This can be obtained by running the optimization provided by\n      LightweightMMM.\n    steps: Number of steps to simulate.\n    percentage_add: Percentage too exceed the maximum historic spend for the\n      simulation of the response curve.\n    apply_log_scale: Whether to apply the log scale to the predictions (Y axis).\n      When some media channels have very large scale compare to others it might\n      be useful to use apply_log_scale=True. Default is False.\n    figure_size: Size of the plot figure.\n    n_columns: Number of columns to display in the subplots grid. Modifying this\n      parameter might require to adjust figure_size accordingly for the plot\n      to still have reasonable structure.\n    marker_size: Size of the marker for the optimization annotations. Only\n      useful if optimal_allocation_per_timeunit is not None. Default is 8.\n    legend_fontsize: Legend font size for individual subplots.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    Plots of response curves.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its response \"\n        \"curves.\")\n  media = media_mix_model.media\n  media_maxes = media.max(axis=0) * (1 + percentage_add)\n  if media_mix_model._extra_features is not None:\n    extra_features = jnp.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      plot in the original scale.\n    prices: Prices to translate the media units to spend. If all your data is\n      already in spend numbers you can leave this as None. If some of your data\n      is media spend and others is media unit, leave the media spend with price\n      1 and add the price to the media unit channels.\n    optimal_allocation_per_timeunit: Optimal allocation per time unit per media\n      channel. This can be obtained by running the optimization provided by\n      LightweightMMM.\n    steps: Number of steps to simulate.\n    percentage_add: Percentage too exceed the maximum historic spend for the\n      simulation of the response curve.\n    apply_log_scale: Whether to apply the log scale to the predictions (Y axis).\n      When some media channels have very large scale compare to others it might\n      be useful to use apply_log_scale=True. Default is False.\n    figure_size: Size of the plot figure.\n    n_columns: Number of columns to display in the subplots grid. Modifying this\n      parameter might require to adjust figure_size accordingly for the plot\n      to still have reasonable structure.\n    marker_size: Size of the marker for the optimization annotations. Only\n      useful if optimal_allocation_per_timeunit is not None. Default is 8.\n    legend_fontsize: Legend font size for individual subplots.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    Plots of response curves.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its response \"\n        \"curves.\")\n  media = media_mix_model.media\n  media_maxes = media.max(axis=0) * (1 + percentage_add)\n  if media_mix_model._extra_features is not None:\n    extra_features = jnp.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(\n      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_405-455", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      simulation of the response curve.\n    apply_log_scale: Whether to apply the log scale to the predictions (Y axis).\n      When some media channels have very large scale compare to others it might\n      be useful to use apply_log_scale=True. Default is False.\n    figure_size: Size of the plot figure.\n    n_columns: Number of columns to display in the subplots grid. Modifying this\n      parameter might require to adjust figure_size accordingly for the plot\n      to still have reasonable structure.\n    marker_size: Size of the marker for the optimization annotations. Only\n      useful if optimal_allocation_per_timeunit is not None. Default is 8.\n    legend_fontsize: Legend font size for individual subplots.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    Plots of response curves.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its response \"\n        \"curves.\")\n  media = media_mix_model.media\n  media_maxes = media.max(axis=0) * (1 + percentage_add)\n  if media_mix_model._extra_features is not None:\n    extra_features = jnp.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(\n      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,\n                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_415-465", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    legend_fontsize: Legend font size for individual subplots.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n      this function and any other function that gets predictions with the same\n      seed.\n\n  Returns:\n    Plots of response curves.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its response \"\n        \"curves.\")\n  media = media_mix_model.media\n  media_maxes = media.max(axis=0) * (1 + percentage_add)\n  if media_mix_model._extra_features is not None:\n    extra_features = jnp.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(\n      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,\n                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal\n  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_425-475", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        \"Model needs to be fit first before attempting to plot its response \"\n        \"curves.\")\n  media = media_mix_model.media\n  media_maxes = media.max(axis=0) * (1 + percentage_add)\n  if media_mix_model._extra_features is not None:\n    extra_features = jnp.expand_dims(\n        media_mix_model._extra_features.mean(axis=0), axis=0)\n  else:\n    extra_features = None\n  media_ranges = jnp.expand_dims(\n      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,\n                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal\n  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:\n    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_435-485", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      jnp.linspace(start=0, stop=media_maxes, num=steps), axis=0)\n\n  make_predictions = jax.vmap(\n      jax.vmap(_make_single_prediction,\n               in_axes=(None, 0, None, None),\n               out_axes=0),\n      in_axes=(None, 0, None, None), out_axes=1)\n  diagonal = jnp.repeat(\n      jnp.eye(media_mix_model.n_media_channels), steps,\n      axis=0).reshape(media_mix_model.n_media_channels, steps,\n                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal\n  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:\n    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)\n\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_445-495", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "                      media_mix_model.n_media_channels)\n\n  prediction_offset = media_mix_model.predict(\n      media=jnp.zeros((1, *media.shape[1:])),\n      extra_features=extra_features).mean(axis=0)\n\n  if media.ndim == 3:\n    diagonal = jnp.expand_dims(diagonal, axis=-1)\n    prediction_offset = jnp.expand_dims(prediction_offset, axis=0)\n  mock_media = media_ranges * diagonal\n  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:\n    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)\n\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_455-505", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  predictions = jnp.squeeze(a=make_predictions(media_mix_model,\n                                               mock_media,\n                                               extra_features,\n                                               seed))\n  predictions = predictions - prediction_offset\n  media_ranges = jnp.squeeze(media_ranges)\n  if target_scaler:\n    predictions = target_scaler.inverse_transform(predictions)\n\n  if media_scaler:\n    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)\n\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(\n          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_465-515", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    media_ranges = media_scaler.inverse_transform(media_ranges)\n\n  if prices is not None:\n    if media.ndim == 3:\n      prices = jnp.expand_dims(prices, axis=-1)\n    media_ranges *= prices\n\n  if predictions.ndim == 3:\n    media_ranges = jnp.sum(media_ranges, axis=-1)\n    predictions = jnp.sum(predictions, axis=-1)\n\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(\n          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"\n  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_475-525", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  if optimal_allocation_per_timeunit is not None:\n    average_allocation = media_mix_model.media.mean(axis=0)\n    average_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=average_allocation,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(\n          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"\n  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_485-535", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    optimal_allocation_predictions = _generate_diagonal_predictions(\n        media_mix_model=media_mix_model,\n        media_values=optimal_allocation_per_timeunit,\n        extra_features=extra_features,\n        target_scaler=target_scaler,\n        prediction_offset=prediction_offset,\n        seed=seed)\n    if media_scaler:\n      average_allocation = media_scaler.inverse_transform(average_allocation)\n      optimal_allocation_per_timeunit = media_scaler.inverse_transform(\n          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"\n  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_495-545", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "          optimal_allocation_per_timeunit)\n    if prices is not None:\n      optimal_allocation_per_timeunit *= prices\n      average_allocation *= prices\n    if media.ndim == 3:\n      average_allocation = jnp.sum(average_allocation, axis=-1)\n      optimal_allocation_per_timeunit = jnp.sum(\n          optimal_allocation_per_timeunit, axis=-1)\n\n  kpi_label = \"KPI\" if target_scaler else \"Normalized KPI\"\n  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_505-555", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  fig = plt.figure(media_mix_model.n_media_channels + 1,\n                   figsize=figure_size,\n                   tight_layout=True)\n  n_rows = _calculate_number_rows_plot(\n      n_media_channels=media_mix_model.n_media_channels, n_columns=n_columns)\n  last_ax = fig.add_subplot(n_rows, 1, n_rows)\n  for i in range(media_mix_model.n_media_channels):\n    ax = fig.add_subplot(n_rows, n_columns, i + 1)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_515-565", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        y=predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=ax)\n    sns.lineplot(\n        x=media_ranges[:, i],\n        y=jnp.log(predictions[:, i]) if apply_log_scale else predictions[:, i],\n        label=media_mix_model.media_names[i],\n        color=_PALETTE[i],\n        ax=last_ax)\n    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_525-575", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    if optimal_allocation_per_timeunit is not None:\n      ax.plot(\n          average_allocation[i],\n          average_allocation_predictions[i],\n          marker=\"o\",\n          markersize=marker_size,\n          label=\"avg_spend\",\n          color=_PALETTE[i])\n      ax.plot(\n          optimal_allocation_per_timeunit[i],\n          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_535-585", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "          optimal_allocation_predictions[i],\n          marker=\"x\",\n          markersize=marker_size + 2,\n          label=\"optimal_spend\",\n          color=_PALETTE[i])\n    ax.set_ylabel(kpi_label)\n    ax.set_xlabel(\"Normalized Spend\" if not media_scaler else \"Spend\")\n    ax.legend(fontsize=legend_fontsize)\n\n  fig.suptitle(\"Response curves\", fontsize=20)\n  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_545-595", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  last_ax.set_ylabel(kpi_label if not apply_log_scale else f\"log({kpi_label})\")\n  last_ax.set_xlabel(\"Normalized spend per channel\"\n                     if not media_scaler else \"Spend per channel\")\n  plt.close()\n  return fig\n\n\ndef plot_cross_correlate(feature: jnp.ndarray,\n                         target: jnp.ndarray,\n                         maxlags: int = 10) -> Tuple[int, float]:\n  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_555-605", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  \"\"\"Plots the cross correlation coefficients between 2 vectors.\n\n  In the chart look for positive peaks, this shows how the lags of the feature\n  lead the target.\n\n  Args:\n    feature: Vector, the lags of which predict target.\n    target: Vector, what is predicted.\n    maxlags: Maximum number of lags.\n\n  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_565-615", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Returns:\n    Lag index and corresponding correlation of the peak correlation.\n\n  Raises:\n    ValueError: If inputs don't have same length.\n  \"\"\"\n  if len(feature) != len(target):\n    raise ValueError(\"feature and target need to have the same length.\")\n  maxlags = jnp.minimum(len(feature) - 1, maxlags)\n  mean_feature, mean_target = feature.mean(), target.mean()\n  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_575-625", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  plot = plt.xcorr(\n      x=feature - mean_feature, y=target - mean_target, maxlags=maxlags)\n  plt.show()\n  maxidx = plot[1][plot[0] <= 0].argmax()\n  return plot[0][maxidx], plot[1][maxidx]\n\n\ndef plot_var_cost(media: jnp.ndarray, costs: jnp.ndarray,\n                  names: List[str]) -> matplotlib.figure.Figure:\n  \"\"\"Plots a a chart between the coefficient of variation and cost.\n\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_585-635", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  Args:\n    media: Media matrix.\n    costs: Cost vector.\n    names: List of variable names.\n\n  Returns:\n    Plot of coefficient of variation and cost.\n\n  Raises:\n    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_595-645", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    ValueError if inputs don't conform to same length.\n  \"\"\"\n  if media.shape[1] != len(costs):\n    raise ValueError(\"media columns and costs needs to have same length.\")\n  if media.shape[1] != len(names):\n    raise ValueError(\"media columns and names needs to have same length.\")\n  coef_of_variation = media.std(axis=0) / media.mean(axis=0)\n\n  fig, ax = plt.subplots(1, 1)\n  ax.scatter(x=costs, y=coef_of_variation)\n  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_605-655", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  # https://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples.\n  for i in range(len(costs)):\n    x, y, label = costs[i], coef_of_variation[i], names[i]\n    ax.annotate(text=label, xy=(x, y))\n  ax.set_xlabel(\"Cost\")\n  ax.set_ylabel(\"Coef of Variation\")\n  plt.close()\n  return fig\n\n\ndef _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_615-665", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "def _create_shaded_line_plot(predictions: jnp.ndarray,\n                             target: jnp.ndarray,\n                             axis: matplotlib.axes.Axes,\n                             title_prefix: str = \"\",\n                             interval_mid_range: float = .9,\n                             digits: int = 3) -> None:\n  \"\"\"Creates a plot of ground truth, predicted value and credibility interval.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Attribute(Name(Load)Load)Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstantConstant)Expr(Constant)If(Compare(Subscript(Attribute(Name(Load)Load)ConstantLoad)NotEqCall(Name(Load)Name(Load)))Raise(Call(Name(Load)Constant)))Assign(Name(Store)BinOp(ConstantSubBinOp(BinOp(ConstantSubName(Load))DivConstant)))Assign(Name(Store)BinOp(BinOp(ConstantSubName(Load))DivConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)BinOp(ConstantMultCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Call(Attribute(Name(Load)Load)keyword(Constant))))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad))Name(Load)keyword(Constant)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad))Call(Attribute(Name(Load)Load)keyword(Constant))keyword(Constant)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)keyword(Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))keyword(Name(Load))keyword(Name(Load))keyword(Constant)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)List(ConstantConstantLoad)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(ConstantLoad)List(Name(Load)ConstantCall(Attribute(ConstantLoad)keyword(Name(Load))keyword(Name(Load)))Call(Attribute(ConstantLoad)keyword(Name(Load))keyword(Name(Load)))Load)))Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_625-675", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    target: Array of true values. Must be same length as predictions.\n    axis: Matplotlib axis in which to plot the data.\n    title_prefix: Prefix to add as the label of the plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n  \"\"\"\n  if predictions.shape[1] != len(target):\n    raise ValueError(\n        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_635-685", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        \"Predicted data and ground-truth data must have same length.\")\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n  upper_bound = jnp.quantile(a=predictions, q=upper_quantile, axis=0)\n  lower_bound = jnp.quantile(a=predictions, q=lower_quantile, axis=0)\n\n  r2, _ = arviz.r2_score(y_true=target, y_pred=predictions)\n  mape = 100 * metrics.mean_absolute_percentage_error(\n      y_true=target, y_pred=predictions.mean(axis=0))\n  axis.plot(jnp.arange(target.shape[0]), target, c=\"grey\", alpha=.9)\n  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_645-695", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  axis.plot(\n      jnp.arange(target.shape[0]),\n      predictions.mean(axis=0),\n      c=\"green\",\n      alpha=.9)\n  axis.fill_between(\n      x=jnp.arange(target.shape[0]),\n      y1=lower_bound,\n      y2=upper_bound,\n      alpha=.35,\n      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_655-705", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      color=\"green\")\n  axis.legend([\"True KPI\", \"Predicted KPI\"])\n  axis.yaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  axis.xaxis.grid(color=\"gray\", linestyle=\"dashed\", alpha=0.3)\n  title = \" \".join([\n      title_prefix,\n      \"True and predicted KPI.\",\n      \"R2 = {r2:.{digits}f}\".format(r2=r2, digits=digits),\n      \"MAPE = {mape:.{digits}f}%\".format(mape=mape, digits=digits)\n  ])\n  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_665-715", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  axis.title.set_text(title)\n  plt.close()\n\n\ndef _call_fit_plotter(\n    predictions: jnp.array,\n    target: jnp.array,\n    interval_mid_range: float,\n    digits: int) -> matplotlib.figure.Figure:\n  \"\"\"Calls the shaded line plot once for national and N times for geo models.\n\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_675-725", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  Args:\n    predictions: 2d array of predicted values.\n    target: Array of true values. Must be same length as prediction.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_685-735", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    Figure of the plot.\n  \"\"\"\n  # TODO(): Allow to pass geo names for fit plots\n  if predictions.ndim == 3:  # Multiple plots for geo model\n    figure, axes = plt.subplots(predictions.shape[-1],\n                                figsize=(10, 5 * predictions.shape[-1]))\n    for i, ax in enumerate(axes):\n      _create_shaded_line_plot(predictions=predictions[..., i],\n                               target=target[..., i],\n                               axis=ax,\n                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_695-745", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "                               title_prefix=f\"Geo {i}:\",\n                               interval_mid_range=interval_mid_range,\n                               digits=digits)\n  else:  # Single plot for national model\n    figure, ax = plt.subplots(1, 1)\n    _create_shaded_line_plot(predictions=predictions,\n                             target=target,\n                             axis=ax,\n                             interval_mid_range=interval_mid_range,\n                             digits=digits)\n  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_705-755", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  return figure\n\n\ndef plot_model_fit(media_mix_model: lightweight_mmm.LightweightMMM,\n                   target_scaler: Optional[preprocessing.CustomScaler] = None,\n                   interval_mid_range: float = .9,\n                   digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the training data.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,\n                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_715-765", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target, to unscaled values and\n      plot in the original scale.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,\n                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_725-775", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Returns:\n    Plot of model fit.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n  target_train = media_mix_model._target\n  posterior_pred = media_mix_model.trace[\"mu\"]\n  if target_scaler:\n    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,\n                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,\n      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_media_channel_posteriors(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    channel_names: Optional[Sequence[Any]] = None,\n    quantiles: Sequence[float] = (0.05, 0.5, 0.95),\n    fig_size: Optional[Tuple[int, int]] = None) -> matplotlib.figure.Figure:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_735-785", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    target_train = target_scaler.inverse_transform(target_train)\n\n  return _call_fit_plotter(\n      predictions=posterior_pred,\n      target=target_train,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_out_of_sample_model_fit(out_of_sample_predictions: jnp.ndarray,\n                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,\n      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_media_channel_posteriors(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    channel_names: Optional[Sequence[Any]] = None,\n    quantiles: Sequence[float] = (0.05, 0.5, 0.95),\n    fig_size: Optional[Tuple[int, int]] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the posterior distributions of estimated media channel effect.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    channel_names: Names of media channels to be added to plot.\n    quantiles: Quantiles to draw on the distribution.\n    fig_size: Size of the figure to plot as used by matplotlib. If not specified\n      it will be determined dynamically based on the number of media channels", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_745-795", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "                                 out_of_sample_target: jnp.ndarray,\n                                 interval_mid_range: float = .9,\n                                 digits: int = 3) -> matplotlib.figure.Figure:\n  \"\"\"Plots the ground truth, predicted value and interval for the test data.\n\n  Args:\n    out_of_sample_predictions: Predictions for the out-of-sample period, as\n      derived from mmm.predict.\n    out_of_sample_target: Target for the out-of-sample period. Needs to be on\n      the same scale as out_of_sample_predictions.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,\n      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_media_channel_posteriors(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    channel_names: Optional[Sequence[Any]] = None,\n    quantiles: Sequence[float] = (0.05, 0.5, 0.95),\n    fig_size: Optional[Tuple[int, int]] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the posterior distributions of estimated media channel effect.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    channel_names: Names of media channels to be added to plot.\n    quantiles: Quantiles to draw on the distribution.\n    fig_size: Size of the figure to plot as used by matplotlib. If not specified\n      it will be determined dynamically based on the number of media channels\n      and geos the model was trained on.\n\n  Returns:\n    Plot of posterior distributions.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  n_media_channels = np.shape(media_mix_model.trace[\"coef_media\"])[1]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_755-805", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n      between 0 and 1.\n    digits: Number of decimals to display on metrics in the plot.\n\n  Returns:\n    Plot of model fit.\n  \"\"\"\n  return _call_fit_plotter(\n      predictions=out_of_sample_predictions,\n      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_media_channel_posteriors(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    channel_names: Optional[Sequence[Any]] = None,\n    quantiles: Sequence[float] = (0.05, 0.5, 0.95),\n    fig_size: Optional[Tuple[int, int]] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the posterior distributions of estimated media channel effect.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    channel_names: Names of media channels to be added to plot.\n    quantiles: Quantiles to draw on the distribution.\n    fig_size: Size of the figure to plot as used by matplotlib. If not specified\n      it will be determined dynamically based on the number of media channels\n      and geos the model was trained on.\n\n  Returns:\n    Plot of posterior distributions.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  n_media_channels = np.shape(media_mix_model.trace[\"coef_media\"])[1]\n  n_geos = (\n      media_mix_model.media.shape[2] if media_mix_model.media.ndim == 3 else 1)\n\n  if not fig_size:\n    fig_size = (5 * n_geos, 3 * n_media_channels)\n\n  media_channel_posteriors = media_mix_model.trace[\"coef_media\"]\n  if channel_names is None:\n    channel_names = np.arange(np.shape(media_channel_posteriors)[1])\n  fig, axes = plt.subplots(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_765-815", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      target=out_of_sample_target,\n      interval_mid_range=interval_mid_range,\n      digits=digits)\n\n\ndef plot_media_channel_posteriors(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    channel_names: Optional[Sequence[Any]] = None,\n    quantiles: Sequence[float] = (0.05, 0.5, 0.95),\n    fig_size: Optional[Tuple[int, int]] = None) -> matplotlib.figure.Figure:\n  \"\"\"Plots the posterior distributions of estimated media channel effect.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    channel_names: Names of media channels to be added to plot.\n    quantiles: Quantiles to draw on the distribution.\n    fig_size: Size of the figure to plot as used by matplotlib. If not specified\n      it will be determined dynamically based on the number of media channels\n      and geos the model was trained on.\n\n  Returns:\n    Plot of posterior distributions.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  n_media_channels = np.shape(media_mix_model.trace[\"coef_media\"])[1]\n  n_geos = (\n      media_mix_model.media.shape[2] if media_mix_model.media.ndim == 3 else 1)\n\n  if not fig_size:\n    fig_size = (5 * n_geos, 3 * n_media_channels)\n\n  media_channel_posteriors = media_mix_model.trace[\"coef_media\"]\n  if channel_names is None:\n    channel_names = np.arange(np.shape(media_channel_posteriors)[1])\n  fig, axes = plt.subplots(\n      nrows=n_media_channels, ncols=n_geos, figsize=fig_size)\n  for channel_i, channel_axis in enumerate(axes):\n    if isinstance(channel_axis, np.ndarray):\n      for geo_i, geo_axis in enumerate(channel_axis):\n        geo_axis = arviz.plot_kde(\n            media_channel_posteriors[:, channel_i, geo_i],\n            quantiles=quantiles,\n            ax=geo_axis)\n        axis_label = f\"media channel {channel_names[channel_i]} geo {geo_i}\"\n        geo_axis.set_xlabel(axis_label)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_775-825", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  \"\"\"Plots the posterior distributions of estimated media channel effect.\n\n  Model needs to be fit before calling this function to plot.\n\n  Args:\n    media_mix_model: Media mix model.\n    channel_names: Names of media channels to be added to plot.\n    quantiles: Quantiles to draw on the distribution.\n    fig_size: Size of the figure to plot as used by matplotlib. If not specified\n      it will be determined dynamically based on the number of media channels\n      and geos the model was trained on.\n\n  Returns:\n    Plot of posterior distributions.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  n_media_channels = np.shape(media_mix_model.trace[\"coef_media\"])[1]\n  n_geos = (\n      media_mix_model.media.shape[2] if media_mix_model.media.ndim == 3 else 1)\n\n  if not fig_size:\n    fig_size = (5 * n_geos, 3 * n_media_channels)\n\n  media_channel_posteriors = media_mix_model.trace[\"coef_media\"]\n  if channel_names is None:\n    channel_names = np.arange(np.shape(media_channel_posteriors)[1])\n  fig, axes = plt.subplots(\n      nrows=n_media_channels, ncols=n_geos, figsize=fig_size)\n  for channel_i, channel_axis in enumerate(axes):\n    if isinstance(channel_axis, np.ndarray):\n      for geo_i, geo_axis in enumerate(channel_axis):\n        geo_axis = arviz.plot_kde(\n            media_channel_posteriors[:, channel_i, geo_i],\n            quantiles=quantiles,\n            ax=geo_axis)\n        axis_label = f\"media channel {channel_names[channel_i]} geo {geo_i}\"\n        geo_axis.set_xlabel(axis_label)\n    else:\n      channel_axis = arviz.plot_kde(\n          media_channel_posteriors[:, channel_i],\n          quantiles=quantiles,\n          ax=channel_axis)\n      axis_label = f\"media channel {channel_names[channel_i]}\"\n      channel_axis.set_xlabel(axis_label)\n\n  fig.tight_layout()\n  plt.close()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_785-835", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      and geos the model was trained on.\n\n  Returns:\n    Plot of posterior distributions.\n  \"\"\"\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  n_media_channels = np.shape(media_mix_model.trace[\"coef_media\"])[1]\n  n_geos = (\n      media_mix_model.media.shape[2] if media_mix_model.media.ndim == 3 else 1)\n\n  if not fig_size:\n    fig_size = (5 * n_geos, 3 * n_media_channels)\n\n  media_channel_posteriors = media_mix_model.trace[\"coef_media\"]\n  if channel_names is None:\n    channel_names = np.arange(np.shape(media_channel_posteriors)[1])\n  fig, axes = plt.subplots(\n      nrows=n_media_channels, ncols=n_geos, figsize=fig_size)\n  for channel_i, channel_axis in enumerate(axes):\n    if isinstance(channel_axis, np.ndarray):\n      for geo_i, geo_axis in enumerate(channel_axis):\n        geo_axis = arviz.plot_kde(\n            media_channel_posteriors[:, channel_i, geo_i],\n            quantiles=quantiles,\n            ax=geo_axis)\n        axis_label = f\"media channel {channel_names[channel_i]} geo {geo_i}\"\n        geo_axis.set_xlabel(axis_label)\n    else:\n      channel_axis = arviz.plot_kde(\n          media_channel_posteriors[:, channel_i],\n          quantiles=quantiles,\n          ax=channel_axis)\n      axis_label = f\"media channel {channel_names[channel_i]}\"\n      channel_axis.set_xlabel(axis_label)\n\n  fig.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_bars_media_metrics(\n    metric: jnp.ndarray,\n    metric_name: str = \"metric\",\n    channel_names: Optional[Tuple[Any]] = None,\n    interval_mid_range: float = .9) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barchart of estimated media effects with their percentile interval.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_795-845", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  n_geos = (\n      media_mix_model.media.shape[2] if media_mix_model.media.ndim == 3 else 1)\n\n  if not fig_size:\n    fig_size = (5 * n_geos, 3 * n_media_channels)\n\n  media_channel_posteriors = media_mix_model.trace[\"coef_media\"]\n  if channel_names is None:\n    channel_names = np.arange(np.shape(media_channel_posteriors)[1])\n  fig, axes = plt.subplots(\n      nrows=n_media_channels, ncols=n_geos, figsize=fig_size)\n  for channel_i, channel_axis in enumerate(axes):\n    if isinstance(channel_axis, np.ndarray):\n      for geo_i, geo_axis in enumerate(channel_axis):\n        geo_axis = arviz.plot_kde(\n            media_channel_posteriors[:, channel_i, geo_i],\n            quantiles=quantiles,\n            ax=geo_axis)\n        axis_label = f\"media channel {channel_names[channel_i]} geo {geo_i}\"\n        geo_axis.set_xlabel(axis_label)\n    else:\n      channel_axis = arviz.plot_kde(\n          media_channel_posteriors[:, channel_i],\n          quantiles=quantiles,\n          ax=channel_axis)\n      axis_label = f\"media channel {channel_names[channel_i]}\"\n      channel_axis.set_xlabel(axis_label)\n\n  fig.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_bars_media_metrics(\n    metric: jnp.ndarray,\n    metric_name: str = \"metric\",\n    channel_names: Optional[Tuple[Any]] = None,\n    interval_mid_range: float = .9) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barchart of estimated media effects with their percentile interval.\n\n  The lower and upper percentile need to be between 0-1.\n\n  Args:\n    metric: Estimated media metric as returned by\n      lightweight_mmm.get_posterior_metrics(). Can be either contribution\n      percentage or ROI.\n    metric_name: Name of the media metric, e.g. contribution percentage or ROI.\n    channel_names: Names of media channels to be added to plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_805-855", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      nrows=n_media_channels, ncols=n_geos, figsize=fig_size)\n  for channel_i, channel_axis in enumerate(axes):\n    if isinstance(channel_axis, np.ndarray):\n      for geo_i, geo_axis in enumerate(channel_axis):\n        geo_axis = arviz.plot_kde(\n            media_channel_posteriors[:, channel_i, geo_i],\n            quantiles=quantiles,\n            ax=geo_axis)\n        axis_label = f\"media channel {channel_names[channel_i]} geo {geo_i}\"\n        geo_axis.set_xlabel(axis_label)\n    else:\n      channel_axis = arviz.plot_kde(\n          media_channel_posteriors[:, channel_i],\n          quantiles=quantiles,\n          ax=channel_axis)\n      axis_label = f\"media channel {channel_names[channel_i]}\"\n      channel_axis.set_xlabel(axis_label)\n\n  fig.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_bars_media_metrics(\n    metric: jnp.ndarray,\n    metric_name: str = \"metric\",\n    channel_names: Optional[Tuple[Any]] = None,\n    interval_mid_range: float = .9) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barchart of estimated media effects with their percentile interval.\n\n  The lower and upper percentile need to be between 0-1.\n\n  Args:\n    metric: Estimated media metric as returned by\n      lightweight_mmm.get_posterior_metrics(). Can be either contribution\n      percentage or ROI.\n    metric_name: Name of the media metric, e.g. contribution percentage or ROI.\n    channel_names: Names of media channels to be added to plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n\n  Returns:\n    Barplot of estimated media effects with defined percentile-bars.\n  \"\"\"\n  if channel_names is None:\n    channel_names = np.arange(np.shape(metric)[1])\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n\n  if metric.ndim == 3:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_815-865", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    else:\n      channel_axis = arviz.plot_kde(\n          media_channel_posteriors[:, channel_i],\n          quantiles=quantiles,\n          ax=channel_axis)\n      axis_label = f\"media channel {channel_names[channel_i]}\"\n      channel_axis.set_xlabel(axis_label)\n\n  fig.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_bars_media_metrics(\n    metric: jnp.ndarray,\n    metric_name: str = \"metric\",\n    channel_names: Optional[Tuple[Any]] = None,\n    interval_mid_range: float = .9) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barchart of estimated media effects with their percentile interval.\n\n  The lower and upper percentile need to be between 0-1.\n\n  Args:\n    metric: Estimated media metric as returned by\n      lightweight_mmm.get_posterior_metrics(). Can be either contribution\n      percentage or ROI.\n    metric_name: Name of the media metric, e.g. contribution percentage or ROI.\n    channel_names: Names of media channels to be added to plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n\n  Returns:\n    Barplot of estimated media effects with defined percentile-bars.\n  \"\"\"\n  if channel_names is None:\n    channel_names = np.arange(np.shape(metric)[1])\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n\n  if metric.ndim == 3:\n    metric = jnp.mean(metric, axis=-1)\n\n  fig, ax = plt.subplots(1, 1)\n  sns.barplot(data=metric, ci=None, ax=ax)\n  quantile_bounds = np.quantile(\n      metric, q=[lower_quantile, upper_quantile], axis=0)\n  quantile_bounds[0] = metric.mean(axis=0) - quantile_bounds[0]\n  quantile_bounds[1] = quantile_bounds[1] - metric.mean(axis=0)\n\n  ax.errorbar(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_825-875", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  return fig\n\n\ndef plot_bars_media_metrics(\n    metric: jnp.ndarray,\n    metric_name: str = \"metric\",\n    channel_names: Optional[Tuple[Any]] = None,\n    interval_mid_range: float = .9) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barchart of estimated media effects with their percentile interval.\n\n  The lower and upper percentile need to be between 0-1.\n\n  Args:\n    metric: Estimated media metric as returned by\n      lightweight_mmm.get_posterior_metrics(). Can be either contribution\n      percentage or ROI.\n    metric_name: Name of the media metric, e.g. contribution percentage or ROI.\n    channel_names: Names of media channels to be added to plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n\n  Returns:\n    Barplot of estimated media effects with defined percentile-bars.\n  \"\"\"\n  if channel_names is None:\n    channel_names = np.arange(np.shape(metric)[1])\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n\n  if metric.ndim == 3:\n    metric = jnp.mean(metric, axis=-1)\n\n  fig, ax = plt.subplots(1, 1)\n  sns.barplot(data=metric, ci=None, ax=ax)\n  quantile_bounds = np.quantile(\n      metric, q=[lower_quantile, upper_quantile], axis=0)\n  quantile_bounds[0] = metric.mean(axis=0) - quantile_bounds[0]\n  quantile_bounds[1] = quantile_bounds[1] - metric.mean(axis=0)\n\n  ax.errorbar(\n      x=np.arange(np.shape(metric)[1]),\n      y=metric.mean(axis=0),\n      yerr=quantile_bounds,\n      fmt=\"none\",\n      c=\"black\")\n  ax.set_xticks(range(len(channel_names)))\n  ax.set_xticklabels(channel_names, rotation=45)\n  fig.suptitle(\n      f\"Estimated media channel {metric_name}. \\n Error bars show \"\n      f\"{np.round(lower_quantile, 2)} - {np.round(upper_quantile, 2)} \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_835-885", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  The lower and upper percentile need to be between 0-1.\n\n  Args:\n    metric: Estimated media metric as returned by\n      lightweight_mmm.get_posterior_metrics(). Can be either contribution\n      percentage or ROI.\n    metric_name: Name of the media metric, e.g. contribution percentage or ROI.\n    channel_names: Names of media channels to be added to plot.\n    interval_mid_range: Mid range interval to take for plotting. Eg. .9 will use\n      .05 and .95 as the lower and upper quantiles. Must be a float number.\n\n  Returns:\n    Barplot of estimated media effects with defined percentile-bars.\n  \"\"\"\n  if channel_names is None:\n    channel_names = np.arange(np.shape(metric)[1])\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n\n  if metric.ndim == 3:\n    metric = jnp.mean(metric, axis=-1)\n\n  fig, ax = plt.subplots(1, 1)\n  sns.barplot(data=metric, ci=None, ax=ax)\n  quantile_bounds = np.quantile(\n      metric, q=[lower_quantile, upper_quantile], axis=0)\n  quantile_bounds[0] = metric.mean(axis=0) - quantile_bounds[0]\n  quantile_bounds[1] = quantile_bounds[1] - metric.mean(axis=0)\n\n  ax.errorbar(\n      x=np.arange(np.shape(metric)[1]),\n      y=metric.mean(axis=0),\n      yerr=quantile_bounds,\n      fmt=\"none\",\n      c=\"black\")\n  ax.set_xticks(range(len(channel_names)))\n  ax.set_xticklabels(channel_names, rotation=45)\n  fig.suptitle(\n      f\"Estimated media channel {metric_name}. \\n Error bars show \"\n      f\"{np.round(lower_quantile, 2)} - {np.round(upper_quantile, 2)} \"\n      \"credibility interval.\"\n  )\n  plt.close()\n  return fig\n\n\ndef plot_pre_post_budget_allocation_comparison(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    kpi_with_optim: jnp.ndarray,\n    kpi_without_optim: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_845-895", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  Returns:\n    Barplot of estimated media effects with defined percentile-bars.\n  \"\"\"\n  if channel_names is None:\n    channel_names = np.arange(np.shape(metric)[1])\n  upper_quantile = 1 - (1 - interval_mid_range) / 2\n  lower_quantile = (1 - interval_mid_range) / 2\n\n  if metric.ndim == 3:\n    metric = jnp.mean(metric, axis=-1)\n\n  fig, ax = plt.subplots(1, 1)\n  sns.barplot(data=metric, ci=None, ax=ax)\n  quantile_bounds = np.quantile(\n      metric, q=[lower_quantile, upper_quantile], axis=0)\n  quantile_bounds[0] = metric.mean(axis=0) - quantile_bounds[0]\n  quantile_bounds[1] = quantile_bounds[1] - metric.mean(axis=0)\n\n  ax.errorbar(\n      x=np.arange(np.shape(metric)[1]),\n      y=metric.mean(axis=0),\n      yerr=quantile_bounds,\n      fmt=\"none\",\n      c=\"black\")\n  ax.set_xticks(range(len(channel_names)))\n  ax.set_xticklabels(channel_names, rotation=45)\n  fig.suptitle(\n      f\"Estimated media channel {metric_name}. \\n Error bars show \"\n      f\"{np.round(lower_quantile, 2)} - {np.round(upper_quantile, 2)} \"\n      \"credibility interval.\"\n  )\n  plt.close()\n  return fig\n\n\ndef plot_pre_post_budget_allocation_comparison(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    kpi_with_optim: jnp.ndarray,\n    kpi_without_optim: jnp.ndarray,\n    optimal_buget_allocation: jnp.ndarray,\n    previous_budget_allocation: jnp.ndarray,\n    channel_names: Optional[Sequence[Any]] = None,\n    figure_size: Tuple[int, int] = (20, 10)\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barcharts to compare pre & post budget allocation.\n\n  Args:\n    media_mix_model: Media mix model to use for the optimization.\n    kpi_with_optim: Negative predicted target variable with optimized budget", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_855-905", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    metric = jnp.mean(metric, axis=-1)\n\n  fig, ax = plt.subplots(1, 1)\n  sns.barplot(data=metric, ci=None, ax=ax)\n  quantile_bounds = np.quantile(\n      metric, q=[lower_quantile, upper_quantile], axis=0)\n  quantile_bounds[0] = metric.mean(axis=0) - quantile_bounds[0]\n  quantile_bounds[1] = quantile_bounds[1] - metric.mean(axis=0)\n\n  ax.errorbar(\n      x=np.arange(np.shape(metric)[1]),\n      y=metric.mean(axis=0),\n      yerr=quantile_bounds,\n      fmt=\"none\",\n      c=\"black\")\n  ax.set_xticks(range(len(channel_names)))\n  ax.set_xticklabels(channel_names, rotation=45)\n  fig.suptitle(\n      f\"Estimated media channel {metric_name}. \\n Error bars show \"\n      f\"{np.round(lower_quantile, 2)} - {np.round(upper_quantile, 2)} \"\n      \"credibility interval.\"\n  )\n  plt.close()\n  return fig\n\n\ndef plot_pre_post_budget_allocation_comparison(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    kpi_with_optim: jnp.ndarray,\n    kpi_without_optim: jnp.ndarray,\n    optimal_buget_allocation: jnp.ndarray,\n    previous_budget_allocation: jnp.ndarray,\n    channel_names: Optional[Sequence[Any]] = None,\n    figure_size: Tuple[int, int] = (20, 10)\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barcharts to compare pre & post budget allocation.\n\n  Args:\n    media_mix_model: Media mix model to use for the optimization.\n    kpi_with_optim: Negative predicted target variable with optimized budget\n      allocation.\n    kpi_without_optim: negative predicted target variable with original budget\n      allocation proportion base on the historical data.\n    optimal_buget_allocation: Optmized budget allocation.\n    previous_budget_allocation: Starting budget allocation based on original\n      budget allocation proportion.\n    channel_names: Names of media channels to be added to plot.\n    figure_size: size of the plot.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_865-915", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      x=np.arange(np.shape(metric)[1]),\n      y=metric.mean(axis=0),\n      yerr=quantile_bounds,\n      fmt=\"none\",\n      c=\"black\")\n  ax.set_xticks(range(len(channel_names)))\n  ax.set_xticklabels(channel_names, rotation=45)\n  fig.suptitle(\n      f\"Estimated media channel {metric_name}. \\n Error bars show \"\n      f\"{np.round(lower_quantile, 2)} - {np.round(upper_quantile, 2)} \"\n      \"credibility interval.\"\n  )\n  plt.close()\n  return fig\n\n\ndef plot_pre_post_budget_allocation_comparison(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    kpi_with_optim: jnp.ndarray,\n    kpi_without_optim: jnp.ndarray,\n    optimal_buget_allocation: jnp.ndarray,\n    previous_budget_allocation: jnp.ndarray,\n    channel_names: Optional[Sequence[Any]] = None,\n    figure_size: Tuple[int, int] = (20, 10)\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barcharts to compare pre & post budget allocation.\n\n  Args:\n    media_mix_model: Media mix model to use for the optimization.\n    kpi_with_optim: Negative predicted target variable with optimized budget\n      allocation.\n    kpi_without_optim: negative predicted target variable with original budget\n      allocation proportion base on the historical data.\n    optimal_buget_allocation: Optmized budget allocation.\n    previous_budget_allocation: Starting budget allocation based on original\n      budget allocation proportion.\n    channel_names: Names of media channels to be added to plot.\n    figure_size: size of the plot.\n\n  Returns:\n    Barplots of budget allocation across media channels pre & post optimization.\n  \"\"\"\n\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  previous_budget_allocation_pct = previous_budget_allocation / jnp.sum(\n      previous_budget_allocation)\n  optimized_budget_allocation_pct = optimal_buget_allocation / jnp.sum(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_875-925", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      \"credibility interval.\"\n  )\n  plt.close()\n  return fig\n\n\ndef plot_pre_post_budget_allocation_comparison(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    kpi_with_optim: jnp.ndarray,\n    kpi_without_optim: jnp.ndarray,\n    optimal_buget_allocation: jnp.ndarray,\n    previous_budget_allocation: jnp.ndarray,\n    channel_names: Optional[Sequence[Any]] = None,\n    figure_size: Tuple[int, int] = (20, 10)\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barcharts to compare pre & post budget allocation.\n\n  Args:\n    media_mix_model: Media mix model to use for the optimization.\n    kpi_with_optim: Negative predicted target variable with optimized budget\n      allocation.\n    kpi_without_optim: negative predicted target variable with original budget\n      allocation proportion base on the historical data.\n    optimal_buget_allocation: Optmized budget allocation.\n    previous_budget_allocation: Starting budget allocation based on original\n      budget allocation proportion.\n    channel_names: Names of media channels to be added to plot.\n    figure_size: size of the plot.\n\n  Returns:\n    Barplots of budget allocation across media channels pre & post optimization.\n  \"\"\"\n\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  previous_budget_allocation_pct = previous_budget_allocation / jnp.sum(\n      previous_budget_allocation)\n  optimized_budget_allocation_pct = optimal_buget_allocation / jnp.sum(\n      optimal_buget_allocation)\n\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n  x_axis = np.arange(len(channel_names))\n\n  pre_optimizaiton_predicted_target = kpi_without_optim * -1\n  post_optimization_predictiond_target = kpi_with_optim * -1\n  predictions = [\n      pre_optimizaiton_predicted_target, post_optimization_predictiond_target", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 925, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_885-935", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    optimal_buget_allocation: jnp.ndarray,\n    previous_budget_allocation: jnp.ndarray,\n    channel_names: Optional[Sequence[Any]] = None,\n    figure_size: Tuple[int, int] = (20, 10)\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots a barcharts to compare pre & post budget allocation.\n\n  Args:\n    media_mix_model: Media mix model to use for the optimization.\n    kpi_with_optim: Negative predicted target variable with optimized budget\n      allocation.\n    kpi_without_optim: negative predicted target variable with original budget\n      allocation proportion base on the historical data.\n    optimal_buget_allocation: Optmized budget allocation.\n    previous_budget_allocation: Starting budget allocation based on original\n      budget allocation proportion.\n    channel_names: Names of media channels to be added to plot.\n    figure_size: size of the plot.\n\n  Returns:\n    Barplots of budget allocation across media channels pre & post optimization.\n  \"\"\"\n\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  previous_budget_allocation_pct = previous_budget_allocation / jnp.sum(\n      previous_budget_allocation)\n  optimized_budget_allocation_pct = optimal_buget_allocation / jnp.sum(\n      optimal_buget_allocation)\n\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n  x_axis = np.arange(len(channel_names))\n\n  pre_optimizaiton_predicted_target = kpi_without_optim * -1\n  post_optimization_predictiond_target = kpi_with_optim * -1\n  predictions = [\n      pre_optimizaiton_predicted_target, post_optimization_predictiond_target\n  ]\n\n  # Create bar chart.\n  fig, axes = plt.subplots(2, 1, figsize=figure_size)\n\n  plots1 = axes[0].bar(\n      x_axis - 0.2,\n      previous_budget_allocation,\n      width=0.4,\n      label=\"previous budget allocation\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 935, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_895-945", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      allocation.\n    kpi_without_optim: negative predicted target variable with original budget\n      allocation proportion base on the historical data.\n    optimal_buget_allocation: Optmized budget allocation.\n    previous_budget_allocation: Starting budget allocation based on original\n      budget allocation proportion.\n    channel_names: Names of media channels to be added to plot.\n    figure_size: size of the plot.\n\n  Returns:\n    Barplots of budget allocation across media channels pre & post optimization.\n  \"\"\"\n\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  previous_budget_allocation_pct = previous_budget_allocation / jnp.sum(\n      previous_budget_allocation)\n  optimized_budget_allocation_pct = optimal_buget_allocation / jnp.sum(\n      optimal_buget_allocation)\n\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n  x_axis = np.arange(len(channel_names))\n\n  pre_optimizaiton_predicted_target = kpi_without_optim * -1\n  post_optimization_predictiond_target = kpi_with_optim * -1\n  predictions = [\n      pre_optimizaiton_predicted_target, post_optimization_predictiond_target\n  ]\n\n  # Create bar chart.\n  fig, axes = plt.subplots(2, 1, figsize=figure_size)\n\n  plots1 = axes[0].bar(\n      x_axis - 0.2,\n      previous_budget_allocation,\n      width=0.4,\n      label=\"previous budget allocation\")\n  plots2 = axes[0].bar(\n      x_axis + 0.2,\n      optimal_buget_allocation,\n      width=0.4,\n      label=\"optimized budget allocation\")\n  axes[0].set_ylabel(\"Budget Allocation\", fontsize=\"x-large\")\n  axes[0].set_title(\n      \"Before and After Optimization Budget Allocation Comparison\",\n      fontsize=\"x-large\")\n  # Iterrating over the bars one-by-one.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 945, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_905-955", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    Barplots of budget allocation across media channels pre & post optimization.\n  \"\"\"\n\n  if not hasattr(media_mix_model, \"trace\"):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first before attempting to plot its fit.\")\n\n  previous_budget_allocation_pct = previous_budget_allocation / jnp.sum(\n      previous_budget_allocation)\n  optimized_budget_allocation_pct = optimal_buget_allocation / jnp.sum(\n      optimal_buget_allocation)\n\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n  x_axis = np.arange(len(channel_names))\n\n  pre_optimizaiton_predicted_target = kpi_without_optim * -1\n  post_optimization_predictiond_target = kpi_with_optim * -1\n  predictions = [\n      pre_optimizaiton_predicted_target, post_optimization_predictiond_target\n  ]\n\n  # Create bar chart.\n  fig, axes = plt.subplots(2, 1, figsize=figure_size)\n\n  plots1 = axes[0].bar(\n      x_axis - 0.2,\n      previous_budget_allocation,\n      width=0.4,\n      label=\"previous budget allocation\")\n  plots2 = axes[0].bar(\n      x_axis + 0.2,\n      optimal_buget_allocation,\n      width=0.4,\n      label=\"optimized budget allocation\")\n  axes[0].set_ylabel(\"Budget Allocation\", fontsize=\"x-large\")\n  axes[0].set_title(\n      \"Before and After Optimization Budget Allocation Comparison\",\n      fontsize=\"x-large\")\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots1.patches)):\n    bar = plots1.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(previous_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 930, "start_line_no": 905, "end_line_no": 955, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_915-965", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      optimal_buget_allocation)\n\n  if channel_names is None:\n    channel_names = media_mix_model.media_names\n  x_axis = np.arange(len(channel_names))\n\n  pre_optimizaiton_predicted_target = kpi_without_optim * -1\n  post_optimization_predictiond_target = kpi_with_optim * -1\n  predictions = [\n      pre_optimizaiton_predicted_target, post_optimization_predictiond_target\n  ]\n\n  # Create bar chart.\n  fig, axes = plt.subplots(2, 1, figsize=figure_size)\n\n  plots1 = axes[0].bar(\n      x_axis - 0.2,\n      previous_budget_allocation,\n      width=0.4,\n      label=\"previous budget allocation\")\n  plots2 = axes[0].bar(\n      x_axis + 0.2,\n      optimal_buget_allocation,\n      width=0.4,\n      label=\"optimized budget allocation\")\n  axes[0].set_ylabel(\"Budget Allocation\", fontsize=\"x-large\")\n  axes[0].set_title(\n      \"Before and After Optimization Budget Allocation Comparison\",\n      fontsize=\"x-large\")\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots1.patches)):\n    bar = plots1.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(previous_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots2.patches)):\n    bar = plots2.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(optimized_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 940, "start_line_no": 915, "end_line_no": 965, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_925-975", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  ]\n\n  # Create bar chart.\n  fig, axes = plt.subplots(2, 1, figsize=figure_size)\n\n  plots1 = axes[0].bar(\n      x_axis - 0.2,\n      previous_budget_allocation,\n      width=0.4,\n      label=\"previous budget allocation\")\n  plots2 = axes[0].bar(\n      x_axis + 0.2,\n      optimal_buget_allocation,\n      width=0.4,\n      label=\"optimized budget allocation\")\n  axes[0].set_ylabel(\"Budget Allocation\", fontsize=\"x-large\")\n  axes[0].set_title(\n      \"Before and After Optimization Budget Allocation Comparison\",\n      fontsize=\"x-large\")\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots1.patches)):\n    bar = plots1.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(previous_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots2.patches)):\n    bar = plots2.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(optimized_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  axes[0].set_xticks(x_axis)\n  axes[0].set_xticklabels(channel_names, fontsize=\"medium\")\n  axes[0].legend(fontsize=\"medium\")\n\n  plots3 = axes[1].bar([\n      \"pre optimization predicted target\",\n      \"post optimization predicted target\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 950, "start_line_no": 925, "end_line_no": 975, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_935-985", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  plots2 = axes[0].bar(\n      x_axis + 0.2,\n      optimal_buget_allocation,\n      width=0.4,\n      label=\"optimized budget allocation\")\n  axes[0].set_ylabel(\"Budget Allocation\", fontsize=\"x-large\")\n  axes[0].set_title(\n      \"Before and After Optimization Budget Allocation Comparison\",\n      fontsize=\"x-large\")\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots1.patches)):\n    bar = plots1.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(previous_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots2.patches)):\n    bar = plots2.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(optimized_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  axes[0].set_xticks(x_axis)\n  axes[0].set_xticklabels(channel_names, fontsize=\"medium\")\n  axes[0].legend(fontsize=\"medium\")\n\n  plots3 = axes[1].bar([\n      \"pre optimization predicted target\",\n      \"post optimization predicted target\"\n  ], predictions)\n  axes[1].set_ylim(\n      min(predictions) - min(predictions) * 0.1,\n      max(predictions) + min(predictions) * 0.1)\n  axes[1].set_ylabel(\"Predicted Target Variable\", fontsize=\"x-large\")\n  axes[1].set_title(\n      \"Pre Post Optimization Target Variable Comparison\", fontsize=\"x-large\")\n  axes[1].set_xticks(range(2))\n  axes[1].set_xticklabels([\n      \"pre optimization predictited target\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 960, "start_line_no": 935, "end_line_no": 985, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_945-995", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  for bar_i in range(len(plots1.patches)):\n    bar = plots1.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(previous_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots2.patches)):\n    bar = plots2.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(optimized_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  axes[0].set_xticks(x_axis)\n  axes[0].set_xticklabels(channel_names, fontsize=\"medium\")\n  axes[0].legend(fontsize=\"medium\")\n\n  plots3 = axes[1].bar([\n      \"pre optimization predicted target\",\n      \"post optimization predicted target\"\n  ], predictions)\n  axes[1].set_ylim(\n      min(predictions) - min(predictions) * 0.1,\n      max(predictions) + min(predictions) * 0.1)\n  axes[1].set_ylabel(\"Predicted Target Variable\", fontsize=\"x-large\")\n  axes[1].set_title(\n      \"Pre Post Optimization Target Variable Comparison\", fontsize=\"x-large\")\n  axes[1].set_xticks(range(2))\n  axes[1].set_xticklabels([\n      \"pre optimization predictited target\",\n      \"post optimization predicted target\"\n  ],\n                          fontsize=\"x-large\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots3.patches)):\n    bar = plots3.patches[bar_i]\n    axes[1].annotate(\n        \"{:,.1f}\".format(predictions[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 970, "start_line_no": 945, "end_line_no": 995, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_955-1005", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots2.patches)):\n    bar = plots2.patches[bar_i]\n    axes[0].annotate(\n        \"{:.0%}\".format(optimized_budget_allocation_pct[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  axes[0].set_xticks(x_axis)\n  axes[0].set_xticklabels(channel_names, fontsize=\"medium\")\n  axes[0].legend(fontsize=\"medium\")\n\n  plots3 = axes[1].bar([\n      \"pre optimization predicted target\",\n      \"post optimization predicted target\"\n  ], predictions)\n  axes[1].set_ylim(\n      min(predictions) - min(predictions) * 0.1,\n      max(predictions) + min(predictions) * 0.1)\n  axes[1].set_ylabel(\"Predicted Target Variable\", fontsize=\"x-large\")\n  axes[1].set_title(\n      \"Pre Post Optimization Target Variable Comparison\", fontsize=\"x-large\")\n  axes[1].set_xticks(range(2))\n  axes[1].set_xticklabels([\n      \"pre optimization predictited target\",\n      \"post optimization predicted target\"\n  ],\n                          fontsize=\"x-large\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots3.patches)):\n    bar = plots3.patches[bar_i]\n    axes[1].annotate(\n        \"{:,.1f}\".format(predictions[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  plt.tight_layout()\n  plt.close()\n  return fig\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 980, "start_line_no": 955, "end_line_no": 1005, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_965-1015", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  axes[0].set_xticks(x_axis)\n  axes[0].set_xticklabels(channel_names, fontsize=\"medium\")\n  axes[0].legend(fontsize=\"medium\")\n\n  plots3 = axes[1].bar([\n      \"pre optimization predicted target\",\n      \"post optimization predicted target\"\n  ], predictions)\n  axes[1].set_ylim(\n      min(predictions) - min(predictions) * 0.1,\n      max(predictions) + min(predictions) * 0.1)\n  axes[1].set_ylabel(\"Predicted Target Variable\", fontsize=\"x-large\")\n  axes[1].set_title(\n      \"Pre Post Optimization Target Variable Comparison\", fontsize=\"x-large\")\n  axes[1].set_xticks(range(2))\n  axes[1].set_xticklabels([\n      \"pre optimization predictited target\",\n      \"post optimization predicted target\"\n  ],\n                          fontsize=\"x-large\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots3.patches)):\n    bar = plots3.patches[bar_i]\n    axes[1].annotate(\n        \"{:,.1f}\".format(predictions[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  plt.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_media_baseline_contribution_area_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[Any]] = None,\n    fig_size: Optional[Tuple[int, int]] = (20, 7),\n    legend_outside: Optional[bool] = False,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots an area chart to visualize weekly media & baseline contribution.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 990, "start_line_no": 965, "end_line_no": 1015, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_975-1025", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  ], predictions)\n  axes[1].set_ylim(\n      min(predictions) - min(predictions) * 0.1,\n      max(predictions) + min(predictions) * 0.1)\n  axes[1].set_ylabel(\"Predicted Target Variable\", fontsize=\"x-large\")\n  axes[1].set_title(\n      \"Pre Post Optimization Target Variable Comparison\", fontsize=\"x-large\")\n  axes[1].set_xticks(range(2))\n  axes[1].set_xticklabels([\n      \"pre optimization predictited target\",\n      \"post optimization predicted target\"\n  ],\n                          fontsize=\"x-large\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots3.patches)):\n    bar = plots3.patches[bar_i]\n    axes[1].annotate(\n        \"{:,.1f}\".format(predictions[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  plt.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_media_baseline_contribution_area_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[Any]] = None,\n    fig_size: Optional[Tuple[int, int]] = (20, 7),\n    legend_outside: Optional[bool] = False,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots an area chart to visualize weekly media & baseline contribution.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n    fig_size: Size of the figure to plot as used by matplotlib.\n    legend_outside: Put the legend outside of the chart, center-right.\n\n  Returns:\n    Stacked area chart of weekly baseline & media contribution.\n  \"\"\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1000, "start_line_no": 975, "end_line_no": 1025, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_985-1035", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      \"post optimization predicted target\"\n  ],\n                          fontsize=\"x-large\")\n\n  # Iterrating over the bars one-by-one.\n  for bar_i in range(len(plots3.patches)):\n    bar = plots3.patches[bar_i]\n    axes[1].annotate(\n        \"{:,.1f}\".format(predictions[bar_i]),\n        (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  plt.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_media_baseline_contribution_area_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[Any]] = None,\n    fig_size: Optional[Tuple[int, int]] = (20, 7),\n    legend_outside: Optional[bool] = False,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots an area chart to visualize weekly media & baseline contribution.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n    fig_size: Size of the figure to plot as used by matplotlib.\n    legend_outside: Put the legend outside of the chart, center-right.\n\n  Returns:\n    Stacked area chart of weekly baseline & media contribution.\n  \"\"\"\n  # Create media channels & baseline contribution dataframe.\n  contribution_df = create_media_baseline_contribution_df(\n      media_mix_model=media_mix_model,\n      target_scaler=target_scaler,\n      channel_names=channel_names)\n\n  # Create contribution dataframe for the plot.\n  contribution_columns = [\n      col for col in contribution_df.columns if \"contribution\" in col\n  ]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1010, "start_line_no": 985, "end_line_no": 1035, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_995-1045", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        ha=\"center\",\n        va=\"center\",\n        size=10,\n        xytext=(0, 8),\n        textcoords=\"offset points\")\n\n  plt.tight_layout()\n  plt.close()\n  return fig\n\n\ndef plot_media_baseline_contribution_area_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[Any]] = None,\n    fig_size: Optional[Tuple[int, int]] = (20, 7),\n    legend_outside: Optional[bool] = False,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots an area chart to visualize weekly media & baseline contribution.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n    fig_size: Size of the figure to plot as used by matplotlib.\n    legend_outside: Put the legend outside of the chart, center-right.\n\n  Returns:\n    Stacked area chart of weekly baseline & media contribution.\n  \"\"\"\n  # Create media channels & baseline contribution dataframe.\n  contribution_df = create_media_baseline_contribution_df(\n      media_mix_model=media_mix_model,\n      target_scaler=target_scaler,\n      channel_names=channel_names)\n\n  # Create contribution dataframe for the plot.\n  contribution_columns = [\n      col for col in contribution_df.columns if \"contribution\" in col\n  ]\n  contribution_df_for_plot = contribution_df.loc[:, contribution_columns]\n  contribution_df_for_plot = contribution_df_for_plot[\n      contribution_df_for_plot.columns[::-1]]\n  period = np.arange(1, contribution_df_for_plot.shape[0] + 1)\n  contribution_df_for_plot.loc[:, \"period\"] = period\n\n  # Plot the stacked area chart.\n  fig, ax = plt.subplots()\n  contribution_df_for_plot.plot.area(\n      x=\"period\", stacked=True, figsize=fig_size, ax=ax)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1020, "start_line_no": 995, "end_line_no": 1045, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1005-1055", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\ndef plot_media_baseline_contribution_area_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    target_scaler: Optional[preprocessing.CustomScaler] = None,\n    channel_names: Optional[Sequence[Any]] = None,\n    fig_size: Optional[Tuple[int, int]] = (20, 7),\n    legend_outside: Optional[bool] = False,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots an area chart to visualize weekly media & baseline contribution.\n\n  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n    fig_size: Size of the figure to plot as used by matplotlib.\n    legend_outside: Put the legend outside of the chart, center-right.\n\n  Returns:\n    Stacked area chart of weekly baseline & media contribution.\n  \"\"\"\n  # Create media channels & baseline contribution dataframe.\n  contribution_df = create_media_baseline_contribution_df(\n      media_mix_model=media_mix_model,\n      target_scaler=target_scaler,\n      channel_names=channel_names)\n\n  # Create contribution dataframe for the plot.\n  contribution_columns = [\n      col for col in contribution_df.columns if \"contribution\" in col\n  ]\n  contribution_df_for_plot = contribution_df.loc[:, contribution_columns]\n  contribution_df_for_plot = contribution_df_for_plot[\n      contribution_df_for_plot.columns[::-1]]\n  period = np.arange(1, contribution_df_for_plot.shape[0] + 1)\n  contribution_df_for_plot.loc[:, \"period\"] = period\n\n  # Plot the stacked area chart.\n  fig, ax = plt.subplots()\n  contribution_df_for_plot.plot.area(\n      x=\"period\", stacked=True, figsize=fig_size, ax=ax)\n  ax.set_title(\"Attribution Over Time\", fontsize=\"x-large\")\n  ax.tick_params(axis=\"y\")\n  ax.set_ylabel(\"Baseline & Media Chanels Attribution\")\n  ax.set_xlabel(\"Period\")\n  ax.set_xlim(1, contribution_df_for_plot[\"period\"].max())\n  ax.set_xticks(contribution_df_for_plot[\"period\"])\n  ax.set_xticklabels(contribution_df_for_plot[\"period\"])\n  # Get handles and labels for sorting.\n  handles, labels = ax.get_legend_handles_labels()\n  # If true, legend_outside reversed the legend and puts the legend center left,\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load))ConstantConstantTuple(ConstantConstantLoad)Constant)Expr(Constant)Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Assign(Name(Store)ListComp(Name(Load)comprehension(Name(Store)Attribute(Name(Load)Load)Compare(ConstantInName(Load)))))Assign(Name(Store)Subscript(Attribute(Name(Load)Load)Tuple(SliceName(Load)Load)Load))Assign(Name(Store)Subscript(Name(Load)Subscript(Attribute(Name(Load)Load)Slice(UnaryOp(USubConstant))Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantBinOp(Subscript(Attribute(Name(Load)Load)ConstantLoad)AddConstant)))Assign(Subscript(Attribute(Name(Load)Load)Tuple(SliceConstantLoad)Store)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Constant)keyword(Constant)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Constantkeyword(Constant)))Expr(Call(Attribute(Name(Load)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Subscript(Name(Load)ConstantLoad)Load))))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))Attribute(Attribute(Name(Load)Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1030, "start_line_no": 1005, "end_line_no": 1055, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1015-1065", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Args:\n    media_mix_model: Media mix model.\n    target_scaler: Scaler used for scaling the target.\n    channel_names: Names of media channels.\n    fig_size: Size of the figure to plot as used by matplotlib.\n    legend_outside: Put the legend outside of the chart, center-right.\n\n  Returns:\n    Stacked area chart of weekly baseline & media contribution.\n  \"\"\"\n  # Create media channels & baseline contribution dataframe.\n  contribution_df = create_media_baseline_contribution_df(\n      media_mix_model=media_mix_model,\n      target_scaler=target_scaler,\n      channel_names=channel_names)\n\n  # Create contribution dataframe for the plot.\n  contribution_columns = [\n      col for col in contribution_df.columns if \"contribution\" in col\n  ]\n  contribution_df_for_plot = contribution_df.loc[:, contribution_columns]\n  contribution_df_for_plot = contribution_df_for_plot[\n      contribution_df_for_plot.columns[::-1]]\n  period = np.arange(1, contribution_df_for_plot.shape[0] + 1)\n  contribution_df_for_plot.loc[:, \"period\"] = period\n\n  # Plot the stacked area chart.\n  fig, ax = plt.subplots()\n  contribution_df_for_plot.plot.area(\n      x=\"period\", stacked=True, figsize=fig_size, ax=ax)\n  ax.set_title(\"Attribution Over Time\", fontsize=\"x-large\")\n  ax.tick_params(axis=\"y\")\n  ax.set_ylabel(\"Baseline & Media Chanels Attribution\")\n  ax.set_xlabel(\"Period\")\n  ax.set_xlim(1, contribution_df_for_plot[\"period\"].max())\n  ax.set_xticks(contribution_df_for_plot[\"period\"])\n  ax.set_xticklabels(contribution_df_for_plot[\"period\"])\n  # Get handles and labels for sorting.\n  handles, labels = ax.get_legend_handles_labels()\n  # If true, legend_outside reversed the legend and puts the legend center left,\n  # outside the chart.\n  # If false, legend_outside only reverses the legend order.\n\n  # Channel order is based on the media input and chart logic.\n  # Chart logic puts the last column onto the bottom.\n  # To be in line with chart order, we reserve the channel order in the legend.\n  if legend_outside:\n    ax.legend(handles[::-1], labels[::-1],\n              loc=\"center left\", bbox_to_anchor=(1, 0.5))\n  # Only sort the legend.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1040, "start_line_no": 1015, "end_line_no": 1065, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1025-1075", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  # Create media channels & baseline contribution dataframe.\n  contribution_df = create_media_baseline_contribution_df(\n      media_mix_model=media_mix_model,\n      target_scaler=target_scaler,\n      channel_names=channel_names)\n\n  # Create contribution dataframe for the plot.\n  contribution_columns = [\n      col for col in contribution_df.columns if \"contribution\" in col\n  ]\n  contribution_df_for_plot = contribution_df.loc[:, contribution_columns]\n  contribution_df_for_plot = contribution_df_for_plot[\n      contribution_df_for_plot.columns[::-1]]\n  period = np.arange(1, contribution_df_for_plot.shape[0] + 1)\n  contribution_df_for_plot.loc[:, \"period\"] = period\n\n  # Plot the stacked area chart.\n  fig, ax = plt.subplots()\n  contribution_df_for_plot.plot.area(\n      x=\"period\", stacked=True, figsize=fig_size, ax=ax)\n  ax.set_title(\"Attribution Over Time\", fontsize=\"x-large\")\n  ax.tick_params(axis=\"y\")\n  ax.set_ylabel(\"Baseline & Media Chanels Attribution\")\n  ax.set_xlabel(\"Period\")\n  ax.set_xlim(1, contribution_df_for_plot[\"period\"].max())\n  ax.set_xticks(contribution_df_for_plot[\"period\"])\n  ax.set_xticklabels(contribution_df_for_plot[\"period\"])\n  # Get handles and labels for sorting.\n  handles, labels = ax.get_legend_handles_labels()\n  # If true, legend_outside reversed the legend and puts the legend center left,\n  # outside the chart.\n  # If false, legend_outside only reverses the legend order.\n\n  # Channel order is based on the media input and chart logic.\n  # Chart logic puts the last column onto the bottom.\n  # To be in line with chart order, we reserve the channel order in the legend.\n  if legend_outside:\n    ax.legend(handles[::-1], labels[::-1],\n              loc=\"center left\", bbox_to_anchor=(1, 0.5))\n  # Only sort the legend.\n  else:\n    ax.legend(handles[::-1], labels[::-1])\n\n  for tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n  plt.close()\n  return fig\n\n\ndef _make_prior_and_posterior_subplot_for_one_feature(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1050, "start_line_no": 1025, "end_line_no": 1075, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1035-1085", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  contribution_df_for_plot = contribution_df.loc[:, contribution_columns]\n  contribution_df_for_plot = contribution_df_for_plot[\n      contribution_df_for_plot.columns[::-1]]\n  period = np.arange(1, contribution_df_for_plot.shape[0] + 1)\n  contribution_df_for_plot.loc[:, \"period\"] = period\n\n  # Plot the stacked area chart.\n  fig, ax = plt.subplots()\n  contribution_df_for_plot.plot.area(\n      x=\"period\", stacked=True, figsize=fig_size, ax=ax)\n  ax.set_title(\"Attribution Over Time\", fontsize=\"x-large\")\n  ax.tick_params(axis=\"y\")\n  ax.set_ylabel(\"Baseline & Media Chanels Attribution\")\n  ax.set_xlabel(\"Period\")\n  ax.set_xlim(1, contribution_df_for_plot[\"period\"].max())\n  ax.set_xticks(contribution_df_for_plot[\"period\"])\n  ax.set_xticklabels(contribution_df_for_plot[\"period\"])\n  # Get handles and labels for sorting.\n  handles, labels = ax.get_legend_handles_labels()\n  # If true, legend_outside reversed the legend and puts the legend center left,\n  # outside the chart.\n  # If false, legend_outside only reverses the legend order.\n\n  # Channel order is based on the media input and chart logic.\n  # Chart logic puts the last column onto the bottom.\n  # To be in line with chart order, we reserve the channel order in the legend.\n  if legend_outside:\n    ax.legend(handles[::-1], labels[::-1],\n              loc=\"center left\", bbox_to_anchor=(1, 0.5))\n  # Only sort the legend.\n  else:\n    ax.legend(handles[::-1], labels[::-1])\n\n  for tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n  plt.close()\n  return fig\n\n\ndef _make_prior_and_posterior_subplot_for_one_feature(\n    prior_distribution: numpyro.distributions.Distribution,\n    posterior_samples: np.ndarray,\n    subplot_title: str,\n    fig: matplotlib.figure.Figure,\n    gridspec_fig: matplotlib.gridspec.GridSpec,\n    i_ax: int,\n    hyperprior: bool = False,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1060, "start_line_no": 1035, "end_line_no": 1085, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1045-1095", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  ax.set_title(\"Attribution Over Time\", fontsize=\"x-large\")\n  ax.tick_params(axis=\"y\")\n  ax.set_ylabel(\"Baseline & Media Chanels Attribution\")\n  ax.set_xlabel(\"Period\")\n  ax.set_xlim(1, contribution_df_for_plot[\"period\"].max())\n  ax.set_xticks(contribution_df_for_plot[\"period\"])\n  ax.set_xticklabels(contribution_df_for_plot[\"period\"])\n  # Get handles and labels for sorting.\n  handles, labels = ax.get_legend_handles_labels()\n  # If true, legend_outside reversed the legend and puts the legend center left,\n  # outside the chart.\n  # If false, legend_outside only reverses the legend order.\n\n  # Channel order is based on the media input and chart logic.\n  # Chart logic puts the last column onto the bottom.\n  # To be in line with chart order, we reserve the channel order in the legend.\n  if legend_outside:\n    ax.legend(handles[::-1], labels[::-1],\n              loc=\"center left\", bbox_to_anchor=(1, 0.5))\n  # Only sort the legend.\n  else:\n    ax.legend(handles[::-1], labels[::-1])\n\n  for tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n  plt.close()\n  return fig\n\n\ndef _make_prior_and_posterior_subplot_for_one_feature(\n    prior_distribution: numpyro.distributions.Distribution,\n    posterior_samples: np.ndarray,\n    subplot_title: str,\n    fig: matplotlib.figure.Figure,\n    gridspec_fig: matplotlib.gridspec.GridSpec,\n    i_ax: int,\n    hyperprior: bool = False,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> Tuple[matplotlib.figure.Figure, matplotlib.gridspec.GridSpec, int]:\n  \"\"\"Helper function to make the prior and posterior distribution subplots.\n\n  This function makes some (hard-coded) choices about how to display the prior\n  and posterior distributions. First, it uses kernel density estimators to\n  smooth the distributions rather than plotting the histograms directly (since\n  the histograms look too noisy unless we take unreasonably large numbers of\n  samples). Second, we found that the default bandwidth works pretty well for\n  visualization of these distributions, though we expose the bw_adjust parameter\n  if users wish to modify this. Finally, kernel density estimators tend to have", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1070, "start_line_no": 1045, "end_line_no": 1095, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1055-1105", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  # outside the chart.\n  # If false, legend_outside only reverses the legend order.\n\n  # Channel order is based on the media input and chart logic.\n  # Chart logic puts the last column onto the bottom.\n  # To be in line with chart order, we reserve the channel order in the legend.\n  if legend_outside:\n    ax.legend(handles[::-1], labels[::-1],\n              loc=\"center left\", bbox_to_anchor=(1, 0.5))\n  # Only sort the legend.\n  else:\n    ax.legend(handles[::-1], labels[::-1])\n\n  for tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n  plt.close()\n  return fig\n\n\ndef _make_prior_and_posterior_subplot_for_one_feature(\n    prior_distribution: numpyro.distributions.Distribution,\n    posterior_samples: np.ndarray,\n    subplot_title: str,\n    fig: matplotlib.figure.Figure,\n    gridspec_fig: matplotlib.gridspec.GridSpec,\n    i_ax: int,\n    hyperprior: bool = False,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> Tuple[matplotlib.figure.Figure, matplotlib.gridspec.GridSpec, int]:\n  \"\"\"Helper function to make the prior and posterior distribution subplots.\n\n  This function makes some (hard-coded) choices about how to display the prior\n  and posterior distributions. First, it uses kernel density estimators to\n  smooth the distributions rather than plotting the histograms directly (since\n  the histograms look too noisy unless we take unreasonably large numbers of\n  samples). Second, we found that the default bandwidth works pretty well for\n  visualization of these distributions, though we expose the bw_adjust parameter\n  if users wish to modify this. Finally, kernel density estimators tend to have\n  issues at the edges of distributions, and this issue persists here too. We\n  clip some distributions (Half-Normal, Beta, Gamma) to keep the KDE plot\n  representations inside the domains where these distributions are defined, and\n  we also set cut=0 for the posterior distributions to give better insight into\n  their exact ranges. We don't do this for the prior distributions, since in\n  general they have wider (or infinite) ranges and the bounds that we show would\n  be mostly determined by number_of_samples_for_prior, which should just be an\n  implementation detail.\n\n  Args:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1080, "start_line_no": 1055, "end_line_no": 1105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1065-1115", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  else:\n    ax.legend(handles[::-1], labels[::-1])\n\n  for tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n  plt.close()\n  return fig\n\n\ndef _make_prior_and_posterior_subplot_for_one_feature(\n    prior_distribution: numpyro.distributions.Distribution,\n    posterior_samples: np.ndarray,\n    subplot_title: str,\n    fig: matplotlib.figure.Figure,\n    gridspec_fig: matplotlib.gridspec.GridSpec,\n    i_ax: int,\n    hyperprior: bool = False,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> Tuple[matplotlib.figure.Figure, matplotlib.gridspec.GridSpec, int]:\n  \"\"\"Helper function to make the prior and posterior distribution subplots.\n\n  This function makes some (hard-coded) choices about how to display the prior\n  and posterior distributions. First, it uses kernel density estimators to\n  smooth the distributions rather than plotting the histograms directly (since\n  the histograms look too noisy unless we take unreasonably large numbers of\n  samples). Second, we found that the default bandwidth works pretty well for\n  visualization of these distributions, though we expose the bw_adjust parameter\n  if users wish to modify this. Finally, kernel density estimators tend to have\n  issues at the edges of distributions, and this issue persists here too. We\n  clip some distributions (Half-Normal, Beta, Gamma) to keep the KDE plot\n  representations inside the domains where these distributions are defined, and\n  we also set cut=0 for the posterior distributions to give better insight into\n  their exact ranges. We don't do this for the prior distributions, since in\n  general they have wider (or infinite) ranges and the bounds that we show would\n  be mostly determined by number_of_samples_for_prior, which should just be an\n  implementation detail.\n\n  Args:\n    prior_distribution: Numpyro distribution specifying the prior from which we\n      will sample to generate the plot.\n    posterior_samples: Array of samples from the posterior distribution,\n      obtained from the trace of the media_mix_model. Might need to be flattened\n      in some cases.\n    subplot_title: Title to display for this particular subplot\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig.\n    hyperprior: Flag which indicates that the prior_distribution is actually a", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1090, "start_line_no": 1065, "end_line_no": 1115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1075-1125", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    prior_distribution: numpyro.distributions.Distribution,\n    posterior_samples: np.ndarray,\n    subplot_title: str,\n    fig: matplotlib.figure.Figure,\n    gridspec_fig: matplotlib.gridspec.GridSpec,\n    i_ax: int,\n    hyperprior: bool = False,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> Tuple[matplotlib.figure.Figure, matplotlib.gridspec.GridSpec, int]:\n  \"\"\"Helper function to make the prior and posterior distribution subplots.\n\n  This function makes some (hard-coded) choices about how to display the prior\n  and posterior distributions. First, it uses kernel density estimators to\n  smooth the distributions rather than plotting the histograms directly (since\n  the histograms look too noisy unless we take unreasonably large numbers of\n  samples). Second, we found that the default bandwidth works pretty well for\n  visualization of these distributions, though we expose the bw_adjust parameter\n  if users wish to modify this. Finally, kernel density estimators tend to have\n  issues at the edges of distributions, and this issue persists here too. We\n  clip some distributions (Half-Normal, Beta, Gamma) to keep the KDE plot\n  representations inside the domains where these distributions are defined, and\n  we also set cut=0 for the posterior distributions to give better insight into\n  their exact ranges. We don't do this for the prior distributions, since in\n  general they have wider (or infinite) ranges and the bounds that we show would\n  be mostly determined by number_of_samples_for_prior, which should just be an\n  implementation detail.\n\n  Args:\n    prior_distribution: Numpyro distribution specifying the prior from which we\n      will sample to generate the plot.\n    posterior_samples: Array of samples from the posterior distribution,\n      obtained from the trace of the media_mix_model. Might need to be flattened\n      in some cases.\n    subplot_title: Title to display for this particular subplot\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig.\n    hyperprior: Flag which indicates that the prior_distribution is actually a\n      hyperprior distribution. LMMM is hierarchical on the channel coefficients\n      when run at the geo-level, so this should currently only be set to True\n      when working with the media channel coefficients.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1100, "start_line_no": 1075, "end_line_no": 1125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1085-1135", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": ") -> Tuple[matplotlib.figure.Figure, matplotlib.gridspec.GridSpec, int]:\n  \"\"\"Helper function to make the prior and posterior distribution subplots.\n\n  This function makes some (hard-coded) choices about how to display the prior\n  and posterior distributions. First, it uses kernel density estimators to\n  smooth the distributions rather than plotting the histograms directly (since\n  the histograms look too noisy unless we take unreasonably large numbers of\n  samples). Second, we found that the default bandwidth works pretty well for\n  visualization of these distributions, though we expose the bw_adjust parameter\n  if users wish to modify this. Finally, kernel density estimators tend to have\n  issues at the edges of distributions, and this issue persists here too. We\n  clip some distributions (Half-Normal, Beta, Gamma) to keep the KDE plot\n  representations inside the domains where these distributions are defined, and\n  we also set cut=0 for the posterior distributions to give better insight into\n  their exact ranges. We don't do this for the prior distributions, since in\n  general they have wider (or infinite) ranges and the bounds that we show would\n  be mostly determined by number_of_samples_for_prior, which should just be an\n  implementation detail.\n\n  Args:\n    prior_distribution: Numpyro distribution specifying the prior from which we\n      will sample to generate the plot.\n    posterior_samples: Array of samples from the posterior distribution,\n      obtained from the trace of the media_mix_model. Might need to be flattened\n      in some cases.\n    subplot_title: Title to display for this particular subplot\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig.\n    hyperprior: Flag which indicates that the prior_distribution is actually a\n      hyperprior distribution. LMMM is hierarchical on the channel coefficients\n      when run at the geo-level, so this should currently only be set to True\n      when working with the media channel coefficients.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig, iterated by one.\n  \"\"\"\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1110, "start_line_no": 1085, "end_line_no": 1135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1095-1145", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  issues at the edges of distributions, and this issue persists here too. We\n  clip some distributions (Half-Normal, Beta, Gamma) to keep the KDE plot\n  representations inside the domains where these distributions are defined, and\n  we also set cut=0 for the posterior distributions to give better insight into\n  their exact ranges. We don't do this for the prior distributions, since in\n  general they have wider (or infinite) ranges and the bounds that we show would\n  be mostly determined by number_of_samples_for_prior, which should just be an\n  implementation detail.\n\n  Args:\n    prior_distribution: Numpyro distribution specifying the prior from which we\n      will sample to generate the plot.\n    posterior_samples: Array of samples from the posterior distribution,\n      obtained from the trace of the media_mix_model. Might need to be flattened\n      in some cases.\n    subplot_title: Title to display for this particular subplot\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig.\n    hyperprior: Flag which indicates that the prior_distribution is actually a\n      hyperprior distribution. LMMM is hierarchical on the channel coefficients\n      when run at the geo-level, so this should currently only be set to True\n      when working with the media channel coefficients.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig, iterated by one.\n  \"\"\"\n\n  if seed is None:\n    seed = utils.get_time_seed()\n  prior_samples = prior_distribution.sample(\n      key=jax.random.PRNGKey(seed=seed),\n      sample_shape=(number_of_samples_for_prior,))\n\n  # Truncate the KDE plot representation of Half-Normal, Beta, and Gamma\n  # distributions, since users might be confused to see a Half-Normal or Gamma\n  # going negative, or a Beta distribution outside of [0, 1].\n  if isinstance(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1120, "start_line_no": 1095, "end_line_no": 1145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1105-1155", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    prior_distribution: Numpyro distribution specifying the prior from which we\n      will sample to generate the plot.\n    posterior_samples: Array of samples from the posterior distribution,\n      obtained from the trace of the media_mix_model. Might need to be flattened\n      in some cases.\n    subplot_title: Title to display for this particular subplot\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig.\n    hyperprior: Flag which indicates that the prior_distribution is actually a\n      hyperprior distribution. LMMM is hierarchical on the channel coefficients\n      when run at the geo-level, so this should currently only be set to True\n      when working with the media channel coefficients.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig, iterated by one.\n  \"\"\"\n\n  if seed is None:\n    seed = utils.get_time_seed()\n  prior_samples = prior_distribution.sample(\n      key=jax.random.PRNGKey(seed=seed),\n      sample_shape=(number_of_samples_for_prior,))\n\n  # Truncate the KDE plot representation of Half-Normal, Beta, and Gamma\n  # distributions, since users might be confused to see a Half-Normal or Gamma\n  # going negative, or a Beta distribution outside of [0, 1].\n  if isinstance(\n      prior_distribution,\n      (numpyro.distributions.HalfNormal, numpyro.distributions.Gamma)):\n    clipping_bounds = [0, None]\n  elif isinstance(prior_distribution, numpyro.distributions.Beta):\n    clipping_bounds = [0, 1]\n  else:\n    clipping_bounds = None\n\n  if hyperprior:\n    square_root_of_number_of_samples_for_prior = int(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1130, "start_line_no": 1105, "end_line_no": 1155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1115-1165", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      hyperprior distribution. LMMM is hierarchical on the channel coefficients\n      when run at the geo-level, so this should currently only be set to True\n      when working with the media channel coefficients.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig, iterated by one.\n  \"\"\"\n\n  if seed is None:\n    seed = utils.get_time_seed()\n  prior_samples = prior_distribution.sample(\n      key=jax.random.PRNGKey(seed=seed),\n      sample_shape=(number_of_samples_for_prior,))\n\n  # Truncate the KDE plot representation of Half-Normal, Beta, and Gamma\n  # distributions, since users might be confused to see a Half-Normal or Gamma\n  # going negative, or a Beta distribution outside of [0, 1].\n  if isinstance(\n      prior_distribution,\n      (numpyro.distributions.HalfNormal, numpyro.distributions.Gamma)):\n    clipping_bounds = [0, None]\n  elif isinstance(prior_distribution, numpyro.distributions.Beta):\n    clipping_bounds = [0, 1]\n  else:\n    clipping_bounds = None\n\n  if hyperprior:\n    square_root_of_number_of_samples_for_prior = int(\n        np.sqrt(number_of_samples_for_prior))\n    prior_distribution = numpyro.distributions.continuous.HalfNormal(\n        scale=prior_samples[:square_root_of_number_of_samples_for_prior])\n    prior_samples = prior_distribution.sample(\n        key=jax.random.PRNGKey(seed=seed),\n        sample_shape=(square_root_of_number_of_samples_for_prior,)).flatten()\n\n  ax = fig.add_subplot(gridspec_fig[i_ax, 0])\n  sns.kdeplot(\n      data=prior_samples,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1140, "start_line_no": 1115, "end_line_no": 1165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1125-1175", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    fig: The matplotlib Figure object for the overall plot.\n    gridspec_fig: The matplotlib GridSpec object for the overall plot.\n    i_ax: Index of the subplot within the gridspec_fig, iterated by one.\n  \"\"\"\n\n  if seed is None:\n    seed = utils.get_time_seed()\n  prior_samples = prior_distribution.sample(\n      key=jax.random.PRNGKey(seed=seed),\n      sample_shape=(number_of_samples_for_prior,))\n\n  # Truncate the KDE plot representation of Half-Normal, Beta, and Gamma\n  # distributions, since users might be confused to see a Half-Normal or Gamma\n  # going negative, or a Beta distribution outside of [0, 1].\n  if isinstance(\n      prior_distribution,\n      (numpyro.distributions.HalfNormal, numpyro.distributions.Gamma)):\n    clipping_bounds = [0, None]\n  elif isinstance(prior_distribution, numpyro.distributions.Beta):\n    clipping_bounds = [0, 1]\n  else:\n    clipping_bounds = None\n\n  if hyperprior:\n    square_root_of_number_of_samples_for_prior = int(\n        np.sqrt(number_of_samples_for_prior))\n    prior_distribution = numpyro.distributions.continuous.HalfNormal(\n        scale=prior_samples[:square_root_of_number_of_samples_for_prior])\n    prior_samples = prior_distribution.sample(\n        key=jax.random.PRNGKey(seed=seed),\n        sample_shape=(square_root_of_number_of_samples_for_prior,)).flatten()\n\n  ax = fig.add_subplot(gridspec_fig[i_ax, 0])\n  sns.kdeplot(\n      data=prior_samples,\n      lw=4,\n      clip=clipping_bounds,\n      color=\"tab:blue\", ax=ax, label=\"prior\")\n  prior_xlims = ax.get_xlim()\n\n  sns.kdeplot(\n      data=posterior_samples.flatten(),\n      lw=4,\n      clip=clipping_bounds,\n      cut=0,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1150, "start_line_no": 1125, "end_line_no": 1175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1135-1185", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  if seed is None:\n    seed = utils.get_time_seed()\n  prior_samples = prior_distribution.sample(\n      key=jax.random.PRNGKey(seed=seed),\n      sample_shape=(number_of_samples_for_prior,))\n\n  # Truncate the KDE plot representation of Half-Normal, Beta, and Gamma\n  # distributions, since users might be confused to see a Half-Normal or Gamma\n  # going negative, or a Beta distribution outside of [0, 1].\n  if isinstance(\n      prior_distribution,\n      (numpyro.distributions.HalfNormal, numpyro.distributions.Gamma)):\n    clipping_bounds = [0, None]\n  elif isinstance(prior_distribution, numpyro.distributions.Beta):\n    clipping_bounds = [0, 1]\n  else:\n    clipping_bounds = None\n\n  if hyperprior:\n    square_root_of_number_of_samples_for_prior = int(\n        np.sqrt(number_of_samples_for_prior))\n    prior_distribution = numpyro.distributions.continuous.HalfNormal(\n        scale=prior_samples[:square_root_of_number_of_samples_for_prior])\n    prior_samples = prior_distribution.sample(\n        key=jax.random.PRNGKey(seed=seed),\n        sample_shape=(square_root_of_number_of_samples_for_prior,)).flatten()\n\n  ax = fig.add_subplot(gridspec_fig[i_ax, 0])\n  sns.kdeplot(\n      data=prior_samples,\n      lw=4,\n      clip=clipping_bounds,\n      color=\"tab:blue\", ax=ax, label=\"prior\")\n  prior_xlims = ax.get_xlim()\n\n  sns.kdeplot(\n      data=posterior_samples.flatten(),\n      lw=4,\n      clip=clipping_bounds,\n      cut=0,\n      bw_adjust=kde_bandwidth_adjust_for_posterior,\n      color=\"tab:orange\", ax=ax, label=\"posterior\")\n  posterior_xlims = ax.get_xlim()\n\n  ax.legend(loc=\"best\")\n  ax.set_xlim(\n      min(prior_xlims[0], posterior_xlims[0]),\n      max(prior_xlims[1], posterior_xlims[1]))\n  ax.set_yticks([])\n  ax.set_ylabel(\"\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1160, "start_line_no": 1135, "end_line_no": 1185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1145-1195", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      prior_distribution,\n      (numpyro.distributions.HalfNormal, numpyro.distributions.Gamma)):\n    clipping_bounds = [0, None]\n  elif isinstance(prior_distribution, numpyro.distributions.Beta):\n    clipping_bounds = [0, 1]\n  else:\n    clipping_bounds = None\n\n  if hyperprior:\n    square_root_of_number_of_samples_for_prior = int(\n        np.sqrt(number_of_samples_for_prior))\n    prior_distribution = numpyro.distributions.continuous.HalfNormal(\n        scale=prior_samples[:square_root_of_number_of_samples_for_prior])\n    prior_samples = prior_distribution.sample(\n        key=jax.random.PRNGKey(seed=seed),\n        sample_shape=(square_root_of_number_of_samples_for_prior,)).flatten()\n\n  ax = fig.add_subplot(gridspec_fig[i_ax, 0])\n  sns.kdeplot(\n      data=prior_samples,\n      lw=4,\n      clip=clipping_bounds,\n      color=\"tab:blue\", ax=ax, label=\"prior\")\n  prior_xlims = ax.get_xlim()\n\n  sns.kdeplot(\n      data=posterior_samples.flatten(),\n      lw=4,\n      clip=clipping_bounds,\n      cut=0,\n      bw_adjust=kde_bandwidth_adjust_for_posterior,\n      color=\"tab:orange\", ax=ax, label=\"posterior\")\n  posterior_xlims = ax.get_xlim()\n\n  ax.legend(loc=\"best\")\n  ax.set_xlim(\n      min(prior_xlims[0], posterior_xlims[0]),\n      max(prior_xlims[1], posterior_xlims[1]))\n  ax.set_yticks([])\n  ax.set_ylabel(\"\")\n  ax.set_title(subplot_title, y=0.85, va=\"top\", fontsize=10)\n\n  i_ax += 1\n\n  return fig, gridspec_fig, i_ax\n\n\ndef _collect_features_for_prior_posterior_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    selected_features: Optional[List[str]] = None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1170, "start_line_no": 1145, "end_line_no": 1195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1155-1205", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        np.sqrt(number_of_samples_for_prior))\n    prior_distribution = numpyro.distributions.continuous.HalfNormal(\n        scale=prior_samples[:square_root_of_number_of_samples_for_prior])\n    prior_samples = prior_distribution.sample(\n        key=jax.random.PRNGKey(seed=seed),\n        sample_shape=(square_root_of_number_of_samples_for_prior,)).flatten()\n\n  ax = fig.add_subplot(gridspec_fig[i_ax, 0])\n  sns.kdeplot(\n      data=prior_samples,\n      lw=4,\n      clip=clipping_bounds,\n      color=\"tab:blue\", ax=ax, label=\"prior\")\n  prior_xlims = ax.get_xlim()\n\n  sns.kdeplot(\n      data=posterior_samples.flatten(),\n      lw=4,\n      clip=clipping_bounds,\n      cut=0,\n      bw_adjust=kde_bandwidth_adjust_for_posterior,\n      color=\"tab:orange\", ax=ax, label=\"posterior\")\n  posterior_xlims = ax.get_xlim()\n\n  ax.legend(loc=\"best\")\n  ax.set_xlim(\n      min(prior_xlims[0], posterior_xlims[0]),\n      max(prior_xlims[1], posterior_xlims[1]))\n  ax.set_yticks([])\n  ax.set_ylabel(\"\")\n  ax.set_title(subplot_title, y=0.85, va=\"top\", fontsize=10)\n\n  i_ax += 1\n\n  return fig, gridspec_fig, i_ax\n\n\ndef _collect_features_for_prior_posterior_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    selected_features: Optional[List[str]] = None,\n) -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n  \"\"\"Helper function to collect features to include in the prior/posterior plot.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n\n  Returns:\n    features: List of all features for the given model type, for which we have", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1180, "start_line_no": 1155, "end_line_no": 1205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1165-1215", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      lw=4,\n      clip=clipping_bounds,\n      color=\"tab:blue\", ax=ax, label=\"prior\")\n  prior_xlims = ax.get_xlim()\n\n  sns.kdeplot(\n      data=posterior_samples.flatten(),\n      lw=4,\n      clip=clipping_bounds,\n      cut=0,\n      bw_adjust=kde_bandwidth_adjust_for_posterior,\n      color=\"tab:orange\", ax=ax, label=\"posterior\")\n  posterior_xlims = ax.get_xlim()\n\n  ax.legend(loc=\"best\")\n  ax.set_xlim(\n      min(prior_xlims[0], posterior_xlims[0]),\n      max(prior_xlims[1], posterior_xlims[1]))\n  ax.set_yticks([])\n  ax.set_ylabel(\"\")\n  ax.set_title(subplot_title, y=0.85, va=\"top\", fontsize=10)\n\n  i_ax += 1\n\n  return fig, gridspec_fig, i_ax\n\n\ndef _collect_features_for_prior_posterior_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    selected_features: Optional[List[str]] = None,\n) -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n  \"\"\"Helper function to collect features to include in the prior/posterior plot.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n\n  Returns:\n    features: List of all features for the given model type, for which we have\n    defined prior distributions.\n    geo_level_features: List of all geo-level features for the given model type.\n    channel_level_features: List of all channel-level features for the given\n    model type.\n    seasonal_features: List of all seasonal features for the given model type.\n    other_features: List of all other features for the given media_mix_model.\n\n  Raises:\n    ValueError: if feature names are passed to selected_features which do not\n    appear in media_mix_model.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1190, "start_line_no": 1165, "end_line_no": 1215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1175-1225", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      bw_adjust=kde_bandwidth_adjust_for_posterior,\n      color=\"tab:orange\", ax=ax, label=\"posterior\")\n  posterior_xlims = ax.get_xlim()\n\n  ax.legend(loc=\"best\")\n  ax.set_xlim(\n      min(prior_xlims[0], posterior_xlims[0]),\n      max(prior_xlims[1], posterior_xlims[1]))\n  ax.set_yticks([])\n  ax.set_ylabel(\"\")\n  ax.set_title(subplot_title, y=0.85, va=\"top\", fontsize=10)\n\n  i_ax += 1\n\n  return fig, gridspec_fig, i_ax\n\n\ndef _collect_features_for_prior_posterior_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    selected_features: Optional[List[str]] = None,\n) -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n  \"\"\"Helper function to collect features to include in the prior/posterior plot.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n\n  Returns:\n    features: List of all features for the given model type, for which we have\n    defined prior distributions.\n    geo_level_features: List of all geo-level features for the given model type.\n    channel_level_features: List of all channel-level features for the given\n    model type.\n    seasonal_features: List of all seasonal features for the given model type.\n    other_features: List of all other features for the given media_mix_model.\n\n  Raises:\n    ValueError: if feature names are passed to selected_features which do not\n    appear in media_mix_model.\n  \"\"\"\n\n  media_mix_model_attributes_to_check_for = [\n      \"trace\",\n      \"_weekday_seasonality\",\n      \"custom_priors\",\n      \"n_geos\",\n      \"n_media_channels\",\n      \"_media_prior\",\n  ]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1200, "start_line_no": 1175, "end_line_no": 1225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1185-1235", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  ax.set_title(subplot_title, y=0.85, va=\"top\", fontsize=10)\n\n  i_ax += 1\n\n  return fig, gridspec_fig, i_ax\n\n\ndef _collect_features_for_prior_posterior_plot(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    selected_features: Optional[List[str]] = None,\n) -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n  \"\"\"Helper function to collect features to include in the prior/posterior plot.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n\n  Returns:\n    features: List of all features for the given model type, for which we have\n    defined prior distributions.\n    geo_level_features: List of all geo-level features for the given model type.\n    channel_level_features: List of all channel-level features for the given\n    model type.\n    seasonal_features: List of all seasonal features for the given model type.\n    other_features: List of all other features for the given media_mix_model.\n\n  Raises:\n    ValueError: if feature names are passed to selected_features which do not\n    appear in media_mix_model.\n  \"\"\"\n\n  media_mix_model_attributes_to_check_for = [\n      \"trace\",\n      \"_weekday_seasonality\",\n      \"custom_priors\",\n      \"n_geos\",\n      \"n_media_channels\",\n      \"_media_prior\",\n  ]\n  if not all([\n      hasattr(media_mix_model, x)\n      for x in media_mix_model_attributes_to_check_for\n  ]):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first in order to plot the posterior.\")\n\n  features = media_mix_model._prior_names\n  if not media_mix_model._weekday_seasonality:\n    features = features.difference([models._WEEKDAY])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1210, "start_line_no": 1185, "end_line_no": 1235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1195-1245", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": ") -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n  \"\"\"Helper function to collect features to include in the prior/posterior plot.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n\n  Returns:\n    features: List of all features for the given model type, for which we have\n    defined prior distributions.\n    geo_level_features: List of all geo-level features for the given model type.\n    channel_level_features: List of all channel-level features for the given\n    model type.\n    seasonal_features: List of all seasonal features for the given model type.\n    other_features: List of all other features for the given media_mix_model.\n\n  Raises:\n    ValueError: if feature names are passed to selected_features which do not\n    appear in media_mix_model.\n  \"\"\"\n\n  media_mix_model_attributes_to_check_for = [\n      \"trace\",\n      \"_weekday_seasonality\",\n      \"custom_priors\",\n      \"n_geos\",\n      \"n_media_channels\",\n      \"_media_prior\",\n  ]\n  if not all([\n      hasattr(media_mix_model, x)\n      for x in media_mix_model_attributes_to_check_for\n  ]):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first in order to plot the posterior.\")\n\n  features = media_mix_model._prior_names\n  if not media_mix_model._weekday_seasonality:\n    features = features.difference([models._WEEKDAY])\n\n  if media_mix_model._extra_features is None:\n    features = features.difference([\"coef_extra_features\"])\n\n  if media_mix_model.media.ndim == 2:\n    features = features.difference(models.GEO_ONLY_PRIORS)\n    features = features.union([\"coef_media\"])\n  else:\n    features = features.union([\"coef_media\", \"channel_coef_media\"])\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1220, "start_line_no": 1195, "end_line_no": 1245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1205-1255", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    defined prior distributions.\n    geo_level_features: List of all geo-level features for the given model type.\n    channel_level_features: List of all channel-level features for the given\n    model type.\n    seasonal_features: List of all seasonal features for the given model type.\n    other_features: List of all other features for the given media_mix_model.\n\n  Raises:\n    ValueError: if feature names are passed to selected_features which do not\n    appear in media_mix_model.\n  \"\"\"\n\n  media_mix_model_attributes_to_check_for = [\n      \"trace\",\n      \"_weekday_seasonality\",\n      \"custom_priors\",\n      \"n_geos\",\n      \"n_media_channels\",\n      \"_media_prior\",\n  ]\n  if not all([\n      hasattr(media_mix_model, x)\n      for x in media_mix_model_attributes_to_check_for\n  ]):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first in order to plot the posterior.\")\n\n  features = media_mix_model._prior_names\n  if not media_mix_model._weekday_seasonality:\n    features = features.difference([models._WEEKDAY])\n\n  if media_mix_model._extra_features is None:\n    features = features.difference([\"coef_extra_features\"])\n\n  if media_mix_model.media.ndim == 2:\n    features = features.difference(models.GEO_ONLY_PRIORS)\n    features = features.union([\"coef_media\"])\n  else:\n    features = features.union([\"coef_media\", \"channel_coef_media\"])\n\n  if selected_features:\n    extraneous_features = set(selected_features).difference(features)\n    if extraneous_features:\n      raise ValueError(\n          f\"Selected_features {extraneous_features} not in media_mix_model.\")\n    features = selected_features\n\n  geo_level_features = [\n      models._COEF_SEASONALITY,\n      models._COEF_TREND,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1230, "start_line_no": 1205, "end_line_no": 1255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1215-1265", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  \"\"\"\n\n  media_mix_model_attributes_to_check_for = [\n      \"trace\",\n      \"_weekday_seasonality\",\n      \"custom_priors\",\n      \"n_geos\",\n      \"n_media_channels\",\n      \"_media_prior\",\n  ]\n  if not all([\n      hasattr(media_mix_model, x)\n      for x in media_mix_model_attributes_to_check_for\n  ]):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first in order to plot the posterior.\")\n\n  features = media_mix_model._prior_names\n  if not media_mix_model._weekday_seasonality:\n    features = features.difference([models._WEEKDAY])\n\n  if media_mix_model._extra_features is None:\n    features = features.difference([\"coef_extra_features\"])\n\n  if media_mix_model.media.ndim == 2:\n    features = features.difference(models.GEO_ONLY_PRIORS)\n    features = features.union([\"coef_media\"])\n  else:\n    features = features.union([\"coef_media\", \"channel_coef_media\"])\n\n  if selected_features:\n    extraneous_features = set(selected_features).difference(features)\n    if extraneous_features:\n      raise ValueError(\n          f\"Selected_features {extraneous_features} not in media_mix_model.\")\n    features = selected_features\n\n  geo_level_features = [\n      models._COEF_SEASONALITY,\n      models._COEF_TREND,\n      models._INTERCEPT,\n      models._SIGMA,\n  ]\n  channel_level_features = [\n      models._AD_EFFECT_RETENTION_RATE,\n      models._EXPONENT,\n      models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n      models._LAG_WEIGHT,\n      models._PEAK_EFFECT_DELAY,\n      models._SLOPE,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1240, "start_line_no": 1215, "end_line_no": 1265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1225-1275", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  if not all([\n      hasattr(media_mix_model, x)\n      for x in media_mix_model_attributes_to_check_for\n  ]):\n    raise lightweight_mmm.NotFittedModelError(\n        \"Model needs to be fit first in order to plot the posterior.\")\n\n  features = media_mix_model._prior_names\n  if not media_mix_model._weekday_seasonality:\n    features = features.difference([models._WEEKDAY])\n\n  if media_mix_model._extra_features is None:\n    features = features.difference([\"coef_extra_features\"])\n\n  if media_mix_model.media.ndim == 2:\n    features = features.difference(models.GEO_ONLY_PRIORS)\n    features = features.union([\"coef_media\"])\n  else:\n    features = features.union([\"coef_media\", \"channel_coef_media\"])\n\n  if selected_features:\n    extraneous_features = set(selected_features).difference(features)\n    if extraneous_features:\n      raise ValueError(\n          f\"Selected_features {extraneous_features} not in media_mix_model.\")\n    features = selected_features\n\n  geo_level_features = [\n      models._COEF_SEASONALITY,\n      models._COEF_TREND,\n      models._INTERCEPT,\n      models._SIGMA,\n  ]\n  channel_level_features = [\n      models._AD_EFFECT_RETENTION_RATE,\n      models._EXPONENT,\n      models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n      models._LAG_WEIGHT,\n      models._PEAK_EFFECT_DELAY,\n      models._SLOPE,\n      \"channel_coef_media\",\n      \"coef_media\",\n  ]\n  seasonal_features = [models._GAMMA_SEASONALITY]\n  if media_mix_model._weekday_seasonality:\n    seasonal_features.append(models._WEEKDAY)\n  other_features = list(set(features) - set(geo_level_features) -\n                        set(channel_level_features) - set(seasonal_features))\n\n  return (list(features), geo_level_features, channel_level_features,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1250, "start_line_no": 1225, "end_line_no": 1275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1235-1285", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  if media_mix_model._extra_features is None:\n    features = features.difference([\"coef_extra_features\"])\n\n  if media_mix_model.media.ndim == 2:\n    features = features.difference(models.GEO_ONLY_PRIORS)\n    features = features.union([\"coef_media\"])\n  else:\n    features = features.union([\"coef_media\", \"channel_coef_media\"])\n\n  if selected_features:\n    extraneous_features = set(selected_features).difference(features)\n    if extraneous_features:\n      raise ValueError(\n          f\"Selected_features {extraneous_features} not in media_mix_model.\")\n    features = selected_features\n\n  geo_level_features = [\n      models._COEF_SEASONALITY,\n      models._COEF_TREND,\n      models._INTERCEPT,\n      models._SIGMA,\n  ]\n  channel_level_features = [\n      models._AD_EFFECT_RETENTION_RATE,\n      models._EXPONENT,\n      models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n      models._LAG_WEIGHT,\n      models._PEAK_EFFECT_DELAY,\n      models._SLOPE,\n      \"channel_coef_media\",\n      \"coef_media\",\n  ]\n  seasonal_features = [models._GAMMA_SEASONALITY]\n  if media_mix_model._weekday_seasonality:\n    seasonal_features.append(models._WEEKDAY)\n  other_features = list(set(features) - set(geo_level_features) -\n                        set(channel_level_features) - set(seasonal_features))\n\n  return (list(features), geo_level_features, channel_level_features,\n          seasonal_features, other_features)\n\n\ndef plot_prior_and_posterior(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    fig_size: Optional[Tuple[int, int]] = None,\n    selected_features: Optional[List[str]] = None,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1260, "start_line_no": 1235, "end_line_no": 1285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1245-1295", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  if selected_features:\n    extraneous_features = set(selected_features).difference(features)\n    if extraneous_features:\n      raise ValueError(\n          f\"Selected_features {extraneous_features} not in media_mix_model.\")\n    features = selected_features\n\n  geo_level_features = [\n      models._COEF_SEASONALITY,\n      models._COEF_TREND,\n      models._INTERCEPT,\n      models._SIGMA,\n  ]\n  channel_level_features = [\n      models._AD_EFFECT_RETENTION_RATE,\n      models._EXPONENT,\n      models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n      models._LAG_WEIGHT,\n      models._PEAK_EFFECT_DELAY,\n      models._SLOPE,\n      \"channel_coef_media\",\n      \"coef_media\",\n  ]\n  seasonal_features = [models._GAMMA_SEASONALITY]\n  if media_mix_model._weekday_seasonality:\n    seasonal_features.append(models._WEEKDAY)\n  other_features = list(set(features) - set(geo_level_features) -\n                        set(channel_level_features) - set(seasonal_features))\n\n  return (list(features), geo_level_features, channel_level_features,\n          seasonal_features, other_features)\n\n\ndef plot_prior_and_posterior(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    fig_size: Optional[Tuple[int, int]] = None,\n    selected_features: Optional[List[str]] = None,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots prior and posterior distributions for parameters in media_mix_model.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    fig_size: Size of the figure to plot as used by matplotlib. Default is a\n      width of 8 and a height of 1.5 for each subplot.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1270, "start_line_no": 1245, "end_line_no": 1295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1255-1305", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      models._INTERCEPT,\n      models._SIGMA,\n  ]\n  channel_level_features = [\n      models._AD_EFFECT_RETENTION_RATE,\n      models._EXPONENT,\n      models._HALF_MAX_EFFECTIVE_CONCENTRATION,\n      models._LAG_WEIGHT,\n      models._PEAK_EFFECT_DELAY,\n      models._SLOPE,\n      \"channel_coef_media\",\n      \"coef_media\",\n  ]\n  seasonal_features = [models._GAMMA_SEASONALITY]\n  if media_mix_model._weekday_seasonality:\n    seasonal_features.append(models._WEEKDAY)\n  other_features = list(set(features) - set(geo_level_features) -\n                        set(channel_level_features) - set(seasonal_features))\n\n  return (list(features), geo_level_features, channel_level_features,\n          seasonal_features, other_features)\n\n\ndef plot_prior_and_posterior(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    fig_size: Optional[Tuple[int, int]] = None,\n    selected_features: Optional[List[str]] = None,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots prior and posterior distributions for parameters in media_mix_model.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    fig_size: Size of the figure to plot as used by matplotlib. Default is a\n      width of 8 and a height of 1.5 for each subplot.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1280, "start_line_no": 1255, "end_line_no": 1305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1265-1315", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      \"channel_coef_media\",\n      \"coef_media\",\n  ]\n  seasonal_features = [models._GAMMA_SEASONALITY]\n  if media_mix_model._weekday_seasonality:\n    seasonal_features.append(models._WEEKDAY)\n  other_features = list(set(features) - set(geo_level_features) -\n                        set(channel_level_features) - set(seasonal_features))\n\n  return (list(features), geo_level_features, channel_level_features,\n          seasonal_features, other_features)\n\n\ndef plot_prior_and_posterior(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    fig_size: Optional[Tuple[int, int]] = None,\n    selected_features: Optional[List[str]] = None,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots prior and posterior distributions for parameters in media_mix_model.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    fig_size: Size of the figure to plot as used by matplotlib. Default is a\n      width of 8 and a height of 1.5 for each subplot.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    Plot with Kernel density estimate smoothing showing prior and posterior\n    distributions for every parameter in the given media_mix_model.\n\n  Raises:\n    NotFittedModelError: media_mix_model has not yet been fit.\n    ValueError: A feature has been created without a well-defined prior.\n  \"\"\"\n\n  (features, geo_level_features, channel_level_features, seasonal_features,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1290, "start_line_no": 1265, "end_line_no": 1315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1275-1325", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "          seasonal_features, other_features)\n\n\ndef plot_prior_and_posterior(\n    media_mix_model: lightweight_mmm.LightweightMMM,\n    fig_size: Optional[Tuple[int, int]] = None,\n    selected_features: Optional[List[str]] = None,\n    number_of_samples_for_prior: int = 5000,\n    kde_bandwidth_adjust_for_posterior: float = 1,\n    seed: Optional[int] = None,\n) -> matplotlib.figure.Figure:\n  \"\"\"Plots prior and posterior distributions for parameters in media_mix_model.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    fig_size: Size of the figure to plot as used by matplotlib. Default is a\n      width of 8 and a height of 1.5 for each subplot.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    Plot with Kernel density estimate smoothing showing prior and posterior\n    distributions for every parameter in the given media_mix_model.\n\n  Raises:\n    NotFittedModelError: media_mix_model has not yet been fit.\n    ValueError: A feature has been created without a well-defined prior.\n  \"\"\"\n\n  (features, geo_level_features, channel_level_features, seasonal_features,\n   other_features) = _collect_features_for_prior_posterior_plot(\n       media_mix_model, selected_features)\n\n  number_of_subplots = int(\n      sum([\n          np.product(media_mix_model.trace[x].shape[1:]) for x in features\n      ]))\n\n  if not fig_size:\n    fig_size = (8, 1.5 * number_of_subplots)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1300, "start_line_no": 1275, "end_line_no": 1325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1285-1335", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": ") -> matplotlib.figure.Figure:\n  \"\"\"Plots prior and posterior distributions for parameters in media_mix_model.\n\n  Args:\n    media_mix_model: Fitted media mix model.\n    fig_size: Size of the figure to plot as used by matplotlib. Default is a\n      width of 8 and a height of 1.5 for each subplot.\n    selected_features: Optional list of feature names to select. If not\n      specified (the default), all features are selected.\n    number_of_samples_for_prior: Controls the level of smoothing for the plotted\n      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    Plot with Kernel density estimate smoothing showing prior and posterior\n    distributions for every parameter in the given media_mix_model.\n\n  Raises:\n    NotFittedModelError: media_mix_model has not yet been fit.\n    ValueError: A feature has been created without a well-defined prior.\n  \"\"\"\n\n  (features, geo_level_features, channel_level_features, seasonal_features,\n   other_features) = _collect_features_for_prior_posterior_plot(\n       media_mix_model, selected_features)\n\n  number_of_subplots = int(\n      sum([\n          np.product(media_mix_model.trace[x].shape[1:]) for x in features\n      ]))\n\n  if not fig_size:\n    fig_size = (8, 1.5 * number_of_subplots)\n\n  fig = plt.figure(figsize=fig_size, constrained_layout=True)\n  gridspec_fig = matplotlib.gridspec.GridSpec(\n      nrows=number_of_subplots, ncols=1, figure=fig, hspace=0.1)\n\n  default_priors = {\n      **models._get_default_priors(),\n      **models._get_transform_default_priors()[media_mix_model.model_name]\n  }\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1310, "start_line_no": 1285, "end_line_no": 1335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1295-1345", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "      version of the prior distribution. The default should be fine unless you\n      want to decrease it to speed up runtime.\n    kde_bandwidth_adjust_for_posterior: Multiplicative factor to adjust the\n      bandwidth of the kernel density estimator, to control the level of\n      smoothing for the posterior distribution. Passed to seaborn.kdeplot as the\n      bw_adjust parameter there.\n    seed: Seed to use for PRNGKey during sampling. For replicability run\n        this function and any other function that utilises predictions with the\n        same seed.\n\n  Returns:\n    Plot with Kernel density estimate smoothing showing prior and posterior\n    distributions for every parameter in the given media_mix_model.\n\n  Raises:\n    NotFittedModelError: media_mix_model has not yet been fit.\n    ValueError: A feature has been created without a well-defined prior.\n  \"\"\"\n\n  (features, geo_level_features, channel_level_features, seasonal_features,\n   other_features) = _collect_features_for_prior_posterior_plot(\n       media_mix_model, selected_features)\n\n  number_of_subplots = int(\n      sum([\n          np.product(media_mix_model.trace[x].shape[1:]) for x in features\n      ]))\n\n  if not fig_size:\n    fig_size = (8, 1.5 * number_of_subplots)\n\n  fig = plt.figure(figsize=fig_size, constrained_layout=True)\n  gridspec_fig = matplotlib.gridspec.GridSpec(\n      nrows=number_of_subplots, ncols=1, figure=fig, hspace=0.1)\n\n  default_priors = {\n      **models._get_default_priors(),\n      **models._get_transform_default_priors()[media_mix_model.model_name]\n  }\n\n  kwargs_for_helper_function = {\n      \"fig\": fig,\n      \"gridspec_fig\": gridspec_fig,\n      \"number_of_samples_for_prior\": number_of_samples_for_prior,\n      \"kde_bandwidth_adjust_for_posterior\": kde_bandwidth_adjust_for_posterior,\n      \"seed\": seed,\n  }\n\n  i_ax = 0\n  for feature in (geo_level_features + channel_level_features +", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1320, "start_line_no": 1295, "end_line_no": 1345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1305-1355", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  Returns:\n    Plot with Kernel density estimate smoothing showing prior and posterior\n    distributions for every parameter in the given media_mix_model.\n\n  Raises:\n    NotFittedModelError: media_mix_model has not yet been fit.\n    ValueError: A feature has been created without a well-defined prior.\n  \"\"\"\n\n  (features, geo_level_features, channel_level_features, seasonal_features,\n   other_features) = _collect_features_for_prior_posterior_plot(\n       media_mix_model, selected_features)\n\n  number_of_subplots = int(\n      sum([\n          np.product(media_mix_model.trace[x].shape[1:]) for x in features\n      ]))\n\n  if not fig_size:\n    fig_size = (8, 1.5 * number_of_subplots)\n\n  fig = plt.figure(figsize=fig_size, constrained_layout=True)\n  gridspec_fig = matplotlib.gridspec.GridSpec(\n      nrows=number_of_subplots, ncols=1, figure=fig, hspace=0.1)\n\n  default_priors = {\n      **models._get_default_priors(),\n      **models._get_transform_default_priors()[media_mix_model.model_name]\n  }\n\n  kwargs_for_helper_function = {\n      \"fig\": fig,\n      \"gridspec_fig\": gridspec_fig,\n      \"number_of_samples_for_prior\": number_of_samples_for_prior,\n      \"kde_bandwidth_adjust_for_posterior\": kde_bandwidth_adjust_for_posterior,\n      \"seed\": seed,\n  }\n\n  i_ax = 0\n  for feature in (geo_level_features + channel_level_features +\n                  seasonal_features + other_features):\n    if feature not in features:\n      continue\n\n    if feature in media_mix_model.custom_priors:\n      prior_distribution = media_mix_model.custom_priors[feature]\n      if not isinstance(prior_distribution, numpyro.distributions.Distribution):\n        raise ValueError(f\"{feature} cannot be plotted.\")\n    elif feature in default_priors.keys():\n      prior_distribution = default_priors[feature]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1330, "start_line_no": 1305, "end_line_no": 1355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1315-1365", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "   other_features) = _collect_features_for_prior_posterior_plot(\n       media_mix_model, selected_features)\n\n  number_of_subplots = int(\n      sum([\n          np.product(media_mix_model.trace[x].shape[1:]) for x in features\n      ]))\n\n  if not fig_size:\n    fig_size = (8, 1.5 * number_of_subplots)\n\n  fig = plt.figure(figsize=fig_size, constrained_layout=True)\n  gridspec_fig = matplotlib.gridspec.GridSpec(\n      nrows=number_of_subplots, ncols=1, figure=fig, hspace=0.1)\n\n  default_priors = {\n      **models._get_default_priors(),\n      **models._get_transform_default_priors()[media_mix_model.model_name]\n  }\n\n  kwargs_for_helper_function = {\n      \"fig\": fig,\n      \"gridspec_fig\": gridspec_fig,\n      \"number_of_samples_for_prior\": number_of_samples_for_prior,\n      \"kde_bandwidth_adjust_for_posterior\": kde_bandwidth_adjust_for_posterior,\n      \"seed\": seed,\n  }\n\n  i_ax = 0\n  for feature in (geo_level_features + channel_level_features +\n                  seasonal_features + other_features):\n    if feature not in features:\n      continue\n\n    if feature in media_mix_model.custom_priors:\n      prior_distribution = media_mix_model.custom_priors[feature]\n      if not isinstance(prior_distribution, numpyro.distributions.Distribution):\n        raise ValueError(f\"{feature} cannot be plotted.\")\n    elif feature in default_priors.keys():\n      prior_distribution = default_priors[feature]\n    elif feature in (\"channel_coef_media\", \"coef_media\"):\n      # We have to fill this in later since the prior varies by channel.\n      prior_distribution = None\n    else:\n      # This should never happen.\n      raise ValueError(f\"{feature} has no prior specified.\")\n    kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n\n    if feature == models._COEF_EXTRA_FEATURES:\n      for i_feature in range(media_mix_model.trace[feature].shape[1]):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1340, "start_line_no": 1315, "end_line_no": 1365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1325-1375", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "\n  fig = plt.figure(figsize=fig_size, constrained_layout=True)\n  gridspec_fig = matplotlib.gridspec.GridSpec(\n      nrows=number_of_subplots, ncols=1, figure=fig, hspace=0.1)\n\n  default_priors = {\n      **models._get_default_priors(),\n      **models._get_transform_default_priors()[media_mix_model.model_name]\n  }\n\n  kwargs_for_helper_function = {\n      \"fig\": fig,\n      \"gridspec_fig\": gridspec_fig,\n      \"number_of_samples_for_prior\": number_of_samples_for_prior,\n      \"kde_bandwidth_adjust_for_posterior\": kde_bandwidth_adjust_for_posterior,\n      \"seed\": seed,\n  }\n\n  i_ax = 0\n  for feature in (geo_level_features + channel_level_features +\n                  seasonal_features + other_features):\n    if feature not in features:\n      continue\n\n    if feature in media_mix_model.custom_priors:\n      prior_distribution = media_mix_model.custom_priors[feature]\n      if not isinstance(prior_distribution, numpyro.distributions.Distribution):\n        raise ValueError(f\"{feature} cannot be plotted.\")\n    elif feature in default_priors.keys():\n      prior_distribution = default_priors[feature]\n    elif feature in (\"channel_coef_media\", \"coef_media\"):\n      # We have to fill this in later since the prior varies by channel.\n      prior_distribution = None\n    else:\n      # This should never happen.\n      raise ValueError(f\"{feature} has no prior specified.\")\n    kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n\n    if feature == models._COEF_EXTRA_FEATURES:\n      for i_feature in range(media_mix_model.trace[feature].shape[1]):\n        for j_geo in range(media_mix_model.n_geos):\n          subplot_title = f\"{feature} feature {i_feature}, geo {j_geo}\"\n          if media_mix_model.n_geos == 1:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature])\n          else:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature, j_geo])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1350, "start_line_no": 1325, "end_line_no": 1375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1335-1385", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "  kwargs_for_helper_function = {\n      \"fig\": fig,\n      \"gridspec_fig\": gridspec_fig,\n      \"number_of_samples_for_prior\": number_of_samples_for_prior,\n      \"kde_bandwidth_adjust_for_posterior\": kde_bandwidth_adjust_for_posterior,\n      \"seed\": seed,\n  }\n\n  i_ax = 0\n  for feature in (geo_level_features + channel_level_features +\n                  seasonal_features + other_features):\n    if feature not in features:\n      continue\n\n    if feature in media_mix_model.custom_priors:\n      prior_distribution = media_mix_model.custom_priors[feature]\n      if not isinstance(prior_distribution, numpyro.distributions.Distribution):\n        raise ValueError(f\"{feature} cannot be plotted.\")\n    elif feature in default_priors.keys():\n      prior_distribution = default_priors[feature]\n    elif feature in (\"channel_coef_media\", \"coef_media\"):\n      # We have to fill this in later since the prior varies by channel.\n      prior_distribution = None\n    else:\n      # This should never happen.\n      raise ValueError(f\"{feature} has no prior specified.\")\n    kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n\n    if feature == models._COEF_EXTRA_FEATURES:\n      for i_feature in range(media_mix_model.trace[feature].shape[1]):\n        for j_geo in range(media_mix_model.n_geos):\n          subplot_title = f\"{feature} feature {i_feature}, geo {j_geo}\"\n          if media_mix_model.n_geos == 1:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature])\n          else:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature, j_geo])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in geo_level_features:\n      for i_geo in range(media_mix_model.n_geos):\n        subplot_title = f\"{feature}, geo {i_geo}\"\n        posterior_samples = np.array(media_mix_model.trace[feature][:, i_geo])\n        (fig, gridspec_fig,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1360, "start_line_no": 1335, "end_line_no": 1385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1345-1395", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "                  seasonal_features + other_features):\n    if feature not in features:\n      continue\n\n    if feature in media_mix_model.custom_priors:\n      prior_distribution = media_mix_model.custom_priors[feature]\n      if not isinstance(prior_distribution, numpyro.distributions.Distribution):\n        raise ValueError(f\"{feature} cannot be plotted.\")\n    elif feature in default_priors.keys():\n      prior_distribution = default_priors[feature]\n    elif feature in (\"channel_coef_media\", \"coef_media\"):\n      # We have to fill this in later since the prior varies by channel.\n      prior_distribution = None\n    else:\n      # This should never happen.\n      raise ValueError(f\"{feature} has no prior specified.\")\n    kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n\n    if feature == models._COEF_EXTRA_FEATURES:\n      for i_feature in range(media_mix_model.trace[feature].shape[1]):\n        for j_geo in range(media_mix_model.n_geos):\n          subplot_title = f\"{feature} feature {i_feature}, geo {j_geo}\"\n          if media_mix_model.n_geos == 1:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature])\n          else:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature, j_geo])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in geo_level_features:\n      for i_geo in range(media_mix_model.n_geos):\n        subplot_title = f\"{feature}, geo {i_geo}\"\n        posterior_samples = np.array(media_mix_model.trace[feature][:, i_geo])\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             **kwargs_for_helper_function)\n\n    if feature in channel_level_features:\n      for i_channel in range(media_mix_model.n_media_channels):\n        subplot_title = f\"{feature}, channel {i_channel}\"\n        if feature in (\"channel_coef_media\", \"coef_media\"):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1370, "start_line_no": 1345, "end_line_no": 1395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1355-1405", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "    elif feature in (\"channel_coef_media\", \"coef_media\"):\n      # We have to fill this in later since the prior varies by channel.\n      prior_distribution = None\n    else:\n      # This should never happen.\n      raise ValueError(f\"{feature} has no prior specified.\")\n    kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n\n    if feature == models._COEF_EXTRA_FEATURES:\n      for i_feature in range(media_mix_model.trace[feature].shape[1]):\n        for j_geo in range(media_mix_model.n_geos):\n          subplot_title = f\"{feature} feature {i_feature}, geo {j_geo}\"\n          if media_mix_model.n_geos == 1:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature])\n          else:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature, j_geo])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in geo_level_features:\n      for i_geo in range(media_mix_model.n_geos):\n        subplot_title = f\"{feature}, geo {i_geo}\"\n        posterior_samples = np.array(media_mix_model.trace[feature][:, i_geo])\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             **kwargs_for_helper_function)\n\n    if feature in channel_level_features:\n      for i_channel in range(media_mix_model.n_media_channels):\n        subplot_title = f\"{feature}, channel {i_channel}\"\n        if feature in (\"channel_coef_media\", \"coef_media\"):\n          prior_distribution = numpyro.distributions.continuous.HalfNormal(\n              scale=jnp.squeeze(media_mix_model._media_prior[i_channel]))\n        posterior_samples = np.array(\n            jnp.squeeze(media_mix_model.trace[feature][:, i_channel]))\n        kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n        hyperprior = feature == \"channel_coef_media\"\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1380, "start_line_no": 1355, "end_line_no": 1405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1365-1415", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "        for j_geo in range(media_mix_model.n_geos):\n          subplot_title = f\"{feature} feature {i_feature}, geo {j_geo}\"\n          if media_mix_model.n_geos == 1:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature])\n          else:\n            posterior_samples = np.array(\n                media_mix_model.trace[feature][:, i_feature, j_geo])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in geo_level_features:\n      for i_geo in range(media_mix_model.n_geos):\n        subplot_title = f\"{feature}, geo {i_geo}\"\n        posterior_samples = np.array(media_mix_model.trace[feature][:, i_geo])\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             **kwargs_for_helper_function)\n\n    if feature in channel_level_features:\n      for i_channel in range(media_mix_model.n_media_channels):\n        subplot_title = f\"{feature}, channel {i_channel}\"\n        if feature in (\"channel_coef_media\", \"coef_media\"):\n          prior_distribution = numpyro.distributions.continuous.HalfNormal(\n              scale=jnp.squeeze(media_mix_model._media_prior[i_channel]))\n        posterior_samples = np.array(\n            jnp.squeeze(media_mix_model.trace[feature][:, i_channel]))\n        kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n        hyperprior = feature == \"channel_coef_media\"\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             hyperprior=hyperprior,\n             **kwargs_for_helper_function)\n\n    if feature in seasonal_features:\n      for i_season in range(media_mix_model._degrees_seasonality):\n        for j_season in range(2):\n          sin_or_cos = \"sin\" if j_season == 0 else \"cos\"\n          subplot_title = f\"{feature}, seasonal mode {i_season}:{sin_or_cos}\"\n          posterior_samples = np.array(media_mix_model.trace[feature][:,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1390, "start_line_no": 1365, "end_line_no": 1415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1375-1425", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in geo_level_features:\n      for i_geo in range(media_mix_model.n_geos):\n        subplot_title = f\"{feature}, geo {i_geo}\"\n        posterior_samples = np.array(media_mix_model.trace[feature][:, i_geo])\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             **kwargs_for_helper_function)\n\n    if feature in channel_level_features:\n      for i_channel in range(media_mix_model.n_media_channels):\n        subplot_title = f\"{feature}, channel {i_channel}\"\n        if feature in (\"channel_coef_media\", \"coef_media\"):\n          prior_distribution = numpyro.distributions.continuous.HalfNormal(\n              scale=jnp.squeeze(media_mix_model._media_prior[i_channel]))\n        posterior_samples = np.array(\n            jnp.squeeze(media_mix_model.trace[feature][:, i_channel]))\n        kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n        hyperprior = feature == \"channel_coef_media\"\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             hyperprior=hyperprior,\n             **kwargs_for_helper_function)\n\n    if feature in seasonal_features:\n      for i_season in range(media_mix_model._degrees_seasonality):\n        for j_season in range(2):\n          sin_or_cos = \"sin\" if j_season == 0 else \"cos\"\n          subplot_title = f\"{feature}, seasonal mode {i_season}:{sin_or_cos}\"\n          posterior_samples = np.array(media_mix_model.trace[feature][:,\n                                                                      i_season,\n                                                                      j_season])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in other_features and feature != models._COEF_EXTRA_FEATURES:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1400, "start_line_no": 1375, "end_line_no": 1425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1385-1434", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             **kwargs_for_helper_function)\n\n    if feature in channel_level_features:\n      for i_channel in range(media_mix_model.n_media_channels):\n        subplot_title = f\"{feature}, channel {i_channel}\"\n        if feature in (\"channel_coef_media\", \"coef_media\"):\n          prior_distribution = numpyro.distributions.continuous.HalfNormal(\n              scale=jnp.squeeze(media_mix_model._media_prior[i_channel]))\n        posterior_samples = np.array(\n            jnp.squeeze(media_mix_model.trace[feature][:, i_channel]))\n        kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n        hyperprior = feature == \"channel_coef_media\"\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             hyperprior=hyperprior,\n             **kwargs_for_helper_function)\n\n    if feature in seasonal_features:\n      for i_season in range(media_mix_model._degrees_seasonality):\n        for j_season in range(2):\n          sin_or_cos = \"sin\" if j_season == 0 else \"cos\"\n          subplot_title = f\"{feature}, seasonal mode {i_season}:{sin_or_cos}\"\n          posterior_samples = np.array(media_mix_model.trace[feature][:,\n                                                                      i_season,\n                                                                      j_season])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in other_features and feature != models._COEF_EXTRA_FEATURES:\n      subplot_title = f\"{feature}\"\n      posterior_samples = np.array(media_mix_model.trace[feature])\n      (fig, gridspec_fig,\n       i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n           posterior_samples=posterior_samples,\n           subplot_title=subplot_title,\n           i_ax=i_ax,\n           **kwargs_for_helper_function)\n  return fig", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1410, "start_line_no": 1385, "end_line_no": 1434, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1395-1434", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "          prior_distribution = numpyro.distributions.continuous.HalfNormal(\n              scale=jnp.squeeze(media_mix_model._media_prior[i_channel]))\n        posterior_samples = np.array(\n            jnp.squeeze(media_mix_model.trace[feature][:, i_channel]))\n        kwargs_for_helper_function[\"prior_distribution\"] = prior_distribution\n        hyperprior = feature == \"channel_coef_media\"\n        (fig, gridspec_fig,\n         i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n             posterior_samples=posterior_samples,\n             subplot_title=subplot_title,\n             i_ax=i_ax,\n             hyperprior=hyperprior,\n             **kwargs_for_helper_function)\n\n    if feature in seasonal_features:\n      for i_season in range(media_mix_model._degrees_seasonality):\n        for j_season in range(2):\n          sin_or_cos = \"sin\" if j_season == 0 else \"cos\"\n          subplot_title = f\"{feature}, seasonal mode {i_season}:{sin_or_cos}\"\n          posterior_samples = np.array(media_mix_model.trace[feature][:,\n                                                                      i_season,\n                                                                      j_season])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in other_features and feature != models._COEF_EXTRA_FEATURES:\n      subplot_title = f\"{feature}\"\n      posterior_samples = np.array(media_mix_model.trace[feature])\n      (fig, gridspec_fig,\n       i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n           posterior_samples=posterior_samples,\n           subplot_title=subplot_title,\n           i_ax=i_ax,\n           **kwargs_for_helper_function)\n  return fig", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1420, "start_line_no": 1395, "end_line_no": 1434, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot.py_1405-1434", "title": "google_lightweight_mmm-lightweight_mmm-plot.py", "text": "             i_ax=i_ax,\n             hyperprior=hyperprior,\n             **kwargs_for_helper_function)\n\n    if feature in seasonal_features:\n      for i_season in range(media_mix_model._degrees_seasonality):\n        for j_season in range(2):\n          sin_or_cos = \"sin\" if j_season == 0 else \"cos\"\n          subplot_title = f\"{feature}, seasonal mode {i_season}:{sin_or_cos}\"\n          posterior_samples = np.array(media_mix_model.trace[feature][:,\n                                                                      i_season,\n                                                                      j_season])\n          (fig, gridspec_fig,\n           i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n               posterior_samples=posterior_samples,\n               subplot_title=subplot_title,\n               i_ax=i_ax,\n               **kwargs_for_helper_function)\n\n    if feature in other_features and feature != models._COEF_EXTRA_FEATURES:\n      subplot_title = f\"{feature}\"\n      posterior_samples = np.array(media_mix_model.trace[feature])\n      (fig, gridspec_fig,\n       i_ax) = _make_prior_and_posterior_subplot_for_one_feature(\n           posterior_samples=posterior_samples,\n           subplot_title=subplot_title,\n           i_ax=i_ax,\n           **kwargs_for_helper_function)\n  return fig", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot.py"], "line_no": 1430, "start_line_no": 1405, "end_line_no": 1434, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for plot.\"\"\"\n\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\nfrom unittest import mock\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro.distributions as dist\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "import pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import models\nfrom lightweight_mmm import plot\nfrom lightweight_mmm import preprocessing\n\nMOCK_NATIONAL_TRACE = {\n    \"coef_extra_features\": np.ones([10, 2]),\n    \"coef_media\": np.ones([10, 5]),\n    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    \"coef_trend\": np.ones([10, 1]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 1]),\n    \"media_transformed\": np.ones([10, 50, 5,]),\n    \"mu\": np.ones([10, 50]),\n    \"sigma\": np.ones([10, 1]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    \"lag_weight\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    }\n\nMOCK_GEO_TRACE = {\n    \"channel_coef_media\": np.ones([10, 5, 1]),\n    \"coef_extra_features\": np.ones([10, 2, 3]),\n    \"coef_media\": np.ones([10, 5, 3]),\n    \"coef_seasonality\": np.ones([10, 3]),\n    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    \"coef_trend\": np.ones([10, 3]),\n    \"expo_trend\": np.ones([10, 1]),\n    \"gamma_seasonality\": np.ones([10, 3, 2]),\n    \"intercept\": np.ones([10, 3]),\n    \"media_transformed\": np.ones([10, 50, 5, 3]),\n    \"mu\": np.ones([10, 50, 3]),\n    \"sigma\": np.ones([10, 3]),\n    \"ad_effect_retention_rate\": np.ones([10, 5]),\n    \"exponent\": np.ones([10, 5]),\n    \"half_max_effective_concentration\": np.ones([10, 5]),\n    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    \"lag_weight\": np.ones([10, 5]),\n    \"peak_effect_delay\": np.ones([10, 5]),\n    \"slope\": np.ones([10, 5]),\n}\n\n\ndef _set_up_mock_mmm(model_name: str,\n                     is_geo_model: bool) -> lightweight_mmm.LightweightMMM:\n  \"\"\"Creates a mock LightweightMMM instance that acts like a fitted model.\n\n  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  These instances are used when we want to run tests on more diverse ranges of\n  models than the two standard national_mmm and geo_mmm defined below but don't\n  need the unit tests to spend time actually running the model fits.\n\n  Args:\n    model_name: One of [\"adstock\", \"carryover\", or \"hill_adstock\"], specifying\n      which model type should be used in the mock LightweightMMM.\n    is_geo_model: Whether to create a geo-level model (True) or a national-level\n      model (False).\n\n  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  Returns:\n    mmm: A LightweightMMM object that can be treated like a fitted model\n    for plotting-related unit tests.\n  \"\"\"\n  initial_mock_trace = MOCK_GEO_TRACE if is_geo_model else MOCK_NATIONAL_TRACE\n  all_model_names = {\"adstock\", \"carryover\", \"hill_adstock\"}\n  model_items_to_delete = frozenset.union(*[\n      models.TRANSFORM_PRIORS_NAMES[x]\n      for x in all_model_names - {model_name}\n  ]) - models.TRANSFORM_PRIORS_NAMES[model_name]\n  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  mock_trace = {\n      key: initial_mock_trace[key]\n      for key in initial_mock_trace\n      if key not in model_items_to_delete\n  }\n  mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n  mmm.n_media_channels = 5\n  mmm.n_geos = 3 if is_geo_model else 1\n  mmm._media_prior = jnp.ones(5)\n  mmm._weekday_seasonality = False\n  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n\n  def setUp(self):\n    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  mmm._degrees_seasonality = 3\n  mmm.custom_priors = {}\n  mmm._extra_features = None\n  mmm.trace = mock_trace\n  mmm.media = jnp.ones_like(mock_trace[\"media_transformed\"][0])\n  mmm.media_names = [f\"channel_{i}\" for i in range(5)]\n  return mmm\n\n\nclass PlotTest(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n\n  def setUp(self):\n    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))\n    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n\n  @parameterized.named_parameters([", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n  @classmethod\n  def setUpClass(cls):\n    super(PlotTest, cls).setUpClass()\n    cls.national_mmm = lightweight_mmm.LightweightMMM()\n    cls.national_mmm.fit(\n        media=jnp.ones((50, 5)),\n        target=jnp.ones(50),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n\n  def setUp(self):\n    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))\n    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=2)\n  ])\n\n  def test_plot_model_fit_plot_called_with_scaler(self, media_mix_model,\n                                                  expected_calls):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "        number_samples=2,\n        number_chains=1)\n    cls.geo_mmm = lightweight_mmm.LightweightMMM()\n    cls.geo_mmm.fit(\n        media=jnp.ones((50, 5, 3)),\n        target=jnp.ones((50, 3)),\n        media_prior=jnp.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n\n  def setUp(self):\n    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))\n    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=2)\n  ])\n\n  def test_plot_model_fit_plot_called_with_scaler(self, media_mix_model,\n                                                  expected_calls):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm, target_scaler=target_scaler)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    cls.not_fitted_mmm = lightweight_mmm.LightweightMMM()\n\n  def setUp(self):\n    super().setUp()\n    self.mock_ax_scatter = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"scatter\", autospec=True))\n    self.mock_sns_lineplot = self.enter_context(\n        mock.patch.object(plot.sns, \"lineplot\", autospec=True))\n    self.mock_plt_plot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"plot\", autospec=True))\n    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=2)\n  ])\n\n  def test_plot_model_fit_plot_called_with_scaler(self, media_mix_model,\n                                                  expected_calls):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm, target_scaler=target_scaler)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\",\n          expected_calls=2)\n  ])\n  def test_plot_model_fit_plot_called_without_scaler(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    self.mock_plt_barplot = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"bar\", autospec=True))\n    self.mock_pd_area_plot = self.enter_context(\n        mock.patch.object(plot.pd.DataFrame.plot, \"area\", autospec=True))\n    self.mock_sns_kdeplot = self.enter_context(\n        mock.patch.object(plot.sns, \"kdeplot\", autospec=True))\n    self.mock_plt_ax_legend = self.enter_context(\n        mock.patch.object(plot.plt.Axes, \"legend\", autospec=True))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=2)\n  ])\n\n  def test_plot_model_fit_plot_called_with_scaler(self, media_mix_model,\n                                                  expected_calls):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm, target_scaler=target_scaler)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\",\n          expected_calls=2)\n  ])\n  def test_plot_model_fit_plot_called_without_scaler(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=2)\n  ])\n\n  def test_plot_model_fit_plot_called_with_scaler(self, media_mix_model,\n                                                  expected_calls):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm, target_scaler=target_scaler)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\",\n          expected_calls=2)\n  ])\n  def test_plot_model_fit_plot_called_without_scaler(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_plots_n_times_with_correct_params(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_geos = (mmm.media.shape[-1] if mmm.media.ndim == 3\n              else 1)\n    plot.plot_response_curves(media_mix_model=mmm)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    target_scaler.fit(jnp.ones(1))\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm, target_scaler=target_scaler)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\",\n          expected_calls=2)\n  ])\n  def test_plot_model_fit_plot_called_without_scaler(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_plots_n_times_with_correct_params(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_geos = (mmm.media.shape[-1] if mmm.media.ndim == 3\n              else 1)\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    _, call_kwargs = self.mock_sns_lineplot.call_args_list[0]\n    # n channels times 2 charts.\n    self.assertEqual(self.mock_sns_lineplot.call_count,\n                     2 * mmm.n_media_channels)\n    self.assertEqual(jnp.round(a=call_kwargs[\"x\"].max(), decimals=4),\n                     jnp.round(a=1.2 * n_geos, decimals=4))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\",\n          expected_calls=2)\n  ])\n  def test_plot_model_fit_plot_called_without_scaler(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_model_fit(media_mix_model=mmm)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_plots_n_times_with_correct_params(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_geos = (mmm.media.shape[-1] if mmm.media.ndim == 3\n              else 1)\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    _, call_kwargs = self.mock_sns_lineplot.call_args_list[0]\n    # n channels times 2 charts.\n    self.assertEqual(self.mock_sns_lineplot.call_count,\n                     2 * mmm.n_media_channels)\n    self.assertEqual(jnp.round(a=call_kwargs[\"x\"].max(), decimals=4),\n                     jnp.round(a=1.2 * n_geos, decimals=4))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_with_prices_plots_n_times_with_correct_params(\n      self, media_mix_model=\"geo_mmm\"):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n    plot.plot_model_fit(media_mix_model=mmm)\n\n    self.assertTrue(self.mock_plt_plot.call_count, expected_calls)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_plots_n_times_with_correct_params(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_geos = (mmm.media.shape[-1] if mmm.media.ndim == 3\n              else 1)\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    _, call_kwargs = self.mock_sns_lineplot.call_args_list[0]\n    # n channels times 2 charts.\n    self.assertEqual(self.mock_sns_lineplot.call_count,\n                     2 * mmm.n_media_channels)\n    self.assertEqual(jnp.round(a=call_kwargs[\"x\"].max(), decimals=4),\n                     jnp.round(a=1.2 * n_geos, decimals=4))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_with_prices_plots_n_times_with_correct_params(\n      self, media_mix_model=\"geo_mmm\"):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n    prices = jnp.array([1., 0.8, 2., 3., 1.])\n    expected_maxes = jnp.repeat(jnp.array([1.2, 0.96, 2.4, 3.6, 1.2]), 2)\n\n    plot.plot_response_curves(media_mix_model=mmm, prices=prices)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    self.assertEqual(self.mock_sns_lineplot.call_count, n_channels * 2)\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(\n          jnp.round(a=call_kwargs[\"x\"].max().item(), decimals=4).item(),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_plots_n_times_with_correct_params(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_geos = (mmm.media.shape[-1] if mmm.media.ndim == 3\n              else 1)\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    _, call_kwargs = self.mock_sns_lineplot.call_args_list[0]\n    # n channels times 2 charts.\n    self.assertEqual(self.mock_sns_lineplot.call_count,\n                     2 * mmm.n_media_channels)\n    self.assertEqual(jnp.round(a=call_kwargs[\"x\"].max(), decimals=4),\n                     jnp.round(a=1.2 * n_geos, decimals=4))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_with_prices_plots_n_times_with_correct_params(\n      self, media_mix_model=\"geo_mmm\"):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n    prices = jnp.array([1., 0.8, 2., 3., 1.])\n    expected_maxes = jnp.repeat(jnp.array([1.2, 0.96, 2.4, 3.6, 1.2]), 2)\n\n    plot.plot_response_curves(media_mix_model=mmm, prices=prices)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    self.assertEqual(self.mock_sns_lineplot.call_count, n_channels * 2)\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(\n          jnp.round(a=call_kwargs[\"x\"].max().item(), decimals=4).item(),\n          jnp.round(a=expected_max, decimals=4).item() * n_geos,\n          places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    _, call_kwargs = self.mock_sns_lineplot.call_args_list[0]\n    # n channels times 2 charts.\n    self.assertEqual(self.mock_sns_lineplot.call_count,\n                     2 * mmm.n_media_channels)\n    self.assertEqual(jnp.round(a=call_kwargs[\"x\"].max(), decimals=4),\n                     jnp.round(a=1.2 * n_geos, decimals=4))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_with_prices_plots_n_times_with_correct_params(\n      self, media_mix_model=\"geo_mmm\"):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n    prices = jnp.array([1., 0.8, 2., 3., 1.])\n    expected_maxes = jnp.repeat(jnp.array([1.2, 0.96, 2.4, 3.6, 1.2]), 2)\n\n    plot.plot_response_curves(media_mix_model=mmm, prices=prices)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    self.assertEqual(self.mock_sns_lineplot.call_count, n_channels * 2)\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(\n          jnp.round(a=call_kwargs[\"x\"].max().item(), decimals=4).item(),\n          jnp.round(a=expected_max, decimals=4).item() * n_geos,\n          places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_produces_y_axis_starting_at_zero(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    for _, call_kwargs in calls_list[:3]:\n      self.assertLessEqual(call_kwargs[\"y\"].min().item(), 0.1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_with_prices_plots_n_times_with_correct_params(\n      self, media_mix_model=\"geo_mmm\"):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n    prices = jnp.array([1., 0.8, 2., 3., 1.])\n    expected_maxes = jnp.repeat(jnp.array([1.2, 0.96, 2.4, 3.6, 1.2]), 2)\n\n    plot.plot_response_curves(media_mix_model=mmm, prices=prices)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    self.assertEqual(self.mock_sns_lineplot.call_count, n_channels * 2)\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(\n          jnp.round(a=call_kwargs[\"x\"].max().item(), decimals=4).item(),\n          jnp.round(a=expected_max, decimals=4).item() * n_geos,\n          places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_produces_y_axis_starting_at_zero(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    for _, call_kwargs in calls_list[:3]:\n      self.assertLessEqual(call_kwargs[\"y\"].min().item(), 0.1)\n      self.assertGreaterEqual(call_kwargs[\"y\"].min().item(), -0.1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    prices = jnp.array([1., 0.8, 2., 3., 1.])\n    expected_maxes = jnp.repeat(jnp.array([1.2, 0.96, 2.4, 3.6, 1.2]), 2)\n\n    plot.plot_response_curves(media_mix_model=mmm, prices=prices)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    self.assertEqual(self.mock_sns_lineplot.call_count, n_channels * 2)\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(\n          jnp.round(a=call_kwargs[\"x\"].max().item(), decimals=4).item(),\n          jnp.round(a=expected_max, decimals=4).item() * n_geos,\n          places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_produces_y_axis_starting_at_zero(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    for _, call_kwargs in calls_list[:3]:\n      self.assertLessEqual(call_kwargs[\"y\"].min().item(), 0.1)\n      self.assertGreaterEqual(call_kwargs[\"y\"].min().item(), -0.1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_media_scaler(\n      self, media_mix_model):\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    media_scaler.fit(jnp.ones(1) * factor)\n    expected_maxes = jnp.repeat(\n        jnp.repeat(jnp.array([1.2]), repeats=5),\n        repeats=2)\n    mmm = getattr(self, media_mix_model)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          jnp.round(a=expected_max, decimals=4).item() * n_geos,\n          places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_produces_y_axis_starting_at_zero(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    for _, call_kwargs in calls_list[:3]:\n      self.assertLessEqual(call_kwargs[\"y\"].min().item(), 0.1)\n      self.assertGreaterEqual(call_kwargs[\"y\"].min().item(), -0.1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_media_scaler(\n      self, media_mix_model):\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    media_scaler.fit(jnp.ones(1) * factor)\n    expected_maxes = jnp.repeat(\n        jnp.repeat(jnp.array([1.2]), repeats=5),\n        repeats=2)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              media_scaler=media_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(call_kwargs[\"x\"].max().item(),\n                             expected_max * factor,\n                             places=4)\n\n  @parameterized.named_parameters([", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  ])\n  def test_plot_response_curves_produces_y_axis_starting_at_zero(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm)\n\n    calls_list = self.mock_sns_lineplot.call_args_list\n    for _, call_kwargs in calls_list[:3]:\n      self.assertLessEqual(call_kwargs[\"y\"].min().item(), 0.1)\n      self.assertGreaterEqual(call_kwargs[\"y\"].min().item(), -0.1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_media_scaler(\n      self, media_mix_model):\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    media_scaler.fit(jnp.ones(1) * factor)\n    expected_maxes = jnp.repeat(\n        jnp.repeat(jnp.array([1.2]), repeats=5),\n        repeats=2)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              media_scaler=media_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(call_kwargs[\"x\"].max().item(),\n                             expected_max * factor,\n                             places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_target_scaler(\n      self, media_mix_model):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "      self.assertGreaterEqual(call_kwargs[\"y\"].min().item(), -0.1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_media_scaler(\n      self, media_mix_model):\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    media_scaler.fit(jnp.ones(1) * factor)\n    expected_maxes = jnp.repeat(\n        jnp.repeat(jnp.array([1.2]), repeats=5),\n        repeats=2)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              media_scaler=media_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(call_kwargs[\"x\"].max().item(),\n                             expected_max * factor,\n                             places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_target_scaler(\n      self, media_mix_model):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    target_scaler.fit(jnp.ones(1) * factor)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              target_scaler=target_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for _, call_kwargs in calls_list:\n      self.assertAlmostEqual(call_kwargs[\"y\"].max().item(),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  def test_plot_response_curves_scales_with_media_scaler(\n      self, media_mix_model):\n    media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    media_scaler.fit(jnp.ones(1) * factor)\n    expected_maxes = jnp.repeat(\n        jnp.repeat(jnp.array([1.2]), repeats=5),\n        repeats=2)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              media_scaler=media_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(call_kwargs[\"x\"].max().item(),\n                             expected_max * factor,\n                             places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_target_scaler(\n      self, media_mix_model):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    target_scaler.fit(jnp.ones(1) * factor)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              target_scaler=target_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for _, call_kwargs in calls_list:\n      self.assertAlmostEqual(call_kwargs[\"y\"].max().item(),\n                             1 * factor,\n                             places=4)\n\n  def test_perfect_correlation_returns_correct_output(self):\n    x = jnp.arange(100)\n    y = jnp.arange(100, 200)\n\n    idx, maxcorr = plot.plot_cross_correlate(x, y)\n\n    self.assertEqual(idx, 0)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    plot.plot_response_curves(media_mix_model=mmm,\n                              media_scaler=media_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for (_, call_kwargs), expected_max in zip(calls_list, expected_maxes):\n      self.assertAlmostEqual(call_kwargs[\"x\"].max().item(),\n                             expected_max * factor,\n                             places=4)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_target_scaler(\n      self, media_mix_model):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    target_scaler.fit(jnp.ones(1) * factor)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              target_scaler=target_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for _, call_kwargs in calls_list:\n      self.assertAlmostEqual(call_kwargs[\"y\"].max().item(),\n                             1 * factor,\n                             places=4)\n\n  def test_perfect_correlation_returns_correct_output(self):\n    x = jnp.arange(100)\n    y = jnp.arange(100, 200)\n\n    idx, maxcorr = plot.plot_cross_correlate(x, y)\n\n    self.assertEqual(idx, 0)\n    self.assertEqual(maxcorr, 1)\n\n  def test_var_cost_plot_called_with_correct_kwargs(self):\n    media = jnp.arange(10).reshape((5, 2))\n    costs = [1, 2]\n    names = [\"a\", \"b\"]\n    std = jnp.repeat(2.82842712, 2)\n    means = jnp.array([4, 5])\n    expected_coef_of_variation = std / means\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_response_curves_scales_with_target_scaler(\n      self, media_mix_model):\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    factor = 5\n    target_scaler.fit(jnp.ones(1) * factor)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              target_scaler=target_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for _, call_kwargs in calls_list:\n      self.assertAlmostEqual(call_kwargs[\"y\"].max().item(),\n                             1 * factor,\n                             places=4)\n\n  def test_perfect_correlation_returns_correct_output(self):\n    x = jnp.arange(100)\n    y = jnp.arange(100, 200)\n\n    idx, maxcorr = plot.plot_cross_correlate(x, y)\n\n    self.assertEqual(idx, 0)\n    self.assertEqual(maxcorr, 1)\n\n  def test_var_cost_plot_called_with_correct_kwargs(self):\n    media = jnp.arange(10).reshape((5, 2))\n    costs = [1, 2]\n    names = [\"a\", \"b\"]\n    std = jnp.repeat(2.82842712, 2)\n    means = jnp.array([4, 5])\n    expected_coef_of_variation = std / means\n\n    _ = plot.plot_var_cost(media, costs, names)\n    _, call_kwargs = self.mock_ax_scatter.call_args_list[0]\n\n    np.testing.assert_array_almost_equal(call_kwargs[\"x\"], costs)\n    np.testing.assert_array_almost_equal(call_kwargs[\"y\"],\n                                         expected_coef_of_variation)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    factor = 5\n    target_scaler.fit(jnp.ones(1) * factor)\n    mmm = getattr(self, media_mix_model)\n\n    plot.plot_response_curves(media_mix_model=mmm,\n                              target_scaler=target_scaler)\n\n    calls_list = self.mock_plt_plot.call_args_list\n    for _, call_kwargs in calls_list:\n      self.assertAlmostEqual(call_kwargs[\"y\"].max().item(),\n                             1 * factor,\n                             places=4)\n\n  def test_perfect_correlation_returns_correct_output(self):\n    x = jnp.arange(100)\n    y = jnp.arange(100, 200)\n\n    idx, maxcorr = plot.plot_cross_correlate(x, y)\n\n    self.assertEqual(idx, 0)\n    self.assertEqual(maxcorr, 1)\n\n  def test_var_cost_plot_called_with_correct_kwargs(self):\n    media = jnp.arange(10).reshape((5, 2))\n    costs = [1, 2]\n    names = [\"a\", \"b\"]\n    std = jnp.repeat(2.82842712, 2)\n    means = jnp.array([4, 5])\n    expected_coef_of_variation = std / means\n\n    _ = plot.plot_var_cost(media, costs, names)\n    _, call_kwargs = self.mock_ax_scatter.call_args_list[0]\n\n    np.testing.assert_array_almost_equal(call_kwargs[\"x\"], costs)\n    np.testing.assert_array_almost_equal(call_kwargs[\"y\"],\n                                         expected_coef_of_variation)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_media_channel_posteriors_plots_right_number_subplots(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "                             1 * factor,\n                             places=4)\n\n  def test_perfect_correlation_returns_correct_output(self):\n    x = jnp.arange(100)\n    y = jnp.arange(100, 200)\n\n    idx, maxcorr = plot.plot_cross_correlate(x, y)\n\n    self.assertEqual(idx, 0)\n    self.assertEqual(maxcorr, 1)\n\n  def test_var_cost_plot_called_with_correct_kwargs(self):\n    media = jnp.arange(10).reshape((5, 2))\n    costs = [1, 2]\n    names = [\"a\", \"b\"]\n    std = jnp.repeat(2.82842712, 2)\n    means = jnp.array([4, 5])\n    expected_coef_of_variation = std / means\n\n    _ = plot.plot_var_cost(media, costs, names)\n    _, call_kwargs = self.mock_ax_scatter.call_args_list[0]\n\n    np.testing.assert_array_almost_equal(call_kwargs[\"x\"], costs)\n    np.testing.assert_array_almost_equal(call_kwargs[\"y\"],\n                                         expected_coef_of_variation)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_media_channel_posteriors_plots_right_number_subplots(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n\n    fig = plot.plot_media_channel_posteriors(media_mix_model=mmm)\n\n    self.assertLen(fig.get_axes(), n_channels * n_geos)\n\n  def test_unequal_length_ground_truth_and_predictions_raises_error(self):\n    prediction = jnp.arange(10).reshape((5, 2))\n    ground_truth = jnp.array([1, 2, 3])\n    with self.assertRaises(ValueError):\n      plot.plot_out_of_sample_model_fit(prediction, ground_truth)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    self.assertEqual(maxcorr, 1)\n\n  def test_var_cost_plot_called_with_correct_kwargs(self):\n    media = jnp.arange(10).reshape((5, 2))\n    costs = [1, 2]\n    names = [\"a\", \"b\"]\n    std = jnp.repeat(2.82842712, 2)\n    means = jnp.array([4, 5])\n    expected_coef_of_variation = std / means\n\n    _ = plot.plot_var_cost(media, costs, names)\n    _, call_kwargs = self.mock_ax_scatter.call_args_list[0]\n\n    np.testing.assert_array_almost_equal(call_kwargs[\"x\"], costs)\n    np.testing.assert_array_almost_equal(call_kwargs[\"y\"],\n                                         expected_coef_of_variation)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_media_channel_posteriors_plots_right_number_subplots(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n\n    fig = plot.plot_media_channel_posteriors(media_mix_model=mmm)\n\n    self.assertLen(fig.get_axes(), n_channels * n_geos)\n\n  def test_unequal_length_ground_truth_and_predictions_raises_error(self):\n    prediction = jnp.arange(10).reshape((5, 2))\n    ground_truth = jnp.array([1, 2, 3])\n    with self.assertRaises(ValueError):\n      plot.plot_out_of_sample_model_fit(prediction, ground_truth)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=3),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=3)\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_n_times_with_correct_params(\n      self, media_mix_model, expected_calls):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    _ = plot.plot_var_cost(media, costs, names)\n    _, call_kwargs = self.mock_ax_scatter.call_args_list[0]\n\n    np.testing.assert_array_almost_equal(call_kwargs[\"x\"], costs)\n    np.testing.assert_array_almost_equal(call_kwargs[\"y\"],\n                                         expected_coef_of_variation)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_media_channel_posteriors_plots_right_number_subplots(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n\n    fig = plot.plot_media_channel_posteriors(media_mix_model=mmm)\n\n    self.assertLen(fig.get_axes(), n_channels * n_geos)\n\n  def test_unequal_length_ground_truth_and_predictions_raises_error(self):\n    prediction = jnp.arange(10).reshape((5, 2))\n    ground_truth = jnp.array([1, 2, 3])\n    with self.assertRaises(ValueError):\n      plot.plot_out_of_sample_model_fit(prediction, ground_truth)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=3),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=3)\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_n_times_with_correct_params(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n\n    plot.plot_pre_post_budget_allocation_comparison(\n        media_mix_model=mmm,\n        kpi_with_optim=kpi_with_optim,\n        kpi_without_optim=kpi_without_optim,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          media_mix_model=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo\",\n          media_mix_model=\"geo_mmm\")\n  ])\n  def test_plot_media_channel_posteriors_plots_right_number_subplots(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    n_channels = mmm.n_media_channels\n    n_geos = mmm.media.shape[-1] if mmm.media.ndim == 3 else 1\n\n    fig = plot.plot_media_channel_posteriors(media_mix_model=mmm)\n\n    self.assertLen(fig.get_axes(), n_channels * n_geos)\n\n  def test_unequal_length_ground_truth_and_predictions_raises_error(self):\n    prediction = jnp.arange(10).reshape((5, 2))\n    ground_truth = jnp.array([1, 2, 3])\n    with self.assertRaises(ValueError):\n      plot.plot_out_of_sample_model_fit(prediction, ground_truth)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=3),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=3)\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_n_times_with_correct_params(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n\n    plot.plot_pre_post_budget_allocation_comparison(\n        media_mix_model=mmm,\n        kpi_with_optim=kpi_with_optim,\n        kpi_without_optim=kpi_without_optim,\n        optimal_buget_allocation=optimal_buget_allocation,\n        previous_budget_allocation=previous_budget_allocation)\n\n    self.assertEqual(self.mock_plt_barplot.call_count, expected_calls)\n    call_list = self.mock_plt_barplot.call_args_list\n\n    np.testing.assert_array_almost_equal(call_list[0][0][-1],\n                                         previous_budget_allocation)\n    np.testing.assert_array_almost_equal(call_list[1][0][-1],\n                                         optimal_buget_allocation)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n    fig = plot.plot_media_channel_posteriors(media_mix_model=mmm)\n\n    self.assertLen(fig.get_axes(), n_channels * n_geos)\n\n  def test_unequal_length_ground_truth_and_predictions_raises_error(self):\n    prediction = jnp.arange(10).reshape((5, 2))\n    ground_truth = jnp.array([1, 2, 3])\n    with self.assertRaises(ValueError):\n      plot.plot_out_of_sample_model_fit(prediction, ground_truth)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=3),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=3)\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_n_times_with_correct_params(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n\n    plot.plot_pre_post_budget_allocation_comparison(\n        media_mix_model=mmm,\n        kpi_with_optim=kpi_with_optim,\n        kpi_without_optim=kpi_without_optim,\n        optimal_buget_allocation=optimal_buget_allocation,\n        previous_budget_allocation=previous_budget_allocation)\n\n    self.assertEqual(self.mock_plt_barplot.call_count, expected_calls)\n    call_list = self.mock_plt_barplot.call_args_list\n\n    np.testing.assert_array_almost_equal(call_list[0][0][-1],\n                                         previous_budget_allocation)\n    np.testing.assert_array_almost_equal(call_list[1][0][-1],\n                                         optimal_buget_allocation)\n    np.testing.assert_array_almost_equal(\n        call_list[2][0][-1], [kpi_without_optim * -1, kpi_with_optim * -1])\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national and geo\", media_mix_model=\"not_fitted_mmm\")\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_raise_notfittedmodelerror(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=3),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=3)\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_n_times_with_correct_params(\n      self, media_mix_model, expected_calls):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n\n    plot.plot_pre_post_budget_allocation_comparison(\n        media_mix_model=mmm,\n        kpi_with_optim=kpi_with_optim,\n        kpi_without_optim=kpi_without_optim,\n        optimal_buget_allocation=optimal_buget_allocation,\n        previous_budget_allocation=previous_budget_allocation)\n\n    self.assertEqual(self.mock_plt_barplot.call_count, expected_calls)\n    call_list = self.mock_plt_barplot.call_args_list\n\n    np.testing.assert_array_almost_equal(call_list[0][0][-1],\n                                         previous_budget_allocation)\n    np.testing.assert_array_almost_equal(call_list[1][0][-1],\n                                         optimal_buget_allocation)\n    np.testing.assert_array_almost_equal(\n        call_list[2][0][-1], [kpi_without_optim * -1, kpi_with_optim * -1])\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national and geo\", media_mix_model=\"not_fitted_mmm\")\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_raise_notfittedmodelerror(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.plot_pre_post_budget_allocation_comparison(\n          media_mix_model=mmm,\n          kpi_with_optim=kpi_with_optim,\n          kpi_without_optim=kpi_without_optim,\n          optimal_buget_allocation=optimal_buget_allocation,\n          previous_budget_allocation=previous_budget_allocation)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n\n    plot.plot_pre_post_budget_allocation_comparison(\n        media_mix_model=mmm,\n        kpi_with_optim=kpi_with_optim,\n        kpi_without_optim=kpi_without_optim,\n        optimal_buget_allocation=optimal_buget_allocation,\n        previous_budget_allocation=previous_budget_allocation)\n\n    self.assertEqual(self.mock_plt_barplot.call_count, expected_calls)\n    call_list = self.mock_plt_barplot.call_args_list\n\n    np.testing.assert_array_almost_equal(call_list[0][0][-1],\n                                         previous_budget_allocation)\n    np.testing.assert_array_almost_equal(call_list[1][0][-1],\n                                         optimal_buget_allocation)\n    np.testing.assert_array_almost_equal(\n        call_list[2][0][-1], [kpi_without_optim * -1, kpi_with_optim * -1])\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national and geo\", media_mix_model=\"not_fitted_mmm\")\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_raise_notfittedmodelerror(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.plot_pre_post_budget_allocation_comparison(\n          media_mix_model=mmm,\n          kpi_with_optim=kpi_with_optim,\n          kpi_without_optim=kpi_without_optim,\n          optimal_buget_allocation=optimal_buget_allocation,\n          previous_budget_allocation=previous_budget_allocation)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"no channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=None),\n      dict(\n          testcase_name=\"channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "        optimal_buget_allocation=optimal_buget_allocation,\n        previous_budget_allocation=previous_budget_allocation)\n\n    self.assertEqual(self.mock_plt_barplot.call_count, expected_calls)\n    call_list = self.mock_plt_barplot.call_args_list\n\n    np.testing.assert_array_almost_equal(call_list[0][0][-1],\n                                         previous_budget_allocation)\n    np.testing.assert_array_almost_equal(call_list[1][0][-1],\n                                         optimal_buget_allocation)\n    np.testing.assert_array_almost_equal(\n        call_list[2][0][-1], [kpi_without_optim * -1, kpi_with_optim * -1])\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national and geo\", media_mix_model=\"not_fitted_mmm\")\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_raise_notfittedmodelerror(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.plot_pre_post_budget_allocation_comparison(\n          media_mix_model=mmm,\n          kpi_with_optim=kpi_with_optim,\n          kpi_without_optim=kpi_without_optim,\n          optimal_buget_allocation=optimal_buget_allocation,\n          previous_budget_allocation=previous_budget_allocation)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"no channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=None),\n      dict(\n          testcase_name=\"channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=[f\"channel_{x}\" for x in range(5)]))\n  def test_create_attribution_over_spend_fractions_raise_notfittedmodelerror(\n      self, media_mix_model, media_spend, channel_names):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_attribution_over_spend_fractions(\n          media_mix_model=getattr(self, media_mix_model),\n          media_spend=media_spend,\n          channel_names=channel_names)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    np.testing.assert_array_almost_equal(\n        call_list[2][0][-1], [kpi_without_optim * -1, kpi_with_optim * -1])\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national and geo\", media_mix_model=\"not_fitted_mmm\")\n  ])\n  def test_plot_pre_post_budget_allocation_comparison_raise_notfittedmodelerror(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    kpi_with_optim = -503\n    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.plot_pre_post_budget_allocation_comparison(\n          media_mix_model=mmm,\n          kpi_with_optim=kpi_with_optim,\n          kpi_without_optim=kpi_without_optim,\n          optimal_buget_allocation=optimal_buget_allocation,\n          previous_budget_allocation=previous_budget_allocation)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"no channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=None),\n      dict(\n          testcase_name=\"channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=[f\"channel_{x}\" for x in range(5)]))\n  def test_create_attribution_over_spend_fractions_raise_notfittedmodelerror(\n      self, media_mix_model, media_spend, channel_names):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_attribution_over_spend_fractions(\n          media_mix_model=getattr(self, media_mix_model),\n          media_spend=media_spend,\n          channel_names=channel_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_number\",\n          media_mix_model=\"national_mmm\",\n          media_spend=-np.ones((50, 5))),\n      dict(\n          testcase_name=\"aggeregated_zero\",\n          media_mix_model=\"national_mmm\",\n          media_spend=np.zeros((50, 5)))\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    kpi_without_optim = -479\n    optimal_buget_allocation = jnp.array([118, 278, 100, 100, 100])\n    previous_budget_allocation = jnp.array([199, 197, 100, 100, 100])\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.plot_pre_post_budget_allocation_comparison(\n          media_mix_model=mmm,\n          kpi_with_optim=kpi_with_optim,\n          kpi_without_optim=kpi_without_optim,\n          optimal_buget_allocation=optimal_buget_allocation,\n          previous_budget_allocation=previous_budget_allocation)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"no channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=None),\n      dict(\n          testcase_name=\"channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=[f\"channel_{x}\" for x in range(5)]))\n  def test_create_attribution_over_spend_fractions_raise_notfittedmodelerror(\n      self, media_mix_model, media_spend, channel_names):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_attribution_over_spend_fractions(\n          media_mix_model=getattr(self, media_mix_model),\n          media_spend=media_spend,\n          channel_names=channel_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_number\",\n          media_mix_model=\"national_mmm\",\n          media_spend=-np.ones((50, 5))),\n      dict(\n          testcase_name=\"aggeregated_zero\",\n          media_mix_model=\"national_mmm\",\n          media_spend=np.zeros((50, 5)))\n  ])\n  def test_create_attribution_over_spend_fractions_raise_error_on_invalid_values(\n      self, media_mix_model, media_spend):\n    expected_message = (\"Values in media must all be non-negative or values in \"\n                        \"aggregated media must be possitive.\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      plot.create_attribution_over_spend_fractions(\n          getattr(self, media_mix_model), media_spend)\n\n  @parameterized.product(\n      (dict(is_geo_model=False, media_spend=np.array([1, 2, 3, 4, 5])),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_405-455", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"no channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=None),\n      dict(\n          testcase_name=\"channel_name input\",\n          media_mix_model=\"not_fitted_mmm\",\n          media_spend=np.ones((50, 5)),\n          channel_names=[f\"channel_{x}\" for x in range(5)]))\n  def test_create_attribution_over_spend_fractions_raise_notfittedmodelerror(\n      self, media_mix_model, media_spend, channel_names):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_attribution_over_spend_fractions(\n          media_mix_model=getattr(self, media_mix_model),\n          media_spend=media_spend,\n          channel_names=channel_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_number\",\n          media_mix_model=\"national_mmm\",\n          media_spend=-np.ones((50, 5))),\n      dict(\n          testcase_name=\"aggeregated_zero\",\n          media_mix_model=\"national_mmm\",\n          media_spend=np.zeros((50, 5)))\n  ])\n  def test_create_attribution_over_spend_fractions_raise_error_on_invalid_values(\n      self, media_mix_model, media_spend):\n    expected_message = (\"Values in media must all be non-negative or values in \"\n                        \"aggregated media must be possitive.\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      plot.create_attribution_over_spend_fractions(\n          getattr(self, media_mix_model), media_spend)\n\n  @parameterized.product(\n      (dict(is_geo_model=False, media_spend=np.array([1, 2, 3, 4, 5])),\n       dict(\n           is_geo_model=False,\n           media_spend=np.resize(np.array([1, 2, 3, 4, 5]), 250).reshape(50,\n                                                                         5)),),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in range(5)])),\n      (dict(time_index=None), dict(time_index=(1, 10))),\n      (dict(model_name=\"adstock\"), dict(model_name=\"carryover\"),\n       dict(model_name=\"hill_adstock\")))\n  def test_create_attribution_over_spend_fractions_results_are_correct(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_415-465", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          media_spend=np.ones((50, 5)),\n          channel_names=[f\"channel_{x}\" for x in range(5)]))\n  def test_create_attribution_over_spend_fractions_raise_notfittedmodelerror(\n      self, media_mix_model, media_spend, channel_names):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_attribution_over_spend_fractions(\n          media_mix_model=getattr(self, media_mix_model),\n          media_spend=media_spend,\n          channel_names=channel_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_number\",\n          media_mix_model=\"national_mmm\",\n          media_spend=-np.ones((50, 5))),\n      dict(\n          testcase_name=\"aggeregated_zero\",\n          media_mix_model=\"national_mmm\",\n          media_spend=np.zeros((50, 5)))\n  ])\n  def test_create_attribution_over_spend_fractions_raise_error_on_invalid_values(\n      self, media_mix_model, media_spend):\n    expected_message = (\"Values in media must all be non-negative or values in \"\n                        \"aggregated media must be possitive.\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      plot.create_attribution_over_spend_fractions(\n          getattr(self, media_mix_model), media_spend)\n\n  @parameterized.product(\n      (dict(is_geo_model=False, media_spend=np.array([1, 2, 3, 4, 5])),\n       dict(\n           is_geo_model=False,\n           media_spend=np.resize(np.array([1, 2, 3, 4, 5]), 250).reshape(50,\n                                                                         5)),),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in range(5)])),\n      (dict(time_index=None), dict(time_index=(1, 10))),\n      (dict(model_name=\"adstock\"), dict(model_name=\"carryover\"),\n       dict(model_name=\"hill_adstock\")))\n  def test_create_attribution_over_spend_fractions_results_are_correct(\n      self, model_name, is_geo_model, media_spend, channel_names, time_index):\n    mmm = _set_up_mock_mmm(model_name, is_geo_model)\n\n    expected_results = pd.DataFrame(\n        np.transpose([np.ones(5)/np.ones(5).sum(),\n                      np.arange(1, 6)/np.arange(1, 6).sum(),\n                      3/np.arange(1, 6)]),\n        index=[f\"channel_{x}\" for x in range(5)],\n        columns=[\"media attribution\", \"media spend\", \"attribution over spend\"])\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_425-475", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"negative_number\",\n          media_mix_model=\"national_mmm\",\n          media_spend=-np.ones((50, 5))),\n      dict(\n          testcase_name=\"aggeregated_zero\",\n          media_mix_model=\"national_mmm\",\n          media_spend=np.zeros((50, 5)))\n  ])\n  def test_create_attribution_over_spend_fractions_raise_error_on_invalid_values(\n      self, media_mix_model, media_spend):\n    expected_message = (\"Values in media must all be non-negative or values in \"\n                        \"aggregated media must be possitive.\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      plot.create_attribution_over_spend_fractions(\n          getattr(self, media_mix_model), media_spend)\n\n  @parameterized.product(\n      (dict(is_geo_model=False, media_spend=np.array([1, 2, 3, 4, 5])),\n       dict(\n           is_geo_model=False,\n           media_spend=np.resize(np.array([1, 2, 3, 4, 5]), 250).reshape(50,\n                                                                         5)),),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in range(5)])),\n      (dict(time_index=None), dict(time_index=(1, 10))),\n      (dict(model_name=\"adstock\"), dict(model_name=\"carryover\"),\n       dict(model_name=\"hill_adstock\")))\n  def test_create_attribution_over_spend_fractions_results_are_correct(\n      self, model_name, is_geo_model, media_spend, channel_names, time_index):\n    mmm = _set_up_mock_mmm(model_name, is_geo_model)\n\n    expected_results = pd.DataFrame(\n        np.transpose([np.ones(5)/np.ones(5).sum(),\n                      np.arange(1, 6)/np.arange(1, 6).sum(),\n                      3/np.arange(1, 6)]),\n        index=[f\"channel_{x}\" for x in range(5)],\n        columns=[\"media attribution\", \"media spend\", \"attribution over spend\"])\n\n    aos_fractions_df = plot.create_attribution_over_spend_fractions(\n        mmm, media_spend, channel_names, time_index)\n\n    pd.testing.assert_frame_equal(aos_fractions_df, expected_results, atol=1e-3)\n\n  def test_create_media_baseline_contribution_df_raise_notfittedmodelerror(\n      self):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_media_baseline_contribution_df(\n          media_mix_model=getattr(self, \"not_fitted_mmm\"))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_435-485", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  def test_create_attribution_over_spend_fractions_raise_error_on_invalid_values(\n      self, media_mix_model, media_spend):\n    expected_message = (\"Values in media must all be non-negative or values in \"\n                        \"aggregated media must be possitive.\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      plot.create_attribution_over_spend_fractions(\n          getattr(self, media_mix_model), media_spend)\n\n  @parameterized.product(\n      (dict(is_geo_model=False, media_spend=np.array([1, 2, 3, 4, 5])),\n       dict(\n           is_geo_model=False,\n           media_spend=np.resize(np.array([1, 2, 3, 4, 5]), 250).reshape(50,\n                                                                         5)),),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in range(5)])),\n      (dict(time_index=None), dict(time_index=(1, 10))),\n      (dict(model_name=\"adstock\"), dict(model_name=\"carryover\"),\n       dict(model_name=\"hill_adstock\")))\n  def test_create_attribution_over_spend_fractions_results_are_correct(\n      self, model_name, is_geo_model, media_spend, channel_names, time_index):\n    mmm = _set_up_mock_mmm(model_name, is_geo_model)\n\n    expected_results = pd.DataFrame(\n        np.transpose([np.ones(5)/np.ones(5).sum(),\n                      np.arange(1, 6)/np.arange(1, 6).sum(),\n                      3/np.arange(1, 6)]),\n        index=[f\"channel_{x}\" for x in range(5)],\n        columns=[\"media attribution\", \"media spend\", \"attribution over spend\"])\n\n    aos_fractions_df = plot.create_attribution_over_spend_fractions(\n        mmm, media_spend, channel_names, time_index)\n\n    pd.testing.assert_frame_equal(aos_fractions_df, expected_results, atol=1e-3)\n\n  def test_create_media_baseline_contribution_df_raise_notfittedmodelerror(\n      self):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_media_baseline_contribution_df(\n          media_mix_model=getattr(self, \"not_fitted_mmm\"))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", media_mix_model=\"national_mmm\"),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\")\n  ])\n  def test_create_media_baseline_contribution_df_contributions_add_up_avg_prediction(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_445-495", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "       dict(\n           is_geo_model=False,\n           media_spend=np.resize(np.array([1, 2, 3, 4, 5]), 250).reshape(50,\n                                                                         5)),),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in range(5)])),\n      (dict(time_index=None), dict(time_index=(1, 10))),\n      (dict(model_name=\"adstock\"), dict(model_name=\"carryover\"),\n       dict(model_name=\"hill_adstock\")))\n  def test_create_attribution_over_spend_fractions_results_are_correct(\n      self, model_name, is_geo_model, media_spend, channel_names, time_index):\n    mmm = _set_up_mock_mmm(model_name, is_geo_model)\n\n    expected_results = pd.DataFrame(\n        np.transpose([np.ones(5)/np.ones(5).sum(),\n                      np.arange(1, 6)/np.arange(1, 6).sum(),\n                      3/np.arange(1, 6)]),\n        index=[f\"channel_{x}\" for x in range(5)],\n        columns=[\"media attribution\", \"media spend\", \"attribution over spend\"])\n\n    aos_fractions_df = plot.create_attribution_over_spend_fractions(\n        mmm, media_spend, channel_names, time_index)\n\n    pd.testing.assert_frame_equal(aos_fractions_df, expected_results, atol=1e-3)\n\n  def test_create_media_baseline_contribution_df_raise_notfittedmodelerror(\n      self):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_media_baseline_contribution_df(\n          media_mix_model=getattr(self, \"not_fitted_mmm\"))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", media_mix_model=\"national_mmm\"),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\")\n  ])\n  def test_create_media_baseline_contribution_df_contributions_add_up_avg_prediction(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    media_channels_baseline_contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm, target_scaler=target_scaler)\n    contribution_pct_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"percentage\" in col\n    ]\n    contribution_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"contribution\" in col\n    ]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_455-505", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "      self, model_name, is_geo_model, media_spend, channel_names, time_index):\n    mmm = _set_up_mock_mmm(model_name, is_geo_model)\n\n    expected_results = pd.DataFrame(\n        np.transpose([np.ones(5)/np.ones(5).sum(),\n                      np.arange(1, 6)/np.arange(1, 6).sum(),\n                      3/np.arange(1, 6)]),\n        index=[f\"channel_{x}\" for x in range(5)],\n        columns=[\"media attribution\", \"media spend\", \"attribution over spend\"])\n\n    aos_fractions_df = plot.create_attribution_over_spend_fractions(\n        mmm, media_spend, channel_names, time_index)\n\n    pd.testing.assert_frame_equal(aos_fractions_df, expected_results, atol=1e-3)\n\n  def test_create_media_baseline_contribution_df_raise_notfittedmodelerror(\n      self):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_media_baseline_contribution_df(\n          media_mix_model=getattr(self, \"not_fitted_mmm\"))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", media_mix_model=\"national_mmm\"),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\")\n  ])\n  def test_create_media_baseline_contribution_df_contributions_add_up_avg_prediction(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    media_channels_baseline_contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm, target_scaler=target_scaler)\n    contribution_pct_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"percentage\" in col\n    ]\n    contribution_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"contribution\" in col\n    ]\n    # test whether contribution percentage sums up to 1 for each period\n    np.testing.assert_array_almost_equal(\n        media_channels_baseline_contribution_df[contribution_pct_cols].sum(\n            axis=1),\n        jnp.repeat(1, media_channels_baseline_contribution_df.shape[0]))\n    # test whether contribution volume sums up to avg predition for each period\n    np.testing.assert_array_almost_equal(\n        np.round(\n            media_channels_baseline_contribution_df[contribution_cols].sum(\n                axis=1), 0),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_465-515", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    aos_fractions_df = plot.create_attribution_over_spend_fractions(\n        mmm, media_spend, channel_names, time_index)\n\n    pd.testing.assert_frame_equal(aos_fractions_df, expected_results, atol=1e-3)\n\n  def test_create_media_baseline_contribution_df_raise_notfittedmodelerror(\n      self):\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      plot.create_media_baseline_contribution_df(\n          media_mix_model=getattr(self, \"not_fitted_mmm\"))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", media_mix_model=\"national_mmm\"),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\")\n  ])\n  def test_create_media_baseline_contribution_df_contributions_add_up_avg_prediction(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    media_channels_baseline_contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm, target_scaler=target_scaler)\n    contribution_pct_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"percentage\" in col\n    ]\n    contribution_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"contribution\" in col\n    ]\n    # test whether contribution percentage sums up to 1 for each period\n    np.testing.assert_array_almost_equal(\n        media_channels_baseline_contribution_df[contribution_pct_cols].sum(\n            axis=1),\n        jnp.repeat(1, media_channels_baseline_contribution_df.shape[0]))\n    # test whether contribution volume sums up to avg predition for each period\n    np.testing.assert_array_almost_equal(\n        np.round(\n            media_channels_baseline_contribution_df[contribution_cols].sum(\n                axis=1), 0),\n        np.round(media_channels_baseline_contribution_df[\"avg_prediction\"], 0))\n\n  def test_create_media_baseline_contribution_df_returns_accurate_contribution_pct(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((1, 3))\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((1, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_475-525", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", media_mix_model=\"national_mmm\"),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\")\n  ])\n  def test_create_media_baseline_contribution_df_contributions_add_up_avg_prediction(\n      self, media_mix_model):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    media_channels_baseline_contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm, target_scaler=target_scaler)\n    contribution_pct_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"percentage\" in col\n    ]\n    contribution_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"contribution\" in col\n    ]\n    # test whether contribution percentage sums up to 1 for each period\n    np.testing.assert_array_almost_equal(\n        media_channels_baseline_contribution_df[contribution_pct_cols].sum(\n            axis=1),\n        jnp.repeat(1, media_channels_baseline_contribution_df.shape[0]))\n    # test whether contribution volume sums up to avg predition for each period\n    np.testing.assert_array_almost_equal(\n        np.round(\n            media_channels_baseline_contribution_df[contribution_cols].sum(\n                axis=1), 0),\n        np.round(media_channels_baseline_contribution_df[\"avg_prediction\"], 0))\n\n  def test_create_media_baseline_contribution_df_returns_accurate_contribution_pct(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((1, 3))\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((1, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 1, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 1)) * 10,\n        \"coef_media\": jnp.ones((500, 3)) * 0.5\n    }\n    expected_contribution_pct = jnp.array([0.05, 0.1, 0.15])\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_485-535", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    media_channels_baseline_contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm, target_scaler=target_scaler)\n    contribution_pct_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"percentage\" in col\n    ]\n    contribution_cols = [\n        col for col in media_channels_baseline_contribution_df.columns\n        if \"contribution\" in col\n    ]\n    # test whether contribution percentage sums up to 1 for each period\n    np.testing.assert_array_almost_equal(\n        media_channels_baseline_contribution_df[contribution_pct_cols].sum(\n            axis=1),\n        jnp.repeat(1, media_channels_baseline_contribution_df.shape[0]))\n    # test whether contribution volume sums up to avg predition for each period\n    np.testing.assert_array_almost_equal(\n        np.round(\n            media_channels_baseline_contribution_df[contribution_cols].sum(\n                axis=1), 0),\n        np.round(media_channels_baseline_contribution_df[\"avg_prediction\"], 0))\n\n  def test_create_media_baseline_contribution_df_returns_accurate_contribution_pct(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((1, 3))\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((1, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 1, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 1)) * 10,\n        \"coef_media\": jnp.ones((500, 3)) * 0.5\n    }\n    expected_contribution_pct = jnp.array([0.05, 0.1, 0.15])\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  def test_create_media_baseline_contribution_df_returns_non_nan_value_for_media_contribution(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.concatenate([jnp.ones((1, 3)), jnp.zeros((1, 3))])\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_495-545", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    # test whether contribution percentage sums up to 1 for each period\n    np.testing.assert_array_almost_equal(\n        media_channels_baseline_contribution_df[contribution_pct_cols].sum(\n            axis=1),\n        jnp.repeat(1, media_channels_baseline_contribution_df.shape[0]))\n    # test whether contribution volume sums up to avg predition for each period\n    np.testing.assert_array_almost_equal(\n        np.round(\n            media_channels_baseline_contribution_df[contribution_cols].sum(\n                axis=1), 0),\n        np.round(media_channels_baseline_contribution_df[\"avg_prediction\"], 0))\n\n  def test_create_media_baseline_contribution_df_returns_accurate_contribution_pct(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((1, 3))\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((1, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 1, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 1)) * 10,\n        \"coef_media\": jnp.ones((500, 3)) * 0.5\n    }\n    expected_contribution_pct = jnp.array([0.05, 0.1, 0.15])\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  def test_create_media_baseline_contribution_df_returns_non_nan_value_for_media_contribution(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.concatenate([jnp.ones((1, 3)), jnp.zeros((1, 3))])\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((2, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.concatenate(\n            [jnp.ones((20, 1, 3)), jnp.zeros((20, 1, 3))], axis=1\n            ) * jnp.arange(1, 4),\n        \"mu\": jnp.concatenate(\n            [jnp.ones((20, 1)), jnp.zeros((20, 1))-1], axis=1)  * 10,\n        \"coef_media\": jnp.ones((20, 3))\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_505-555", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "        np.round(media_channels_baseline_contribution_df[\"avg_prediction\"], 0))\n\n  def test_create_media_baseline_contribution_df_returns_accurate_contribution_pct(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.ones((1, 3))\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((1, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.ones((500, 1, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 1)) * 10,\n        \"coef_media\": jnp.ones((500, 3)) * 0.5\n    }\n    expected_contribution_pct = jnp.array([0.05, 0.1, 0.15])\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  def test_create_media_baseline_contribution_df_returns_non_nan_value_for_media_contribution(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.concatenate([jnp.ones((1, 3)), jnp.zeros((1, 3))])\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((2, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.concatenate(\n            [jnp.ones((20, 1, 3)), jnp.zeros((20, 1, 3))], axis=1\n            ) * jnp.arange(1, 4),\n        \"mu\": jnp.concatenate(\n            [jnp.ones((20, 1)), jnp.zeros((20, 1))-1], axis=1)  * 10,\n        \"coef_media\": jnp.ones((20, 3))\n    }\n    expected_contribution_pct = jnp.array([0.1, 0.2, 0.3, 0, 0, 0])\n    # Columns want to be tested whether they will return nan value.\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n\n    np.testing.assert_array_almost_equal(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_515-565", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "        \"media_transformed\": jnp.ones((500, 1, 3)) * jnp.arange(1, 4),\n        \"mu\": jnp.ones((500, 1)) * 10,\n        \"coef_media\": jnp.ones((500, 3)) * 0.5\n    }\n    expected_contribution_pct = jnp.array([0.05, 0.1, 0.15])\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  def test_create_media_baseline_contribution_df_returns_non_nan_value_for_media_contribution(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.concatenate([jnp.ones((1, 3)), jnp.zeros((1, 3))])\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((2, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.concatenate(\n            [jnp.ones((20, 1, 3)), jnp.zeros((20, 1, 3))], axis=1\n            ) * jnp.arange(1, 4),\n        \"mu\": jnp.concatenate(\n            [jnp.ones((20, 1)), jnp.zeros((20, 1))-1], axis=1)  * 10,\n        \"coef_media\": jnp.ones((20, 3))\n    }\n    expected_contribution_pct = jnp.array([0.1, 0.2, 0.3, 0, 0, 0])\n    # Columns want to be tested whether they will return nan value.\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=1)\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_525-575", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    ]\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  def test_create_media_baseline_contribution_df_returns_non_nan_value_for_media_contribution(\n      self):\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.media = jnp.concatenate([jnp.ones((1, 3)), jnp.zeros((1, 3))])\n    mmm_object._total_costs = jnp.array([2., 1., 3.]) * 15\n    mmm_object._target = jnp.ones((2, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.concatenate(\n            [jnp.ones((20, 1, 3)), jnp.zeros((20, 1, 3))], axis=1\n            ) * jnp.arange(1, 4),\n        \"mu\": jnp.concatenate(\n            [jnp.ones((20, 1)), jnp.zeros((20, 1))-1], axis=1)  * 10,\n        \"coef_media\": jnp.ones((20, 3))\n    }\n    expected_contribution_pct = jnp.array([0.1, 0.2, 0.3, 0, 0, 0])\n    # Columns want to be tested whether they will return nan value.\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=1)\n  ])\n  def test_plot_media_baseline_contribution_area_plot(self, media_mix_model,\n                                                      expected_calls):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    self.assertEqual(self.mock_pd_area_plot.call_count, expected_calls)\n\n  def test_legend_plot_media_baseline_contribution_area_plot(self):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_535-585", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    mmm_object._target = jnp.ones((2, 1)) * 5\n    mmm_object.media_names = [\"channel_0\", \"channel_1\", \"channel_2\"]\n    mmm_object.trace = {\n        \"media_transformed\": jnp.concatenate(\n            [jnp.ones((20, 1, 3)), jnp.zeros((20, 1, 3))], axis=1\n            ) * jnp.arange(1, 4),\n        \"mu\": jnp.concatenate(\n            [jnp.ones((20, 1)), jnp.zeros((20, 1))-1], axis=1)  * 10,\n        \"coef_media\": jnp.ones((20, 3))\n    }\n    expected_contribution_pct = jnp.array([0.1, 0.2, 0.3, 0, 0, 0])\n    # Columns want to be tested whether they will return nan value.\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=1)\n  ])\n  def test_plot_media_baseline_contribution_area_plot(self, media_mix_model,\n                                                      expected_calls):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    self.assertEqual(self.mock_pd_area_plot.call_count, expected_calls)\n\n  def test_legend_plot_media_baseline_contribution_area_plot(self):\n    mmm = getattr(self, \"national_mmm\")\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    call_args, call_kwargs = self.mock_plt_ax_legend.call_args_list[0]\n    self.assertEqual(call_kwargs[\"loc\"], \"center left\")\n    self.assertEqual(call_kwargs[\"bbox_to_anchor\"], (1, 0.5))\n\n    mmm_object = lightweight_mmm.LightweightMMM()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_545-595", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    expected_contribution_pct = jnp.array([0.1, 0.2, 0.3, 0, 0, 0])\n    # Columns want to be tested whether they will return nan value.\n    contribution_percentage_cols = [\n        \"{}_percentage\".format(col) for col in mmm_object.media_names\n    ]\n\n    contribution_df = plot.create_media_baseline_contribution_df(\n        media_mix_model=mmm_object)\n\n    np.testing.assert_array_almost_equal(\n        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=1)\n  ])\n  def test_plot_media_baseline_contribution_area_plot(self, media_mix_model,\n                                                      expected_calls):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    self.assertEqual(self.mock_pd_area_plot.call_count, expected_calls)\n\n  def test_legend_plot_media_baseline_contribution_area_plot(self):\n    mmm = getattr(self, \"national_mmm\")\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    call_args, call_kwargs = self.mock_plt_ax_legend.call_args_list[0]\n    self.assertEqual(call_kwargs[\"loc\"], \"center left\")\n    self.assertEqual(call_kwargs[\"bbox_to_anchor\"], (1, 0.5))\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=np.ones((50, 5)),\n        target=np.ones(50),\n        media_prior=np.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    _ = plot.plot_response_curves(media_mix_model=mmm_object)\n    fig = plot.plot_response_curves(media_mix_model=mmm_object)\n    self.assertIsInstance(fig, matplotlib.figure.Figure)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_555-605", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "        expected_contribution_pct,\n        contribution_df[contribution_percentage_cols].values.flatten().tolist())\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          media_mix_model=\"national_mmm\",\n          expected_calls=1),\n      dict(testcase_name=\"geo\", media_mix_model=\"geo_mmm\", expected_calls=1)\n  ])\n  def test_plot_media_baseline_contribution_area_plot(self, media_mix_model,\n                                                      expected_calls):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    self.assertEqual(self.mock_pd_area_plot.call_count, expected_calls)\n\n  def test_legend_plot_media_baseline_contribution_area_plot(self):\n    mmm = getattr(self, \"national_mmm\")\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    call_args, call_kwargs = self.mock_plt_ax_legend.call_args_list[0]\n    self.assertEqual(call_kwargs[\"loc\"], \"center left\")\n    self.assertEqual(call_kwargs[\"bbox_to_anchor\"], (1, 0.5))\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=np.ones((50, 5)),\n        target=np.ones(50),\n        media_prior=np.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    _ = plot.plot_response_curves(media_mix_model=mmm_object)\n    fig = plot.plot_response_curves(media_mix_model=mmm_object)\n    self.assertIsInstance(fig, matplotlib.figure.Figure)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace_missing\", missing_attribute=\"trace\"),\n      dict(\n          testcase_name=\"weekday_seasonality_missing\",\n          missing_attribute=\"_weekday_seasonality\"),\n      dict(\n          testcase_name=\"custom_priors_missing\",\n          missing_attribute=\"custom_priors\"),\n      dict(testcase_name=\"n_geos_missing\", missing_attribute=\"n_geos\"),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_565-615", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  def test_plot_media_baseline_contribution_area_plot(self, media_mix_model,\n                                                      expected_calls):\n    mmm = getattr(self, media_mix_model)\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    self.assertEqual(self.mock_pd_area_plot.call_count, expected_calls)\n\n  def test_legend_plot_media_baseline_contribution_area_plot(self):\n    mmm = getattr(self, \"national_mmm\")\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    call_args, call_kwargs = self.mock_plt_ax_legend.call_args_list[0]\n    self.assertEqual(call_kwargs[\"loc\"], \"center left\")\n    self.assertEqual(call_kwargs[\"bbox_to_anchor\"], (1, 0.5))\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=np.ones((50, 5)),\n        target=np.ones(50),\n        media_prior=np.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    _ = plot.plot_response_curves(media_mix_model=mmm_object)\n    fig = plot.plot_response_curves(media_mix_model=mmm_object)\n    self.assertIsInstance(fig, matplotlib.figure.Figure)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace_missing\", missing_attribute=\"trace\"),\n      dict(\n          testcase_name=\"weekday_seasonality_missing\",\n          missing_attribute=\"_weekday_seasonality\"),\n      dict(\n          testcase_name=\"custom_priors_missing\",\n          missing_attribute=\"custom_priors\"),\n      dict(testcase_name=\"n_geos_missing\", missing_attribute=\"n_geos\"),\n      dict(\n          testcase_name=\"n_media_channels_missing\",\n          missing_attribute=\"n_media_channels\"),\n      dict(\n          testcase_name=\"media_prior_missing\",\n          missing_attribute=\"_media_prior\"),\n  ])\n  def test_prior_posterior_plot_raises_notfittedmodelerror(\n      self, missing_attribute):\n    mmm = getattr(self, \"not_fitted_mmm\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_575-625", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    mmm = getattr(self, \"national_mmm\")\n    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n    target_scaler.fit(jnp.ones(1))\n    _ = plot.plot_media_baseline_contribution_area_plot(\n        media_mix_model=mmm, target_scaler=target_scaler, legend_outside=True)\n    call_args, call_kwargs = self.mock_plt_ax_legend.call_args_list[0]\n    self.assertEqual(call_kwargs[\"loc\"], \"center left\")\n    self.assertEqual(call_kwargs[\"bbox_to_anchor\"], (1, 0.5))\n\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=np.ones((50, 5)),\n        target=np.ones(50),\n        media_prior=np.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    _ = plot.plot_response_curves(media_mix_model=mmm_object)\n    fig = plot.plot_response_curves(media_mix_model=mmm_object)\n    self.assertIsInstance(fig, matplotlib.figure.Figure)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace_missing\", missing_attribute=\"trace\"),\n      dict(\n          testcase_name=\"weekday_seasonality_missing\",\n          missing_attribute=\"_weekday_seasonality\"),\n      dict(\n          testcase_name=\"custom_priors_missing\",\n          missing_attribute=\"custom_priors\"),\n      dict(testcase_name=\"n_geos_missing\", missing_attribute=\"n_geos\"),\n      dict(\n          testcase_name=\"n_media_channels_missing\",\n          missing_attribute=\"n_media_channels\"),\n      dict(\n          testcase_name=\"media_prior_missing\",\n          missing_attribute=\"_media_prior\"),\n  ])\n  def test_prior_posterior_plot_raises_notfittedmodelerror(\n      self, missing_attribute):\n    mmm = getattr(self, \"not_fitted_mmm\")\n    mmm.trace = jnp.ones((50, 5))\n    mmm._weekday_seasonality = True\n    mmm.custom_priors = None\n    mmm.n_geos = 1\n    mmm.n_media_channels = 3\n    mmm._media_prior = jnp.ones(5) * 50\n\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      delattr(mmm, missing_attribute)\n      plot.plot_prior_and_posterior(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_585-635", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    mmm_object.fit(\n        media=np.ones((50, 5)),\n        target=np.ones(50),\n        media_prior=np.ones(5) * 50,\n        number_warmup=2,\n        number_samples=2,\n        number_chains=1)\n    _ = plot.plot_response_curves(media_mix_model=mmm_object)\n    fig = plot.plot_response_curves(media_mix_model=mmm_object)\n    self.assertIsInstance(fig, matplotlib.figure.Figure)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace_missing\", missing_attribute=\"trace\"),\n      dict(\n          testcase_name=\"weekday_seasonality_missing\",\n          missing_attribute=\"_weekday_seasonality\"),\n      dict(\n          testcase_name=\"custom_priors_missing\",\n          missing_attribute=\"custom_priors\"),\n      dict(testcase_name=\"n_geos_missing\", missing_attribute=\"n_geos\"),\n      dict(\n          testcase_name=\"n_media_channels_missing\",\n          missing_attribute=\"n_media_channels\"),\n      dict(\n          testcase_name=\"media_prior_missing\",\n          missing_attribute=\"_media_prior\"),\n  ])\n  def test_prior_posterior_plot_raises_notfittedmodelerror(\n      self, missing_attribute):\n    mmm = getattr(self, \"not_fitted_mmm\")\n    mmm.trace = jnp.ones((50, 5))\n    mmm._weekday_seasonality = True\n    mmm.custom_priors = None\n    mmm.n_geos = 1\n    mmm.n_media_channels = 3\n    mmm._media_prior = jnp.ones(5) * 50\n\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      delattr(mmm, missing_attribute)\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm, number_of_samples_for_prior=100)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"half_normal_prior\",\n          prior_distribution=dist.HalfNormal(3),\n          expected_clipping_bounds=[0, None]),\n      dict(\n          testcase_name=\"normal_prior\",\n          prior_distribution=dist.Normal(0, 1),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_595-645", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(testcase_name=\"trace_missing\", missing_attribute=\"trace\"),\n      dict(\n          testcase_name=\"weekday_seasonality_missing\",\n          missing_attribute=\"_weekday_seasonality\"),\n      dict(\n          testcase_name=\"custom_priors_missing\",\n          missing_attribute=\"custom_priors\"),\n      dict(testcase_name=\"n_geos_missing\", missing_attribute=\"n_geos\"),\n      dict(\n          testcase_name=\"n_media_channels_missing\",\n          missing_attribute=\"n_media_channels\"),\n      dict(\n          testcase_name=\"media_prior_missing\",\n          missing_attribute=\"_media_prior\"),\n  ])\n  def test_prior_posterior_plot_raises_notfittedmodelerror(\n      self, missing_attribute):\n    mmm = getattr(self, \"not_fitted_mmm\")\n    mmm.trace = jnp.ones((50, 5))\n    mmm._weekday_seasonality = True\n    mmm.custom_priors = None\n    mmm.n_geos = 1\n    mmm.n_media_channels = 3\n    mmm._media_prior = jnp.ones(5) * 50\n\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      delattr(mmm, missing_attribute)\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm, number_of_samples_for_prior=100)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"half_normal_prior\",\n          prior_distribution=dist.HalfNormal(3),\n          expected_clipping_bounds=[0, None]),\n      dict(\n          testcase_name=\"normal_prior\",\n          prior_distribution=dist.Normal(0, 1),\n          expected_clipping_bounds=None),\n      dict(\n          testcase_name=\"beta_prior\",\n          prior_distribution=\"beta distribution placeholder\",\n          expected_clipping_bounds=[0, 1]),\n      dict(\n          testcase_name=\"gamma_prior\",\n          prior_distribution=dist.Gamma(0.5),\n          expected_clipping_bounds=[0, None]),\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_605-655", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "      dict(\n          testcase_name=\"n_media_channels_missing\",\n          missing_attribute=\"n_media_channels\"),\n      dict(\n          testcase_name=\"media_prior_missing\",\n          missing_attribute=\"_media_prior\"),\n  ])\n  def test_prior_posterior_plot_raises_notfittedmodelerror(\n      self, missing_attribute):\n    mmm = getattr(self, \"not_fitted_mmm\")\n    mmm.trace = jnp.ones((50, 5))\n    mmm._weekday_seasonality = True\n    mmm.custom_priors = None\n    mmm.n_geos = 1\n    mmm.n_media_channels = 3\n    mmm._media_prior = jnp.ones(5) * 50\n\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      delattr(mmm, missing_attribute)\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm, number_of_samples_for_prior=100)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"half_normal_prior\",\n          prior_distribution=dist.HalfNormal(3),\n          expected_clipping_bounds=[0, None]),\n      dict(\n          testcase_name=\"normal_prior\",\n          prior_distribution=dist.Normal(0, 1),\n          expected_clipping_bounds=None),\n      dict(\n          testcase_name=\"beta_prior\",\n          prior_distribution=\"beta distribution placeholder\",\n          expected_clipping_bounds=[0, 1]),\n      dict(\n          testcase_name=\"gamma_prior\",\n          prior_distribution=dist.Gamma(0.5),\n          expected_clipping_bounds=[0, None]),\n  ])\n  def test_prior_posterior_plot_clipping_bounds_for_kdeplots(\n      self, prior_distribution, expected_clipping_bounds):\n    fig = plt.figure()\n    gridspec_fig = matplotlib.gridspec.GridSpec(\n        nrows=1, ncols=1, figure=fig, hspace=10)\n\n    # dist.Beta() calls jnp.broadcast_to() upon instantiation, so this\n    # distribution can't be left in the decorator or it raises a\n    # RuntimeError: \"Attempted call to JAX before absl.app.run() is called\".\n    # Thus we have to instantiate it inside the unit test instead.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_615-665", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    mmm.trace = jnp.ones((50, 5))\n    mmm._weekday_seasonality = True\n    mmm.custom_priors = None\n    mmm.n_geos = 1\n    mmm.n_media_channels = 3\n    mmm._media_prior = jnp.ones(5) * 50\n\n    with self.assertRaises(lightweight_mmm.NotFittedModelError):\n      delattr(mmm, missing_attribute)\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm, number_of_samples_for_prior=100)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"half_normal_prior\",\n          prior_distribution=dist.HalfNormal(3),\n          expected_clipping_bounds=[0, None]),\n      dict(\n          testcase_name=\"normal_prior\",\n          prior_distribution=dist.Normal(0, 1),\n          expected_clipping_bounds=None),\n      dict(\n          testcase_name=\"beta_prior\",\n          prior_distribution=\"beta distribution placeholder\",\n          expected_clipping_bounds=[0, 1]),\n      dict(\n          testcase_name=\"gamma_prior\",\n          prior_distribution=dist.Gamma(0.5),\n          expected_clipping_bounds=[0, None]),\n  ])\n  def test_prior_posterior_plot_clipping_bounds_for_kdeplots(\n      self, prior_distribution, expected_clipping_bounds):\n    fig = plt.figure()\n    gridspec_fig = matplotlib.gridspec.GridSpec(\n        nrows=1, ncols=1, figure=fig, hspace=10)\n\n    # dist.Beta() calls jnp.broadcast_to() upon instantiation, so this\n    # distribution can't be left in the decorator or it raises a\n    # RuntimeError: \"Attempted call to JAX before absl.app.run() is called\".\n    # Thus we have to instantiate it inside the unit test instead.\n    if prior_distribution == \"beta distribution placeholder\":\n      prior_distribution = dist.Beta(0.5, 0.5)\n\n    plot._make_prior_and_posterior_subplot_for_one_feature(\n        prior_distribution=prior_distribution,\n        posterior_samples=jnp.ones(50),\n        subplot_title=\"title\",\n        fig=fig,\n        gridspec_fig=gridspec_fig,\n        i_ax=0, number_of_samples_for_prior=100)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_625-675", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          media_mix_model=mmm, number_of_samples_for_prior=100)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"half_normal_prior\",\n          prior_distribution=dist.HalfNormal(3),\n          expected_clipping_bounds=[0, None]),\n      dict(\n          testcase_name=\"normal_prior\",\n          prior_distribution=dist.Normal(0, 1),\n          expected_clipping_bounds=None),\n      dict(\n          testcase_name=\"beta_prior\",\n          prior_distribution=\"beta distribution placeholder\",\n          expected_clipping_bounds=[0, 1]),\n      dict(\n          testcase_name=\"gamma_prior\",\n          prior_distribution=dist.Gamma(0.5),\n          expected_clipping_bounds=[0, None]),\n  ])\n  def test_prior_posterior_plot_clipping_bounds_for_kdeplots(\n      self, prior_distribution, expected_clipping_bounds):\n    fig = plt.figure()\n    gridspec_fig = matplotlib.gridspec.GridSpec(\n        nrows=1, ncols=1, figure=fig, hspace=10)\n\n    # dist.Beta() calls jnp.broadcast_to() upon instantiation, so this\n    # distribution can't be left in the decorator or it raises a\n    # RuntimeError: \"Attempted call to JAX before absl.app.run() is called\".\n    # Thus we have to instantiate it inside the unit test instead.\n    if prior_distribution == \"beta distribution placeholder\":\n      prior_distribution = dist.Beta(0.5, 0.5)\n\n    plot._make_prior_and_posterior_subplot_for_one_feature(\n        prior_distribution=prior_distribution,\n        posterior_samples=jnp.ones(50),\n        subplot_title=\"title\",\n        fig=fig,\n        gridspec_fig=gridspec_fig,\n        i_ax=0, number_of_samples_for_prior=100)\n    call_details = self.mock_sns_kdeplot.call_args_list\n    called_clipping_bounds = call_details[0][1][\"clip\"]\n\n    self.assertEqual(called_clipping_bounds, expected_clipping_bounds)\n\n  @parameterized.product(\n      (dict(\n          is_geo_model=False,\n          has_extra_features=False,\n          extra_expected_number_of_subplots=0),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_635-685", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "          expected_clipping_bounds=None),\n      dict(\n          testcase_name=\"beta_prior\",\n          prior_distribution=\"beta distribution placeholder\",\n          expected_clipping_bounds=[0, 1]),\n      dict(\n          testcase_name=\"gamma_prior\",\n          prior_distribution=dist.Gamma(0.5),\n          expected_clipping_bounds=[0, None]),\n  ])\n  def test_prior_posterior_plot_clipping_bounds_for_kdeplots(\n      self, prior_distribution, expected_clipping_bounds):\n    fig = plt.figure()\n    gridspec_fig = matplotlib.gridspec.GridSpec(\n        nrows=1, ncols=1, figure=fig, hspace=10)\n\n    # dist.Beta() calls jnp.broadcast_to() upon instantiation, so this\n    # distribution can't be left in the decorator or it raises a\n    # RuntimeError: \"Attempted call to JAX before absl.app.run() is called\".\n    # Thus we have to instantiate it inside the unit test instead.\n    if prior_distribution == \"beta distribution placeholder\":\n      prior_distribution = dist.Beta(0.5, 0.5)\n\n    plot._make_prior_and_posterior_subplot_for_one_feature(\n        prior_distribution=prior_distribution,\n        posterior_samples=jnp.ones(50),\n        subplot_title=\"title\",\n        fig=fig,\n        gridspec_fig=gridspec_fig,\n        i_ax=0, number_of_samples_for_prior=100)\n    call_details = self.mock_sns_kdeplot.call_args_list\n    called_clipping_bounds = call_details[0][1][\"clip\"]\n\n    self.assertEqual(called_clipping_bounds, expected_clipping_bounds)\n\n  @parameterized.product(\n      (dict(\n          is_geo_model=False,\n          has_extra_features=False,\n          extra_expected_number_of_subplots=0),\n       dict(\n           is_geo_model=False,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=2),\n       dict(\n           is_geo_model=True,\n           has_extra_features=False,\n           extra_expected_number_of_subplots=14),\n       dict(\n           is_geo_model=True,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_645-695", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "  def test_prior_posterior_plot_clipping_bounds_for_kdeplots(\n      self, prior_distribution, expected_clipping_bounds):\n    fig = plt.figure()\n    gridspec_fig = matplotlib.gridspec.GridSpec(\n        nrows=1, ncols=1, figure=fig, hspace=10)\n\n    # dist.Beta() calls jnp.broadcast_to() upon instantiation, so this\n    # distribution can't be left in the decorator or it raises a\n    # RuntimeError: \"Attempted call to JAX before absl.app.run() is called\".\n    # Thus we have to instantiate it inside the unit test instead.\n    if prior_distribution == \"beta distribution placeholder\":\n      prior_distribution = dist.Beta(0.5, 0.5)\n\n    plot._make_prior_and_posterior_subplot_for_one_feature(\n        prior_distribution=prior_distribution,\n        posterior_samples=jnp.ones(50),\n        subplot_title=\"title\",\n        fig=fig,\n        gridspec_fig=gridspec_fig,\n        i_ax=0, number_of_samples_for_prior=100)\n    call_details = self.mock_sns_kdeplot.call_args_list\n    called_clipping_bounds = call_details[0][1][\"clip\"]\n\n    self.assertEqual(called_clipping_bounds, expected_clipping_bounds)\n\n  @parameterized.product(\n      (dict(\n          is_geo_model=False,\n          has_extra_features=False,\n          extra_expected_number_of_subplots=0),\n       dict(\n           is_geo_model=False,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=2),\n       dict(\n           is_geo_model=True,\n           has_extra_features=False,\n           extra_expected_number_of_subplots=14),\n       dict(\n           is_geo_model=True,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=20)),\n      (dict(model_name=\"adstock\", base_expected_number_of_subplots=25),\n       dict(model_name=\"carryover\", base_expected_number_of_subplots=30),\n       dict(model_name=\"hill_adstock\", base_expected_number_of_subplots=30)))\n  def test_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, is_geo_model, has_extra_features,\n      base_expected_number_of_subplots, extra_expected_number_of_subplots):\n    expected_number_of_subplots = (\n        base_expected_number_of_subplots + extra_expected_number_of_subplots)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_655-705", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    if prior_distribution == \"beta distribution placeholder\":\n      prior_distribution = dist.Beta(0.5, 0.5)\n\n    plot._make_prior_and_posterior_subplot_for_one_feature(\n        prior_distribution=prior_distribution,\n        posterior_samples=jnp.ones(50),\n        subplot_title=\"title\",\n        fig=fig,\n        gridspec_fig=gridspec_fig,\n        i_ax=0, number_of_samples_for_prior=100)\n    call_details = self.mock_sns_kdeplot.call_args_list\n    called_clipping_bounds = call_details[0][1][\"clip\"]\n\n    self.assertEqual(called_clipping_bounds, expected_clipping_bounds)\n\n  @parameterized.product(\n      (dict(\n          is_geo_model=False,\n          has_extra_features=False,\n          extra_expected_number_of_subplots=0),\n       dict(\n           is_geo_model=False,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=2),\n       dict(\n           is_geo_model=True,\n           has_extra_features=False,\n           extra_expected_number_of_subplots=14),\n       dict(\n           is_geo_model=True,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=20)),\n      (dict(model_name=\"adstock\", base_expected_number_of_subplots=25),\n       dict(model_name=\"carryover\", base_expected_number_of_subplots=30),\n       dict(model_name=\"hill_adstock\", base_expected_number_of_subplots=30)))\n  def test_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, is_geo_model, has_extra_features,\n      base_expected_number_of_subplots, extra_expected_number_of_subplots):\n    expected_number_of_subplots = (\n        base_expected_number_of_subplots + extra_expected_number_of_subplots)\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n    if not has_extra_features:\n      del mmm.trace[\"coef_extra_features\"]\n    mmm._extra_features = jnp.ones_like(\n        mmm.trace[\"coef_extra_features\"][0]) if has_extra_features else None\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm, number_of_samples_for_prior=10, seed=0)\n\n    # each subplot gets two calls, one for the prior and one for the posterior", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_665-715", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    call_details = self.mock_sns_kdeplot.call_args_list\n    called_clipping_bounds = call_details[0][1][\"clip\"]\n\n    self.assertEqual(called_clipping_bounds, expected_clipping_bounds)\n\n  @parameterized.product(\n      (dict(\n          is_geo_model=False,\n          has_extra_features=False,\n          extra_expected_number_of_subplots=0),\n       dict(\n           is_geo_model=False,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=2),\n       dict(\n           is_geo_model=True,\n           has_extra_features=False,\n           extra_expected_number_of_subplots=14),\n       dict(\n           is_geo_model=True,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=20)),\n      (dict(model_name=\"adstock\", base_expected_number_of_subplots=25),\n       dict(model_name=\"carryover\", base_expected_number_of_subplots=30),\n       dict(model_name=\"hill_adstock\", base_expected_number_of_subplots=30)))\n  def test_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, is_geo_model, has_extra_features,\n      base_expected_number_of_subplots, extra_expected_number_of_subplots):\n    expected_number_of_subplots = (\n        base_expected_number_of_subplots + extra_expected_number_of_subplots)\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n    if not has_extra_features:\n      del mmm.trace[\"coef_extra_features\"]\n    mmm._extra_features = jnp.ones_like(\n        mmm.trace[\"coef_extra_features\"][0]) if has_extra_features else None\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm, number_of_samples_for_prior=10, seed=0)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.product(\n      (dict(is_geo_model=True, expected_number_of_subplots=11),\n       dict(is_geo_model=False, expected_number_of_subplots=7)),\n      (dict(model_name=\"adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"carryover\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_675-725", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "       dict(\n           is_geo_model=False,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=2),\n       dict(\n           is_geo_model=True,\n           has_extra_features=False,\n           extra_expected_number_of_subplots=14),\n       dict(\n           is_geo_model=True,\n           has_extra_features=True,\n           extra_expected_number_of_subplots=20)),\n      (dict(model_name=\"adstock\", base_expected_number_of_subplots=25),\n       dict(model_name=\"carryover\", base_expected_number_of_subplots=30),\n       dict(model_name=\"hill_adstock\", base_expected_number_of_subplots=30)))\n  def test_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, is_geo_model, has_extra_features,\n      base_expected_number_of_subplots, extra_expected_number_of_subplots):\n    expected_number_of_subplots = (\n        base_expected_number_of_subplots + extra_expected_number_of_subplots)\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n    if not has_extra_features:\n      del mmm.trace[\"coef_extra_features\"]\n    mmm._extra_features = jnp.ones_like(\n        mmm.trace[\"coef_extra_features\"][0]) if has_extra_features else None\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm, number_of_samples_for_prior=10, seed=0)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.product(\n      (dict(is_geo_model=True, expected_number_of_subplots=11),\n       dict(is_geo_model=False, expected_number_of_subplots=7)),\n      (dict(model_name=\"adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"carryover\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"hill_adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"slope\"]))\n      )\n  def test_selected_features_for_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, selected_features, is_geo_model,\n      expected_number_of_subplots):\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_685-735", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "           has_extra_features=True,\n           extra_expected_number_of_subplots=20)),\n      (dict(model_name=\"adstock\", base_expected_number_of_subplots=25),\n       dict(model_name=\"carryover\", base_expected_number_of_subplots=30),\n       dict(model_name=\"hill_adstock\", base_expected_number_of_subplots=30)))\n  def test_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, is_geo_model, has_extra_features,\n      base_expected_number_of_subplots, extra_expected_number_of_subplots):\n    expected_number_of_subplots = (\n        base_expected_number_of_subplots + extra_expected_number_of_subplots)\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n    if not has_extra_features:\n      del mmm.trace[\"coef_extra_features\"]\n    mmm._extra_features = jnp.ones_like(\n        mmm.trace[\"coef_extra_features\"][0]) if has_extra_features else None\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm, number_of_samples_for_prior=10, seed=0)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.product(\n      (dict(is_geo_model=True, expected_number_of_subplots=11),\n       dict(is_geo_model=False, expected_number_of_subplots=7)),\n      (dict(model_name=\"adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"carryover\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"hill_adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"slope\"]))\n      )\n  def test_selected_features_for_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, selected_features, is_geo_model,\n      expected_number_of_subplots):\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm,\n        number_of_samples_for_prior=10,\n        seed=0,\n        selected_features=selected_features)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_695-745", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n    if not has_extra_features:\n      del mmm.trace[\"coef_extra_features\"]\n    mmm._extra_features = jnp.ones_like(\n        mmm.trace[\"coef_extra_features\"][0]) if has_extra_features else None\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm, number_of_samples_for_prior=10, seed=0)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.product(\n      (dict(is_geo_model=True, expected_number_of_subplots=11),\n       dict(is_geo_model=False, expected_number_of_subplots=7)),\n      (dict(model_name=\"adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"carryover\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"hill_adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"slope\"]))\n      )\n  def test_selected_features_for_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, selected_features, is_geo_model,\n      expected_number_of_subplots):\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm,\n        number_of_samples_for_prior=10,\n        seed=0,\n        selected_features=selected_features)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_model\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_model\",\n          model_name=\"geo_mmm\"),\n  ])\n  def test_selected_features_raises_value_error(self, model_name):\n    mmm = getattr(self, model_name)\n\n    # The expected error message here is an f-string whose value is filled in by", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_705-755", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.product(\n      (dict(is_geo_model=True, expected_number_of_subplots=11),\n       dict(is_geo_model=False, expected_number_of_subplots=7)),\n      (dict(model_name=\"adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"carryover\",\n            selected_features=[\"sigma\", \"intercept\", \"exponent\"]),\n       dict(model_name=\"hill_adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"slope\"]))\n      )\n  def test_selected_features_for_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, selected_features, is_geo_model,\n      expected_number_of_subplots):\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm,\n        number_of_samples_for_prior=10,\n        seed=0,\n        selected_features=selected_features)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_model\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_model\",\n          model_name=\"geo_mmm\"),\n  ])\n  def test_selected_features_raises_value_error(self, model_name):\n    mmm = getattr(self, model_name)\n\n    # The expected error message here is an f-string whose value is filled in by\n    # a set that contains a single element. The message should look exactly as\n    # it appears below; no special regex parsing should be happening here.\n    expected_error = (\"Selected_features {'misspelled_feature'}\"\n                      \" not in media_mix_model.\")\n\n    with self.assertRaisesRegex(ValueError, expected_regex=expected_error):\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm,\n          number_of_samples_for_prior=10,\n          selected_features=[", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_715-760", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "       dict(model_name=\"hill_adstock\",\n            selected_features=[\"sigma\", \"intercept\", \"slope\"]))\n      )\n  def test_selected_features_for_prior_posterior_plot_makes_correct_number_of_subplots(\n      self, model_name, selected_features, is_geo_model,\n      expected_number_of_subplots):\n    mmm = _set_up_mock_mmm(model_name=model_name, is_geo_model=is_geo_model)\n\n    plot.plot_prior_and_posterior(\n        media_mix_model=mmm,\n        number_of_samples_for_prior=10,\n        seed=0,\n        selected_features=selected_features)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_model\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_model\",\n          model_name=\"geo_mmm\"),\n  ])\n  def test_selected_features_raises_value_error(self, model_name):\n    mmm = getattr(self, model_name)\n\n    # The expected error message here is an f-string whose value is filled in by\n    # a set that contains a single element. The message should look exactly as\n    # it appears below; no special regex parsing should be happening here.\n    expected_error = (\"Selected_features {'misspelled_feature'}\"\n                      \" not in media_mix_model.\")\n\n    with self.assertRaisesRegex(ValueError, expected_regex=expected_error):\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm,\n          number_of_samples_for_prior=10,\n          selected_features=[\n              \"sigma\", \"intercept\", \"misspelled_feature\"\n          ])\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 760, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-plot_test.py_725-760", "title": "google_lightweight_mmm-lightweight_mmm-plot_test.py", "text": "        number_of_samples_for_prior=10,\n        seed=0,\n        selected_features=selected_features)\n\n    # each subplot gets two calls, one for the prior and one for the posterior\n    number_of_subplots_created = self.mock_sns_kdeplot.call_count / 2\n    self.assertEqual(number_of_subplots_created, expected_number_of_subplots)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_model\",\n          model_name=\"national_mmm\"),\n      dict(\n          testcase_name=\"geo_model\",\n          model_name=\"geo_mmm\"),\n  ])\n  def test_selected_features_raises_value_error(self, model_name):\n    mmm = getattr(self, model_name)\n\n    # The expected error message here is an f-string whose value is filled in by\n    # a set that contains a single element. The message should look exactly as\n    # it appears below; no special regex parsing should be happening here.\n    expected_error = (\"Selected_features {'misspelled_feature'}\"\n                      \" not in media_mix_model.\")\n\n    with self.assertRaisesRegex(ValueError, expected_regex=expected_error):\n      plot.plot_prior_and_posterior(\n          media_mix_model=mmm,\n          number_of_samples_for_prior=10,\n          selected_features=[\n              \"sigma\", \"intercept\", \"misspelled_feature\"\n          ])\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "plot_test.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 760, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for preprocessing dataset for training LightweightMMM.\"\"\"\n\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for preprocessing dataset for training LightweightMMM.\"\"\"\n\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for preprocessing dataset for training LightweightMMM.\"\"\"\n\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n\n  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for preprocessing dataset for training LightweightMMM.\"\"\"\n\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n\n  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any\n  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\nimport copy\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport jax.numpy as jnp\nimport pandas as pd\nfrom sklearn import base\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n\n  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any\n  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.\n    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "from lightweight_mmm.core import core_utils\n\n\nclass NotFittedScalerError(Exception):\n  pass\n\n\nclass CustomScaler(base.TransformerMixin):\n  \"\"\"Class to scale your data based on multiplications and divisions.\n\n  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any\n  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.\n    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "  This scaler can be used in two fashions for both the multiplication and\n  division operation.\n  - By specifying a value to use for the scaling operation.\n  - By specifying an operation used at column level to calculate the value\n    for the actual scaling operation.\n\n  Eg. if one wants to scale the dataset by multiply by 100 you can directly\n  pass multiply_by=100. Value can also be an array with as many values\n  as column has the data being scaled. But if you want to multiply by the mean\n  value of each column, then you can pass multiply_operation=jnp.mean (or any\n  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.\n    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "  other operation desired).\n\n  Operation parameters have the upper hand in the cases where both values and\n  operations are passed, values will be ignored in this case.\n\n  Scaler must be fit first in order to call the transform method.\n\n  Attributes.\n    divide_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for division during scaling.\n    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    divide_by: Numbers(s) by which to divide data in the scaling process. Since\n      the scaler is applied to axis 0 of the data, the shape of divide_by must\n      be consistent with division into the data. For example, if data.shape =\n      (100, 3, 5) then divide_by.shape can be (3, 5) or (5,) or a number. If\n      divide_operation is given, this divide_by value will be ignored.\n    multiply_operation: Operation to apply over axis 0 of the fitting data to\n      obtain the value that will be used for multiplication during scaling.\n    multiply_by: Numbers(s) by which to multiply data in the scaling process.\n      Since the scaler is applied to axis 0 of the data, the shape of\n      multiply_by must be consistent with multiplication into the data. For\n      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      example, if data.shape = (100, 3, 5) then multiply_by.shape can be (3, 5)\n      or (5,) or a number. If multiply_operation is given, this multiply_by\n      value will be ignored.\n  \"\"\"\n\n  def __init__(\n      self,\n      divide_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      divide_by: Optional[Union[float, int, jnp.ndarray]] = 1,\n      multiply_operation: Optional[Callable[[jnp.ndarray], jnp.float32]] = None,\n      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      multiply_by: Optional[Union[float, int, jnp.ndarray]] = 1.) -> None:\n    \"\"\"Constructor for the CustomScaler class.\"\"\"\n    if all([\n        divide_by is None, divide_operation is None, multiply_by is None,\n        multiply_operation is None\n    ]):\n      raise ValueError(\"No values for transformations were provided and this \"\n                       \"scaler will fail. Please instantiate a valid one\")\n\n    if divide_operation is None and divide_by is None:\n      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      raise ValueError(\n          \"Either a division operation or value needs to be passed. If \"\n          \"you dont want to use a division to scale your data just \"\n          \"pass divide_by=1.\")\n    elif divide_operation is not None:\n      self.divide_operation = divide_operation\n    else:\n      self.divide_by = divide_by\n\n    if multiply_operation is None and multiply_by is None:\n      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      raise ValueError(\n          \"Either a multiplication operation or value needs to be passed. If \"\n          \"you dont want to use a multiplication to scale your data just \"\n          \"pass multiply_by=1.\")\n    elif multiply_operation is not None:\n      self.multiply_operation = multiply_operation\n    else:\n      self.multiply_by = multiply_by\n\n  def fit(self, data: jnp.ndarray) -> None:\n    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    \"\"\"Figures out values for transformations based on the specified operations.\n\n    Args:\n      data: Input dataset to use for fitting.\n    \"\"\"\n    if hasattr(self, \"divide_operation\"):\n      self.divide_by = jnp.apply_along_axis(\n          func1d=self.divide_operation, axis=0, arr=data)\n    elif isinstance(self.divide_by, int) or isinstance(self.divide_by, float):\n      self.divide_by = self.divide_by * jnp.ones(data.shape[1:])\n    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    if hasattr(self, \"multiply_operation\"):\n      self.multiply_by = jnp.apply_along_axis(\n          func1d=self.multiply_operation, axis=0, arr=data)\n    elif isinstance(self.multiply_by, int) or isinstance(\n        self.multiply_by, float):\n      self.multiply_by = self.multiply_by * jnp.ones(data.shape[1:])\n\n  def transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies transformation based on fitted values.\n\n    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    It can only be called if scaler was fit first.\n\n    Args:\n      data: Input dataset to transform.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    if not hasattr(self, \"divide_by\") or not hasattr(self, \"multiply_by\"):\n      raise NotFittedScalerError(\n          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "          \"transform is called without fit being called previously. Please \"\n          \"fit scaler first.\")\n    return self.multiply_by * data / self.divide_by\n\n  def fit_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Fits the values and applies transformation to the input data.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    Returns:\n      Transformed array.\n    \"\"\"\n    self.fit(data)\n    return self.transform(data)\n\n  def inverse_transform(self, data: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Runs inverse transformation to get original values.\n\n    Args:\n      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      data: Input dataset.\n\n    Returns:\n      Dataset with the inverse transformation applied.\n    \"\"\"\n    return self.divide_by * data / self.multiply_by\n\n\ndef _compute_correlations(\n    features: jnp.ndarray,\n    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    target: jnp.ndarray,\n    feature_names: List[str],\n    ) -> List[pd.DataFrame]:\n  \"\"\"Computes feature-feature and feature-target correlations.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    target: Target variable for media mix model.\n    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    feature_names: Names of media channels to be added to the output dataframes.\n\n  Returns:\n    List of dataframes containing Pearson correlation coefficients between each\n      feature, as well as between features and the target variable. For\n      national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n\n  Raises:\n    ValueError: If features and target have incompatible shapes (e.g. one is\n      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      geo-level and the other national-level).\n  \"\"\"\n  if not ((features.ndim == 2 and target.ndim == 1) or\n          (features.ndim == 3 and target.ndim == 2)):\n    raise ValueError(f\"Incompatible shapes between features {features.shape}\"\n                     f\" and target {target.shape}.\")\n\n  number_of_geos = core_utils.get_number_geos(features)\n  correlation_matrix_output = []\n  for i_geo in range(number_of_geos):\n\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n    if number_of_geos == 1:\n      features_and_target = jnp.concatenate(\n          [features, jnp.expand_dims(target, axis=1)], axis=1)\n    else:\n      features_and_target = jnp.concatenate(\n          [features[:, :, i_geo],\n           jnp.expand_dims(target[:, i_geo], axis=1)],\n          axis=1)\n\n    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    covariance_matrix = jnp.cov(features_and_target, rowvar=False)\n    standard_deviations = jnp.std(features_and_target, axis=0, ddof=1)\n    correlation_matrix = covariance_matrix / jnp.outer(standard_deviations,\n                                                       standard_deviations)\n    correlation_matrix = pd.DataFrame(\n        correlation_matrix,\n        columns=feature_names + [\"target\"],\n        index=feature_names + [\"target\"],\n        dtype=float)\n    correlation_matrix_output.append(correlation_matrix)\n\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n  return correlation_matrix_output\n\n\ndef _compute_variances(\n    features: jnp.ndarray,\n    feature_names: Sequence[str],\n    geo_names: Sequence[str],\n) -> pd.DataFrame:\n  \"\"\"Computes variances over time for each feature.\n\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n  In general, higher variance is better since it creates more signal for the\n  regression analysis. However, if the features have not been scaled (divided by\n  the mean), then the variance can take any value and this analysis is not\n  meaningful.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n  Returns:\n    Dataframe containing the variance over time for each feature. This dataframe\n      contains one row per geo, and just a single row for national data.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  variances_as_series = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    variances_as_series.append(\n        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n\n\ndef _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "        pd.DataFrame(data=features_for_this_geo).var(axis=0, ddof=0))\n\n  variances = pd.concat(variances_as_series, axis=1)\n  variances.columns = geo_names\n  variances.index = copy.copy(feature_names)\n\n  return variances\n\n\ndef _compute_spend_fractions(\n    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n\n\ndef _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:\n  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    cost_data: jnp.ndarray,\n    channel_names: Optional[Sequence[str]] = None,\n    output_column_name: str = \"fraction of spend\") -> pd.DataFrame:\n  \"\"\"Computes fraction of total spend for each media channel.\n\n  Args:\n    cost_data: Spend (can be normalized or not) per channel.\n    channel_names: Names of media channels to be added to the output dataframe.\n    output_column_name: Name of the column in the output dataframe, denoting the\n      fraction of the total spend in each media channel.\n\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n\n\ndef _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:\n  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n  Returns:\n    Dataframe containing fraction of the total spend in each channel.\n\n  Raises:\n    ValueError if any of the costs are zero or negative.\n  \"\"\"\n  cost_df = pd.DataFrame(\n      cost_data, index=channel_names, columns=[output_column_name])\n\n  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n\n\ndef _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:\n  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "  if (cost_df[output_column_name] <= 0).any():\n    raise ValueError(\"Values in cost_data must all be positive.\")\n\n  normalized_cost_df = cost_df.div(cost_df.sum(axis=0), axis=1).round(4)\n  return normalized_cost_df\n\n\ndef _compute_variance_inflation_factors(\n    features: jnp.ndarray, feature_names: Sequence[str],\n    geo_names: Sequence[str]) -> pd.DataFrame:\n  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")\n\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "  \"\"\"Computes variance inflation factors for all features.\n\n  Helper function for DataQualityCheck.\n\n  Args:\n    features: Features for media mix model (media and non-media variables).\n    feature_names: Names of media channels to be added to the output dataframe.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")\n\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)\n  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n\n\ndef check_data_quality(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    cost_data: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    Dataframe containing variance inflation factors for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of geos in features does not match the number of\n    supplied geo_names.\n  \"\"\"\n  number_of_geos = core_utils.get_number_geos(features)\n\n  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")\n\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)\n  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n\n\ndef check_data_quality(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    cost_data: jnp.ndarray,\n    extra_features_data: Optional[jnp.ndarray] = None,\n    channel_names: Optional[Sequence[str]] = None,\n    extra_features_names: Optional[Sequence[str]] = None,\n    geo_names: Optional[Sequence[str]] = None,\n    ) -> Tuple[List[pd.DataFrame], pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n  \"\"\"Checks LMMM data quality, to be used before fitting a model.\n\n  Args:\n    media_data: National-level or geo-level media impressions data, such as\n      media_data_train or media_data in the example Collaboratory. This dataset", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "  if len(geo_names) != number_of_geos:\n    raise ValueError(\"The number of geos in features does not match the length \"\n                     \"of geo_names\")\n\n  vifs_for_each_geo = []\n  for i_geo in range(number_of_geos):\n    features_for_this_geo = features[...,\n                                     i_geo] if number_of_geos > 1 else features\n    features_for_this_geo = add_constant(\n        pd.DataFrame(features_for_this_geo, dtype=float), has_constant=\"skip\")\n\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)\n  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n\n\ndef check_data_quality(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    cost_data: jnp.ndarray,\n    extra_features_data: Optional[jnp.ndarray] = None,\n    channel_names: Optional[Sequence[str]] = None,\n    extra_features_names: Optional[Sequence[str]] = None,\n    geo_names: Optional[Sequence[str]] = None,\n    ) -> Tuple[List[pd.DataFrame], pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n  \"\"\"Checks LMMM data quality, to be used before fitting a model.\n\n  Args:\n    media_data: National-level or geo-level media impressions data, such as\n      media_data_train or media_data in the example Collaboratory. This dataset\n      should be scaled so that it has a similar order of magnitude to the\n      target_data and extra_features_data (if applicable).\n    target_data: National-level or geo-level sales or revenue data, such as\n      target_train or target in the example Colabs. This dataset should be\n      scaled so that it has a similar order of magnitude to the media_data and\n      extra_features_data (if applicable).\n    cost_data: National-level cost data, identified as \"costs\" in the example\n      Colabs, with one value per media channel denoting the total cost for that\n      channel over the time period covered by the media_data. The costs can be\n      scaled (mean-normalized) or not scaled.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "\n    vifs_for_this_geo = []\n    for i, feature in enumerate(features_for_this_geo.columns):\n      if feature != \"const\":\n        vifs_for_this_geo.append(\n            variance_inflation_factor(features_for_this_geo.values, i))\n\n    vifs_for_each_geo.append(vifs_for_this_geo)\n\n  vif_df = pd.DataFrame(data=zip(*vifs_for_each_geo), dtype=float)\n  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n\n\ndef check_data_quality(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    cost_data: jnp.ndarray,\n    extra_features_data: Optional[jnp.ndarray] = None,\n    channel_names: Optional[Sequence[str]] = None,\n    extra_features_names: Optional[Sequence[str]] = None,\n    geo_names: Optional[Sequence[str]] = None,\n    ) -> Tuple[List[pd.DataFrame], pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n  \"\"\"Checks LMMM data quality, to be used before fitting a model.\n\n  Args:\n    media_data: National-level or geo-level media impressions data, such as\n      media_data_train or media_data in the example Collaboratory. This dataset\n      should be scaled so that it has a similar order of magnitude to the\n      target_data and extra_features_data (if applicable).\n    target_data: National-level or geo-level sales or revenue data, such as\n      target_train or target in the example Colabs. This dataset should be\n      scaled so that it has a similar order of magnitude to the media_data and\n      extra_features_data (if applicable).\n    cost_data: National-level cost data, identified as \"costs\" in the example\n      Colabs, with one value per media channel denoting the total cost for that\n      channel over the time period covered by the media_data. The costs can be\n      scaled (mean-normalized) or not scaled.\n    extra_features_data: Optional national-level or geo-level extra features\n      data, such as extra_features_train or extra_features in the example\n      Colabs. This dataset should be scaled so that it has a similar order of\n      magnitude to the media_data and target_data.\n    channel_names: Names of media channels to be added to the output dataframes.\n    extra_features_names: Names of extra features to be added to the output\n      dataframes.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "  vif_df.columns = geo_names\n  vif_df.index = copy.copy(feature_names)\n\n  return vif_df\n\n\ndef check_data_quality(\n    media_data: jnp.ndarray,\n    target_data: jnp.ndarray,\n    cost_data: jnp.ndarray,\n    extra_features_data: Optional[jnp.ndarray] = None,\n    channel_names: Optional[Sequence[str]] = None,\n    extra_features_names: Optional[Sequence[str]] = None,\n    geo_names: Optional[Sequence[str]] = None,\n    ) -> Tuple[List[pd.DataFrame], pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n  \"\"\"Checks LMMM data quality, to be used before fitting a model.\n\n  Args:\n    media_data: National-level or geo-level media impressions data, such as\n      media_data_train or media_data in the example Collaboratory. This dataset\n      should be scaled so that it has a similar order of magnitude to the\n      target_data and extra_features_data (if applicable).\n    target_data: National-level or geo-level sales or revenue data, such as\n      target_train or target in the example Colabs. This dataset should be\n      scaled so that it has a similar order of magnitude to the media_data and\n      extra_features_data (if applicable).\n    cost_data: National-level cost data, identified as \"costs\" in the example\n      Colabs, with one value per media channel denoting the total cost for that\n      channel over the time period covered by the media_data. The costs can be\n      scaled (mean-normalized) or not scaled.\n    extra_features_data: Optional national-level or geo-level extra features\n      data, such as extra_features_train or extra_features in the example\n      Colabs. This dataset should be scaled so that it has a similar order of\n      magnitude to the media_data and target_data.\n    channel_names: Names of media channels to be added to the output dataframes.\n    extra_features_names: Names of extra features to be added to the output\n      dataframes.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    correlations: List of dataframes containing Pearson correlation coefficients\n      between each feature, as well as between features and the target variable.\n      For national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n    variances: Dataframe containing the variance over time for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n    spend_fractions: Dataframe containing fraction of the total spend in each\n      channel.\n    variance_inflation_factors: Dataframes containing variance inflation factors", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    extra_features_data: Optional[jnp.ndarray] = None,\n    channel_names: Optional[Sequence[str]] = None,\n    extra_features_names: Optional[Sequence[str]] = None,\n    geo_names: Optional[Sequence[str]] = None,\n    ) -> Tuple[List[pd.DataFrame], pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n  \"\"\"Checks LMMM data quality, to be used before fitting a model.\n\n  Args:\n    media_data: National-level or geo-level media impressions data, such as\n      media_data_train or media_data in the example Collaboratory. This dataset\n      should be scaled so that it has a similar order of magnitude to the\n      target_data and extra_features_data (if applicable).\n    target_data: National-level or geo-level sales or revenue data, such as\n      target_train or target in the example Colabs. This dataset should be\n      scaled so that it has a similar order of magnitude to the media_data and\n      extra_features_data (if applicable).\n    cost_data: National-level cost data, identified as \"costs\" in the example\n      Colabs, with one value per media channel denoting the total cost for that\n      channel over the time period covered by the media_data. The costs can be\n      scaled (mean-normalized) or not scaled.\n    extra_features_data: Optional national-level or geo-level extra features\n      data, such as extra_features_train or extra_features in the example\n      Colabs. This dataset should be scaled so that it has a similar order of\n      magnitude to the media_data and target_data.\n    channel_names: Names of media channels to be added to the output dataframes.\n    extra_features_names: Names of extra features to be added to the output\n      dataframes.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    correlations: List of dataframes containing Pearson correlation coefficients\n      between each feature, as well as between features and the target variable.\n      For national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n    variances: Dataframe containing the variance over time for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n    spend_fractions: Dataframe containing fraction of the total spend in each\n      channel.\n    variance_inflation_factors: Dataframes containing variance inflation factors\n      for each feature. For national-level data the dataframe contains just one\n      column, and for geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of channel_names does not match size of media_data\n      or cost_data, or if the number of extra_features_names does not match size\n      of extra_features_data.\n  \"\"\"\n\n  if channel_names is not None and media_data.shape[1] != len(channel_names):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      should be scaled so that it has a similar order of magnitude to the\n      target_data and extra_features_data (if applicable).\n    target_data: National-level or geo-level sales or revenue data, such as\n      target_train or target in the example Colabs. This dataset should be\n      scaled so that it has a similar order of magnitude to the media_data and\n      extra_features_data (if applicable).\n    cost_data: National-level cost data, identified as \"costs\" in the example\n      Colabs, with one value per media channel denoting the total cost for that\n      channel over the time period covered by the media_data. The costs can be\n      scaled (mean-normalized) or not scaled.\n    extra_features_data: Optional national-level or geo-level extra features\n      data, such as extra_features_train or extra_features in the example\n      Colabs. This dataset should be scaled so that it has a similar order of\n      magnitude to the media_data and target_data.\n    channel_names: Names of media channels to be added to the output dataframes.\n    extra_features_names: Names of extra features to be added to the output\n      dataframes.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    correlations: List of dataframes containing Pearson correlation coefficients\n      between each feature, as well as between features and the target variable.\n      For national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n    variances: Dataframe containing the variance over time for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n    spend_fractions: Dataframe containing fraction of the total spend in each\n      channel.\n    variance_inflation_factors: Dataframes containing variance inflation factors\n      for each feature. For national-level data the dataframe contains just one\n      column, and for geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of channel_names does not match size of media_data\n      or cost_data, or if the number of extra_features_names does not match size\n      of extra_features_data.\n  \"\"\"\n\n  if channel_names is not None and media_data.shape[1] != len(channel_names):\n    raise ValueError(\"Number of channels in media_data does not match length \"\n                     \"of channel_names.\")\n\n  if channel_names is not None and len(cost_data) != len(channel_names):\n    raise ValueError(\"Number of channels in cost_data does not match length \"\n                     \"of channel_names.\")\n\n  if (extra_features_data is not None and\n      extra_features_names is not None and\n      extra_features_data.shape[1] != len(extra_features_names)):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    extra_features_data: Optional national-level or geo-level extra features\n      data, such as extra_features_train or extra_features in the example\n      Colabs. This dataset should be scaled so that it has a similar order of\n      magnitude to the media_data and target_data.\n    channel_names: Names of media channels to be added to the output dataframes.\n    extra_features_names: Names of extra features to be added to the output\n      dataframes.\n    geo_names: Names of geos to be added to the output dataframes.\n\n  Returns:\n    correlations: List of dataframes containing Pearson correlation coefficients\n      between each feature, as well as between features and the target variable.\n      For national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n    variances: Dataframe containing the variance over time for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n    spend_fractions: Dataframe containing fraction of the total spend in each\n      channel.\n    variance_inflation_factors: Dataframes containing variance inflation factors\n      for each feature. For national-level data the dataframe contains just one\n      column, and for geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of channel_names does not match size of media_data\n      or cost_data, or if the number of extra_features_names does not match size\n      of extra_features_data.\n  \"\"\"\n\n  if channel_names is not None and media_data.shape[1] != len(channel_names):\n    raise ValueError(\"Number of channels in media_data does not match length \"\n                     \"of channel_names.\")\n\n  if channel_names is not None and len(cost_data) != len(channel_names):\n    raise ValueError(\"Number of channels in cost_data does not match length \"\n                     \"of channel_names.\")\n\n  if (extra_features_data is not None and\n      extra_features_names is not None and\n      extra_features_data.shape[1] != len(extra_features_names)):\n    raise ValueError(\"Number of features in extra_features_data does not match \"\n                     \"length of extra_features_names.\")\n\n  if channel_names is None:\n    all_features_names = [f\"feature_{i}\" for i in range(media_data.shape[1])]\n  else:\n    all_features_names = list(channel_names)\n\n  if geo_names is None:\n    geo_names = [", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    correlations: List of dataframes containing Pearson correlation coefficients\n      between each feature, as well as between features and the target variable.\n      For national-level data the list contains just one dataframe, and for\n      geo-level data the list contains one dataframe for each geo.\n    variances: Dataframe containing the variance over time for each feature. For\n      national-level data the dataframe contains just one column, and for\n      geo-level data the list contains one column for each geo.\n    spend_fractions: Dataframe containing fraction of the total spend in each\n      channel.\n    variance_inflation_factors: Dataframes containing variance inflation factors\n      for each feature. For national-level data the dataframe contains just one\n      column, and for geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of channel_names does not match size of media_data\n      or cost_data, or if the number of extra_features_names does not match size\n      of extra_features_data.\n  \"\"\"\n\n  if channel_names is not None and media_data.shape[1] != len(channel_names):\n    raise ValueError(\"Number of channels in media_data does not match length \"\n                     \"of channel_names.\")\n\n  if channel_names is not None and len(cost_data) != len(channel_names):\n    raise ValueError(\"Number of channels in cost_data does not match length \"\n                     \"of channel_names.\")\n\n  if (extra_features_data is not None and\n      extra_features_names is not None and\n      extra_features_data.shape[1] != len(extra_features_names)):\n    raise ValueError(\"Number of features in extra_features_data does not match \"\n                     \"length of extra_features_names.\")\n\n  if channel_names is None:\n    all_features_names = [f\"feature_{i}\" for i in range(media_data.shape[1])]\n  else:\n    all_features_names = list(channel_names)\n\n  if geo_names is None:\n    geo_names = [\n        f\"geo_{i}\" for i in range(core_utils.get_number_geos(media_data))\n    ]\n\n  # Spend fractions are computed for the media channels only, so we run this\n  # before concatentating the extra_features_names.\n  spend_fractions = _compute_spend_fractions(cost_data, all_features_names)\n\n  if extra_features_data is not None:\n    all_features_data = jnp.concatenate(\n        [media_data, extra_features_data], axis=1", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "      for each feature. For national-level data the dataframe contains just one\n      column, and for geo-level data the list contains one column for each geo.\n\n  Raises:\n    ValueError: If the number of channel_names does not match size of media_data\n      or cost_data, or if the number of extra_features_names does not match size\n      of extra_features_data.\n  \"\"\"\n\n  if channel_names is not None and media_data.shape[1] != len(channel_names):\n    raise ValueError(\"Number of channels in media_data does not match length \"\n                     \"of channel_names.\")\n\n  if channel_names is not None and len(cost_data) != len(channel_names):\n    raise ValueError(\"Number of channels in cost_data does not match length \"\n                     \"of channel_names.\")\n\n  if (extra_features_data is not None and\n      extra_features_names is not None and\n      extra_features_data.shape[1] != len(extra_features_names)):\n    raise ValueError(\"Number of features in extra_features_data does not match \"\n                     \"length of extra_features_names.\")\n\n  if channel_names is None:\n    all_features_names = [f\"feature_{i}\" for i in range(media_data.shape[1])]\n  else:\n    all_features_names = list(channel_names)\n\n  if geo_names is None:\n    geo_names = [\n        f\"geo_{i}\" for i in range(core_utils.get_number_geos(media_data))\n    ]\n\n  # Spend fractions are computed for the media channels only, so we run this\n  # before concatentating the extra_features_names.\n  spend_fractions = _compute_spend_fractions(cost_data, all_features_names)\n\n  if extra_features_data is not None:\n    all_features_data = jnp.concatenate(\n        [media_data, extra_features_data], axis=1\n    )\n    if extra_features_names is None:\n      extra_features_names = [\n          f\"extra_feature_{i}\" for i in range(extra_features_data.shape[1])\n      ]\n    all_features_names += list(extra_features_names)\n  else:\n    all_features_data = jnp.array(media_data)\n\n  correlations = _compute_correlations(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    raise ValueError(\"Number of channels in media_data does not match length \"\n                     \"of channel_names.\")\n\n  if channel_names is not None and len(cost_data) != len(channel_names):\n    raise ValueError(\"Number of channels in cost_data does not match length \"\n                     \"of channel_names.\")\n\n  if (extra_features_data is not None and\n      extra_features_names is not None and\n      extra_features_data.shape[1] != len(extra_features_names)):\n    raise ValueError(\"Number of features in extra_features_data does not match \"\n                     \"length of extra_features_names.\")\n\n  if channel_names is None:\n    all_features_names = [f\"feature_{i}\" for i in range(media_data.shape[1])]\n  else:\n    all_features_names = list(channel_names)\n\n  if geo_names is None:\n    geo_names = [\n        f\"geo_{i}\" for i in range(core_utils.get_number_geos(media_data))\n    ]\n\n  # Spend fractions are computed for the media channels only, so we run this\n  # before concatentating the extra_features_names.\n  spend_fractions = _compute_spend_fractions(cost_data, all_features_names)\n\n  if extra_features_data is not None:\n    all_features_data = jnp.concatenate(\n        [media_data, extra_features_data], axis=1\n    )\n    if extra_features_names is None:\n      extra_features_names = [\n          f\"extra_feature_{i}\" for i in range(extra_features_data.shape[1])\n      ]\n    all_features_names += list(extra_features_names)\n  else:\n    all_features_data = jnp.array(media_data)\n\n  correlations = _compute_correlations(\n      features=all_features_data,\n      target=target_data,\n      feature_names=all_features_names)\n\n  variance_inflation_factors = _compute_variance_inflation_factors(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  variances = _compute_variances(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_405-451", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    raise ValueError(\"Number of features in extra_features_data does not match \"\n                     \"length of extra_features_names.\")\n\n  if channel_names is None:\n    all_features_names = [f\"feature_{i}\" for i in range(media_data.shape[1])]\n  else:\n    all_features_names = list(channel_names)\n\n  if geo_names is None:\n    geo_names = [\n        f\"geo_{i}\" for i in range(core_utils.get_number_geos(media_data))\n    ]\n\n  # Spend fractions are computed for the media channels only, so we run this\n  # before concatentating the extra_features_names.\n  spend_fractions = _compute_spend_fractions(cost_data, all_features_names)\n\n  if extra_features_data is not None:\n    all_features_data = jnp.concatenate(\n        [media_data, extra_features_data], axis=1\n    )\n    if extra_features_names is None:\n      extra_features_names = [\n          f\"extra_feature_{i}\" for i in range(extra_features_data.shape[1])\n      ]\n    all_features_names += list(extra_features_names)\n  else:\n    all_features_data = jnp.array(media_data)\n\n  correlations = _compute_correlations(\n      features=all_features_data,\n      target=target_data,\n      feature_names=all_features_names)\n\n  variance_inflation_factors = _compute_variance_inflation_factors(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  variances = _compute_variances(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  # TODO(): clean up output list\n  return correlations, variances, spend_fractions, variance_inflation_factors", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 451, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_415-451", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "        f\"geo_{i}\" for i in range(core_utils.get_number_geos(media_data))\n    ]\n\n  # Spend fractions are computed for the media channels only, so we run this\n  # before concatentating the extra_features_names.\n  spend_fractions = _compute_spend_fractions(cost_data, all_features_names)\n\n  if extra_features_data is not None:\n    all_features_data = jnp.concatenate(\n        [media_data, extra_features_data], axis=1\n    )\n    if extra_features_names is None:\n      extra_features_names = [\n          f\"extra_feature_{i}\" for i in range(extra_features_data.shape[1])\n      ]\n    all_features_names += list(extra_features_names)\n  else:\n    all_features_data = jnp.array(media_data)\n\n  correlations = _compute_correlations(\n      features=all_features_data,\n      target=target_data,\n      feature_names=all_features_names)\n\n  variance_inflation_factors = _compute_variance_inflation_factors(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  variances = _compute_variances(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  # TODO(): clean up output list\n  return correlations, variances, spend_fractions, variance_inflation_factors", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 451, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing.py_425-451", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing.py", "text": "    )\n    if extra_features_names is None:\n      extra_features_names = [\n          f\"extra_feature_{i}\" for i in range(extra_features_data.shape[1])\n      ]\n    all_features_names += list(extra_features_names)\n  else:\n    all_features_data = jnp.array(media_data)\n\n  correlations = _compute_correlations(\n      features=all_features_data,\n      target=target_data,\n      feature_names=all_features_names)\n\n  variance_inflation_factors = _compute_variance_inflation_factors(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  variances = _compute_variances(\n      features=all_features_data,\n      feature_names=all_features_names,\n      geo_names=geo_names)\n\n  # TODO(): clean up output list\n  return correlations, variances, spend_fractions, variance_inflation_factors", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 451, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for preprocessing.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm.core import core_utils\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for preprocessing.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm.core import core_utils\n\n\n_GEO_DATA_FOR_TESTS = [[[0.1, 0.5], [0.2, 0.4], [0.3, 0.7]],\n                       [[0.2, 0.5], [0.4, 0.6], [0.2, 0.9]],\n                       [[0.3, 0.5], [0.7, 0.8], [0.1, 0.1]],\n                       [[0.4, 0.5], [0.8, 0.5], [0, 0.2]],\n                       [[0.5, 0.5], [0.9, 0.6], [0, 0.5]]]\n_NATIONAL_DATA_FOR_TESTS = [[1, 2, 3, 5], [2, 3, 4, 5], [3, 4, 6, 5],\n                            [2, 5, 7, 5], [3, 7, 9, 5], [2, 6, 9, 5],\n                            [3, 5, 9, 5], [4, 8, 9, 5], [5, 7, 8, 5],\n                            [6, 9, 9, 5]]\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)List(List(List(ConstantConstantLoad)List(ConstantConstantLoad)List(ConstantConstantLoad)Load)List(List(ConstantConstantLoad)List(ConstantConstantLoad)List(ConstantConstantLoad)Load)List(List(ConstantConstantLoad)List(ConstantConstantLoad)List(ConstantConstantLoad)Load)List(List(ConstantConstantLoad)List(ConstantConstantLoad)List(ConstantConstantLoad)Load)List(List(ConstantConstantLoad)List(ConstantConstantLoad)List(ConstantConstantLoad)Load)Load))Assign(Name(Store)List(List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantLoad)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for preprocessing.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm.core import core_utils\n\n\n_GEO_DATA_FOR_TESTS = [[[0.1, 0.5], [0.2, 0.4], [0.3, 0.7]],\n                       [[0.2, 0.5], [0.4, 0.6], [0.2, 0.9]],\n                       [[0.3, 0.5], [0.7, 0.8], [0.1, 0.1]],\n                       [[0.4, 0.5], [0.8, 0.5], [0, 0.2]],\n                       [[0.5, 0.5], [0.9, 0.6], [0, 0.5]]]\n_NATIONAL_DATA_FOR_TESTS = [[1, 2, 3, 5], [2, 3, 4, 5], [3, 4, 6, 5],\n                            [2, 5, 7, 5], [3, 7, 9, 5], [2, 6, 9, 5],\n                            [3, 5, 9, 5], [4, 8, 9, 5], [5, 7, 8, 5],\n                            [6, 9, 9, 5]]\n_NATIONAL_TARGET_DATA = [0.2, 0.4, 0.6, 0.8, 0.5, 0.7, 0.9, 1.0, 0.9, 1.2]\n_GEO_TARGET_DATA = [[0.2, 0.1, 1], [0.4, 0.2, 0.5], [0.6, 0.3, 1],\n                    [0.8, 0.4, 0], [1.0, 0.5, 0.2]]\n_NATIONAL_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.83381, 0.60244, np.nan, 0.81846],\n              [0.83381, 1, 0.86645, np.nan, 0.82736],\n              [0.60245, 0.86645, 1, np.nan, 0.77283],\n              [np.nan, np.nan, np.nan, np.nan, np.nan],\n              [0.81847, 0.82736, 0.77283, np.nan, 1]],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for preprocessing.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm.core import core_utils\n\n\n_GEO_DATA_FOR_TESTS = [[[0.1, 0.5], [0.2, 0.4], [0.3, 0.7]],\n                       [[0.2, 0.5], [0.4, 0.6], [0.2, 0.9]],\n                       [[0.3, 0.5], [0.7, 0.8], [0.1, 0.1]],\n                       [[0.4, 0.5], [0.8, 0.5], [0, 0.2]],\n                       [[0.5, 0.5], [0.9, 0.6], [0, 0.5]]]\n_NATIONAL_DATA_FOR_TESTS = [[1, 2, 3, 5], [2, 3, 4, 5], [3, 4, 6, 5],\n                            [2, 5, 7, 5], [3, 7, 9, 5], [2, 6, 9, 5],\n                            [3, 5, 9, 5], [4, 8, 9, 5], [5, 7, 8, 5],\n                            [6, 9, 9, 5]]\n_NATIONAL_TARGET_DATA = [0.2, 0.4, 0.6, 0.8, 0.5, 0.7, 0.9, 1.0, 0.9, 1.2]\n_GEO_TARGET_DATA = [[0.2, 0.1, 1], [0.4, 0.2, 0.5], [0.6, 0.3, 1],\n                    [0.8, 0.4, 0], [1.0, 0.5, 0.2]]\n_NATIONAL_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.83381, 0.60244, np.nan, 0.81846],\n              [0.83381, 1, 0.86645, np.nan, 0.82736],\n              [0.60245, 0.86645, 1, np.nan, 0.77283],\n              [np.nan, np.nan, np.nan, np.nan, np.nan],\n              [0.81847, 0.82736, 0.77283, np.nan, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        dtype=float),\n]\n_GEO_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.97619, -0.97014, 1],\n              [0.97619, 1, -0.98650, 0.97610],\n              [-0.97014254, -0.9865005, 1, -0.97014,],\n              [1, 0.97619, -0.97014, 1]],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import preprocessing\nfrom lightweight_mmm.core import core_utils\n\n\n_GEO_DATA_FOR_TESTS = [[[0.1, 0.5], [0.2, 0.4], [0.3, 0.7]],\n                       [[0.2, 0.5], [0.4, 0.6], [0.2, 0.9]],\n                       [[0.3, 0.5], [0.7, 0.8], [0.1, 0.1]],\n                       [[0.4, 0.5], [0.8, 0.5], [0, 0.2]],\n                       [[0.5, 0.5], [0.9, 0.6], [0, 0.5]]]\n_NATIONAL_DATA_FOR_TESTS = [[1, 2, 3, 5], [2, 3, 4, 5], [3, 4, 6, 5],\n                            [2, 5, 7, 5], [3, 7, 9, 5], [2, 6, 9, 5],\n                            [3, 5, 9, 5], [4, 8, 9, 5], [5, 7, 8, 5],\n                            [6, 9, 9, 5]]\n_NATIONAL_TARGET_DATA = [0.2, 0.4, 0.6, 0.8, 0.5, 0.7, 0.9, 1.0, 0.9, 1.2]\n_GEO_TARGET_DATA = [[0.2, 0.1, 1], [0.4, 0.2, 0.5], [0.6, 0.3, 1],\n                    [0.8, 0.4, 0], [1.0, 0.5, 0.2]]\n_NATIONAL_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.83381, 0.60244, np.nan, 0.81846],\n              [0.83381, 1, 0.86645, np.nan, 0.82736],\n              [0.60245, 0.86645, 1, np.nan, 0.77283],\n              [np.nan, np.nan, np.nan, np.nan, np.nan],\n              [0.81847, 0.82736, 0.77283, np.nan, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        dtype=float),\n]\n_GEO_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.97619, -0.97014, 1],\n              [0.97619, 1, -0.98650, 0.97610],\n              [-0.97014254, -0.9865005, 1, -0.97014,],\n              [1, 0.97619, -0.97014, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float),\n    pd.DataFrame(\n        data=[[np.nan, np.nan, np.nan, np.nan],\n              [np.nan, 1, -0.46335, 0.31980],\n              [np.nan, -0.46335, 1, -0.51970],\n              [np.nan, 0.31980, -0.51970, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\n_GEO_DATA_FOR_TESTS = [[[0.1, 0.5], [0.2, 0.4], [0.3, 0.7]],\n                       [[0.2, 0.5], [0.4, 0.6], [0.2, 0.9]],\n                       [[0.3, 0.5], [0.7, 0.8], [0.1, 0.1]],\n                       [[0.4, 0.5], [0.8, 0.5], [0, 0.2]],\n                       [[0.5, 0.5], [0.9, 0.6], [0, 0.5]]]\n_NATIONAL_DATA_FOR_TESTS = [[1, 2, 3, 5], [2, 3, 4, 5], [3, 4, 6, 5],\n                            [2, 5, 7, 5], [3, 7, 9, 5], [2, 6, 9, 5],\n                            [3, 5, 9, 5], [4, 8, 9, 5], [5, 7, 8, 5],\n                            [6, 9, 9, 5]]\n_NATIONAL_TARGET_DATA = [0.2, 0.4, 0.6, 0.8, 0.5, 0.7, 0.9, 1.0, 0.9, 1.2]\n_GEO_TARGET_DATA = [[0.2, 0.1, 1], [0.4, 0.2, 0.5], [0.6, 0.3, 1],\n                    [0.8, 0.4, 0], [1.0, 0.5, 0.2]]\n_NATIONAL_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.83381, 0.60244, np.nan, 0.81846],\n              [0.83381, 1, 0.86645, np.nan, 0.82736],\n              [0.60245, 0.86645, 1, np.nan, 0.77283],\n              [np.nan, np.nan, np.nan, np.nan, np.nan],\n              [0.81847, 0.82736, 0.77283, np.nan, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        dtype=float),\n]\n_GEO_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.97619, -0.97014, 1],\n              [0.97619, 1, -0.98650, 0.97610],\n              [-0.97014254, -0.9865005, 1, -0.97014,],\n              [1, 0.97619, -0.97014, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float),\n    pd.DataFrame(\n        data=[[np.nan, np.nan, np.nan, np.nan],\n              [np.nan, 1, -0.46335, 0.31980],\n              [np.nan, -0.46335, 1, -0.51970],\n              [np.nan, 0.31980, -0.51970, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float)\n]\n_NATIONAL_VARIANCES = pd.DataFrame(\n    data=[2.09, 4.44, 4.61, 0],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VARIANCES = pd.DataFrame(\n    data=[[0.02, 0], [0.068, 0.0176], [0.0136, 0.0896]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "_NATIONAL_TARGET_DATA = [0.2, 0.4, 0.6, 0.8, 0.5, 0.7, 0.9, 1.0, 0.9, 1.2]\n_GEO_TARGET_DATA = [[0.2, 0.1, 1], [0.4, 0.2, 0.5], [0.6, 0.3, 1],\n                    [0.8, 0.4, 0], [1.0, 0.5, 0.2]]\n_NATIONAL_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.83381, 0.60244, np.nan, 0.81846],\n              [0.83381, 1, 0.86645, np.nan, 0.82736],\n              [0.60245, 0.86645, 1, np.nan, 0.77283],\n              [np.nan, np.nan, np.nan, np.nan, np.nan],\n              [0.81847, 0.82736, 0.77283, np.nan, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        dtype=float),\n]\n_GEO_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.97619, -0.97014, 1],\n              [0.97619, 1, -0.98650, 0.97610],\n              [-0.97014254, -0.9865005, 1, -0.97014,],\n              [1, 0.97619, -0.97014, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float),\n    pd.DataFrame(\n        data=[[np.nan, np.nan, np.nan, np.nan],\n              [np.nan, 1, -0.46335, 0.31980],\n              [np.nan, -0.46335, 1, -0.51970],\n              [np.nan, 0.31980, -0.51970, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float)\n]\n_NATIONAL_VARIANCES = pd.DataFrame(\n    data=[2.09, 4.44, 4.61, 0],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VARIANCES = pd.DataFrame(\n    data=[[0.02, 0], [0.068, 0.0176], [0.0136, 0.0896]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n_NATIONAL_VIFS = pd.DataFrame(\n    data=[4.0491, 10.3485, 4.9505, 13.7372],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VIFS = pd.DataFrame(\n    data=[[22.1429, 36.8863], [48.5715, 1.2734], [38.8572, 1.2734]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\", \"target\"],\n        dtype=float),\n]\n_GEO_CORRELATION_MATRICES = [\n    pd.DataFrame(\n        data=[[1, 0.97619, -0.97014, 1],\n              [0.97619, 1, -0.98650, 0.97610],\n              [-0.97014254, -0.9865005, 1, -0.97014,],\n              [1, 0.97619, -0.97014, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float),\n    pd.DataFrame(\n        data=[[np.nan, np.nan, np.nan, np.nan],\n              [np.nan, 1, -0.46335, 0.31980],\n              [np.nan, -0.46335, 1, -0.51970],\n              [np.nan, 0.31980, -0.51970, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float)\n]\n_NATIONAL_VARIANCES = pd.DataFrame(\n    data=[2.09, 4.44, 4.61, 0],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VARIANCES = pd.DataFrame(\n    data=[[0.02, 0], [0.068, 0.0176], [0.0136, 0.0896]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n_NATIONAL_VIFS = pd.DataFrame(\n    data=[4.0491, 10.3485, 4.9505, 13.7372],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VIFS = pd.DataFrame(\n    data=[[22.1429, 36.8863], [48.5715, 1.2734], [38.8572, 1.2734]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n\n\nclass PreprocessingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"all_nones\",\n          divide_operation=None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float),\n    pd.DataFrame(\n        data=[[np.nan, np.nan, np.nan, np.nan],\n              [np.nan, 1, -0.46335, 0.31980],\n              [np.nan, -0.46335, 1, -0.51970],\n              [np.nan, 0.31980, -0.51970, 1]],\n        columns=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        index=[\"feature_0\", \"feature_1\", \"feature_2\", \"target\"],\n        dtype=float)\n]\n_NATIONAL_VARIANCES = pd.DataFrame(\n    data=[2.09, 4.44, 4.61, 0],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VARIANCES = pd.DataFrame(\n    data=[[0.02, 0], [0.068, 0.0176], [0.0136, 0.0896]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n_NATIONAL_VIFS = pd.DataFrame(\n    data=[4.0491, 10.3485, 4.9505, 13.7372],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VIFS = pd.DataFrame(\n    data=[[22.1429, 36.8863], [48.5715, 1.2734], [38.8572, 1.2734]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n\n\nclass PreprocessingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"all_nones\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_divides\",\n          divide_operation=1,\n          divide_by=1,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "        dtype=float)\n]\n_NATIONAL_VARIANCES = pd.DataFrame(\n    data=[2.09, 4.44, 4.61, 0],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VARIANCES = pd.DataFrame(\n    data=[[0.02, 0], [0.068, 0.0176], [0.0136, 0.0896]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n_NATIONAL_VIFS = pd.DataFrame(\n    data=[4.0491, 10.3485, 4.9505, 13.7372],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VIFS = pd.DataFrame(\n    data=[[22.1429, 36.8863], [48.5715, 1.2734], [38.8572, 1.2734]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n\n\nclass PreprocessingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"all_nones\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_divides\",\n          divide_operation=1,\n          divide_by=1,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_multiplies\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=1,\n          multiply_by=1),\n  ])\n  def test_custom_scaler_constructor_wrong_params_raises_valueerror(\n      self, divide_operation, divide_by, multiply_operation, multiply_by):\n    with self.assertRaises(ValueError):\n      preprocessing.CustomScaler(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n_NATIONAL_VIFS = pd.DataFrame(\n    data=[4.0491, 10.3485, 4.9505, 13.7372],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\", \"feature_3\"],\n    columns=[\"geo_0\"],\n    dtype=float)\n_GEO_VIFS = pd.DataFrame(\n    data=[[22.1429, 36.8863], [48.5715, 1.2734], [38.8572, 1.2734]],\n    index=[\"feature_0\", \"feature_1\", \"feature_2\"],\n    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n\n\nclass PreprocessingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"all_nones\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_divides\",\n          divide_operation=1,\n          divide_by=1,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_multiplies\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=1,\n          multiply_by=1),\n  ])\n  def test_custom_scaler_constructor_wrong_params_raises_valueerror(\n      self, divide_operation, divide_by, multiply_operation, multiply_by):\n    with self.assertRaises(ValueError):\n      preprocessing.CustomScaler(\n          divide_operation=divide_operation,\n          divide_by=divide_by,\n          multiply_operation=multiply_operation,\n          multiply_by=multiply_by)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=1,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    columns=[\"geo_0\", \"geo_1\"],\n    dtype=float)\n\n\nclass PreprocessingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"all_nones\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_divides\",\n          divide_operation=1,\n          divide_by=1,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_multiplies\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=1,\n          multiply_by=1),\n  ])\n  def test_custom_scaler_constructor_wrong_params_raises_valueerror(\n      self, divide_operation, divide_by, multiply_operation, multiply_by):\n    with self.assertRaises(ValueError):\n      preprocessing.CustomScaler(\n          divide_operation=divide_operation,\n          divide_by=divide_by,\n          multiply_operation=multiply_operation,\n          multiply_by=multiply_by)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=1,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          divide_by=None,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_divides\",\n          divide_operation=1,\n          divide_by=1,\n          multiply_operation=None,\n          multiply_by=None),\n      dict(\n          testcase_name=\"both_multiplies\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=1,\n          multiply_by=1),\n  ])\n  def test_custom_scaler_constructor_wrong_params_raises_valueerror(\n      self, divide_operation, divide_by, multiply_operation, multiply_by):\n    with self.assertRaises(ValueError):\n      preprocessing.CustomScaler(\n          divide_operation=divide_operation,\n          divide_by=divide_by,\n          multiply_operation=multiply_operation,\n          multiply_by=multiply_by)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"both_multiplies\",\n          divide_operation=None,\n          divide_by=None,\n          multiply_operation=1,\n          multiply_by=1),\n  ])\n  def test_custom_scaler_constructor_wrong_params_raises_valueerror(\n      self, divide_operation, divide_by, multiply_operation, multiply_by):\n    with self.assertRaises(ValueError):\n      preprocessing.CustomScaler(\n          divide_operation=divide_operation,\n          divide_by=divide_by,\n          multiply_operation=multiply_operation,\n          multiply_by=multiply_by)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"5\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          divide_operation=divide_operation,\n          divide_by=divide_by,\n          multiply_operation=multiply_operation,\n          multiply_by=multiply_by)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"5\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_by\"],\n          missing_attributes=[\"divide_operation\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"6\",\n          divide_operation=jnp.mean,\n          divide_by=5,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=1,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"5\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_by\"],\n          missing_attributes=[\"divide_operation\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"6\",\n          divide_operation=jnp.mean,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_operation\", \"multiply_by\"],\n          missing_attributes=[\"divide_by\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"7\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=jnp.mean,\n          multiply_by=5,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=1,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"5\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_by\"],\n          missing_attributes=[\"divide_operation\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"6\",\n          divide_operation=jnp.mean,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_operation\", \"multiply_by\"],\n          missing_attributes=[\"divide_by\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"7\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=jnp.mean,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_operation\"],\n          missing_attributes=[\"divide_operation\", \"multiply_by\"]),\n  ])\n  def test_custom_scaler_constructor_sets_correct_attributes(\n      self, divide_operation, divide_by, multiply_operation, multiply_by,\n      has_attributes, missing_attributes):\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=divide_by,\n        multiply_operation=multiply_operation,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      dict(\n          testcase_name=\"4\",\n          divide_operation=jnp.mean,\n          divide_by=None,\n          multiply_operation=jnp.mean,\n          multiply_by=None,\n          has_attributes=[\"divide_operation\", \"multiply_operation\"],\n          missing_attributes=[\"divide_by\", \"multiply_by\"]),\n      dict(\n          testcase_name=\"5\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_by\"],\n          missing_attributes=[\"divide_operation\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"6\",\n          divide_operation=jnp.mean,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_operation\", \"multiply_by\"],\n          missing_attributes=[\"divide_by\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"7\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=jnp.mean,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_operation\"],\n          missing_attributes=[\"divide_operation\", \"multiply_by\"]),\n  ])\n  def test_custom_scaler_constructor_sets_correct_attributes(\n      self, divide_operation, divide_by, multiply_operation, multiply_by,\n      has_attributes, missing_attributes):\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=divide_by,\n        multiply_operation=multiply_operation,\n        multiply_by=multiply_by)\n\n    for attribute in has_attributes:\n      self.assertTrue(hasattr(custom_scaler, attribute))\n\n    for attribute in missing_attributes:\n      self.assertFalse(hasattr(custom_scaler, attribute))\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          divide_operation=None,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_by\"],\n          missing_attributes=[\"divide_operation\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"6\",\n          divide_operation=jnp.mean,\n          divide_by=5,\n          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_operation\", \"multiply_by\"],\n          missing_attributes=[\"divide_by\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"7\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=jnp.mean,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_operation\"],\n          missing_attributes=[\"divide_operation\", \"multiply_by\"]),\n  ])\n  def test_custom_scaler_constructor_sets_correct_attributes(\n      self, divide_operation, divide_by, multiply_operation, multiply_by,\n      has_attributes, missing_attributes):\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=divide_by,\n        multiply_operation=multiply_operation,\n        multiply_by=multiply_by)\n\n    for attribute in has_attributes:\n      self.assertTrue(hasattr(custom_scaler, attribute))\n\n    for attribute in missing_attributes:\n      self.assertFalse(hasattr(custom_scaler, attribute))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_operation=None,\n          multiply_by=5,\n          has_attributes=[\"divide_operation\", \"multiply_by\"],\n          missing_attributes=[\"divide_by\", \"multiply_operation\"]),\n      dict(\n          testcase_name=\"7\",\n          divide_operation=None,\n          divide_by=5,\n          multiply_operation=jnp.mean,\n          multiply_by=5,\n          has_attributes=[\"divide_by\", \"multiply_operation\"],\n          missing_attributes=[\"divide_operation\", \"multiply_by\"]),\n  ])\n  def test_custom_scaler_constructor_sets_correct_attributes(\n      self, divide_operation, divide_by, multiply_operation, multiply_by,\n      has_attributes, missing_attributes):\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=divide_by,\n        multiply_operation=multiply_operation,\n        multiply_by=multiply_by)\n\n    for attribute in has_attributes:\n      self.assertTrue(hasattr(custom_scaler, attribute))\n\n    for attribute in missing_attributes:\n      self.assertFalse(hasattr(custom_scaler, attribute))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          has_attributes=[\"divide_by\", \"multiply_operation\"],\n          missing_attributes=[\"divide_operation\", \"multiply_by\"]),\n  ])\n  def test_custom_scaler_constructor_sets_correct_attributes(\n      self, divide_operation, divide_by, multiply_operation, multiply_by,\n      has_attributes, missing_attributes):\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=divide_by,\n        multiply_operation=multiply_operation,\n        multiply_by=multiply_by)\n\n    for attribute in has_attributes:\n      self.assertTrue(hasattr(custom_scaler, attribute))\n\n    for attribute in missing_attributes:\n      self.assertFalse(hasattr(custom_scaler, attribute))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[1, 1, 1]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "        multiply_by=multiply_by)\n\n    for attribute in has_attributes:\n      self.assertTrue(hasattr(custom_scaler, attribute))\n\n    for attribute in missing_attributes:\n      self.assertFalse(hasattr(custom_scaler, attribute))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[1, 1, 1]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[1, 1, 1]),\n  ])\n  def test_fit_overrides_or_sets_correct_values(self, divide_operation,\n                                                divide_by, multiply_operation,\n                                                multiply_by, expected_divide_by,\n                                                expected_multiply_by):\n    data = jnp.ones((10, 3)) * 2\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=jnp.array(divide_by),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"1\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"2\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[1, 1, 1]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[1, 1, 1]),\n  ])\n  def test_fit_overrides_or_sets_correct_values(self, divide_operation,\n                                                divide_by, multiply_operation,\n                                                multiply_by, expected_divide_by,\n                                                expected_multiply_by):\n    data = jnp.ones((10, 3)) * 2\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=jnp.array(divide_by),\n        multiply_operation=multiply_operation,\n        multiply_by=jnp.array(multiply_by))\n\n    custom_scaler.fit(data)\n\n    self.assertTrue(hasattr(custom_scaler, \"divide_by\"))\n    self.assertTrue(hasattr(custom_scaler, \"multiply_by\"))\n    np.testing.assert_array_equal(custom_scaler.divide_by,\n                                  jnp.array(expected_divide_by))\n    np.testing.assert_array_equal(custom_scaler.multiply_by,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          divide_by=[1, 1, 1],\n          multiply_operation=jnp.mean,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[2, 2, 2]),\n      dict(\n          testcase_name=\"3\",\n          divide_operation=jnp.mean,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[1, 1, 1]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[1, 1, 1]),\n  ])\n  def test_fit_overrides_or_sets_correct_values(self, divide_operation,\n                                                divide_by, multiply_operation,\n                                                multiply_by, expected_divide_by,\n                                                expected_multiply_by):\n    data = jnp.ones((10, 3)) * 2\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=jnp.array(divide_by),\n        multiply_operation=multiply_operation,\n        multiply_by=jnp.array(multiply_by))\n\n    custom_scaler.fit(data)\n\n    self.assertTrue(hasattr(custom_scaler, \"divide_by\"))\n    self.assertTrue(hasattr(custom_scaler, \"multiply_by\"))\n    np.testing.assert_array_equal(custom_scaler.divide_by,\n                                  jnp.array(expected_divide_by))\n    np.testing.assert_array_equal(custom_scaler.multiply_by,\n                                  jnp.array(expected_multiply_by))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"2\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_by=[1, 1, 1],\n          expected_divide_by=[2, 2, 2],\n          expected_multiply_by=[1, 1, 1]),\n      dict(\n          testcase_name=\"4\",\n          divide_operation=None,\n          divide_by=[1, 1, 1],\n          multiply_operation=None,\n          multiply_by=[1, 1, 1],\n          expected_divide_by=[1, 1, 1],\n          expected_multiply_by=[1, 1, 1]),\n  ])\n  def test_fit_overrides_or_sets_correct_values(self, divide_operation,\n                                                divide_by, multiply_operation,\n                                                multiply_by, expected_divide_by,\n                                                expected_multiply_by):\n    data = jnp.ones((10, 3)) * 2\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=jnp.array(divide_by),\n        multiply_operation=multiply_operation,\n        multiply_by=jnp.array(multiply_by))\n\n    custom_scaler.fit(data)\n\n    self.assertTrue(hasattr(custom_scaler, \"divide_by\"))\n    self.assertTrue(hasattr(custom_scaler, \"multiply_by\"))\n    np.testing.assert_array_equal(custom_scaler.divide_by,\n                                  jnp.array(expected_divide_by))\n    np.testing.assert_array_equal(custom_scaler.multiply_by,\n                                  jnp.array(expected_multiply_by))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"2\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"3\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          expected_multiply_by=[1, 1, 1]),\n  ])\n  def test_fit_overrides_or_sets_correct_values(self, divide_operation,\n                                                divide_by, multiply_operation,\n                                                multiply_by, expected_divide_by,\n                                                expected_multiply_by):\n    data = jnp.ones((10, 3)) * 2\n    custom_scaler = preprocessing.CustomScaler(\n        divide_operation=divide_operation,\n        divide_by=jnp.array(divide_by),\n        multiply_operation=multiply_operation,\n        multiply_by=jnp.array(multiply_by))\n\n    custom_scaler.fit(data)\n\n    self.assertTrue(hasattr(custom_scaler, \"divide_by\"))\n    self.assertTrue(hasattr(custom_scaler, \"multiply_by\"))\n    np.testing.assert_array_equal(custom_scaler.divide_by,\n                                  jnp.array(expected_divide_by))\n    np.testing.assert_array_equal(custom_scaler.multiply_by,\n                                  jnp.array(expected_multiply_by))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"2\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"3\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"4\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"5\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "        multiply_operation=multiply_operation,\n        multiply_by=jnp.array(multiply_by))\n\n    custom_scaler.fit(data)\n\n    self.assertTrue(hasattr(custom_scaler, \"divide_by\"))\n    self.assertTrue(hasattr(custom_scaler, \"multiply_by\"))\n    np.testing.assert_array_equal(custom_scaler.divide_by,\n                                  jnp.array(expected_divide_by))\n    np.testing.assert_array_equal(custom_scaler.multiply_by,\n                                  jnp.array(expected_multiply_by))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"2\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"3\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"4\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"5\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"6\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"7\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                  jnp.array(expected_multiply_by))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"2\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"3\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"4\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"5\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"6\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"7\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"8\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_transform_produces_correct_values(self, multiply_by, divide_by,\n                                             expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"3\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"4\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"5\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"6\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"7\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"8\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_transform_produces_correct_values(self, multiply_by, divide_by,\n                                             expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    if isinstance(multiply_by, int) and isinstance(divide_by, int):\n      scaler = preprocessing.CustomScaler(\n          divide_by=divide_by, multiply_by=multiply_by)\n    else:\n      scaler = preprocessing.CustomScaler(\n          divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    scaler.fit(data)\n    transformed_data = scaler.transform(data)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"4\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"5\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"6\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"7\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"8\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_transform_produces_correct_values(self, multiply_by, divide_by,\n                                             expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    if isinstance(multiply_by, int) and isinstance(divide_by, int):\n      scaler = preprocessing.CustomScaler(\n          divide_by=divide_by, multiply_by=multiply_by)\n    else:\n      scaler = preprocessing.CustomScaler(\n          divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    scaler.fit(data)\n    transformed_data = scaler.transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      dict(\n          testcase_name=\"6\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"7\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"8\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_transform_produces_correct_values(self, multiply_by, divide_by,\n                                             expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    if isinstance(multiply_by, int) and isinstance(divide_by, int):\n      scaler = preprocessing.CustomScaler(\n          divide_by=divide_by, multiply_by=multiply_by)\n    else:\n      scaler = preprocessing.CustomScaler(\n          divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    scaler.fit(data)\n    transformed_data = scaler.transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"8\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_transform_produces_correct_values(self, multiply_by, divide_by,\n                                             expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    if isinstance(multiply_by, int) and isinstance(divide_by, int):\n      scaler = preprocessing.CustomScaler(\n          divide_by=divide_by, multiply_by=multiply_by)\n    else:\n      scaler = preprocessing.CustomScaler(\n          divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    scaler.fit(data)\n    transformed_data = scaler.transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\n    if isinstance(multiply_by, int) and isinstance(divide_by, int):\n      scaler = preprocessing.CustomScaler(\n          divide_by=divide_by, multiply_by=multiply_by)\n    else:\n      scaler = preprocessing.CustomScaler(\n          divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    scaler.fit(data)\n    transformed_data = scaler.transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"ones_ones\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_fit_transform_produces_correct_values(self, multiply_by, divide_by,\n                                                 expected_transformed):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                [3.75, 4.375, 5.]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[0., 2., 4.], [6., 8., 10.], [12., 14., 16.]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"ones_ones\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_fit_transform_produces_correct_values(self, multiply_by, divide_by,\n                                                 expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                [1.5, 1.75, 2.]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"ones_ones\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_fit_transform_produces_correct_values(self, multiply_by, divide_by,\n                                                 expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"one_one\", multiply_by=1, divide_by=1),\n      dict(testcase_name=\"five_eight\", multiply_by=5, divide_by=8),\n      dict(testcase_name=\"two_one\", multiply_by=2, divide_by=1),\n      dict(testcase_name=\"one_four\", multiply_by=1, divide_by=4),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3]),\n      dict(\n          testcase_name=\"ones_arange\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[0., 0.5, 0.66666667], [1., 2., 1.66666667],\n                                [2., 3.5, 2.66666667]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"ones_ones\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_fit_transform_produces_correct_values(self, multiply_by, divide_by,\n                                                 expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"one_one\", multiply_by=1, divide_by=1),\n      dict(testcase_name=\"five_eight\", multiply_by=5, divide_by=8),\n      dict(testcase_name=\"two_one\", multiply_by=2, divide_by=1),\n      dict(testcase_name=\"one_four\", multiply_by=1, divide_by=4),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1]),\n      dict(\n          testcase_name=\"ones_ones\", multiply_by=[1, 1, 1], divide_by=[1, 1,\n                                                                       1]),\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          divide_by=[3, 2, 1],\n          expected_transformed=[[0., 1., 6.], [1., 4., 15.], [2., 7., 24.]]),\n      dict(\n          testcase_name=\"ones_ones\",\n          multiply_by=[1, 1, 1],\n          divide_by=[1, 1, 1],\n          expected_transformed=[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]),\n  ])\n  def test_fit_transform_produces_correct_values(self, multiply_by, divide_by,\n                                                 expected_transformed):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"one_one\", multiply_by=1, divide_by=1),\n      dict(testcase_name=\"five_eight\", multiply_by=5, divide_by=8),\n      dict(testcase_name=\"two_one\", multiply_by=2, divide_by=1),\n      dict(testcase_name=\"one_four\", multiply_by=1, divide_by=4),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1]),\n      dict(\n          testcase_name=\"ones_ones\", multiply_by=[1, 1, 1], divide_by=[1, 1,\n                                                                       1]),\n  ])\n  def test_reverse_transform_returns_original_values(self, multiply_by,\n                                                     divide_by):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n    inverse_transformed_data = scaler.inverse_transform(transformed_data)\n\n    np.testing.assert_array_almost_equal(data, inverse_transformed_data)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"one_one\", multiply_by=1, divide_by=1),\n      dict(testcase_name=\"five_eight\", multiply_by=5, divide_by=8),\n      dict(testcase_name=\"two_one\", multiply_by=2, divide_by=1),\n      dict(testcase_name=\"one_four\", multiply_by=1, divide_by=4),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1]),\n      dict(\n          testcase_name=\"ones_ones\", multiply_by=[1, 1, 1], divide_by=[1, 1,\n                                                                       1]),\n  ])\n  def test_reverse_transform_returns_original_values(self, multiply_by,\n                                                     divide_by):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n    inverse_transformed_data = scaler.inverse_transform(transformed_data)\n\n    np.testing.assert_array_almost_equal(data, inverse_transformed_data)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      dict(testcase_name=\"one_one\", multiply_by=1, divide_by=1),\n      dict(testcase_name=\"five_eight\", multiply_by=5, divide_by=8),\n      dict(testcase_name=\"two_one\", multiply_by=2, divide_by=1),\n      dict(testcase_name=\"one_four\", multiply_by=1, divide_by=4),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1]),\n      dict(\n          testcase_name=\"ones_ones\", multiply_by=[1, 1, 1], divide_by=[1, 1,\n                                                                       1]),\n  ])\n  def test_reverse_transform_returns_original_values(self, multiply_by,\n                                                     divide_by):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n    inverse_transformed_data = scaler.inverse_transform(transformed_data)\n\n    np.testing.assert_array_almost_equal(data, inverse_transformed_data)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                 [3.75, 4.375, 5.]],\n                                [[5.625, 6.25, 6.875], [7.5, 8.125, 8.75],\n                                 [9.375, 10., 10.625]],\n                                [[11.25, 11.875, 12.5], [13.125, 13.75, 14.375],\n                                 [15., 15.625, 16.25]]]),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1]),\n      dict(\n          testcase_name=\"ones_ones\", multiply_by=[1, 1, 1], divide_by=[1, 1,\n                                                                       1]),\n  ])\n  def test_reverse_transform_returns_original_values(self, multiply_by,\n                                                     divide_by):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n    inverse_transformed_data = scaler.inverse_transform(transformed_data)\n\n    np.testing.assert_array_almost_equal(data, inverse_transformed_data)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                 [3.75, 4.375, 5.]],\n                                [[5.625, 6.25, 6.875], [7.5, 8.125, 8.75],\n                                 [9.375, 10., 10.625]],\n                                [[11.25, 11.875, 12.5], [13.125, 13.75, 14.375],\n                                 [15., 15.625, 16.25]]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[[0, 2, 4], [6, 8, 10], [12, 14, 16]],\n                                [[18, 20, 22], [24, 26, 28], [30, 32, 34]],\n                                [[36, 38, 40], [42, 44, 46], [48, 50, 52]]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "  def test_reverse_transform_returns_original_values(self, multiply_by,\n                                                     divide_by):\n    data = jnp.arange(9).reshape((3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n    inverse_transformed_data = scaler.inverse_transform(transformed_data)\n\n    np.testing.assert_array_almost_equal(data, inverse_transformed_data)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                 [3.75, 4.375, 5.]],\n                                [[5.625, 6.25, 6.875], [7.5, 8.125, 8.75],\n                                 [9.375, 10., 10.625]],\n                                [[11.25, 11.875, 12.5], [13.125, 13.75, 14.375],\n                                 [15., 15.625, 16.25]]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[[0, 2, 4], [6, 8, 10], [12, 14, 16]],\n                                [[18, 20, 22], [24, 26, 28], [30, 32, 34]],\n                                [[36, 38, 40], [42, 44, 46], [48, 50, 52]]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                 [1.5, 1.75, 2.]],\n                                [[2.25, 2.5, 2.75], [3., 3.25, 3.5],\n                                 [3.75, 4., 4.25]],\n                                [[4.5, 4.75, 5.], [5.25, 5.5, 5.75],\n                                 [6., 6.25, 6.5]]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_one\",\n          multiply_by=1,\n          divide_by=1,\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                 [3.75, 4.375, 5.]],\n                                [[5.625, 6.25, 6.875], [7.5, 8.125, 8.75],\n                                 [9.375, 10., 10.625]],\n                                [[11.25, 11.875, 12.5], [13.125, 13.75, 14.375],\n                                 [15., 15.625, 16.25]]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[[0, 2, 4], [6, 8, 10], [12, 14, 16]],\n                                [[18, 20, 22], [24, 26, 28], [30, 32, 34]],\n                                [[36, 38, 40], [42, 44, 46], [48, 50, 52]]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                 [1.5, 1.75, 2.]],\n                                [[2.25, 2.5, 2.75], [3., 3.25, 3.5],\n                                 [3.75, 4., 4.25]],\n                                [[4.5, 4.75, 5.], [5.25, 5.5, 5.75],\n                                 [6., 6.25, 6.5]]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 1.6666666],\n                                 [2., 3.5, 2.6666667]],\n                                [[3., 5., 3.6666667], [4., 6.5, 4.6666665],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_405-455", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"five_eight\",\n          multiply_by=5,\n          divide_by=8,\n          expected_transformed=[[[0., 0.625, 1.25], [1.875, 2.5, 3.125],\n                                 [3.75, 4.375, 5.]],\n                                [[5.625, 6.25, 6.875], [7.5, 8.125, 8.75],\n                                 [9.375, 10., 10.625]],\n                                [[11.25, 11.875, 12.5], [13.125, 13.75, 14.375],\n                                 [15., 15.625, 16.25]]]),\n      dict(\n          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[[0, 2, 4], [6, 8, 10], [12, 14, 16]],\n                                [[18, 20, 22], [24, 26, 28], [30, 32, 34]],\n                                [[36, 38, 40], [42, 44, 46], [48, 50, 52]]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                 [1.5, 1.75, 2.]],\n                                [[2.25, 2.5, 2.75], [3., 3.25, 3.5],\n                                 [3.75, 4., 4.25]],\n                                [[4.5, 4.75, 5.], [5.25, 5.5, 5.75],\n                                 [6., 6.25, 6.5]]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 1.6666666],\n                                 [2., 3.5, 2.6666667]],\n                                [[3., 5., 3.6666667], [4., 6.5, 4.6666665],\n                                 [5., 8., 5.6666665]],\n                                [[6., 9.5, 6.6666665], [7., 11., 7.6666665],\n                                 [8., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[[0, 1, 6], [1, 4, 15], [2, 7, 24]],\n                                [[3, 10, 33], [4, 13, 42], [5, 16, 51]],\n                                [[6, 19, 60], [7, 22, 69], [8, 25, 78]]]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_415-465", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"two_one\",\n          multiply_by=2,\n          divide_by=1,\n          expected_transformed=[[[0, 2, 4], [6, 8, 10], [12, 14, 16]],\n                                [[18, 20, 22], [24, 26, 28], [30, 32, 34]],\n                                [[36, 38, 40], [42, 44, 46], [48, 50, 52]]]),\n      dict(\n          testcase_name=\"one_four\",\n          multiply_by=1,\n          divide_by=4,\n          expected_transformed=[[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                 [1.5, 1.75, 2.]],\n                                [[2.25, 2.5, 2.75], [3., 3.25, 3.5],\n                                 [3.75, 4., 4.25]],\n                                [[4.5, 4.75, 5.], [5.25, 5.5, 5.75],\n                                 [6., 6.25, 6.5]]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 1.6666666],\n                                 [2., 3.5, 2.6666667]],\n                                [[3., 5., 3.6666667], [4., 6.5, 4.6666665],\n                                 [5., 8., 5.6666665]],\n                                [[6., 9.5, 6.6666665], [7., 11., 7.6666665],\n                                 [8., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[[0, 1, 6], [1, 4, 15], [2, 7, 24]],\n                                [[3, 10, 33], [4, 13, 42], [5, 16, 51]],\n                                [[6, 19, 60], [7, 22, 69], [8, 25, 78]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=1,\n          expected_transformed=[[[0., 2., 6.], [9., 8., 5.], [6., 14., 24.]],\n                                [[9., 20., 33.], [36., 26., 14.],\n                                 [15., 32., 51.]],\n                                [[18., 38., 60.], [63., 44., 23.],\n                                 [24., 50., 78.]]]),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_425-475", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          expected_transformed=[[[0., 0.25, 0.5], [0.75, 1., 1.25],\n                                 [1.5, 1.75, 2.]],\n                                [[2.25, 2.5, 2.75], [3., 3.25, 3.5],\n                                 [3.75, 4., 4.25]],\n                                [[4.5, 4.75, 5.], [5.25, 5.5, 5.75],\n                                 [6., 6.25, 6.5]]]),\n      dict(\n          testcase_name=\"arange_arange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[1, 2, 3],\n          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 1.6666666],\n                                 [2., 3.5, 2.6666667]],\n                                [[3., 5., 3.6666667], [4., 6.5, 4.6666665],\n                                 [5., 8., 5.6666665]],\n                                [[6., 9.5, 6.6666665], [7., 11., 7.6666665],\n                                 [8., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[[0, 1, 6], [1, 4, 15], [2, 7, 24]],\n                                [[3, 10, 33], [4, 13, 42], [5, 16, 51]],\n                                [[6, 19, 60], [7, 22, 69], [8, 25, 78]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=1,\n          expected_transformed=[[[0., 2., 6.], [9., 8., 5.], [6., 14., 24.]],\n                                [[9., 20., 33.], [36., 26., 14.],\n                                 [15., 32., 51.]],\n                                [[18., 38., 60.], [63., 44., 23.],\n                                 [24., 50., 78.]]]),\n      dict(\n          testcase_name=\"two_dimensional_divide_by\",\n          multiply_by=1,\n          divide_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 5.],\n                                 [6., 3.5, 2.6666667]],\n                                [[9., 5., 3.6666667], [4., 6.5, 14.],\n                                 [15., 8., 5.6666665]],\n                                [[18., 9.5, 6.6666665], [7., 11., 23.],\n                                 [24., 12.5, 8.666667]]]),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_435-485", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          expected_transformed=[[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n                                [[9, 10, 11], [12, 13, 14], [15, 16, 17]],\n                                [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]),\n      dict(\n          testcase_name=\"ones_arange\",\n          multiply_by=[1, 1, 1],\n          divide_by=[3, 2, 3],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 1.6666666],\n                                 [2., 3.5, 2.6666667]],\n                                [[3., 5., 3.6666667], [4., 6.5, 4.6666665],\n                                 [5., 8., 5.6666665]],\n                                [[6., 9.5, 6.6666665], [7., 11., 7.6666665],\n                                 [8., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[[0, 1, 6], [1, 4, 15], [2, 7, 24]],\n                                [[3, 10, 33], [4, 13, 42], [5, 16, 51]],\n                                [[6, 19, 60], [7, 22, 69], [8, 25, 78]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=1,\n          expected_transformed=[[[0., 2., 6.], [9., 8., 5.], [6., 14., 24.]],\n                                [[9., 20., 33.], [36., 26., 14.],\n                                 [15., 32., 51.]],\n                                [[18., 38., 60.], [63., 44., 23.],\n                                 [24., 50., 78.]]]),\n      dict(\n          testcase_name=\"two_dimensional_divide_by\",\n          multiply_by=1,\n          divide_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 5.],\n                                 [6., 3.5, 2.6666667]],\n                                [[9., 5., 3.6666667], [4., 6.5, 14.],\n                                 [15., 8., 5.6666665]],\n                                [[18., 9.5, 6.6666665], [7., 11., 23.],\n                                 [24., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by_and_divide_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=[[3, 2, 1], [1, 2, 3], [2, 1, 3]],\n          expected_transformed=[[[0., 1., 6.], [9., 4., 1.6666666],\n                                 [3., 14., 8.]],\n                                [[3., 10., 33.], [36., 13., 4.6666665],\n                                 [7.5, 32., 17.]],\n                                [[6., 19., 60.], [63., 22., 7.6666665],\n                                 [12., 50., 26.]]]),\n  ])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_445-495", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                 [5., 8., 5.6666665]],\n                                [[6., 9.5, 6.6666665], [7., 11., 7.6666665],\n                                 [8., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"arange_invarange\",\n          multiply_by=[1, 2, 3],\n          divide_by=[3, 2, 1],\n          expected_transformed=[[[0, 1, 6], [1, 4, 15], [2, 7, 24]],\n                                [[3, 10, 33], [4, 13, 42], [5, 16, 51]],\n                                [[6, 19, 60], [7, 22, 69], [8, 25, 78]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=1,\n          expected_transformed=[[[0., 2., 6.], [9., 8., 5.], [6., 14., 24.]],\n                                [[9., 20., 33.], [36., 26., 14.],\n                                 [15., 32., 51.]],\n                                [[18., 38., 60.], [63., 44., 23.],\n                                 [24., 50., 78.]]]),\n      dict(\n          testcase_name=\"two_dimensional_divide_by\",\n          multiply_by=1,\n          divide_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 5.],\n                                 [6., 3.5, 2.6666667]],\n                                [[9., 5., 3.6666667], [4., 6.5, 14.],\n                                 [15., 8., 5.6666665]],\n                                [[18., 9.5, 6.6666665], [7., 11., 23.],\n                                 [24., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by_and_divide_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=[[3, 2, 1], [1, 2, 3], [2, 1, 3]],\n          expected_transformed=[[[0., 1., 6.], [9., 4., 1.6666666],\n                                 [3., 14., 8.]],\n                                [[3., 10., 33.], [36., 13., 4.6666665],\n                                 [7.5, 32., 17.]],\n                                [[6., 19., 60.], [63., 22., 7.6666665],\n                                 [12., 50., 26.]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_three_dimensions(\n      self, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(27).reshape((3, 3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_455-505", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      dict(\n          testcase_name=\"two_dimensional_multiply_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=1,\n          expected_transformed=[[[0., 2., 6.], [9., 8., 5.], [6., 14., 24.]],\n                                [[9., 20., 33.], [36., 26., 14.],\n                                 [15., 32., 51.]],\n                                [[18., 38., 60.], [63., 44., 23.],\n                                 [24., 50., 78.]]]),\n      dict(\n          testcase_name=\"two_dimensional_divide_by\",\n          multiply_by=1,\n          divide_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 5.],\n                                 [6., 3.5, 2.6666667]],\n                                [[9., 5., 3.6666667], [4., 6.5, 14.],\n                                 [15., 8., 5.6666665]],\n                                [[18., 9.5, 6.6666665], [7., 11., 23.],\n                                 [24., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by_and_divide_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=[[3, 2, 1], [1, 2, 3], [2, 1, 3]],\n          expected_transformed=[[[0., 1., 6.], [9., 4., 1.6666666],\n                                 [3., 14., 8.]],\n                                [[3., 10., 33.], [36., 13., 4.6666665],\n                                 [7.5, 32., 17.]],\n                                [[6., 19., 60.], [63., 22., 7.6666665],\n                                 [12., 50., 26.]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_three_dimensions(\n      self, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(27).reshape((3, 3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"mean_multiply\",\n          multiply_operation=jnp.mean,\n          divide_operation=None,\n          expected_transformed=[[[0., 21., 44., 69., 96.],\n                                 [125., 156., 189., 224., 261.],\n                                 [300., 341., 384., 429., 476.],\n                                 [525., 576., 629., 684., 741.]],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_465-515", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"two_dimensional_divide_by\",\n          multiply_by=1,\n          divide_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          expected_transformed=[[[0., 0.5, 0.6666667], [1., 2., 5.],\n                                 [6., 3.5, 2.6666667]],\n                                [[9., 5., 3.6666667], [4., 6.5, 14.],\n                                 [15., 8., 5.6666665]],\n                                [[18., 9.5, 6.6666665], [7., 11., 23.],\n                                 [24., 12.5, 8.666667]]]),\n      dict(\n          testcase_name=\"two_dimensional_multiply_by_and_divide_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=[[3, 2, 1], [1, 2, 3], [2, 1, 3]],\n          expected_transformed=[[[0., 1., 6.], [9., 4., 1.6666666],\n                                 [3., 14., 8.]],\n                                [[3., 10., 33.], [36., 13., 4.6666665],\n                                 [7.5, 32., 17.]],\n                                [[6., 19., 60.], [63., 22., 7.6666665],\n                                 [12., 50., 26.]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_three_dimensions(\n      self, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(27).reshape((3, 3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"mean_multiply\",\n          multiply_operation=jnp.mean,\n          divide_operation=None,\n          expected_transformed=[[[0., 21., 44., 69., 96.],\n                                 [125., 156., 189., 224., 261.],\n                                 [300., 341., 384., 429., 476.],\n                                 [525., 576., 629., 684., 741.]],\n                                [[400., 441., 484., 529., 576.],\n                                 [625., 676., 729., 784., 841.],\n                                 [900., 961., 1024., 1089., 1156.],\n                                 [1225., 1296., 1369., 1444., 1521.]],\n                                [[800., 861., 924., 989., 1056.],\n                                 [1125., 1196., 1269., 1344., 1421.],\n                                 [1500., 1581., 1664., 1749., 1836.],\n                                 [1925., 2016., 2109., 2204., 2301.]]]),\n      dict(\n          testcase_name=\"mean_divide\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_475-525", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"two_dimensional_multiply_by_and_divide_by\",\n          multiply_by=[[1, 2, 3], [3, 2, 1], [1, 2, 3]],\n          divide_by=[[3, 2, 1], [1, 2, 3], [2, 1, 3]],\n          expected_transformed=[[[0., 1., 6.], [9., 4., 1.6666666],\n                                 [3., 14., 8.]],\n                                [[3., 10., 33.], [36., 13., 4.6666665],\n                                 [7.5, 32., 17.]],\n                                [[6., 19., 60.], [63., 22., 7.6666665],\n                                 [12., 50., 26.]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_three_dimensions(\n      self, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(27).reshape((3, 3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"mean_multiply\",\n          multiply_operation=jnp.mean,\n          divide_operation=None,\n          expected_transformed=[[[0., 21., 44., 69., 96.],\n                                 [125., 156., 189., 224., 261.],\n                                 [300., 341., 384., 429., 476.],\n                                 [525., 576., 629., 684., 741.]],\n                                [[400., 441., 484., 529., 576.],\n                                 [625., 676., 729., 784., 841.],\n                                 [900., 961., 1024., 1089., 1156.],\n                                 [1225., 1296., 1369., 1444., 1521.]],\n                                [[800., 861., 924., 989., 1056.],\n                                 [1125., 1196., 1269., 1344., 1421.],\n                                 [1500., 1581., 1664., 1749., 1836.],\n                                 [1925., 2016., 2109., 2204., 2301.]]]),\n      dict(\n          testcase_name=\"mean_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.09090909, 0.13043478, 0.16666667],\n               [0.2, 0.23076923, 0.25925925, 0.2857143, 0.31034482],\n               [0.33333334, 0.3548387, 0.375, 0.3939394, 0.4117647],\n               [0.42857143, 0.44444445, 0.45945945, 0.47368422, 0.4871795]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]],\n              [[2., 1.9523809, 1.9090909, 1.8695652, 1.8333334],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_485-535", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "  def test_fit_transform_produces_correct_values_in_three_dimensions(\n      self, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(27).reshape((3, 3, 3))\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"mean_multiply\",\n          multiply_operation=jnp.mean,\n          divide_operation=None,\n          expected_transformed=[[[0., 21., 44., 69., 96.],\n                                 [125., 156., 189., 224., 261.],\n                                 [300., 341., 384., 429., 476.],\n                                 [525., 576., 629., 684., 741.]],\n                                [[400., 441., 484., 529., 576.],\n                                 [625., 676., 729., 784., 841.],\n                                 [900., 961., 1024., 1089., 1156.],\n                                 [1225., 1296., 1369., 1444., 1521.]],\n                                [[800., 861., 924., 989., 1056.],\n                                 [1125., 1196., 1269., 1344., 1421.],\n                                 [1500., 1581., 1664., 1749., 1836.],\n                                 [1925., 2016., 2109., 2204., 2301.]]]),\n      dict(\n          testcase_name=\"mean_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.09090909, 0.13043478, 0.16666667],\n               [0.2, 0.23076923, 0.25925925, 0.2857143, 0.31034482],\n               [0.33333334, 0.3548387, 0.375, 0.3939394, 0.4117647],\n               [0.42857143, 0.44444445, 0.45945945, 0.47368422, 0.4871795]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]],\n              [[2., 1.9523809, 1.9090909, 1.8695652, 1.8333334],\n               [1.8, 1.7692307, 1.7407408, 1.7142857, 1.6896552],\n               [1.6666666, 1.6451613, 1.625, 1.6060606, 1.5882353],\n               [1.5714285, 1.5555556, 1.5405406, 1.5263158, 1.5128205]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply\",\n          multiply_operation=jnp.min,\n          divide_operation=None,\n          expected_transformed=[[[0, 1, 4, 9, 16], [25, 36, 49, 64, 81],\n                                 [100, 121, 144, 169, 196],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_495-545", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"mean_multiply\",\n          multiply_operation=jnp.mean,\n          divide_operation=None,\n          expected_transformed=[[[0., 21., 44., 69., 96.],\n                                 [125., 156., 189., 224., 261.],\n                                 [300., 341., 384., 429., 476.],\n                                 [525., 576., 629., 684., 741.]],\n                                [[400., 441., 484., 529., 576.],\n                                 [625., 676., 729., 784., 841.],\n                                 [900., 961., 1024., 1089., 1156.],\n                                 [1225., 1296., 1369., 1444., 1521.]],\n                                [[800., 861., 924., 989., 1056.],\n                                 [1125., 1196., 1269., 1344., 1421.],\n                                 [1500., 1581., 1664., 1749., 1836.],\n                                 [1925., 2016., 2109., 2204., 2301.]]]),\n      dict(\n          testcase_name=\"mean_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.09090909, 0.13043478, 0.16666667],\n               [0.2, 0.23076923, 0.25925925, 0.2857143, 0.31034482],\n               [0.33333334, 0.3548387, 0.375, 0.3939394, 0.4117647],\n               [0.42857143, 0.44444445, 0.45945945, 0.47368422, 0.4871795]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]],\n              [[2., 1.9523809, 1.9090909, 1.8695652, 1.8333334],\n               [1.8, 1.7692307, 1.7407408, 1.7142857, 1.6896552],\n               [1.6666666, 1.6451613, 1.625, 1.6060606, 1.5882353],\n               [1.5714285, 1.5555556, 1.5405406, 1.5263158, 1.5128205]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply\",\n          multiply_operation=jnp.min,\n          divide_operation=None,\n          expected_transformed=[[[0, 1, 4, 9, 16], [25, 36, 49, 64, 81],\n                                 [100, 121, 144, 169, 196],\n                                 [225, 256, 289, 324, 361]],\n                                [[0, 21, 44, 69, 96], [125, 156, 189, 224, 261],\n                                 [300, 341, 384, 429, 476],\n                                 [525, 576, 629, 684, 741]],\n                                [[0, 41, 84, 129, 176],\n                                 [225, 276, 329, 384, 441],\n                                 [500, 561, 624, 689, 756],\n                                 [825, 896, 969, 1044, 1121]]]),\n      dict(\n          testcase_name=\"max_divide\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_505-555", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                [[400., 441., 484., 529., 576.],\n                                 [625., 676., 729., 784., 841.],\n                                 [900., 961., 1024., 1089., 1156.],\n                                 [1225., 1296., 1369., 1444., 1521.]],\n                                [[800., 861., 924., 989., 1056.],\n                                 [1125., 1196., 1269., 1344., 1421.],\n                                 [1500., 1581., 1664., 1749., 1836.],\n                                 [1925., 2016., 2109., 2204., 2301.]]]),\n      dict(\n          testcase_name=\"mean_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.09090909, 0.13043478, 0.16666667],\n               [0.2, 0.23076923, 0.25925925, 0.2857143, 0.31034482],\n               [0.33333334, 0.3548387, 0.375, 0.3939394, 0.4117647],\n               [0.42857143, 0.44444445, 0.45945945, 0.47368422, 0.4871795]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]],\n              [[2., 1.9523809, 1.9090909, 1.8695652, 1.8333334],\n               [1.8, 1.7692307, 1.7407408, 1.7142857, 1.6896552],\n               [1.6666666, 1.6451613, 1.625, 1.6060606, 1.5882353],\n               [1.5714285, 1.5555556, 1.5405406, 1.5263158, 1.5128205]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply\",\n          multiply_operation=jnp.min,\n          divide_operation=None,\n          expected_transformed=[[[0, 1, 4, 9, 16], [25, 36, 49, 64, 81],\n                                 [100, 121, 144, 169, 196],\n                                 [225, 256, 289, 324, 361]],\n                                [[0, 21, 44, 69, 96], [125, 156, 189, 224, 261],\n                                 [300, 341, 384, 429, 476],\n                                 [525, 576, 629, 684, 741]],\n                                [[0, 41, 84, 129, 176],\n                                 [225, 276, 329, 384, 441],\n                                 [500, 561, 624, 689, 756],\n                                 [825, 896, 969, 1044, 1121]]]),\n      dict(\n          testcase_name=\"max_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.max,\n          expected_transformed=[\n              [[0., 0.02439024, 0.04761905, 0.06976745, 0.09090909],\n               [0.11111111, 0.13043478, 0.14893617, 0.16666667, 0.18367347],\n               [0.2, 0.21568628, 0.23076923, 0.24528302, 0.25925925],\n               [0.27272728, 0.2857143, 0.2982456, 0.31034482, 0.3220339]],\n              [[0.5, 0.5121951, 0.52380955, 0.53488374, 0.54545456],\n               [0.5555556, 0.5652174, 0.5744681, 0.5833333, 0.59183675],\n               [0.6, 0.60784316, 0.61538464, 0.6226415, 0.6296296],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_515-565", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_operation=None,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.09090909, 0.13043478, 0.16666667],\n               [0.2, 0.23076923, 0.25925925, 0.2857143, 0.31034482],\n               [0.33333334, 0.3548387, 0.375, 0.3939394, 0.4117647],\n               [0.42857143, 0.44444445, 0.45945945, 0.47368422, 0.4871795]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]],\n              [[2., 1.9523809, 1.9090909, 1.8695652, 1.8333334],\n               [1.8, 1.7692307, 1.7407408, 1.7142857, 1.6896552],\n               [1.6666666, 1.6451613, 1.625, 1.6060606, 1.5882353],\n               [1.5714285, 1.5555556, 1.5405406, 1.5263158, 1.5128205]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply\",\n          multiply_operation=jnp.min,\n          divide_operation=None,\n          expected_transformed=[[[0, 1, 4, 9, 16], [25, 36, 49, 64, 81],\n                                 [100, 121, 144, 169, 196],\n                                 [225, 256, 289, 324, 361]],\n                                [[0, 21, 44, 69, 96], [125, 156, 189, 224, 261],\n                                 [300, 341, 384, 429, 476],\n                                 [525, 576, 629, 684, 741]],\n                                [[0, 41, 84, 129, 176],\n                                 [225, 276, 329, 384, 441],\n                                 [500, 561, 624, 689, 756],\n                                 [825, 896, 969, 1044, 1121]]]),\n      dict(\n          testcase_name=\"max_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.max,\n          expected_transformed=[\n              [[0., 0.02439024, 0.04761905, 0.06976745, 0.09090909],\n               [0.11111111, 0.13043478, 0.14893617, 0.16666667, 0.18367347],\n               [0.2, 0.21568628, 0.23076923, 0.24528302, 0.25925925],\n               [0.27272728, 0.2857143, 0.2982456, 0.31034482, 0.3220339]],\n              [[0.5, 0.5121951, 0.52380955, 0.53488374, 0.54545456],\n               [0.5555556, 0.5652174, 0.5744681, 0.5833333, 0.59183675],\n               [0.6, 0.60784316, 0.61538464, 0.6226415, 0.6296296],\n               [0.6363636, 0.64285713, 0.64912283, 0.6551724, 0.66101694]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply_mean_divide\",\n          multiply_operation=jnp.min,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.18181819, 0.39130434, 0.6666667],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_525-575", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "               [1.8, 1.7692307, 1.7407408, 1.7142857, 1.6896552],\n               [1.6666666, 1.6451613, 1.625, 1.6060606, 1.5882353],\n               [1.5714285, 1.5555556, 1.5405406, 1.5263158, 1.5128205]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply\",\n          multiply_operation=jnp.min,\n          divide_operation=None,\n          expected_transformed=[[[0, 1, 4, 9, 16], [25, 36, 49, 64, 81],\n                                 [100, 121, 144, 169, 196],\n                                 [225, 256, 289, 324, 361]],\n                                [[0, 21, 44, 69, 96], [125, 156, 189, 224, 261],\n                                 [300, 341, 384, 429, 476],\n                                 [525, 576, 629, 684, 741]],\n                                [[0, 41, 84, 129, 176],\n                                 [225, 276, 329, 384, 441],\n                                 [500, 561, 624, 689, 756],\n                                 [825, 896, 969, 1044, 1121]]]),\n      dict(\n          testcase_name=\"max_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.max,\n          expected_transformed=[\n              [[0., 0.02439024, 0.04761905, 0.06976745, 0.09090909],\n               [0.11111111, 0.13043478, 0.14893617, 0.16666667, 0.18367347],\n               [0.2, 0.21568628, 0.23076923, 0.24528302, 0.25925925],\n               [0.27272728, 0.2857143, 0.2982456, 0.31034482, 0.3220339]],\n              [[0.5, 0.5121951, 0.52380955, 0.53488374, 0.54545456],\n               [0.5555556, 0.5652174, 0.5744681, 0.5833333, 0.59183675],\n               [0.6, 0.60784316, 0.61538464, 0.6226415, 0.6296296],\n               [0.6363636, 0.64285713, 0.64912283, 0.6551724, 0.66101694]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply_mean_divide\",\n          multiply_operation=jnp.min,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.18181819, 0.39130434, 0.6666667],\n               [1., 1.3846154, 1.8148148, 2.2857144, 2.7931035],\n               [3.3333333, 3.903226, 4.5, 5.121212, 5.7647057],\n               [6.428571, 7.111111, 7.810811, 8.526316, 9.256411]],\n              [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.],\n               [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]],\n              [[0., 1.9523809, 3.8181818, 5.6086955, 7.3333335],\n               [9., 10.615385, 12.185185, 13.714286, 15.206897],\n               [16.666666, 18.096775, 19.5, 20.878788, 22.235294],\n               [23.571428, 24.88889, 26.18919, 27.473684, 28.74359]]\n          ]),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_535-585", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                 [225, 256, 289, 324, 361]],\n                                [[0, 21, 44, 69, 96], [125, 156, 189, 224, 261],\n                                 [300, 341, 384, 429, 476],\n                                 [525, 576, 629, 684, 741]],\n                                [[0, 41, 84, 129, 176],\n                                 [225, 276, 329, 384, 441],\n                                 [500, 561, 624, 689, 756],\n                                 [825, 896, 969, 1044, 1121]]]),\n      dict(\n          testcase_name=\"max_divide\",\n          multiply_operation=None,\n          divide_operation=jnp.max,\n          expected_transformed=[\n              [[0., 0.02439024, 0.04761905, 0.06976745, 0.09090909],\n               [0.11111111, 0.13043478, 0.14893617, 0.16666667, 0.18367347],\n               [0.2, 0.21568628, 0.23076923, 0.24528302, 0.25925925],\n               [0.27272728, 0.2857143, 0.2982456, 0.31034482, 0.3220339]],\n              [[0.5, 0.5121951, 0.52380955, 0.53488374, 0.54545456],\n               [0.5555556, 0.5652174, 0.5744681, 0.5833333, 0.59183675],\n               [0.6, 0.60784316, 0.61538464, 0.6226415, 0.6296296],\n               [0.6363636, 0.64285713, 0.64912283, 0.6551724, 0.66101694]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply_mean_divide\",\n          multiply_operation=jnp.min,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.18181819, 0.39130434, 0.6666667],\n               [1., 1.3846154, 1.8148148, 2.2857144, 2.7931035],\n               [3.3333333, 3.903226, 4.5, 5.121212, 5.7647057],\n               [6.428571, 7.111111, 7.810811, 8.526316, 9.256411]],\n              [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.],\n               [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]],\n              [[0., 1.9523809, 3.8181818, 5.6086955, 7.3333335],\n               [9., 10.615385, 12.185185, 13.714286, 15.206897],\n               [16.666666, 18.096775, 19.5, 20.878788, 22.235294],\n               [23.571428, 24.88889, 26.18919, 27.473684, 28.74359]]\n          ]),\n  ])\n  def test_fit_transform_works_with_operations_in_three_dimensions(\n      self, multiply_operation, divide_operation, expected_transformed):\n    data = jnp.arange(60).reshape((3, 4, 5))\n\n    scaler = preprocessing.CustomScaler(\n        multiply_operation=multiply_operation,\n        divide_operation=divide_operation)\n    transformed_data = scaler.fit_transform(data)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_545-595", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          multiply_operation=None,\n          divide_operation=jnp.max,\n          expected_transformed=[\n              [[0., 0.02439024, 0.04761905, 0.06976745, 0.09090909],\n               [0.11111111, 0.13043478, 0.14893617, 0.16666667, 0.18367347],\n               [0.2, 0.21568628, 0.23076923, 0.24528302, 0.25925925],\n               [0.27272728, 0.2857143, 0.2982456, 0.31034482, 0.3220339]],\n              [[0.5, 0.5121951, 0.52380955, 0.53488374, 0.54545456],\n               [0.5555556, 0.5652174, 0.5744681, 0.5833333, 0.59183675],\n               [0.6, 0.60784316, 0.61538464, 0.6226415, 0.6296296],\n               [0.6363636, 0.64285713, 0.64912283, 0.6551724, 0.66101694]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply_mean_divide\",\n          multiply_operation=jnp.min,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.18181819, 0.39130434, 0.6666667],\n               [1., 1.3846154, 1.8148148, 2.2857144, 2.7931035],\n               [3.3333333, 3.903226, 4.5, 5.121212, 5.7647057],\n               [6.428571, 7.111111, 7.810811, 8.526316, 9.256411]],\n              [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.],\n               [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]],\n              [[0., 1.9523809, 3.8181818, 5.6086955, 7.3333335],\n               [9., 10.615385, 12.185185, 13.714286, 15.206897],\n               [16.666666, 18.096775, 19.5, 20.878788, 22.235294],\n               [23.571428, 24.88889, 26.18919, 27.473684, 28.74359]]\n          ]),\n  ])\n  def test_fit_transform_works_with_operations_in_three_dimensions(\n      self, multiply_operation, divide_operation, expected_transformed):\n    data = jnp.arange(60).reshape((3, 4, 5))\n\n    scaler = preprocessing.CustomScaler(\n        multiply_operation=multiply_operation,\n        divide_operation=divide_operation)\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"four_dims\",\n          number_of_dimensions=4,\n          multiply_by=5,\n          divide_by=2,\n          expected_transformed=[[[[0., 2.5], [5, 7.5]], [[10, 12.5], [15,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_555-605", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "               [0.6363636, 0.64285713, 0.64912283, 0.6551724, 0.66101694]],\n              [[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1.]]\n          ]),\n      dict(\n          testcase_name=\"min_multiply_mean_divide\",\n          multiply_operation=jnp.min,\n          divide_operation=jnp.mean,\n          expected_transformed=[\n              [[0., 0.04761905, 0.18181819, 0.39130434, 0.6666667],\n               [1., 1.3846154, 1.8148148, 2.2857144, 2.7931035],\n               [3.3333333, 3.903226, 4.5, 5.121212, 5.7647057],\n               [6.428571, 7.111111, 7.810811, 8.526316, 9.256411]],\n              [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.],\n               [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]],\n              [[0., 1.9523809, 3.8181818, 5.6086955, 7.3333335],\n               [9., 10.615385, 12.185185, 13.714286, 15.206897],\n               [16.666666, 18.096775, 19.5, 20.878788, 22.235294],\n               [23.571428, 24.88889, 26.18919, 27.473684, 28.74359]]\n          ]),\n  ])\n  def test_fit_transform_works_with_operations_in_three_dimensions(\n      self, multiply_operation, divide_operation, expected_transformed):\n    data = jnp.arange(60).reshape((3, 4, 5))\n\n    scaler = preprocessing.CustomScaler(\n        multiply_operation=multiply_operation,\n        divide_operation=divide_operation)\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"four_dims\",\n          number_of_dimensions=4,\n          multiply_by=5,\n          divide_by=2,\n          expected_transformed=[[[[0., 2.5], [5, 7.5]], [[10, 12.5], [15,\n                                                                      17.5]]],\n                                [[[20, 22.5], [25, 27.5]],\n                                 [[30, 32.5], [35, 37.5]]]]),\n      dict(\n          testcase_name=\"five_dims\",\n          number_of_dimensions=5,\n          multiply_by=3,\n          divide_by=2,\n          expected_transformed=[[[[[0, 1.5], [3, 4.5]], [[6, 7.5], [9, 10.5]]],\n                                 [[[12, 13.5], [15, 16.5]],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_565-615", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "               [1., 1.3846154, 1.8148148, 2.2857144, 2.7931035],\n               [3.3333333, 3.903226, 4.5, 5.121212, 5.7647057],\n               [6.428571, 7.111111, 7.810811, 8.526316, 9.256411]],\n              [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.],\n               [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]],\n              [[0., 1.9523809, 3.8181818, 5.6086955, 7.3333335],\n               [9., 10.615385, 12.185185, 13.714286, 15.206897],\n               [16.666666, 18.096775, 19.5, 20.878788, 22.235294],\n               [23.571428, 24.88889, 26.18919, 27.473684, 28.74359]]\n          ]),\n  ])\n  def test_fit_transform_works_with_operations_in_three_dimensions(\n      self, multiply_operation, divide_operation, expected_transformed):\n    data = jnp.arange(60).reshape((3, 4, 5))\n\n    scaler = preprocessing.CustomScaler(\n        multiply_operation=multiply_operation,\n        divide_operation=divide_operation)\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"four_dims\",\n          number_of_dimensions=4,\n          multiply_by=5,\n          divide_by=2,\n          expected_transformed=[[[[0., 2.5], [5, 7.5]], [[10, 12.5], [15,\n                                                                      17.5]]],\n                                [[[20, 22.5], [25, 27.5]],\n                                 [[30, 32.5], [35, 37.5]]]]),\n      dict(\n          testcase_name=\"five_dims\",\n          number_of_dimensions=5,\n          multiply_by=3,\n          divide_by=2,\n          expected_transformed=[[[[[0, 1.5], [3, 4.5]], [[6, 7.5], [9, 10.5]]],\n                                 [[[12, 13.5], [15, 16.5]],\n                                  [[18, 19.5], [21, 22.5]]]],\n                                [[[[24, 25.5], [27, 28.5]],\n                                  [[30, 31.5], [33, 34.5]]],\n                                 [[[36, 37.5], [39, 40.5]],\n                                  [[42, 43.5], [45, 46.5]]]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_higher_dimensions(\n      self, number_of_dimensions, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(2**number_of_dimensions).reshape([2] *\n                                                       number_of_dimensions)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_575-625", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "  ])\n  def test_fit_transform_works_with_operations_in_three_dimensions(\n      self, multiply_operation, divide_operation, expected_transformed):\n    data = jnp.arange(60).reshape((3, 4, 5))\n\n    scaler = preprocessing.CustomScaler(\n        multiply_operation=multiply_operation,\n        divide_operation=divide_operation)\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"four_dims\",\n          number_of_dimensions=4,\n          multiply_by=5,\n          divide_by=2,\n          expected_transformed=[[[[0., 2.5], [5, 7.5]], [[10, 12.5], [15,\n                                                                      17.5]]],\n                                [[[20, 22.5], [25, 27.5]],\n                                 [[30, 32.5], [35, 37.5]]]]),\n      dict(\n          testcase_name=\"five_dims\",\n          number_of_dimensions=5,\n          multiply_by=3,\n          divide_by=2,\n          expected_transformed=[[[[[0, 1.5], [3, 4.5]], [[6, 7.5], [9, 10.5]]],\n                                 [[[12, 13.5], [15, 16.5]],\n                                  [[18, 19.5], [21, 22.5]]]],\n                                [[[[24, 25.5], [27, 28.5]],\n                                  [[30, 31.5], [33, 34.5]]],\n                                 [[[36, 37.5], [39, 40.5]],\n                                  [[42, 43.5], [45, 46.5]]]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_higher_dimensions(\n      self, number_of_dimensions, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(2**number_of_dimensions).reshape([2] *\n                                                       number_of_dimensions)\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_585-635", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"four_dims\",\n          number_of_dimensions=4,\n          multiply_by=5,\n          divide_by=2,\n          expected_transformed=[[[[0., 2.5], [5, 7.5]], [[10, 12.5], [15,\n                                                                      17.5]]],\n                                [[[20, 22.5], [25, 27.5]],\n                                 [[30, 32.5], [35, 37.5]]]]),\n      dict(\n          testcase_name=\"five_dims\",\n          number_of_dimensions=5,\n          multiply_by=3,\n          divide_by=2,\n          expected_transformed=[[[[[0, 1.5], [3, 4.5]], [[6, 7.5], [9, 10.5]]],\n                                 [[[12, 13.5], [15, 16.5]],\n                                  [[18, 19.5], [21, 22.5]]]],\n                                [[[[24, 25.5], [27, 28.5]],\n                                  [[30, 31.5], [33, 34.5]]],\n                                 [[[36, 37.5], [39, 40.5]],\n                                  [[42, 43.5], [45, 46.5]]]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_higher_dimensions(\n      self, number_of_dimensions, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(2**number_of_dimensions).reshape([2] *\n                                                       number_of_dimensions)\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES,)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_595-645", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                                                      17.5]]],\n                                [[[20, 22.5], [25, 27.5]],\n                                 [[30, 32.5], [35, 37.5]]]]),\n      dict(\n          testcase_name=\"five_dims\",\n          number_of_dimensions=5,\n          multiply_by=3,\n          divide_by=2,\n          expected_transformed=[[[[[0, 1.5], [3, 4.5]], [[6, 7.5], [9, 10.5]]],\n                                 [[[12, 13.5], [15, 16.5]],\n                                  [[18, 19.5], [21, 22.5]]]],\n                                [[[[24, 25.5], [27, 28.5]],\n                                  [[30, 31.5], [33, 34.5]]],\n                                 [[[36, 37.5], [39, 40.5]],\n                                  [[42, 43.5], [45, 46.5]]]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_higher_dimensions(\n      self, number_of_dimensions, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(2**number_of_dimensions).reshape([2] *\n                                                       number_of_dimensions)\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES,)\n  ])\n  def test_compute_correlations_returns_expected_values(\n      self, features, target, expected_correlations):\n    features = jnp.array(features)\n    target = jnp.array(target)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    correlations = preprocessing._compute_correlations(\n        features=features, target=target, feature_names=feature_names)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_605-655", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                  [[18, 19.5], [21, 22.5]]]],\n                                [[[[24, 25.5], [27, 28.5]],\n                                  [[30, 31.5], [33, 34.5]]],\n                                 [[[36, 37.5], [39, 40.5]],\n                                  [[42, 43.5], [45, 46.5]]]]]),\n  ])\n  def test_fit_transform_produces_correct_values_in_higher_dimensions(\n      self, number_of_dimensions, multiply_by, divide_by, expected_transformed):\n    data = jnp.arange(2**number_of_dimensions).reshape([2] *\n                                                       number_of_dimensions)\n\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES,)\n  ])\n  def test_compute_correlations_returns_expected_values(\n      self, features, target, expected_correlations):\n    features = jnp.array(features)\n    target = jnp.array(target)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    correlations = preprocessing._compute_correlations(\n        features=features, target=target, feature_names=feature_names)\n\n    for i, expected_correlation in enumerate(expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1_dimensional_target\",\n          features=np.ones([5, 2, 3]),\n          target=np.zeros(5),\n          expected_message=(r\"Incompatible shapes between features \\(5, 2, 3\\) \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_615-665", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\n    scaler = preprocessing.CustomScaler(\n        divide_by=jnp.array(divide_by), multiply_by=jnp.array(multiply_by))\n    transformed_data = scaler.fit_transform(data)\n\n    np.testing.assert_array_almost_equal(transformed_data,\n                                         jnp.array(expected_transformed))\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES,)\n  ])\n  def test_compute_correlations_returns_expected_values(\n      self, features, target, expected_correlations):\n    features = jnp.array(features)\n    target = jnp.array(target)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    correlations = preprocessing._compute_correlations(\n        features=features, target=target, feature_names=feature_names)\n\n    for i, expected_correlation in enumerate(expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1_dimensional_target\",\n          features=np.ones([5, 2, 3]),\n          target=np.zeros(5),\n          expected_message=(r\"Incompatible shapes between features \\(5, 2, 3\\) \"\n                            r\"and target \\(5,\\)\\.\")\n      ),\n      dict(\n          testcase_name=\"2_dimensional_target\",\n          features=np.ones([10, 5]),\n          target=np.zeros([5, 5]),\n          expected_message=(r\"Incompatible shapes between features \\(10, 5\\) \"\n                            r\"and target \\(5, 5\\)\\.\"))\n  ])\n  def test_compute_correlations_raises_value_error(self, features, target,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_625-675", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES,)\n  ])\n  def test_compute_correlations_returns_expected_values(\n      self, features, target, expected_correlations):\n    features = jnp.array(features)\n    target = jnp.array(target)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    correlations = preprocessing._compute_correlations(\n        features=features, target=target, feature_names=feature_names)\n\n    for i, expected_correlation in enumerate(expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1_dimensional_target\",\n          features=np.ones([5, 2, 3]),\n          target=np.zeros(5),\n          expected_message=(r\"Incompatible shapes between features \\(5, 2, 3\\) \"\n                            r\"and target \\(5,\\)\\.\")\n      ),\n      dict(\n          testcase_name=\"2_dimensional_target\",\n          features=np.ones([10, 5]),\n          target=np.zeros([5, 5]),\n          expected_message=(r\"Incompatible shapes between features \\(10, 5\\) \"\n                            r\"and target \\(5, 5\\)\\.\"))\n  ])\n  def test_compute_correlations_raises_value_error(self, features, target,\n                                                   expected_message):\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_correlations(\n          features=features, target=target, feature_names=feature_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_635-685", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "  ])\n  def test_compute_correlations_returns_expected_values(\n      self, features, target, expected_correlations):\n    features = jnp.array(features)\n    target = jnp.array(target)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    correlations = preprocessing._compute_correlations(\n        features=features, target=target, feature_names=feature_names)\n\n    for i, expected_correlation in enumerate(expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1_dimensional_target\",\n          features=np.ones([5, 2, 3]),\n          target=np.zeros(5),\n          expected_message=(r\"Incompatible shapes between features \\(5, 2, 3\\) \"\n                            r\"and target \\(5,\\)\\.\")\n      ),\n      dict(\n          testcase_name=\"2_dimensional_target\",\n          features=np.ones([10, 5]),\n          target=np.zeros([5, 5]),\n          expected_message=(r\"Incompatible shapes between features \\(10, 5\\) \"\n                            r\"and target \\(5, 5\\)\\.\"))\n  ])\n  def test_compute_correlations_raises_value_error(self, features, target,\n                                                   expected_message):\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_correlations(\n          features=features, target=target, feature_names=feature_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES),\n      ])\n  def test_check_data_quality_with_extra_features(self, features, target,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_645-695", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    for i, expected_correlation in enumerate(expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"1_dimensional_target\",\n          features=np.ones([5, 2, 3]),\n          target=np.zeros(5),\n          expected_message=(r\"Incompatible shapes between features \\(5, 2, 3\\) \"\n                            r\"and target \\(5,\\)\\.\")\n      ),\n      dict(\n          testcase_name=\"2_dimensional_target\",\n          features=np.ones([10, 5]),\n          target=np.zeros([5, 5]),\n          expected_message=(r\"Incompatible shapes between features \\(10, 5\\) \"\n                            r\"and target \\(5, 5\\)\\.\"))\n  ])\n  def test_compute_correlations_raises_value_error(self, features, target,\n                                                   expected_message):\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_correlations(\n          features=features, target=target, feature_names=feature_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES),\n      ])\n  def test_check_data_quality_with_extra_features(self, features, target,\n                                                  expected_correlations):\n    media_data = jnp.array(features)[:, :2]\n    costs = np.ones(media_data.shape[1])\n    extra_features = jnp.array(features)[:, 2:]\n    extra_features_transformer = {\n        \"feature_2\": \"extra_feature_0\",\n        \"feature_3\": \"extra_feature_1\",\n    }\n    updated_expected_correlations = [\n        x.rename(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_655-705", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                            r\"and target \\(5,\\)\\.\")\n      ),\n      dict(\n          testcase_name=\"2_dimensional_target\",\n          features=np.ones([10, 5]),\n          target=np.zeros([5, 5]),\n          expected_message=(r\"Incompatible shapes between features \\(10, 5\\) \"\n                            r\"and target \\(5, 5\\)\\.\"))\n  ])\n  def test_compute_correlations_raises_value_error(self, features, target,\n                                                   expected_message):\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_correlations(\n          features=features, target=target, feature_names=feature_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES),\n      ])\n  def test_check_data_quality_with_extra_features(self, features, target,\n                                                  expected_correlations):\n    media_data = jnp.array(features)[:, :2]\n    costs = np.ones(media_data.shape[1])\n    extra_features = jnp.array(features)[:, 2:]\n    extra_features_transformer = {\n        \"feature_2\": \"extra_feature_0\",\n        \"feature_3\": \"extra_feature_1\",\n    }\n    updated_expected_correlations = [\n        x.rename(\n            index=extra_features_transformer,\n            columns=extra_features_transformer) for x in expected_correlations\n    ]\n\n    correlations, _, _, _ = preprocessing.check_data_quality(\n        media_data=media_data,\n        target_data=jnp.array(target),\n        cost_data=costs,\n        extra_features_data=extra_features)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_665-715", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                                   expected_message):\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_correlations(\n          features=features, target=target, feature_names=feature_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES),\n      ])\n  def test_check_data_quality_with_extra_features(self, features, target,\n                                                  expected_correlations):\n    media_data = jnp.array(features)[:, :2]\n    costs = np.ones(media_data.shape[1])\n    extra_features = jnp.array(features)[:, 2:]\n    extra_features_transformer = {\n        \"feature_2\": \"extra_feature_0\",\n        \"feature_3\": \"extra_feature_1\",\n    }\n    updated_expected_correlations = [\n        x.rename(\n            index=extra_features_transformer,\n            columns=extra_features_transformer) for x in expected_correlations\n    ]\n\n    correlations, _, _, _ = preprocessing.check_data_quality(\n        media_data=media_data,\n        target_data=jnp.array(target),\n        cost_data=costs,\n        extra_features_data=extra_features)\n\n    for i, expected_correlation in enumerate(updated_expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_variances=_NATIONAL_VARIANCES,\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_675-725", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          features=_NATIONAL_DATA_FOR_TESTS,\n          target=_NATIONAL_TARGET_DATA,\n          expected_correlations=_NATIONAL_CORRELATION_MATRICES),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          target=_GEO_TARGET_DATA,\n          expected_correlations=_GEO_CORRELATION_MATRICES),\n      ])\n  def test_check_data_quality_with_extra_features(self, features, target,\n                                                  expected_correlations):\n    media_data = jnp.array(features)[:, :2]\n    costs = np.ones(media_data.shape[1])\n    extra_features = jnp.array(features)[:, 2:]\n    extra_features_transformer = {\n        \"feature_2\": \"extra_feature_0\",\n        \"feature_3\": \"extra_feature_1\",\n    }\n    updated_expected_correlations = [\n        x.rename(\n            index=extra_features_transformer,\n            columns=extra_features_transformer) for x in expected_correlations\n    ]\n\n    correlations, _, _, _ = preprocessing.check_data_quality(\n        media_data=media_data,\n        target_data=jnp.array(target),\n        cost_data=costs,\n        extra_features_data=extra_features)\n\n    for i, expected_correlation in enumerate(updated_expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_variances=_NATIONAL_VARIANCES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_variances=_GEO_VARIANCES,\n      )\n  ])\n  def test_compute_variances_returns_expected_values(self, features,\n                                                     expected_variances):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_685-735", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                                                  expected_correlations):\n    media_data = jnp.array(features)[:, :2]\n    costs = np.ones(media_data.shape[1])\n    extra_features = jnp.array(features)[:, 2:]\n    extra_features_transformer = {\n        \"feature_2\": \"extra_feature_0\",\n        \"feature_3\": \"extra_feature_1\",\n    }\n    updated_expected_correlations = [\n        x.rename(\n            index=extra_features_transformer,\n            columns=extra_features_transformer) for x in expected_correlations\n    ]\n\n    correlations, _, _, _ = preprocessing.check_data_quality(\n        media_data=media_data,\n        target_data=jnp.array(target),\n        cost_data=costs,\n        extra_features_data=extra_features)\n\n    for i, expected_correlation in enumerate(updated_expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_variances=_NATIONAL_VARIANCES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_variances=_GEO_VARIANCES,\n      )\n  ])\n  def test_compute_variances_returns_expected_values(self, features,\n                                                     expected_variances):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    variances = preprocessing._compute_variances(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        variances, expected_variances, atol=1e-3, check_dtype=False)\n\n  def test_check_data_quality_raises_error_on_media_channel_name_mismatch(self):\n    expected_message = (\"Number of channels in media_data does not match \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_695-745", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "            index=extra_features_transformer,\n            columns=extra_features_transformer) for x in expected_correlations\n    ]\n\n    correlations, _, _, _ = preprocessing.check_data_quality(\n        media_data=media_data,\n        target_data=jnp.array(target),\n        cost_data=costs,\n        extra_features_data=extra_features)\n\n    for i, expected_correlation in enumerate(updated_expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_variances=_NATIONAL_VARIANCES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_variances=_GEO_VARIANCES,\n      )\n  ])\n  def test_compute_variances_returns_expected_values(self, features,\n                                                     expected_variances):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    variances = preprocessing._compute_variances(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        variances, expected_variances, atol=1e-3, check_dtype=False)\n\n  def test_check_data_quality_raises_error_on_media_channel_name_mismatch(self):\n    expected_message = (\"Number of channels in media_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\"],\n          cost_data=jnp.ones(3))\n\n  def test_check_data_quality_raises_error_on_extra_feature_name_mismatch(self):\n    expected_message = (\"Number of features in extra_features_data does not \"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_705-755", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    for i, expected_correlation in enumerate(updated_expected_correlations):\n      pd.testing.assert_frame_equal(\n          correlations[i], expected_correlation, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_variances=_NATIONAL_VARIANCES,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_variances=_GEO_VARIANCES,\n      )\n  ])\n  def test_compute_variances_returns_expected_values(self, features,\n                                                     expected_variances):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    variances = preprocessing._compute_variances(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        variances, expected_variances, atol=1e-3, check_dtype=False)\n\n  def test_check_data_quality_raises_error_on_media_channel_name_mismatch(self):\n    expected_message = (\"Number of channels in media_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\"],\n          cost_data=jnp.ones(3))\n\n  def test_check_data_quality_raises_error_on_extra_feature_name_mismatch(self):\n    expected_message = (\"Number of features in extra_features_data does not \"\n                        \"match length of extra_features\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          cost_data=jnp.ones(3),\n          extra_features_data=jnp.ones([3, 4]),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          extra_features_names=[\"extra_feature_0\", \"extra_feature_1\"])\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_715-765", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_variances=_GEO_VARIANCES,\n      )\n  ])\n  def test_compute_variances_returns_expected_values(self, features,\n                                                     expected_variances):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    variances = preprocessing._compute_variances(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        variances, expected_variances, atol=1e-3, check_dtype=False)\n\n  def test_check_data_quality_raises_error_on_media_channel_name_mismatch(self):\n    expected_message = (\"Number of channels in media_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\"],\n          cost_data=jnp.ones(3))\n\n  def test_check_data_quality_raises_error_on_extra_feature_name_mismatch(self):\n    expected_message = (\"Number of features in extra_features_data does not \"\n                        \"match length of extra_features\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          cost_data=jnp.ones(3),\n          extra_features_data=jnp.ones([3, 4]),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          extra_features_names=[\"extra_feature_0\", \"extra_feature_1\"])\n\n  def test_check_data_quality_raises_error_on_cost_data_mismatch(self):\n    expected_message = (\"Number of channels in cost_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          cost_data=jnp.ones(5))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_725-775", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    variances = preprocessing._compute_variances(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        variances, expected_variances, atol=1e-3, check_dtype=False)\n\n  def test_check_data_quality_raises_error_on_media_channel_name_mismatch(self):\n    expected_message = (\"Number of channels in media_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\"],\n          cost_data=jnp.ones(3))\n\n  def test_check_data_quality_raises_error_on_extra_feature_name_mismatch(self):\n    expected_message = (\"Number of features in extra_features_data does not \"\n                        \"match length of extra_features\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          cost_data=jnp.ones(3),\n          extra_features_data=jnp.ones([3, 4]),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          extra_features_names=[\"extra_feature_0\", \"extra_feature_1\"])\n\n  def test_check_data_quality_raises_error_on_cost_data_mismatch(self):\n    expected_message = (\"Number of channels in cost_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          cost_data=jnp.ones(5))\n\n  @parameterized.product(\n      (dict(\n          costs=np.arange(1, 10),\n          expected_spend_fractions=np.arange(1, 10) / np.arange(1, 10).sum()),\n       dict(costs=np.ones(5), expected_spend_fractions=np.ones(5) / 5.)),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in \"ABCDEFGHIJ\"])))\n  def test_compute_spend_fraction_results_are_correct(self, costs,\n                                                      expected_spend_fractions,\n                                                      channel_names):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_735-785", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\"],\n          cost_data=jnp.ones(3))\n\n  def test_check_data_quality_raises_error_on_extra_feature_name_mismatch(self):\n    expected_message = (\"Number of features in extra_features_data does not \"\n                        \"match length of extra_features\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          cost_data=jnp.ones(3),\n          extra_features_data=jnp.ones([3, 4]),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          extra_features_names=[\"extra_feature_0\", \"extra_feature_1\"])\n\n  def test_check_data_quality_raises_error_on_cost_data_mismatch(self):\n    expected_message = (\"Number of channels in cost_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          cost_data=jnp.ones(5))\n\n  @parameterized.product(\n      (dict(\n          costs=np.arange(1, 10),\n          expected_spend_fractions=np.arange(1, 10) / np.arange(1, 10).sum()),\n       dict(costs=np.ones(5), expected_spend_fractions=np.ones(5) / 5.)),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in \"ABCDEFGHIJ\"])))\n  def test_compute_spend_fraction_results_are_correct(self, costs,\n                                                      expected_spend_fractions,\n                                                      channel_names):\n    if channel_names is not None:\n      channel_names = channel_names[:len(costs)]\n    expected_output = pd.DataFrame(\n        expected_spend_fractions,\n        index=channel_names,\n        columns=[\"fraction of spend\"])\n\n    spend_fractions = preprocessing._compute_spend_fractions(\n        costs, channel_names)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_745-795", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "                        \"match length of extra_features\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          cost_data=jnp.ones(3),\n          extra_features_data=jnp.ones([3, 4]),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          extra_features_names=[\"extra_feature_0\", \"extra_feature_1\"])\n\n  def test_check_data_quality_raises_error_on_cost_data_mismatch(self):\n    expected_message = (\"Number of channels in cost_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          cost_data=jnp.ones(5))\n\n  @parameterized.product(\n      (dict(\n          costs=np.arange(1, 10),\n          expected_spend_fractions=np.arange(1, 10) / np.arange(1, 10).sum()),\n       dict(costs=np.ones(5), expected_spend_fractions=np.ones(5) / 5.)),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in \"ABCDEFGHIJ\"])))\n  def test_compute_spend_fraction_results_are_correct(self, costs,\n                                                      expected_spend_fractions,\n                                                      channel_names):\n    if channel_names is not None:\n      channel_names = channel_names[:len(costs)]\n    expected_output = pd.DataFrame(\n        expected_spend_fractions,\n        index=channel_names,\n        columns=[\"fraction of spend\"])\n\n    spend_fractions = preprocessing._compute_spend_fractions(\n        costs, channel_names)\n\n    pd.testing.assert_frame_equal(spend_fractions, expected_output, atol=1e-3)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"single_zero\", costs=np.arange(10)),\n      dict(testcase_name=\"all_zeros\", costs=np.zeros(10)),\n      dict(testcase_name=\"negative_number_and_zero\", costs=np.arange(-1, 10)),\n  ])\n  def test_compute_spend_fraction_raises_error_on_non_positive_costs(\n      self, costs):\n    expected_message = (\"Values in cost_data must all be positive\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_755-805", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "  def test_check_data_quality_raises_error_on_cost_data_mismatch(self):\n    expected_message = (\"Number of channels in cost_data does not match \"\n                        \"length of channel_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing.check_data_quality(\n          media_data=jnp.ones([3, 3]),\n          target_data=jnp.ones(3),\n          channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n          cost_data=jnp.ones(5))\n\n  @parameterized.product(\n      (dict(\n          costs=np.arange(1, 10),\n          expected_spend_fractions=np.arange(1, 10) / np.arange(1, 10).sum()),\n       dict(costs=np.ones(5), expected_spend_fractions=np.ones(5) / 5.)),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in \"ABCDEFGHIJ\"])))\n  def test_compute_spend_fraction_results_are_correct(self, costs,\n                                                      expected_spend_fractions,\n                                                      channel_names):\n    if channel_names is not None:\n      channel_names = channel_names[:len(costs)]\n    expected_output = pd.DataFrame(\n        expected_spend_fractions,\n        index=channel_names,\n        columns=[\"fraction of spend\"])\n\n    spend_fractions = preprocessing._compute_spend_fractions(\n        costs, channel_names)\n\n    pd.testing.assert_frame_equal(spend_fractions, expected_output, atol=1e-3)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"single_zero\", costs=np.arange(10)),\n      dict(testcase_name=\"all_zeros\", costs=np.zeros(10)),\n      dict(testcase_name=\"negative_number_and_zero\", costs=np.arange(-1, 10)),\n  ])\n  def test_compute_spend_fraction_raises_error_on_non_positive_costs(\n      self, costs):\n    expected_message = (\"Values in cost_data must all be positive\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_spend_fractions(costs)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_vifs=_NATIONAL_VIFS,\n      ),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_765-815", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "  @parameterized.product(\n      (dict(\n          costs=np.arange(1, 10),\n          expected_spend_fractions=np.arange(1, 10) / np.arange(1, 10).sum()),\n       dict(costs=np.ones(5), expected_spend_fractions=np.ones(5) / 5.)),\n      (dict(channel_names=None),\n       dict(channel_names=[f\"channel_{x}\" for x in \"ABCDEFGHIJ\"])))\n  def test_compute_spend_fraction_results_are_correct(self, costs,\n                                                      expected_spend_fractions,\n                                                      channel_names):\n    if channel_names is not None:\n      channel_names = channel_names[:len(costs)]\n    expected_output = pd.DataFrame(\n        expected_spend_fractions,\n        index=channel_names,\n        columns=[\"fraction of spend\"])\n\n    spend_fractions = preprocessing._compute_spend_fractions(\n        costs, channel_names)\n\n    pd.testing.assert_frame_equal(spend_fractions, expected_output, atol=1e-3)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"single_zero\", costs=np.arange(10)),\n      dict(testcase_name=\"all_zeros\", costs=np.zeros(10)),\n      dict(testcase_name=\"negative_number_and_zero\", costs=np.arange(-1, 10)),\n  ])\n  def test_compute_spend_fraction_raises_error_on_non_positive_costs(\n      self, costs):\n    expected_message = (\"Values in cost_data must all be positive\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_spend_fractions(costs)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_vifs=_NATIONAL_VIFS,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_vifs=_GEO_VIFS,\n      )\n  ])\n  def test_compute_vifs_returns_expected_values(self, features, expected_vifs):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_775-825", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    if channel_names is not None:\n      channel_names = channel_names[:len(costs)]\n    expected_output = pd.DataFrame(\n        expected_spend_fractions,\n        index=channel_names,\n        columns=[\"fraction of spend\"])\n\n    spend_fractions = preprocessing._compute_spend_fractions(\n        costs, channel_names)\n\n    pd.testing.assert_frame_equal(spend_fractions, expected_output, atol=1e-3)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"single_zero\", costs=np.arange(10)),\n      dict(testcase_name=\"all_zeros\", costs=np.zeros(10)),\n      dict(testcase_name=\"negative_number_and_zero\", costs=np.arange(-1, 10)),\n  ])\n  def test_compute_spend_fraction_raises_error_on_non_positive_costs(\n      self, costs):\n    expected_message = (\"Values in cost_data must all be positive\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_spend_fractions(costs)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_vifs=_NATIONAL_VIFS,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_vifs=_GEO_VIFS,\n      )\n  ])\n  def test_compute_vifs_returns_expected_values(self, features, expected_vifs):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  def test_extreme_values_for_compute_vifs(self):\n    df = pd.DataFrame(\n        data={\n            \"column_A\": np.arange(25),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_785-835", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    pd.testing.assert_frame_equal(spend_fractions, expected_output, atol=1e-3)\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"single_zero\", costs=np.arange(10)),\n      dict(testcase_name=\"all_zeros\", costs=np.zeros(10)),\n      dict(testcase_name=\"negative_number_and_zero\", costs=np.arange(-1, 10)),\n  ])\n  def test_compute_spend_fraction_raises_error_on_non_positive_costs(\n      self, costs):\n    expected_message = (\"Values in cost_data must all be positive\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_spend_fractions(costs)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_vifs=_NATIONAL_VIFS,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_vifs=_GEO_VIFS,\n      )\n  ])\n  def test_compute_vifs_returns_expected_values(self, features, expected_vifs):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  def test_extreme_values_for_compute_vifs(self):\n    df = pd.DataFrame(\n        data={\n            \"column_A\": np.arange(25),\n            \"column_B\": np.arange(25)**0.5 + 2,\n            \"column_C\": np.arange(25)**0.25 - 5,\n            \"all_ones\": np.ones(25),\n            \"all_zeros\": np.zeros(25)\n        })\n    df[\"linear_transform_of_column_A\"] = 30 - df[\"column_A\"]\n    df[\"copy_of_column_B\"] = df[\"column_B\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=df.values, feature_names=df.columns, geo_names=[\"a_geo\"])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_795-845", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_spend_fractions(costs)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          expected_vifs=_NATIONAL_VIFS,\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_vifs=_GEO_VIFS,\n      )\n  ])\n  def test_compute_vifs_returns_expected_values(self, features, expected_vifs):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  def test_extreme_values_for_compute_vifs(self):\n    df = pd.DataFrame(\n        data={\n            \"column_A\": np.arange(25),\n            \"column_B\": np.arange(25)**0.5 + 2,\n            \"column_C\": np.arange(25)**0.25 - 5,\n            \"all_ones\": np.ones(25),\n            \"all_zeros\": np.zeros(25)\n        })\n    df[\"linear_transform_of_column_A\"] = 30 - df[\"column_A\"]\n    df[\"copy_of_column_B\"] = df[\"column_B\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=df.values, feature_names=df.columns, geo_names=[\"a_geo\"])\n    expected_vifs = pd.DataFrame(\n        data=[np.inf, np.inf, 57.8253, 0, np.nan, np.inf, np.inf],\n        columns=[\"a_geo\"],\n        index=[\n            \"column_A\", \"column_B\", \"column_C\", \"all_ones\", \"all_zeros\",\n            \"linear_transform_of_column_A\", \"copy_of_column_B\"\n        ])\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_805-855", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          expected_vifs=_GEO_VIFS,\n      )\n  ])\n  def test_compute_vifs_returns_expected_values(self, features, expected_vifs):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    geo_names = [\"geo_0\", \"geo_1\"] if features.ndim == 3 else [\"geo_0\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  def test_extreme_values_for_compute_vifs(self):\n    df = pd.DataFrame(\n        data={\n            \"column_A\": np.arange(25),\n            \"column_B\": np.arange(25)**0.5 + 2,\n            \"column_C\": np.arange(25)**0.25 - 5,\n            \"all_ones\": np.ones(25),\n            \"all_zeros\": np.zeros(25)\n        })\n    df[\"linear_transform_of_column_A\"] = 30 - df[\"column_A\"]\n    df[\"copy_of_column_B\"] = df[\"column_B\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=df.values, feature_names=df.columns, geo_names=[\"a_geo\"])\n    expected_vifs = pd.DataFrame(\n        data=[np.inf, np.inf, 57.8253, 0, np.nan, np.inf, np.inf],\n        columns=[\"a_geo\"],\n        index=[\n            \"column_A\", \"column_B\", \"column_C\", \"all_ones\", \"all_zeros\",\n            \"linear_transform_of_column_A\", \"copy_of_column_B\"\n        ])\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_815-865", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    vifs = preprocessing._compute_variance_inflation_factors(\n        features=features, feature_names=feature_names, geo_names=geo_names)\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  def test_extreme_values_for_compute_vifs(self):\n    df = pd.DataFrame(\n        data={\n            \"column_A\": np.arange(25),\n            \"column_B\": np.arange(25)**0.5 + 2,\n            \"column_C\": np.arange(25)**0.25 - 5,\n            \"all_ones\": np.ones(25),\n            \"all_zeros\": np.zeros(25)\n        })\n    df[\"linear_transform_of_column_A\"] = 30 - df[\"column_A\"]\n    df[\"copy_of_column_B\"] = df[\"column_B\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=df.values, feature_names=df.columns, geo_names=[\"a_geo\"])\n    expected_vifs = pd.DataFrame(\n        data=[np.inf, np.inf, 57.8253, 0, np.nan, np.inf, np.inf],\n        columns=[\"a_geo\"],\n        index=[\n            \"column_A\", \"column_B\", \"column_C\", \"all_ones\", \"all_zeros\",\n            \"linear_transform_of_column_A\", \"copy_of_column_B\"\n        ])\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      )\n  ])\n  def test_compute_vifs_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\"The number of geos in features does not match the \"\n                        \"length of geo_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_825-875", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "            \"column_B\": np.arange(25)**0.5 + 2,\n            \"column_C\": np.arange(25)**0.25 - 5,\n            \"all_ones\": np.ones(25),\n            \"all_zeros\": np.zeros(25)\n        })\n    df[\"linear_transform_of_column_A\"] = 30 - df[\"column_A\"]\n    df[\"copy_of_column_B\"] = df[\"column_B\"]\n\n    vifs = preprocessing._compute_variance_inflation_factors(\n        features=df.values, feature_names=df.columns, geo_names=[\"a_geo\"])\n    expected_vifs = pd.DataFrame(\n        data=[np.inf, np.inf, 57.8253, 0, np.nan, np.inf, np.inf],\n        columns=[\"a_geo\"],\n        index=[\n            \"column_A\", \"column_B\", \"column_C\", \"all_ones\", \"all_zeros\",\n            \"linear_transform_of_column_A\", \"copy_of_column_B\"\n        ])\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      )\n  ])\n  def test_compute_vifs_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\"The number of geos in features does not match the \"\n                        \"length of geo_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variance_inflation_factors(\n          features=features, feature_names=feature_names, geo_names=geo_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_835-885", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    expected_vifs = pd.DataFrame(\n        data=[np.inf, np.inf, 57.8253, 0, np.nan, np.inf, np.inf],\n        columns=[\"a_geo\"],\n        index=[\n            \"column_A\", \"column_B\", \"column_C\", \"all_ones\", \"all_zeros\",\n            \"linear_transform_of_column_A\", \"copy_of_column_B\"\n        ])\n\n    pd.testing.assert_frame_equal(\n        vifs, expected_vifs, atol=1e-3, check_dtype=False)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      )\n  ])\n  def test_compute_vifs_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\"The number of geos in features does not match the \"\n                        \"length of geo_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variance_inflation_factors(\n          features=features, feature_names=feature_names, geo_names=geo_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      ),\n  ])\n  def test_compute_variances_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names\n  ):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_845-895", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      )\n  ])\n  def test_compute_vifs_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\"The number of geos in features does not match the \"\n                        \"length of geo_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variance_inflation_factors(\n          features=features, feature_names=feature_names, geo_names=geo_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      ),\n  ])\n  def test_compute_variances_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names\n  ):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\n        \"The number of geos in features does not match the length of geo_names\"\n    )\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variances(\n          features=features, feature_names=feature_names, geo_names=geo_names\n      )\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_855-905", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          geo_names=[\"geo_0\"],\n      )\n  ])\n  def test_compute_vifs_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\"The number of geos in features does not match the \"\n                        \"length of geo_names\")\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variance_inflation_factors(\n          features=features, feature_names=feature_names, geo_names=geo_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      ),\n  ])\n  def test_compute_variances_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names\n  ):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\n        \"The number of geos in features does not match the length of geo_names\"\n    )\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variances(\n          features=features, feature_names=feature_names, geo_names=geo_names\n      )\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"default_names\",\n          extra_features_names=[\n              \"extra_feature_0\",\n              \"extra_feature_1\",\n              \"extra_feature_2\",\n          ],\n      ),\n      dict(\n          testcase_name=\"custom_names\",\n          extra_features_names=[", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_865-915", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "      preprocessing._compute_variance_inflation_factors(\n          features=features, feature_names=feature_names, geo_names=geo_names)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_data\",\n          features=_NATIONAL_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\", \"geo_1\"],\n      ),\n      dict(\n          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      ),\n  ])\n  def test_compute_variances_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names\n  ):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\n        \"The number of geos in features does not match the length of geo_names\"\n    )\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variances(\n          features=features, feature_names=feature_names, geo_names=geo_names\n      )\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"default_names\",\n          extra_features_names=[\n              \"extra_feature_0\",\n              \"extra_feature_1\",\n              \"extra_feature_2\",\n          ],\n      ),\n      dict(\n          testcase_name=\"custom_names\",\n          extra_features_names=[\n              \"my_feature_A\",\n              \"my_feature_1\",\n              \"my_feature_gamma\",\n          ],\n      ),\n  ])\n  def test_check_data_quality_propagates_extra_features_names_into_output(\n      self, extra_features_names\n  ):\n    correlations, variances, _, variance_inflation_factors = (", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_875-925", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"geo_data\",\n          features=_GEO_DATA_FOR_TESTS,\n          geo_names=[\"geo_0\"],\n      ),\n  ])\n  def test_compute_variances_raises_error_for_incorrect_number_of_geo_names(\n      self, features, geo_names\n  ):\n    features = jnp.array(features)\n    feature_names = [f\"feature_{i}\" for i in range(features.shape[1])]\n    expected_message = (\n        \"The number of geos in features does not match the length of geo_names\"\n    )\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variances(\n          features=features, feature_names=feature_names, geo_names=geo_names\n      )\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"default_names\",\n          extra_features_names=[\n              \"extra_feature_0\",\n              \"extra_feature_1\",\n              \"extra_feature_2\",\n          ],\n      ),\n      dict(\n          testcase_name=\"custom_names\",\n          extra_features_names=[\n              \"my_feature_A\",\n              \"my_feature_1\",\n              \"my_feature_gamma\",\n          ],\n      ),\n  ])\n  def test_check_data_quality_propagates_extra_features_names_into_output(\n      self, extra_features_names\n  ):\n    correlations, variances, _, variance_inflation_factors = (\n        preprocessing.check_data_quality(\n            media_data=jnp.ones([3, 3]),\n            extra_features_data=jnp.ones([3, 3]),\n            extra_features_names=extra_features_names,\n            target_data=jnp.ones(3),\n            channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n            cost_data=jnp.ones(3),\n        )\n    )\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 925, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_885-935", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "    expected_message = (\n        \"The number of geos in features does not match the length of geo_names\"\n    )\n    with self.assertRaisesRegex(ValueError, expected_message):\n      preprocessing._compute_variances(\n          features=features, feature_names=feature_names, geo_names=geo_names\n      )\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"default_names\",\n          extra_features_names=[\n              \"extra_feature_0\",\n              \"extra_feature_1\",\n              \"extra_feature_2\",\n          ],\n      ),\n      dict(\n          testcase_name=\"custom_names\",\n          extra_features_names=[\n              \"my_feature_A\",\n              \"my_feature_1\",\n              \"my_feature_gamma\",\n          ],\n      ),\n  ])\n  def test_check_data_quality_propagates_extra_features_names_into_output(\n      self, extra_features_names\n  ):\n    correlations, variances, _, variance_inflation_factors = (\n        preprocessing.check_data_quality(\n            media_data=jnp.ones([3, 3]),\n            extra_features_data=jnp.ones([3, 3]),\n            extra_features_names=extra_features_names,\n            target_data=jnp.ones(3),\n            channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n            cost_data=jnp.ones(3),\n        )\n    )\n\n    self.assertContainsSubset(\n        extra_features_names, correlations[0].index.to_list()\n    )\n    self.assertContainsSubset(extra_features_names, variances.index.to_list())\n    self.assertContainsSubset(\n        extra_features_names, variance_inflation_factors.index.to_list()\n    )\n\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 935, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_895-936", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "          testcase_name=\"default_names\",\n          extra_features_names=[\n              \"extra_feature_0\",\n              \"extra_feature_1\",\n              \"extra_feature_2\",\n          ],\n      ),\n      dict(\n          testcase_name=\"custom_names\",\n          extra_features_names=[\n              \"my_feature_A\",\n              \"my_feature_1\",\n              \"my_feature_gamma\",\n          ],\n      ),\n  ])\n  def test_check_data_quality_propagates_extra_features_names_into_output(\n      self, extra_features_names\n  ):\n    correlations, variances, _, variance_inflation_factors = (\n        preprocessing.check_data_quality(\n            media_data=jnp.ones([3, 3]),\n            extra_features_data=jnp.ones([3, 3]),\n            extra_features_names=extra_features_names,\n            target_data=jnp.ones(3),\n            channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n            cost_data=jnp.ones(3),\n        )\n    )\n\n    self.assertContainsSubset(\n        extra_features_names, correlations[0].index.to_list()\n    )\n    self.assertContainsSubset(extra_features_names, variances.index.to_list())\n    self.assertContainsSubset(\n        extra_features_names, variance_inflation_factors.index.to_list()\n    )\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 936, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-preprocessing_test.py_905-936", "title": "google_lightweight_mmm-lightweight_mmm-preprocessing_test.py", "text": "              \"my_feature_A\",\n              \"my_feature_1\",\n              \"my_feature_gamma\",\n          ],\n      ),\n  ])\n  def test_check_data_quality_propagates_extra_features_names_into_output(\n      self, extra_features_names\n  ):\n    correlations, variances, _, variance_inflation_factors = (\n        preprocessing.check_data_quality(\n            media_data=jnp.ones([3, 3]),\n            extra_features_data=jnp.ones([3, 3]),\n            extra_features_names=extra_features_names,\n            target_data=jnp.ones(3),\n            channel_names=[\"channel_one\", \"channel_two\", \"channel_three\"],\n            cost_data=jnp.ones(3),\n        )\n    )\n\n    self.assertContainsSubset(\n        extra_features_names, correlations[0].index.to_list()\n    )\n    self.assertContainsSubset(extra_features_names, variances.index.to_list())\n    self.assertContainsSubset(\n        extra_features_names, variance_inflation_factors.index.to_list()\n    )\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "preprocessing_test.py"], "line_no": 930, "start_line_no": 905, "end_line_no": 936, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of utilities for LightweighMMM package.\"\"\"\nimport pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of utilities for LightweighMMM package.\"\"\"\nimport pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nfrom scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of utilities for LightweighMMM package.\"\"\"\nimport pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nfrom scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,\n    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of utilities for LightweighMMM package.\"\"\"\nimport pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nfrom scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,\n    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "import pickle\nimport time\nfrom typing import Any, List, Optional, Tuple\n\nfrom absl import logging\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nfrom scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,\n    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\nAST=Module(Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Constant)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load)))))For(Name(Store)Call(Name(Load)Name(Load))If(Call(Attribute(Name(Load)Load)Constant)Continue)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Expr(Call(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))))Name(Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "from scipy import optimize\nfrom scipy import spatial\nfrom scipy import stats\nfrom tensorflow.io import gfile\n\nfrom lightweight_mmm import media_transforms\n\n\ndef save_model(\n    media_mix_model: Any,\n    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n\n\ndef get_time_seed() -> int:\n  \"\"\"Generates an integer using the last decimals of time.time().\n\n  Returns:\n    Integer to be used as seed.\n  \"\"\"\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load)))))Constant)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load)))))For(Name(Store)Call(Name(Load)Name(Load))If(Call(Attribute(Name(Load)Load)Constant)Continue)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Expr(Call(Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))))Return(Name(Load))Name(Load))FunctionDef(argumentsExpr(Constant)Name(Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    file_path: str\n    ) -> None:\n  \"\"\"Saves the given model in the given path.\n\n  Args:\n    media_mix_model: Model to save on disk.\n    file_path: File path where the model should be placed.\n  \"\"\"\n  with gfile.GFile(file_path, \"wb\") as file:\n    pickle.dump(obj=media_mix_model, file=file)\n\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n\n\ndef get_time_seed() -> int:\n  \"\"\"Generates an integer using the last decimals of time.time().\n\n  Returns:\n    Integer to be used as seed.\n  \"\"\"\n  # time.time() has the following format: 1645174953.0429401\n  return int(str(time.time()).split(\".\")[1])\n\n\ndef simulate_dummy_data(\n    data_size: int,\n    n_media_channels: int,\n    n_extra_features: int,\n    geos: int = 1,\n    seed: int = 5", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n\ndef load_model(file_path: str) -> Any:\n  \"\"\"Loads a model given a string path.\n\n  Args:\n    file_path: Path of the file containing the model.\n\n  Returns:\n    The LightweightMMM object that was stored in the given path.\n  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n\n\ndef get_time_seed() -> int:\n  \"\"\"Generates an integer using the last decimals of time.time().\n\n  Returns:\n    Integer to be used as seed.\n  \"\"\"\n  # time.time() has the following format: 1645174953.0429401\n  return int(str(time.time()).split(\".\")[1])\n\n\ndef simulate_dummy_data(\n    data_size: int,\n    n_media_channels: int,\n    n_extra_features: int,\n    geos: int = 1,\n    seed: int = 5\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Simulates dummy data needed for media mix modelling.\n\n  This function's goal is to be super simple and not have many parameters,\n  although it does not generate a fully realistic dataset is only meant to be\n  used for demos/tutorial purposes. Uses carryover for lagging but has no\n  saturation and no trend.\n\n  The data simulated includes the media data, extra features, a target/KPI and\n  costs.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  \"\"\"\n  with gfile.GFile(file_path, \"rb\") as file:\n    media_mix_model = pickle.load(file=file)\n\n  for attr in dir(media_mix_model):\n    if attr.startswith(\"__\"):\n      continue\n    attr_value = getattr(media_mix_model, attr)\n    if isinstance(attr_value, np.ndarray):\n      setattr(media_mix_model, attr, jnp.array(attr_value))\n\n  return media_mix_model\n\n\ndef get_time_seed() -> int:\n  \"\"\"Generates an integer using the last decimals of time.time().\n\n  Returns:\n    Integer to be used as seed.\n  \"\"\"\n  # time.time() has the following format: 1645174953.0429401\n  return int(str(time.time()).split(\".\")[1])\n\n\ndef simulate_dummy_data(\n    data_size: int,\n    n_media_channels: int,\n    n_extra_features: int,\n    geos: int = 1,\n    seed: int = 5\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Simulates dummy data needed for media mix modelling.\n\n  This function's goal is to be super simple and not have many parameters,\n  although it does not generate a fully realistic dataset is only meant to be\n  used for demos/tutorial purposes. Uses carryover for lagging but has no\n  saturation and no trend.\n\n  The data simulated includes the media data, extra features, a target/KPI and\n  costs.\n\n  Args:\n    data_size: Number of rows to generate.\n    n_media_channels: Number of media channels to generate.\n    n_extra_features: Number of extra features to generate.\n    geos: Number of geos for geo level data (default = 1 for national).\n    seed: Random seed.\n\n  Returns:\n    The simulated media, extra features, target and costs.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n  return media_mix_model\n\n\ndef get_time_seed() -> int:\n  \"\"\"Generates an integer using the last decimals of time.time().\n\n  Returns:\n    Integer to be used as seed.\n  \"\"\"\n  # time.time() has the following format: 1645174953.0429401\n  return int(str(time.time()).split(\".\")[1])\n\n\ndef simulate_dummy_data(\n    data_size: int,\n    n_media_channels: int,\n    n_extra_features: int,\n    geos: int = 1,\n    seed: int = 5\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Simulates dummy data needed for media mix modelling.\n\n  This function's goal is to be super simple and not have many parameters,\n  although it does not generate a fully realistic dataset is only meant to be\n  used for demos/tutorial purposes. Uses carryover for lagging but has no\n  saturation and no trend.\n\n  The data simulated includes the media data, extra features, a target/KPI and\n  costs.\n\n  Args:\n    data_size: Number of rows to generate.\n    n_media_channels: Number of media channels to generate.\n    n_extra_features: Number of extra features to generate.\n    geos: Number of geos for geo level data (default = 1 for national).\n    seed: Random seed.\n\n  Returns:\n    The simulated media, extra features, target and costs.\n  \"\"\"\n  if data_size < 1 or n_media_channels < 1 or n_extra_features < 1:\n    raise ValueError(\n        \"Data size, n_media_channels and n_extra_features must be greater than\"\n        \" 0. Please check the values introduced are greater than zero.\")\n  data_offset = int(data_size * 0.2)\n  data_size += data_offset\n  key = random.PRNGKey(seed)\n  sub_keys = random.split(key=key, num=7)\n  media_data = random.normal(key=sub_keys[0],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  # time.time() has the following format: 1645174953.0429401\n  return int(str(time.time()).split(\".\")[1])\n\n\ndef simulate_dummy_data(\n    data_size: int,\n    n_media_channels: int,\n    n_extra_features: int,\n    geos: int = 1,\n    seed: int = 5\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Simulates dummy data needed for media mix modelling.\n\n  This function's goal is to be super simple and not have many parameters,\n  although it does not generate a fully realistic dataset is only meant to be\n  used for demos/tutorial purposes. Uses carryover for lagging but has no\n  saturation and no trend.\n\n  The data simulated includes the media data, extra features, a target/KPI and\n  costs.\n\n  Args:\n    data_size: Number of rows to generate.\n    n_media_channels: Number of media channels to generate.\n    n_extra_features: Number of extra features to generate.\n    geos: Number of geos for geo level data (default = 1 for national).\n    seed: Random seed.\n\n  Returns:\n    The simulated media, extra features, target and costs.\n  \"\"\"\n  if data_size < 1 or n_media_channels < 1 or n_extra_features < 1:\n    raise ValueError(\n        \"Data size, n_media_channels and n_extra_features must be greater than\"\n        \" 0. Please check the values introduced are greater than zero.\")\n  data_offset = int(data_size * 0.2)\n  data_size += data_offset\n  key = random.PRNGKey(seed)\n  sub_keys = random.split(key=key, num=7)\n  media_data = random.normal(key=sub_keys[0],\n                             shape=(data_size, n_media_channels)) * 1.5 + 20\n\n  extra_features = random.normal(key=sub_keys[1],\n                                 shape=(data_size, n_extra_features)) + 5\n  # Reduce the costs to make ROI realistic.\n  costs = media_data[data_offset:].sum(axis=0) * .1\n\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=2,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Simulates dummy data needed for media mix modelling.\n\n  This function's goal is to be super simple and not have many parameters,\n  although it does not generate a fully realistic dataset is only meant to be\n  used for demos/tutorial purposes. Uses carryover for lagging but has no\n  saturation and no trend.\n\n  The data simulated includes the media data, extra features, a target/KPI and\n  costs.\n\n  Args:\n    data_size: Number of rows to generate.\n    n_media_channels: Number of media channels to generate.\n    n_extra_features: Number of extra features to generate.\n    geos: Number of geos for geo level data (default = 1 for national).\n    seed: Random seed.\n\n  Returns:\n    The simulated media, extra features, target and costs.\n  \"\"\"\n  if data_size < 1 or n_media_channels < 1 or n_extra_features < 1:\n    raise ValueError(\n        \"Data size, n_media_channels and n_extra_features must be greater than\"\n        \" 0. Please check the values introduced are greater than zero.\")\n  data_offset = int(data_size * 0.2)\n  data_size += data_offset\n  key = random.PRNGKey(seed)\n  sub_keys = random.split(key=key, num=7)\n  media_data = random.normal(key=sub_keys[0],\n                             shape=(data_size, n_media_channels)) * 1.5 + 20\n\n  extra_features = random.normal(key=sub_keys[1],\n                                 shape=(data_size, n_extra_features)) + 5\n  # Reduce the costs to make ROI realistic.\n  costs = media_data[data_offset:].sum(axis=0) * .1\n\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=2,\n      frequency=52,\n      gamma_seasonality=1)\n  target_noise = random.normal(key=sub_keys[2], shape=(data_size,)) + 3\n\n  # media_data_transformed = media_transforms.adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n  Args:\n    data_size: Number of rows to generate.\n    n_media_channels: Number of media channels to generate.\n    n_extra_features: Number of extra features to generate.\n    geos: Number of geos for geo level data (default = 1 for national).\n    seed: Random seed.\n\n  Returns:\n    The simulated media, extra features, target and costs.\n  \"\"\"\n  if data_size < 1 or n_media_channels < 1 or n_extra_features < 1:\n    raise ValueError(\n        \"Data size, n_media_channels and n_extra_features must be greater than\"\n        \" 0. Please check the values introduced are greater than zero.\")\n  data_offset = int(data_size * 0.2)\n  data_size += data_offset\n  key = random.PRNGKey(seed)\n  sub_keys = random.split(key=key, num=7)\n  media_data = random.normal(key=sub_keys[0],\n                             shape=(data_size, n_media_channels)) * 1.5 + 20\n\n  extra_features = random.normal(key=sub_keys[1],\n                                 shape=(data_size, n_extra_features)) + 5\n  # Reduce the costs to make ROI realistic.\n  costs = media_data[data_offset:].sum(axis=0) * .1\n\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=2,\n      frequency=52,\n      gamma_seasonality=1)\n  target_noise = random.normal(key=sub_keys[2], shape=(data_size,)) + 3\n\n  # media_data_transformed = media_transforms.adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1\n  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  \"\"\"\n  if data_size < 1 or n_media_channels < 1 or n_extra_features < 1:\n    raise ValueError(\n        \"Data size, n_media_channels and n_extra_features must be greater than\"\n        \" 0. Please check the values introduced are greater than zero.\")\n  data_offset = int(data_size * 0.2)\n  data_size += data_offset\n  key = random.PRNGKey(seed)\n  sub_keys = random.split(key=key, num=7)\n  media_data = random.normal(key=sub_keys[0],\n                             shape=(data_size, n_media_channels)) * 1.5 + 20\n\n  extra_features = random.normal(key=sub_keys[1],\n                                 shape=(data_size, n_extra_features)) + 5\n  # Reduce the costs to make ROI realistic.\n  costs = media_data[data_offset:].sum(axis=0) * .1\n\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=2,\n      frequency=52,\n      gamma_seasonality=1)\n  target_noise = random.normal(key=sub_keys[2], shape=(data_size,)) + 3\n\n  # media_data_transformed = media_transforms.adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1\n  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]\n      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "                             shape=(data_size, n_media_channels)) * 1.5 + 20\n\n  extra_features = random.normal(key=sub_keys[1],\n                                 shape=(data_size, n_extra_features)) + 5\n  # Reduce the costs to make ROI realistic.\n  costs = media_data[data_offset:].sum(axis=0) * .1\n\n  seasonality = media_transforms.calculate_seasonality(\n      number_periods=data_size,\n      degrees=2,\n      frequency=52,\n      gamma_seasonality=1)\n  target_noise = random.normal(key=sub_keys[2], shape=(data_size,)) + 3\n\n  # media_data_transformed = media_transforms.adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1\n  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]\n      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "      frequency=52,\n      gamma_seasonality=1)\n  target_noise = random.normal(key=sub_keys[2], shape=(data_size,)) + 3\n\n  # media_data_transformed = media_transforms.adstock(media_data)\n  media_data_transformed = media_transforms.carryover(\n      data=media_data,\n      ad_effect_retention_rate=jnp.full((n_media_channels,), fill_value=.5),\n      peak_effect_delay=jnp.full((n_media_channels,), fill_value=1.))\n  beta_media = random.normal(key=sub_keys[3], shape=(n_media_channels,)) + 1\n  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]\n      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  beta_extra_features = random.normal(key=sub_keys[4],\n                                      shape=(n_extra_features,))\n  # There is no trend to keep this very simple.\n  target = 15 + seasonality + media_data_transformed.dot(\n      beta_media) + extra_features.dot(beta_extra_features) + target_noise\n\n  logging.info(\"Correlation between transformed media and target\")\n  logging.info([\n      np.corrcoef(target[data_offset:], media_data_transformed[data_offset:,\n                                                               i])[0, 1]\n      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "      for i in range(n_media_channels)\n  ])\n\n  logging.info(\"True ROI for media channels\")\n  logging.info([\n      sum(media_data_transformed[data_offset:, i] * beta_media[i]) / costs[i]\n      for i in range(n_media_channels)\n  ])\n\n  if geos > 1:\n    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    # Distribute national data to geo and add some more noise.\n    weights = random.uniform(key=sub_keys[5], shape=(1, geos))\n    weights /= sum(weights)\n    target_noise = random.normal(key=sub_keys[6], shape=(data_size, geos)) * .5\n    target = target[:, np.newaxis].dot(weights) + target_noise\n    media_data = media_data[:, :, np.newaxis].dot(weights)\n    extra_features = extra_features[:, :, np.newaxis].dot(weights)\n\n  return (media_data[data_offset:], extra_features[data_offset:],\n          target[data_offset:], costs)\n\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n\ndef _split_array_into_list(\n    dataframe: pd.DataFrame,\n    split_level_feature: str,\n    features: List[str],\n    national_model_flag: bool = True) -> List[np.ndarray]:\n  \"\"\"Splits data frame into list of jax arrays.\n\n  Args:\n    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    dataframe: Dataframe with all the modeling feature.\n    split_level_feature: Feature that will be used to split.\n    features: List of feature to export from data frame.\n    national_model_flag: Whether the data frame is used for national model.\n\n  Returns:\n    List of jax arrays.\n  \"\"\"\n  split_level = dataframe[split_level_feature].unique()\n  array_list_by_level = [\n      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "      dataframe.loc[dataframe[split_level_feature] == level, features].values.T\n      for level in split_level\n  ]\n  feature_array = jnp.stack(array_list_by_level)\n  if national_model_flag:\n    feature_array = jnp.squeeze(feature_array, axis=2)\n  return feature_array\n\n\ndef dataframe_to_jax(\n    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    dataframe: pd.DataFrame,\n    media_features: List[str],\n    extra_features: List[str],\n    date_feature: str,\n    target: str,\n    geo_feature: Optional[str] = None,\n    cost_features: Optional[List[str]] = None\n    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Converts pandas dataframe to right data format for media mix model.\n\n  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  This function's goal is to convert dataframe which is most familar with data\n  scientists to jax arrays to help the users who are not familar with array to\n  use the lightweight MMM library easier.\n\n  Args:\n    dataframe: Dataframe with geo, KPI, media and non-media features.\n    media_features: List of media feature names.\n    extra_features: List of non media feature names.\n    date_feature: Date feature name.\n    target: Target variables name.\n    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    geo_feature: Geo feature name and it is optional if the data is at national\n      level.\n    cost_features: List of media cost variables and it is optional if user\n      use actual media cost as their media features in the model.\n\n  Returns:\n    Media, extra features, target and costs arrays.\n\n  Raises:\n    ValueError: If each geo has unequal number of weeks or there is only one\n    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    value in the geo feature.\n  \"\"\"\n  if geo_feature is not None:\n    if dataframe[geo_feature].nunique() == 1:\n      raise ValueError(\n          \"Geo feature has at least two geos or keep default for national model\"\n          )\n    count_by_geo = dataframe.groupby(\n        geo_feature)[date_feature].count().reset_index()\n    unique_date_count = count_by_geo[date_feature].nunique()\n    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    if unique_date_count != 1:\n      raise ValueError(\"Not all the geos have same number of weeks.\")\n    national_model_flag = False\n    features_to_sort = [date_feature, geo_feature]\n  else:\n    national_model_flag = True\n    features_to_sort = [date_feature]\n\n  df_sorted = dataframe.sort_values(by=features_to_sort)\n  media_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=media_features,\n      national_model_flag=national_model_flag)\n\n  extra_features_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=extra_features,\n      national_model_flag=national_model_flag)\n\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n  target_data = _split_array_into_list(\n      dataframe=df_sorted,\n      split_level_feature=date_feature,\n      features=[target],\n      national_model_flag=national_model_flag)\n  target_data = jnp.squeeze(target_data)\n\n  if cost_features:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "        jnp.full(len(dataframe), 1), dataframe[cost_features].values)\n  else:\n    cost_data = jnp.dot(\n        jnp.full(len(dataframe), 1), dataframe[media_features].values)\n  return (media_features_data, extra_features_data, target_data, cost_data)# jax-ndarray\n\n\ndef get_halfnormal_mean_from_scale(scale: float) -> float:\n  \"\"\"Returns the mean of the half-normal distribition.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  return scale * np.sqrt(2) / np.sqrt(np.pi)\n\n\ndef get_halfnormal_scale_from_mean(mean: float) -> float:\n  \"\"\"Returns the scale of the half-normal distribution.\"\"\"\n  # https://en.wikipedia.org/wiki/Half-normal_distribution\n  return mean * np.sqrt(np.pi) / np.sqrt(2)\n\n\ndef get_beta_params_from_mu_sigma(mu: float,\n                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "                                  sigma: float,\n                                  bracket: Tuple[float, float] = (.5, 100.)\n                                  ) -> Tuple[float, float]:\n  \"\"\"Deterministically estimates (a, b) from (mu, sigma) of a beta variable.\n\n  https://en.wikipedia.org/wiki/Beta_distribution\n\n  Args:\n    mu: The sample mean of the beta distributed variable.\n    sigma: The sample standard deviation of the beta distributed variable.\n    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    bracket: Search bracket for b.\n\n  Returns:\n    Tuple of the (a, b) parameters.\n  \"\"\"\n  # Assume a = 1 to find b.\n  def _f(x):\n    return x ** 2 + 4 * x + 5 + 2 / x - 1 / sigma ** 2\n  b = optimize.root_scalar(_f, bracket=bracket, method=\"brentq\").root\n  # Given b, now find a better a.\n  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  a = b / (1 / mu - 1)\n  return a, b\n\n\ndef _estimate_pdf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates smooth pdf with Gaussian kernel.\n\n  Args:\n    p: Samples.\n    x: The continuous x space (sorted).\n\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n  Returns:\n    A density vector.\n  \"\"\"\n  density = sum(stats.norm(xi).pdf(x) for xi in p)\n  return density / density.sum()\n\n\ndef _pmf(p: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimates discrete pmf.\n\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"\n\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "\n  Args:\n    p: Samples.\n    x: The discrete x space (sorted).\n\n  Returns:\n    A pmf vector.\n  \"\"\"\n  p_cdf = jnp.array([jnp.sum(p <= x[i]) for i in range(len(x))])\n  p_pmf = np.concatenate([[p_cdf[0]], jnp.diff(p_cdf)])\n  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"\n\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  return p_pmf / p_pmf.sum()\n\n\ndef distance_pior_posterior(p: jnp.ndarray, q: jnp.ndarray, method: str = \"KS\",\n                            discrete: bool = True) -> float:\n  \"\"\"Quantifies the distance between two distributions.\n\n  Note we do not use KL divergence because it's not defined when a probability\n  is 0.\n\n  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"\n\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n\n\ndef interpolate_outliers(x: jnp.ndarray,\n                         outlier_idx: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Overwrites outliers in x with interpolated values.\n\n  Args:\n    x: The original univariate variable with outliers.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  https://en.wikipedia.org/wiki/Hellinger_distance\n\n  Args:\n    p: Samples for distribution 1.\n    q: Samples for distribution 2.\n    method: We can have four methods: KS, Hellinger, JS and min.\n    discrete: Whether input data is discrete or continuous.\n\n  Returns:\n    The distance metric (between 0 and 1).\n  \"\"\"\n\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n\n\ndef interpolate_outliers(x: jnp.ndarray,\n                         outlier_idx: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Overwrites outliers in x with interpolated values.\n\n  Args:\n    x: The original univariate variable with outliers.\n    outlier_idx: Indices of the outliers in x.\n\n  Returns:\n    A cleaned x with outliers overwritten.\n\n  \"\"\"\n  time_idx = jnp.arange(len(x))\n  inverse_idx = jnp.array([i for i in range(len(x)) if i not in outlier_idx])\n  interp_func = interpolate.interp1d(\n      time_idx[inverse_idx], x[inverse_idx], kind=\"linear\")", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_355-397", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "  \"\"\"\n\n  if method == \"KS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html\n    return stats.ks_2samp(p, q).statistic\n  elif method in [\"Hellinger\", \"JS\", \"min\"]:\n    if discrete:\n      x = jnp.unique(jnp.concatenate((p, q)))\n      p_pdf = _pmf(p, x)\n      q_pdf = _pmf(q, x)\n    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n\n\ndef interpolate_outliers(x: jnp.ndarray,\n                         outlier_idx: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Overwrites outliers in x with interpolated values.\n\n  Args:\n    x: The original univariate variable with outliers.\n    outlier_idx: Indices of the outliers in x.\n\n  Returns:\n    A cleaned x with outliers overwritten.\n\n  \"\"\"\n  time_idx = jnp.arange(len(x))\n  inverse_idx = jnp.array([i for i in range(len(x)) if i not in outlier_idx])\n  interp_func = interpolate.interp1d(\n      time_idx[inverse_idx], x[inverse_idx], kind=\"linear\")\n  x = x.at[outlier_idx].set(interp_func(time_idx[outlier_idx]))\n  return x", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 397, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils.py_365-397", "title": "google_lightweight_mmm-lightweight_mmm-utils.py", "text": "    else:\n      minx, maxx = min(p.min(), q.min()), max(p.max(), q.max())\n      x = np.linspace(minx, maxx, 100)\n      p_pdf = _estimate_pdf(p, x)\n      q_pdf = _estimate_pdf(q, x)\n  if method == \"Hellinger\":\n    return np.sqrt(jnp.sum((np.sqrt(p_pdf) - np.sqrt(q_pdf)) ** 2)) / np.sqrt(2)\n  elif method == \"JS\":\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    return spatial.distance.jensenshannon(p_pdf, q_pdf)\n  else:\n    return 1 - np.minimum(p_pdf, q_pdf).sum()\n\n\ndef interpolate_outliers(x: jnp.ndarray,\n                         outlier_idx: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Overwrites outliers in x with interpolated values.\n\n  Args:\n    x: The original univariate variable with outliers.\n    outlier_idx: Indices of the outliers in x.\n\n  Returns:\n    A cleaned x with outliers overwritten.\n\n  \"\"\"\n  time_idx = jnp.arange(len(x))\n  inverse_idx = jnp.array([i for i in range(len(x)) if i not in outlier_idx])\n  interp_func = interpolate.interp1d(\n      time_idx[inverse_idx], x[inverse_idx], kind=\"linear\")\n  x = x.at[outlier_idx].set(interp_func(time_idx[outlier_idx]))\n  return x", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 397, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for utils.\"\"\"\n\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for utils.\"\"\"\n\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import utils\n\n_MEDIA_DATAFRAME = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1]],\n    columns=[", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for utils.\"\"\"\n\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import utils\n\n_MEDIA_DATAFRAME = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n_MEDIA_DATAFRAME_UNSORTED = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for utils.\"\"\"\n\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import utils\n\n_MEDIA_DATAFRAME = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n_MEDIA_DATAFRAME_UNSORTED = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_save_model_file_is_correctly_saved(self):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\n\nfrom lightweight_mmm import lightweight_mmm\nfrom lightweight_mmm import utils\n\n_MEDIA_DATAFRAME = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n_MEDIA_DATAFRAME_UNSORTED = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_save_model_file_is_correctly_saved(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "from lightweight_mmm import utils\n\n_MEDIA_DATAFRAME = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n_MEDIA_DATAFRAME_UNSORTED = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_save_model_file_is_correctly_saved(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    self.assertTrue(os.path.exists(file_path))\n\n\nAST=Module(ImportFrom(alias)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(List(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)Load))keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(List(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)Load))keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)keyword(Attribute(Name(Load)Load))))Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load)Constant)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Call(Attribute(Name(Load)Load))Load)Constant))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n_MEDIA_DATAFRAME_UNSORTED = pd.DataFrame(\n    data=[[\"2020-01-01\", \"geo2\", 40, 6, 2, 2, 1, 0, 1],\n          [\"2020-01-08\", \"geo1\", 21, 1, 7, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo3\", 20, 5, 3, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo1\", 10, 2, 3, 2, 1, 1, 1],\n          [\"2020-01-08\", \"geo2\", 27, 3, 9, 2, 1, 1, 1],\n          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_save_model_file_is_correctly_saved(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    self.assertTrue(os.path.exists(file_path))\n\n  def test_load_model_with_all_attributes(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          [\"2020-01-01\", \"geo3\", 33, 7, 5, 2, 1, 0, 1]],\n    columns=[\n        \"date\", \"geo\", \"kpi\", \"channel1_imp\", \"channel2_imp\",\n        \"channel1_cost\", \"channel2_cost\", \"promo_1\", \"promo_2\"\n    ])\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_save_model_file_is_correctly_saved(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    self.assertTrue(os.path.exists(file_path))\n\n  def test_load_model_with_all_attributes(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    loaded_mmm = utils.load_model(file_path)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    self.assertTrue(os.path.exists(file_path))\n\n  def test_load_model_with_all_attributes(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    loaded_mmm = utils.load_model(file_path)\n\n    self.assertEqual(mmm_object, loaded_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_100_3_3\",\n          data_size=100,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_8_1\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    self.assertTrue(os.path.exists(file_path))\n\n  def test_load_model_with_all_attributes(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    loaded_mmm = utils.load_model(file_path)\n\n    self.assertEqual(mmm_object, loaded_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_100_3_3\",\n          data_size=100,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_8_1\",\n          data_size=200,\n          n_media_channels=8,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=2),\n      dict(\n          testcase_name=\"shape_400_4_10\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "  def test_load_model_with_all_attributes(self):\n    media = jnp.ones((20, 2), dtype=jnp.float32)\n    extra_features = jnp.arange(20).reshape((20, 1))\n    costs = jnp.arange(1, 3)\n    target = jnp.arange(1, 21)\n    mmm_object = lightweight_mmm.LightweightMMM()\n    mmm_object.fit(\n        media=media,\n        extra_features=extra_features,\n        media_prior=costs,\n        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    loaded_mmm = utils.load_model(file_path)\n\n    self.assertEqual(mmm_object, loaded_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_100_3_3\",\n          data_size=100,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_8_1\",\n          data_size=200,\n          n_media_channels=8,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=2),\n      dict(\n          testcase_name=\"shape_400_4_10\",\n          data_size=400,\n          n_media_channels=4,\n          n_extra_features=10)\n  ])\n  def test_simulate_dummy_data_produces_correct_shape(self,\n                                                      data_size,\n                                                      n_media_channels,\n                                                      n_extra_features):\n    media_data, extra_features, target, costs = utils.simulate_dummy_data(\n        data_size=data_size,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        target=target,\n        number_warmup=10,\n        number_samples=100,\n        number_chains=1)\n    file_path = os.path.join(self.create_tempdir().full_path, \"model.pkl\")\n    utils.save_model(media_mix_model=mmm_object,\n                     file_path=file_path)\n\n    loaded_mmm = utils.load_model(file_path)\n\n    self.assertEqual(mmm_object, loaded_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_100_3_3\",\n          data_size=100,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_8_1\",\n          data_size=200,\n          n_media_channels=8,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=2),\n      dict(\n          testcase_name=\"shape_400_4_10\",\n          data_size=400,\n          n_media_channels=4,\n          n_extra_features=10)\n  ])\n  def test_simulate_dummy_data_produces_correct_shape(self,\n                                                      data_size,\n                                                      n_media_channels,\n                                                      n_extra_features):\n    media_data, extra_features, target, costs = utils.simulate_dummy_data(\n        data_size=data_size,\n        n_media_channels=n_media_channels,\n        n_extra_features=n_extra_features)\n\n    self.assertEqual(media_data.shape, (data_size, n_media_channels))\n    self.assertEqual(extra_features.shape, (data_size, n_extra_features))\n    self.assertEqual(target.shape, (data_size,))\n    self.assertLen(costs, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    self.assertEqual(mmm_object, loaded_mmm)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_100_3_3\",\n          data_size=100,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_8_1\",\n          data_size=200,\n          n_media_channels=8,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=2),\n      dict(\n          testcase_name=\"shape_400_4_10\",\n          data_size=400,\n          n_media_channels=4,\n          n_extra_features=10)\n  ])\n  def test_simulate_dummy_data_produces_correct_shape(self,\n                                                      data_size,\n                                                      n_media_channels,\n                                                      n_extra_features):\n    media_data, extra_features, target, costs = utils.simulate_dummy_data(\n        data_size=data_size,\n        n_media_channels=n_media_channels,\n        n_extra_features=n_extra_features)\n\n    self.assertEqual(media_data.shape, (data_size, n_media_channels))\n    self.assertEqual(extra_features.shape, (data_size, n_extra_features))\n    self.assertEqual(target.shape, (data_size,))\n    self.assertLen(costs, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_geo_in_geo_feature\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"]),\n      dict(\n          testcase_name=\"unequal_weeks_in_each_geo\",\n          dataframe=_MEDIA_DATAFRAME.iloc[0:5, :])\n  ])\n  def test_dataframe_to_jax_wrong_params_raises_valueerror(self, dataframe):\n    with self.assertRaises(ValueError):\n      utils.dataframe_to_jax(\n          dataframe=dataframe,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          data_size=200,\n          n_media_channels=8,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=2),\n      dict(\n          testcase_name=\"shape_400_4_10\",\n          data_size=400,\n          n_media_channels=4,\n          n_extra_features=10)\n  ])\n  def test_simulate_dummy_data_produces_correct_shape(self,\n                                                      data_size,\n                                                      n_media_channels,\n                                                      n_extra_features):\n    media_data, extra_features, target, costs = utils.simulate_dummy_data(\n        data_size=data_size,\n        n_media_channels=n_media_channels,\n        n_extra_features=n_extra_features)\n\n    self.assertEqual(media_data.shape, (data_size, n_media_channels))\n    self.assertEqual(extra_features.shape, (data_size, n_extra_features))\n    self.assertEqual(target.shape, (data_size,))\n    self.assertLen(costs, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_geo_in_geo_feature\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"]),\n      dict(\n          testcase_name=\"unequal_weeks_in_each_geo\",\n          dataframe=_MEDIA_DATAFRAME.iloc[0:5, :])\n  ])\n  def test_dataframe_to_jax_wrong_params_raises_valueerror(self, dataframe):\n    with self.assertRaises(ValueError):\n      utils.dataframe_to_jax(\n          dataframe=dataframe,\n          media_features=[\"channel1_imp\", \"channel2_imp\"],\n          extra_features=[\"promo_1\"],\n          geo_feature=\"geo\",\n          date_feature=\"date\",\n          target=\"kpi\",\n          cost_features=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_regional_model\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          data_size=400,\n          n_media_channels=4,\n          n_extra_features=10)\n  ])\n  def test_simulate_dummy_data_produces_correct_shape(self,\n                                                      data_size,\n                                                      n_media_channels,\n                                                      n_extra_features):\n    media_data, extra_features, target, costs = utils.simulate_dummy_data(\n        data_size=data_size,\n        n_media_channels=n_media_channels,\n        n_extra_features=n_extra_features)\n\n    self.assertEqual(media_data.shape, (data_size, n_media_channels))\n    self.assertEqual(extra_features.shape, (data_size, n_extra_features))\n    self.assertEqual(target.shape, (data_size,))\n    self.assertLen(costs, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_geo_in_geo_feature\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"]),\n      dict(\n          testcase_name=\"unequal_weeks_in_each_geo\",\n          dataframe=_MEDIA_DATAFRAME.iloc[0:5, :])\n  ])\n  def test_dataframe_to_jax_wrong_params_raises_valueerror(self, dataframe):\n    with self.assertRaises(ValueError):\n      utils.dataframe_to_jax(\n          dataframe=dataframe,\n          media_features=[\"channel1_imp\", \"channel2_imp\"],\n          extra_features=[\"promo_1\"],\n          geo_feature=\"geo\",\n          date_feature=\"date\",\n          target=\"kpi\",\n          cost_features=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_regional_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_regional(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    n_geos = _MEDIA_DATAFRAME[\"geo\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        n_media_channels=n_media_channels,\n        n_extra_features=n_extra_features)\n\n    self.assertEqual(media_data.shape, (data_size, n_media_channels))\n    self.assertEqual(extra_features.shape, (data_size, n_extra_features))\n    self.assertEqual(target.shape, (data_size,))\n    self.assertLen(costs, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"one_geo_in_geo_feature\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"]),\n      dict(\n          testcase_name=\"unequal_weeks_in_each_geo\",\n          dataframe=_MEDIA_DATAFRAME.iloc[0:5, :])\n  ])\n  def test_dataframe_to_jax_wrong_params_raises_valueerror(self, dataframe):\n    with self.assertRaises(ValueError):\n      utils.dataframe_to_jax(\n          dataframe=dataframe,\n          media_features=[\"channel1_imp\", \"channel2_imp\"],\n          extra_features=[\"promo_1\"],\n          geo_feature=\"geo\",\n          date_feature=\"date\",\n          target=\"kpi\",\n          cost_features=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_regional_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_regional(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    n_geos = _MEDIA_DATAFRAME[\"geo\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          testcase_name=\"one_geo_in_geo_feature\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"]),\n      dict(\n          testcase_name=\"unequal_weeks_in_each_geo\",\n          dataframe=_MEDIA_DATAFRAME.iloc[0:5, :])\n  ])\n  def test_dataframe_to_jax_wrong_params_raises_valueerror(self, dataframe):\n    with self.assertRaises(ValueError):\n      utils.dataframe_to_jax(\n          dataframe=dataframe,\n          media_features=[\"channel1_imp\", \"channel2_imp\"],\n          extra_features=[\"promo_1\"],\n          geo_feature=\"geo\",\n          date_feature=\"date\",\n          target=\"kpi\",\n          cost_features=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_regional_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_regional(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    n_geos = _MEDIA_DATAFRAME[\"geo\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n        )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels, n_geos))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features, n_geos))\n    self.assertEqual(target_data.shape, (n_weeks, n_geos))\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          media_features=[\"channel1_imp\", \"channel2_imp\"],\n          extra_features=[\"promo_1\"],\n          geo_feature=\"geo\",\n          date_feature=\"date\",\n          target=\"kpi\",\n          cost_features=None)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_regional_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_regional(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    n_geos = _MEDIA_DATAFRAME[\"geo\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n        )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels, n_geos))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features, n_geos))\n    self.assertEqual(target_data.shape, (n_weeks, n_geos))\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_multiple_features(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_regional(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    n_geos = _MEDIA_DATAFRAME[\"geo\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n        )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels, n_geos))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features, n_geos))\n    self.assertEqual(target_data.shape, (n_weeks, n_geos))\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_multiple_features(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n        )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels, n_geos))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features, n_geos))\n    self.assertEqual(target_data.shape, (n_weeks, n_geos))\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_multiple_features(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels, n_geos))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features, n_geos))\n    self.assertEqual(target_data.shape, (n_weeks, n_geos))\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_multiple_features(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_one_feature(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_multiple_features(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_one_feature(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\"])\n    n_extra_features = len([\"promo_1\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    n_media_channels = len([\"channel1_imp\", \"channel2_imp\"])\n    n_extra_features = len([\"promo_1\", \"promo_2\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\", \"promo_2\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_one_feature(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\"])\n    n_extra_features = len([\"promo_1\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_one_feature(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\"])\n    n_extra_features = len([\"promo_1\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model\",\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_regional(\n      self, cost_features, actual_cost_features_list):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          testcase_name=\"array_shape_without_cost_feature_national_model\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_shape_with_cost_feature_national_model\",\n          cost_features=[\"channel1_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_shape_with_or_without_cost_national_one_feature(\n      self, cost_features):\n    n_weeks = _MEDIA_DATAFRAME.loc[\n        _MEDIA_DATAFRAME[\"geo\"] == \"geo1\", \"date\"].nunique()\n    n_media_channels = len([\"channel1_imp\"])\n    n_extra_features = len([\"promo_1\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model\",\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_regional(\n      self, cost_features, actual_cost_features_list):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    geo1_media = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\",\n                                      [\"channel1_imp\", \"channel2_imp\"]].values\n    geo1_extra_features = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                               \"geo1\", [\"promo_1\"]].values\n    geo1_target = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                       \"geo1\", [\"kpi\"]].values\n    geo1_target = geo1_target.reshape(n_weeks,)\n    cost_by_channel = _MEDIA_DATAFRAME.loc[\n        :, actual_cost_features_list].sum().values", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    n_media_channels = len([\"channel1_imp\"])\n    n_extra_features = len([\"promo_1\"])\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n        media_features=[\"channel1_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features\n    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model\",\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_regional(\n      self, cost_features, actual_cost_features_list):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    geo1_media = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\",\n                                      [\"channel1_imp\", \"channel2_imp\"]].values\n    geo1_extra_features = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                               \"geo1\", [\"promo_1\"]].values\n    geo1_target = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                       \"geo1\", [\"kpi\"]].values\n    geo1_target = geo1_target.reshape(n_weeks,)\n    cost_by_channel = _MEDIA_DATAFRAME.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    )\n\n    self.assertEqual(media_data.shape, (n_weeks, n_media_channels))\n    self.assertEqual(extra_features_data.shape,\n                     (n_weeks, n_extra_features))\n    self.assertLen(target_data, n_weeks)\n    self.assertLen(costs_data, n_media_channels)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model\",\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_regional(\n      self, cost_features, actual_cost_features_list):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    geo1_media = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\",\n                                      [\"channel1_imp\", \"channel2_imp\"]].values\n    geo1_extra_features = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                               \"geo1\", [\"promo_1\"]].values\n    geo1_target = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                       \"geo1\", [\"kpi\"]].values\n    geo1_target = geo1_target.reshape(n_weeks,)\n    cost_by_channel = _MEDIA_DATAFRAME.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data[:, :, 0], geo1_media)\n    np.testing.assert_array_equal(extra_features_data[:, :, 0],\n                                  geo1_extra_features)\n    np.testing.assert_array_equal(target_data[:, 0],\n                                  geo1_target.reshape(n_weeks,))\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_national_model\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          testcase_name=\"array_value_without_cost_feature_regional_model\",\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_regional(\n      self, cost_features, actual_cost_features_list):\n    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    geo1_media = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\",\n                                      [\"channel1_imp\", \"channel2_imp\"]].values\n    geo1_extra_features = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                               \"geo1\", [\"promo_1\"]].values\n    geo1_target = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                       \"geo1\", [\"kpi\"]].values\n    geo1_target = geo1_target.reshape(n_weeks,)\n    cost_by_channel = _MEDIA_DATAFRAME.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data[:, :, 0], geo1_media)\n    np.testing.assert_array_equal(extra_features_data[:, :, 0],\n                                  geo1_extra_features)\n    np.testing.assert_array_equal(target_data[:, 0],\n                                  geo1_target.reshape(n_weeks,))\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_national(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    n_weeks = _MEDIA_DATAFRAME[\"date\"].nunique()\n    geo1_media = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\",\n                                      [\"channel1_imp\", \"channel2_imp\"]].values\n    geo1_extra_features = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                               \"geo1\", [\"promo_1\"]].values\n    geo1_target = _MEDIA_DATAFRAME.loc[_MEDIA_DATAFRAME[\"geo\"] ==\n                                       \"geo1\", [\"kpi\"]].values\n    geo1_target = geo1_target.reshape(n_weeks,)\n    cost_by_channel = _MEDIA_DATAFRAME.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data[:, :, 0], geo1_media)\n    np.testing.assert_array_equal(extra_features_data[:, :, 0],\n                                  geo1_extra_features)\n    np.testing.assert_array_equal(target_data[:, 0],\n                                  geo1_target.reshape(n_weeks,))\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_national(\n      self, dataframe, cost_features, actual_cost_features_list):\n    media_from_test_dataframe = dataframe[\n        [\"channel1_imp\", \"channel2_imp\"]].values\n    extra_features_from_test_dataframe = dataframe[[\"promo_1\"]].values\n    target_from_test_dataframe = dataframe[\"kpi\"].values\n    cost_by_channel = dataframe.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=dataframe,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data[:, :, 0], geo1_media)\n    np.testing.assert_array_equal(extra_features_data[:, :, 0],\n                                  geo1_extra_features)\n    np.testing.assert_array_equal(target_data[:, 0],\n                                  geo1_target.reshape(n_weeks,))\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_national(\n      self, dataframe, cost_features, actual_cost_features_list):\n    media_from_test_dataframe = dataframe[\n        [\"channel1_imp\", \"channel2_imp\"]].values\n    extra_features_from_test_dataframe = dataframe[[\"promo_1\"]].values\n    target_from_test_dataframe = dataframe[\"kpi\"].values\n    cost_by_channel = dataframe.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=dataframe,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data, media_from_test_dataframe)\n    np.testing.assert_array_equal(\n        extra_features_data,\n        extra_features_from_test_dataframe)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    np.testing.assert_array_equal(media_data[:, :, 0], geo1_media)\n    np.testing.assert_array_equal(extra_features_data[:, :, 0],\n                                  geo1_extra_features)\n    np.testing.assert_array_equal(target_data[:, 0],\n                                  geo1_target.reshape(n_weeks,))\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_national(\n      self, dataframe, cost_features, actual_cost_features_list):\n    media_from_test_dataframe = dataframe[\n        [\"channel1_imp\", \"channel2_imp\"]].values\n    extra_features_from_test_dataframe = dataframe[[\"promo_1\"]].values\n    target_from_test_dataframe = dataframe[\"kpi\"].values\n    cost_by_channel = dataframe.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=dataframe,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data, media_from_test_dataframe)\n    np.testing.assert_array_equal(\n        extra_features_data,\n        extra_features_from_test_dataframe)\n    np.testing.assert_array_equal(target_data, target_from_test_dataframe)\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model_unsorted\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model_unsorted\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_285-335", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=None,\n          actual_cost_features_list=[\"channel1_imp\", \"channel2_imp\"]),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_national_model\",\n          dataframe=_MEDIA_DATAFRAME[_MEDIA_DATAFRAME[\"geo\"] == \"geo1\"],\n          cost_features=[\"channel1_cost\", \"channel2_cost\"],\n          actual_cost_features_list=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_or_without_cost_national(\n      self, dataframe, cost_features, actual_cost_features_list):\n    media_from_test_dataframe = dataframe[\n        [\"channel1_imp\", \"channel2_imp\"]].values\n    extra_features_from_test_dataframe = dataframe[[\"promo_1\"]].values\n    target_from_test_dataframe = dataframe[\"kpi\"].values\n    cost_by_channel = dataframe.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=dataframe,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data, media_from_test_dataframe)\n    np.testing.assert_array_equal(\n        extra_features_data,\n        extra_features_from_test_dataframe)\n    np.testing.assert_array_equal(target_data, target_from_test_dataframe)\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model_unsorted\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model_unsorted\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_unsorted_dataframe(\n      self, cost_features):\n    media_data_sorted, extra_features_data_sorted, target_data_sorted, costs_data_sorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_295-345", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "      self, dataframe, cost_features, actual_cost_features_list):\n    media_from_test_dataframe = dataframe[\n        [\"channel1_imp\", \"channel2_imp\"]].values\n    extra_features_from_test_dataframe = dataframe[[\"promo_1\"]].values\n    target_from_test_dataframe = dataframe[\"kpi\"].values\n    cost_by_channel = dataframe.loc[\n        :, actual_cost_features_list].sum().values\n\n    media_data, extra_features_data, target_data, costs_data = utils.dataframe_to_jax(\n        dataframe=dataframe,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data, media_from_test_dataframe)\n    np.testing.assert_array_equal(\n        extra_features_data,\n        extra_features_from_test_dataframe)\n    np.testing.assert_array_equal(target_data, target_from_test_dataframe)\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model_unsorted\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model_unsorted\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_unsorted_dataframe(\n      self, cost_features):\n    media_data_sorted, extra_features_data_sorted, target_data_sorted, costs_data_sorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    media_data_unsorted, extra_features_data_unsorted, target_data_unsorted, costs_data_unsorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME_UNSORTED,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_305-355", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data, media_from_test_dataframe)\n    np.testing.assert_array_equal(\n        extra_features_data,\n        extra_features_from_test_dataframe)\n    np.testing.assert_array_equal(target_data, target_from_test_dataframe)\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model_unsorted\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model_unsorted\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_unsorted_dataframe(\n      self, cost_features):\n    media_data_sorted, extra_features_data_sorted, target_data_sorted, costs_data_sorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    media_data_unsorted, extra_features_data_unsorted, target_data_unsorted, costs_data_unsorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME_UNSORTED,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data_sorted, media_data_unsorted)\n    np.testing.assert_array_equal(\n        extra_features_data_sorted, extra_features_data_unsorted)\n    np.testing.assert_array_equal(target_data_sorted, target_data_unsorted)\n    np.testing.assert_array_equal(costs_data_sorted, costs_data_unsorted)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_0_3_3\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_315-365", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    np.testing.assert_array_equal(target_data, target_from_test_dataframe)\n    np.testing.assert_array_equal(costs_data, cost_by_channel)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"array_value_without_cost_feature_regional_model_unsorted\",\n          cost_features=None),\n      dict(\n          testcase_name=\"array_value_with_cost_feature_regional_model_unsorted\",\n          cost_features=[\"channel1_cost\", \"channel2_cost\"])\n  ])\n  def test_dataframe_to_jax_produce_correct_value_with_unsorted_dataframe(\n      self, cost_features):\n    media_data_sorted, extra_features_data_sorted, target_data_sorted, costs_data_sorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    media_data_unsorted, extra_features_data_unsorted, target_data_unsorted, costs_data_unsorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME_UNSORTED,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data_sorted, media_data_unsorted)\n    np.testing.assert_array_equal(\n        extra_features_data_sorted, extra_features_data_unsorted)\n    np.testing.assert_array_equal(target_data_sorted, target_data_unsorted)\n    np.testing.assert_array_equal(costs_data_sorted, costs_data_unsorted)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_0_3_3\",\n          data_size=0,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_-1_1\",\n          data_size=200,\n          n_media_channels=-1,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_-2\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_325-375", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "  ])\n  def test_dataframe_to_jax_produce_correct_value_with_unsorted_dataframe(\n      self, cost_features):\n    media_data_sorted, extra_features_data_sorted, target_data_sorted, costs_data_sorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    media_data_unsorted, extra_features_data_unsorted, target_data_unsorted, costs_data_unsorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME_UNSORTED,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data_sorted, media_data_unsorted)\n    np.testing.assert_array_equal(\n        extra_features_data_sorted, extra_features_data_unsorted)\n    np.testing.assert_array_equal(target_data_sorted, target_data_unsorted)\n    np.testing.assert_array_equal(costs_data_sorted, costs_data_unsorted)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_0_3_3\",\n          data_size=0,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_-1_1\",\n          data_size=200,\n          n_media_channels=-1,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_-2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=-2),\n      dict(\n          testcase_name=\"shape_-400_-4_-10\",\n          data_size=-400,\n          n_media_channels=-4,\n          n_extra_features=-10)\n  ])\n  def test_simulate_dummy_data_with_zero_or_neg_parameter_raises_value_error(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_335-385", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "        cost_features=cost_features)\n\n    media_data_unsorted, extra_features_data_unsorted, target_data_unsorted, costs_data_unsorted = utils.dataframe_to_jax(\n        dataframe=_MEDIA_DATAFRAME_UNSORTED,\n        media_features=[\"channel1_imp\", \"channel2_imp\"],\n        extra_features=[\"promo_1\"],\n        geo_feature=\"geo\",\n        date_feature=\"date\",\n        target=\"kpi\",\n        cost_features=cost_features)\n\n    np.testing.assert_array_equal(media_data_sorted, media_data_unsorted)\n    np.testing.assert_array_equal(\n        extra_features_data_sorted, extra_features_data_unsorted)\n    np.testing.assert_array_equal(target_data_sorted, target_data_unsorted)\n    np.testing.assert_array_equal(costs_data_sorted, costs_data_unsorted)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_0_3_3\",\n          data_size=0,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_-1_1\",\n          data_size=200,\n          n_media_channels=-1,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_-2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=-2),\n      dict(\n          testcase_name=\"shape_-400_-4_-10\",\n          data_size=-400,\n          n_media_channels=-4,\n          n_extra_features=-10)\n  ])\n  def test_simulate_dummy_data_with_zero_or_neg_parameter_raises_value_error(\n      self, data_size, n_media_channels, n_extra_features):\n\n    with self.assertRaises(ValueError):\n      utils.simulate_dummy_data(\n          data_size=data_size,\n          n_media_channels=n_media_channels,\n          n_extra_features=n_extra_features)\n\n  def test_simulate_geo_data_has_right_shape(self):\n    data_size = 100", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_345-395", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "\n    np.testing.assert_array_equal(media_data_sorted, media_data_unsorted)\n    np.testing.assert_array_equal(\n        extra_features_data_sorted, extra_features_data_unsorted)\n    np.testing.assert_array_equal(target_data_sorted, target_data_unsorted)\n    np.testing.assert_array_equal(costs_data_sorted, costs_data_unsorted)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"shape_0_3_3\",\n          data_size=0,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_-1_1\",\n          data_size=200,\n          n_media_channels=-1,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_-2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=-2),\n      dict(\n          testcase_name=\"shape_-400_-4_-10\",\n          data_size=-400,\n          n_media_channels=-4,\n          n_extra_features=-10)\n  ])\n  def test_simulate_dummy_data_with_zero_or_neg_parameter_raises_value_error(\n      self, data_size, n_media_channels, n_extra_features):\n\n    with self.assertRaises(ValueError):\n      utils.simulate_dummy_data(\n          data_size=data_size,\n          n_media_channels=n_media_channels,\n          n_extra_features=n_extra_features)\n\n  def test_simulate_geo_data_has_right_shape(self):\n    data_size = 100\n    geos = 3\n    media_data, _, target, _ = utils.simulate_dummy_data(\n        data_size, 2, 2, geos=geos)\n    self.assertEqual(target.shape, (data_size, geos))\n    self.assertEqual(media_data.shape, (data_size, 2, geos))\n\n  def test_halfnormal_mean_and_scale(self):\n    mean = 1.\n    scale = utils.get_halfnormal_scale_from_mean(mean)\n    new_mean = utils.get_halfnormal_mean_from_scale(scale)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_355-405", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          data_size=0,\n          n_media_channels=3,\n          n_extra_features=3),\n      dict(\n          testcase_name=\"shape_200_-1_1\",\n          data_size=200,\n          n_media_channels=-1,\n          n_extra_features=1),\n      dict(\n          testcase_name=\"shape_300_2_-2\",\n          data_size=300,\n          n_media_channels=2,\n          n_extra_features=-2),\n      dict(\n          testcase_name=\"shape_-400_-4_-10\",\n          data_size=-400,\n          n_media_channels=-4,\n          n_extra_features=-10)\n  ])\n  def test_simulate_dummy_data_with_zero_or_neg_parameter_raises_value_error(\n      self, data_size, n_media_channels, n_extra_features):\n\n    with self.assertRaises(ValueError):\n      utils.simulate_dummy_data(\n          data_size=data_size,\n          n_media_channels=n_media_channels,\n          n_extra_features=n_extra_features)\n\n  def test_simulate_geo_data_has_right_shape(self):\n    data_size = 100\n    geos = 3\n    media_data, _, target, _ = utils.simulate_dummy_data(\n        data_size, 2, 2, geos=geos)\n    self.assertEqual(target.shape, (data_size, geos))\n    self.assertEqual(media_data.shape, (data_size, 2, geos))\n\n  def test_halfnormal_mean_and_scale(self):\n    mean = 1.\n    scale = utils.get_halfnormal_scale_from_mean(mean)\n    new_mean = utils.get_halfnormal_mean_from_scale(scale)\n    self.assertEqual(scale, mean * np.sqrt(np.pi) / np.sqrt(2))\n    self.assertEqual(mean, new_mean)\n\n  def test_beta_params_match(self):\n    a, b = 2., 3.\n    # Expected mean is 2 / 5.\n    mu = a / (a + b)\n    sigma = np.sqrt(a * b / ((a + b) ** 2 * (a + b + 1)))\n    ahat, bhat = utils.get_beta_params_from_mu_sigma(mu, sigma)\n    self.assertAlmostEqual(ahat / (ahat + bhat), 2 / 5)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_365-415", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "          data_size=300,\n          n_media_channels=2,\n          n_extra_features=-2),\n      dict(\n          testcase_name=\"shape_-400_-4_-10\",\n          data_size=-400,\n          n_media_channels=-4,\n          n_extra_features=-10)\n  ])\n  def test_simulate_dummy_data_with_zero_or_neg_parameter_raises_value_error(\n      self, data_size, n_media_channels, n_extra_features):\n\n    with self.assertRaises(ValueError):\n      utils.simulate_dummy_data(\n          data_size=data_size,\n          n_media_channels=n_media_channels,\n          n_extra_features=n_extra_features)\n\n  def test_simulate_geo_data_has_right_shape(self):\n    data_size = 100\n    geos = 3\n    media_data, _, target, _ = utils.simulate_dummy_data(\n        data_size, 2, 2, geos=geos)\n    self.assertEqual(target.shape, (data_size, geos))\n    self.assertEqual(media_data.shape, (data_size, 2, geos))\n\n  def test_halfnormal_mean_and_scale(self):\n    mean = 1.\n    scale = utils.get_halfnormal_scale_from_mean(mean)\n    new_mean = utils.get_halfnormal_mean_from_scale(scale)\n    self.assertEqual(scale, mean * np.sqrt(np.pi) / np.sqrt(2))\n    self.assertEqual(mean, new_mean)\n\n  def test_beta_params_match(self):\n    a, b = 2., 3.\n    # Expected mean is 2 / 5.\n    mu = a / (a + b)\n    sigma = np.sqrt(a * b / ((a + b) ** 2 * (a + b + 1)))\n    ahat, bhat = utils.get_beta_params_from_mu_sigma(mu, sigma)\n    self.assertAlmostEqual(ahat / (ahat + bhat), 2 / 5)\n\n  def test_prior_posterior_distance_discrete(self):\n    p = jnp.array([0] * 2 + [1] * 3)\n    q = jnp.array([0] * 3 + [1] * 2 + [2] * 1)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=True)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=True)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=True)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=True)\n    print(ks, js, hell, mindist)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_375-425", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "      self, data_size, n_media_channels, n_extra_features):\n\n    with self.assertRaises(ValueError):\n      utils.simulate_dummy_data(\n          data_size=data_size,\n          n_media_channels=n_media_channels,\n          n_extra_features=n_extra_features)\n\n  def test_simulate_geo_data_has_right_shape(self):\n    data_size = 100\n    geos = 3\n    media_data, _, target, _ = utils.simulate_dummy_data(\n        data_size, 2, 2, geos=geos)\n    self.assertEqual(target.shape, (data_size, geos))\n    self.assertEqual(media_data.shape, (data_size, 2, geos))\n\n  def test_halfnormal_mean_and_scale(self):\n    mean = 1.\n    scale = utils.get_halfnormal_scale_from_mean(mean)\n    new_mean = utils.get_halfnormal_mean_from_scale(scale)\n    self.assertEqual(scale, mean * np.sqrt(np.pi) / np.sqrt(2))\n    self.assertEqual(mean, new_mean)\n\n  def test_beta_params_match(self):\n    a, b = 2., 3.\n    # Expected mean is 2 / 5.\n    mu = a / (a + b)\n    sigma = np.sqrt(a * b / ((a + b) ** 2 * (a + b + 1)))\n    ahat, bhat = utils.get_beta_params_from_mu_sigma(mu, sigma)\n    self.assertAlmostEqual(ahat / (ahat + bhat), 2 / 5)\n\n  def test_prior_posterior_distance_discrete(self):\n    p = jnp.array([0] * 2 + [1] * 3)\n    q = jnp.array([0] * 3 + [1] * 2 + [2] * 1)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=True)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=True)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=True)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=True)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 1 / 6)\n    self.assertAlmostEqual(js, 0.283, 3)\n    self.assertAlmostEqual(hell, 0.325, 3)\n    self.assertAlmostEqual(mindist, 0.267, 3)\n\n  def test_prior_posterior_distance_continuous(self):\n    p = jnp.array([0] * 2 + [.5] * 3 + [1] * 2)\n    q = jnp.array([0] * 2 + [.5] * 4 + [1] * 2 + [1.5] * 3)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=False)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=False)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_385-435", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    geos = 3\n    media_data, _, target, _ = utils.simulate_dummy_data(\n        data_size, 2, 2, geos=geos)\n    self.assertEqual(target.shape, (data_size, geos))\n    self.assertEqual(media_data.shape, (data_size, 2, geos))\n\n  def test_halfnormal_mean_and_scale(self):\n    mean = 1.\n    scale = utils.get_halfnormal_scale_from_mean(mean)\n    new_mean = utils.get_halfnormal_mean_from_scale(scale)\n    self.assertEqual(scale, mean * np.sqrt(np.pi) / np.sqrt(2))\n    self.assertEqual(mean, new_mean)\n\n  def test_beta_params_match(self):\n    a, b = 2., 3.\n    # Expected mean is 2 / 5.\n    mu = a / (a + b)\n    sigma = np.sqrt(a * b / ((a + b) ** 2 * (a + b + 1)))\n    ahat, bhat = utils.get_beta_params_from_mu_sigma(mu, sigma)\n    self.assertAlmostEqual(ahat / (ahat + bhat), 2 / 5)\n\n  def test_prior_posterior_distance_discrete(self):\n    p = jnp.array([0] * 2 + [1] * 3)\n    q = jnp.array([0] * 3 + [1] * 2 + [2] * 1)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=True)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=True)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=True)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=True)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 1 / 6)\n    self.assertAlmostEqual(js, 0.283, 3)\n    self.assertAlmostEqual(hell, 0.325, 3)\n    self.assertAlmostEqual(mindist, 0.267, 3)\n\n  def test_prior_posterior_distance_continuous(self):\n    p = jnp.array([0] * 2 + [.5] * 3 + [1] * 2)\n    q = jnp.array([0] * 2 + [.5] * 4 + [1] * 2 + [1.5] * 3)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=False)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=False)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=False)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=False)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 0.2727, 4)\n    self.assertAlmostEqual(js, 0.034, 3)\n    self.assertAlmostEqual(hell, 0.034, 3)\n    self.assertAlmostEqual(mindist, 0.041, 3)\n\n  def test_outlier_interpolation_straight_line(self):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_395-445", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    self.assertEqual(scale, mean * np.sqrt(np.pi) / np.sqrt(2))\n    self.assertEqual(mean, new_mean)\n\n  def test_beta_params_match(self):\n    a, b = 2., 3.\n    # Expected mean is 2 / 5.\n    mu = a / (a + b)\n    sigma = np.sqrt(a * b / ((a + b) ** 2 * (a + b + 1)))\n    ahat, bhat = utils.get_beta_params_from_mu_sigma(mu, sigma)\n    self.assertAlmostEqual(ahat / (ahat + bhat), 2 / 5)\n\n  def test_prior_posterior_distance_discrete(self):\n    p = jnp.array([0] * 2 + [1] * 3)\n    q = jnp.array([0] * 3 + [1] * 2 + [2] * 1)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=True)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=True)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=True)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=True)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 1 / 6)\n    self.assertAlmostEqual(js, 0.283, 3)\n    self.assertAlmostEqual(hell, 0.325, 3)\n    self.assertAlmostEqual(mindist, 0.267, 3)\n\n  def test_prior_posterior_distance_continuous(self):\n    p = jnp.array([0] * 2 + [.5] * 3 + [1] * 2)\n    q = jnp.array([0] * 2 + [.5] * 4 + [1] * 2 + [1.5] * 3)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=False)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=False)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=False)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=False)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 0.2727, 4)\n    self.assertAlmostEqual(js, 0.034, 3)\n    self.assertAlmostEqual(hell, 0.034, 3)\n    self.assertAlmostEqual(mindist, 0.041, 3)\n\n  def test_outlier_interpolation_straight_line(self):\n    x = np.arange(10) * 1.\n    x[3:5] += 10\n    x = jnp.array(x)\n    outlier_idx = jnp.array([3, 4])\n    new_x = utils.interpolate_outliers(x, outlier_idx)\n    self.assertTrue(all(np.equal(new_x[outlier_idx], [3, 4])))\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_405-445", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "\n  def test_prior_posterior_distance_discrete(self):\n    p = jnp.array([0] * 2 + [1] * 3)\n    q = jnp.array([0] * 3 + [1] * 2 + [2] * 1)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=True)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=True)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=True)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=True)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 1 / 6)\n    self.assertAlmostEqual(js, 0.283, 3)\n    self.assertAlmostEqual(hell, 0.325, 3)\n    self.assertAlmostEqual(mindist, 0.267, 3)\n\n  def test_prior_posterior_distance_continuous(self):\n    p = jnp.array([0] * 2 + [.5] * 3 + [1] * 2)\n    q = jnp.array([0] * 2 + [.5] * 4 + [1] * 2 + [1.5] * 3)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=False)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=False)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=False)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=False)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 0.2727, 4)\n    self.assertAlmostEqual(js, 0.034, 3)\n    self.assertAlmostEqual(hell, 0.034, 3)\n    self.assertAlmostEqual(mindist, 0.041, 3)\n\n  def test_outlier_interpolation_straight_line(self):\n    x = np.arange(10) * 1.\n    x[3:5] += 10\n    x = jnp.array(x)\n    outlier_idx = jnp.array([3, 4])\n    new_x = utils.interpolate_outliers(x, outlier_idx)\n    self.assertTrue(all(np.equal(new_x[outlier_idx], [3, 4])))\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-utils_test.py_415-445", "title": "google_lightweight_mmm-lightweight_mmm-utils_test.py", "text": "    self.assertAlmostEqual(ks, 1 / 6)\n    self.assertAlmostEqual(js, 0.283, 3)\n    self.assertAlmostEqual(hell, 0.325, 3)\n    self.assertAlmostEqual(mindist, 0.267, 3)\n\n  def test_prior_posterior_distance_continuous(self):\n    p = jnp.array([0] * 2 + [.5] * 3 + [1] * 2)\n    q = jnp.array([0] * 2 + [.5] * 4 + [1] * 2 + [1.5] * 3)\n    ks = utils.distance_pior_posterior(p, q, method=\"KS\", discrete=False)\n    js = utils.distance_pior_posterior(p, q, method=\"JS\", discrete=False)\n    hell = utils.distance_pior_posterior(\n        p, q, method=\"Hellinger\", discrete=False)\n    mindist = utils.distance_pior_posterior(p, q, method=\"min\", discrete=False)\n    print(ks, js, hell, mindist)\n    self.assertAlmostEqual(ks, 0.2727, 4)\n    self.assertAlmostEqual(js, 0.034, 3)\n    self.assertAlmostEqual(hell, 0.034, 3)\n    self.assertAlmostEqual(mindist, 0.041, 3)\n\n  def test_outlier_interpolation_straight_line(self):\n    x = np.arange(10) * 1.\n    x[3:5] += 10\n    x = jnp.array(x)\n    outlier_idx = jnp.array([3, 4])\n    new_x = utils.interpolate_outliers(x, outlier_idx)\n    self.assertTrue(all(np.equal(new_x[outlier_idx], [3, 4])))\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "utils_test.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 445, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-__init__.py_0-20", "title": "google_lightweight_mmm-lightweight_mmm-__init__.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"LightweightMMM library.\n\nDetailed documentation and examples can be found in the\n[Github repository](https://github.com/google/lightweight_mmm).\n\"\"\"\n__version__ = \"0.1.7\"\n\nAST=Module(Expr(Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 20, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets of utilities used across the core components of LightweightMMM.\"\"\"\n\nimport sys\nfrom typing import Any, Mapping, Tuple, Union\n\nimport jax.numpy as jnp\n\nfrom numpyro import distributions as dist\n\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets of utilities used across the core components of LightweightMMM.\"\"\"\n\nimport sys\nfrom typing import Any, Mapping, Tuple, Union\n\nimport jax.numpy as jnp\n\nfrom numpyro import distributions as dist\n\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      data: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets of utilities used across the core components of LightweightMMM.\"\"\"\n\nimport sys\nfrom typing import Any, Mapping, Tuple, Union\n\nimport jax.numpy as jnp\n\nfrom numpyro import distributions as dist\n\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      data: jnp.ndarray,\n      custom_priors: Mapping[str, dist.Distribution],\n      prefix: str,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\nclass Module(Protocol):\n\n  def __call__(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets of utilities used across the core components of LightweightMMM.\"\"\"\n\nimport sys\nfrom typing import Any, Mapping, Tuple, Union\n\nimport jax.numpy as jnp\n\nfrom numpyro import distributions as dist\n\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      data: jnp.ndarray,\n      custom_priors: Mapping[str, dist.Distribution],\n      prefix: str,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\nclass Module(Protocol):\n\n  def __call__(\n      self,\n      *args: Any,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\ndef get_number_geos(data: jnp.ndarray) -> int:\n  return data.shape[2] if data.ndim == 3 else 1\n\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasaliasalias)Import(alias)ImportFrom(alias)If(Compare(Attribute(Name(Load)Load)GtETuple(ConstantConstantLoad))ImportFrom(alias)ImportFrom(alias))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))Expr(Constant)Attribute(Name(Load)Load)))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Return(IfExp(Compare(Attribute(Name(Load)Load)EqConstant)Subscript(Attribute(Name(Load)Load)ConstantLoad)Constant))Name(Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "\nimport sys\nfrom typing import Any, Mapping, Tuple, Union\n\nimport jax.numpy as jnp\n\nfrom numpyro import distributions as dist\n\n#  pylint: disable=g-import-not-at-top\nif sys.version_info >= (3, 8):\n  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      data: jnp.ndarray,\n      custom_priors: Mapping[str, dist.Distribution],\n      prefix: str,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\nclass Module(Protocol):\n\n  def __call__(\n      self,\n      *args: Any,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\ndef get_number_geos(data: jnp.ndarray) -> int:\n  return data.shape[2] if data.ndim == 3 else 1\n\n\ndef get_geo_shape(data: jnp.ndarray) -> Union[Tuple[int], Tuple[()]]:\n  return (data.shape[2],) if data.ndim == 3 else ()\n\n\ndef apply_exponent_safe(data: jnp.ndarray,\n                        exponent: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "  from typing import Protocol\nelse:\n  from typing_extensions import Protocol\n\n\nclass TransformFunction(Protocol):\n\n  def __call__(\n      self,\n      data: jnp.ndarray,\n      custom_priors: Mapping[str, dist.Distribution],\n      prefix: str,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\nclass Module(Protocol):\n\n  def __call__(\n      self,\n      *args: Any,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\ndef get_number_geos(data: jnp.ndarray) -> int:\n  return data.shape[2] if data.ndim == 3 else 1\n\n\ndef get_geo_shape(data: jnp.ndarray) -> Union[Tuple[int], Tuple[()]]:\n  return (data.shape[2],) if data.ndim == 3 else ()\n\n\ndef apply_exponent_safe(data: jnp.ndarray,\n                        exponent: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:\n  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf\n\n  Args:\n    data: Input data to use.\n    exponent: Exponent required for the operations.\n\n  Returns:\n    The result of the exponent operation with the inputs provided.\n  \"\"\"\n  exponent_safe = jnp.where(condition=(data == 0), x=1, y=data)**exponent", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_35-76", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "      custom_priors: Mapping[str, dist.Distribution],\n      prefix: str,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\nclass Module(Protocol):\n\n  def __call__(\n      self,\n      *args: Any,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\ndef get_number_geos(data: jnp.ndarray) -> int:\n  return data.shape[2] if data.ndim == 3 else 1\n\n\ndef get_geo_shape(data: jnp.ndarray) -> Union[Tuple[int], Tuple[()]]:\n  return (data.shape[2],) if data.ndim == 3 else ()\n\n\ndef apply_exponent_safe(data: jnp.ndarray,\n                        exponent: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:\n  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf\n\n  Args:\n    data: Input data to use.\n    exponent: Exponent required for the operations.\n\n  Returns:\n    The result of the exponent operation with the inputs provided.\n  \"\"\"\n  exponent_safe = jnp.where(condition=(data == 0), x=1, y=data)**exponent\n  return jnp.where(condition=(data == 0), x=0, y=exponent_safe)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 76, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils.py_45-76", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils.py", "text": "      self,\n      *args: Any,\n      **kwargs: Any,\n  ) -> jnp.ndarray:\n    ...\n\n\ndef get_number_geos(data: jnp.ndarray) -> int:\n  return data.shape[2] if data.ndim == 3 else 1\n\n\ndef get_geo_shape(data: jnp.ndarray) -> Union[Tuple[int], Tuple[()]]:\n  return (data.shape[2],) if data.ndim == 3 else ()\n\n\ndef apply_exponent_safe(data: jnp.ndarray,\n                        exponent: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies an exponent to given data in a gradient safe way.\n\n  More info on the double jnp.where can be found:\n  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf\n\n  Args:\n    data: Input data to use.\n    exponent: Exponent required for the operations.\n\n  Returns:\n    The result of the exponent operation with the inputs provided.\n  \"\"\"\n  exponent_safe = jnp.where(condition=(data == 0), x=1, y=data)**exponent\n  return jnp.where(condition=(data == 0), x=0, y=exponent_safe)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 76, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for core_utils.\"\"\"\n\nfrom lightweight_mmm.core import core_utils\nfrom absl.testing import absltest\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\nclass CoreUtilsTest(absltest.TestCase):\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for core_utils.\"\"\"\n\nfrom lightweight_mmm.core import core_utils\nfrom absl.testing import absltest\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\nclass CoreUtilsTest(absltest.TestCase):\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.ones((10, 5))\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load)Constant)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(BinOp(Name(Load)PowName(Load))))))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for core_utils.\"\"\"\n\nfrom lightweight_mmm.core import core_utils\nfrom absl.testing import absltest\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\nclass CoreUtilsTest(absltest.TestCase):\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n\n    def f_safe(data, exponent):\n      x = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load)Constant)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(BinOp(Name(Load)PowName(Load))))))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load))))FunctionDef(arguments(arg)FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for core_utils.\"\"\"\n\nfrom lightweight_mmm.core import core_utils\nfrom absl.testing import absltest\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\nclass CoreUtilsTest(absltest.TestCase):\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n\n    def f_safe(data, exponent):\n      x = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load)Constant)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(BinOp(Name(Load)PowName(Load))))))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load))))FunctionDef(arguments(arg)FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Return(Call(Attribute(Name(Load)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Subscript(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Call(Attribute(Name(Load)Load)Name(Load))Name(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py_15-59", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py", "text": "\nfrom lightweight_mmm.core import core_utils\nfrom absl.testing import absltest\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\nclass CoreUtilsTest(absltest.TestCase):\n\n  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n\n    def f_safe(data, exponent):\n      x = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n\nif __name__ == '__main__':\n  absltest.main()\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load)Constant)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(BinOp(Name(Load)PowName(Load))))))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load))))FunctionDef(arguments(arg)FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Return(Call(Attribute(Name(Load)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)))Assign(Name(Store)Call(Attribute(Subscript(Attribute(Name(Load)Load)Tuple(ConstantConstantLoad)Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)Call(Call(Attribute(Name(Load)Load)Name(Load))Name(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load))))))If(Compare(Name(Load)EqConstant)Expr(Call(Attribute(Name(Load)Load)))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 59, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py_25-59", "title": "google_lightweight_mmm-lightweight_mmm-core-core_utils_test.py", "text": "  def test_apply_exponent_safe_produces_same_exponent_results(self):\n    data = jnp.arange(50).reshape((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    np.testing.assert_array_equal(x=output, y=data**exponent)\n\n  def test_apply_exponent_safe_produces_correct_shape(self):\n    data = jnp.ones((10, 5))\n    exponent = jnp.full(5, 0.5)\n\n    output = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n\n    self.assertEqual(output.shape, data.shape)\n\n  def test_apply_exponent_safe_produces_non_nan_or_inf_grads(self):\n\n    def f_safe(data, exponent):\n      x = core_utils.apply_exponent_safe(data=data, exponent=exponent)\n      return x.sum()\n\n    data = jnp.ones((10, 5))\n    data = data.at[0, 0].set(0.)\n    exponent = jnp.full(5, 0.5)\n\n    grads = jax.grad(f_safe)(data, exponent)\n\n    self.assertFalse(np.isnan(grads).any())\n    self.assertFalse(np.isinf(grads).any())\n\n\nif __name__ == '__main__':\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "core_utils_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 59, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets priors and prior related constants for LMMM.\"\"\"\nfrom typing import Mapping\n\nimport immutabledict\nfrom numpyro import distributions as dist\n\n# Core model priors\nINTERCEPT = \"intercept\"\nCOEF_TREND = \"coef_trend\"\nEXPO_TREND = \"expo_trend\"\nSIGMA = \"sigma\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)ImportFrom(alias)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets priors and prior related constants for LMMM.\"\"\"\nfrom typing import Mapping\n\nimport immutabledict\nfrom numpyro import distributions as dist\n\n# Core model priors\nINTERCEPT = \"intercept\"\nCOEF_TREND = \"coef_trend\"\nEXPO_TREND = \"expo_trend\"\nSIGMA = \"sigma\"\nGAMMA_SEASONALITY = \"gamma_seasonality\"\nWEEKDAY = \"weekday\"\nCOEF_EXTRA_FEATURES = \"coef_extra_features\"\nCOEF_SEASONALITY = \"coef_seasonality\"\n\n# Lagging priors\nLAG_WEIGHT = \"lag_weight\"\nAD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\nPEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)ImportFrom(alias)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets priors and prior related constants for LMMM.\"\"\"\nfrom typing import Mapping\n\nimport immutabledict\nfrom numpyro import distributions as dist\n\n# Core model priors\nINTERCEPT = \"intercept\"\nCOEF_TREND = \"coef_trend\"\nEXPO_TREND = \"expo_trend\"\nSIGMA = \"sigma\"\nGAMMA_SEASONALITY = \"gamma_seasonality\"\nWEEKDAY = \"weekday\"\nCOEF_EXTRA_FEATURES = \"coef_extra_features\"\nCOEF_SEASONALITY = \"coef_seasonality\"\n\n# Lagging priors\nLAG_WEIGHT = \"lag_weight\"\nAD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\nPEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\n# Saturation priors\nEXPONENT = \"exponent\"\nHALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\nSLOPE = \"slope\"\n\n# Dynamic trend priors\nDYNAMIC_TREND_INITIAL_LEVEL = \"dynamic_trend_initial_level\"\nDYNAMIC_TREND_INITIAL_SLOPE = \"dynamic_trend_initial_slope\"\nDYNAMIC_TREND_LEVEL_VARIANCE = \"dynamic_trend_level_variance\"\nDYNAMIC_TREND_SLOPE_VARIANCE = \"dynamic_trend_slope_variance\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)ImportFrom(alias)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Constant))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sets priors and prior related constants for LMMM.\"\"\"\nfrom typing import Mapping\n\nimport immutabledict\nfrom numpyro import distributions as dist\n\n# Core model priors\nINTERCEPT = \"intercept\"\nCOEF_TREND = \"coef_trend\"\nEXPO_TREND = \"expo_trend\"\nSIGMA = \"sigma\"\nGAMMA_SEASONALITY = \"gamma_seasonality\"\nWEEKDAY = \"weekday\"\nCOEF_EXTRA_FEATURES = \"coef_extra_features\"\nCOEF_SEASONALITY = \"coef_seasonality\"\n\n# Lagging priors\nLAG_WEIGHT = \"lag_weight\"\nAD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\nPEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\n# Saturation priors\nEXPONENT = \"exponent\"\nHALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\nSLOPE = \"slope\"\n\n# Dynamic trend priors\nDYNAMIC_TREND_INITIAL_LEVEL = \"dynamic_trend_initial_level\"\nDYNAMIC_TREND_INITIAL_SLOPE = \"dynamic_trend_initial_slope\"\nDYNAMIC_TREND_LEVEL_VARIANCE = \"dynamic_trend_level_variance\"\nDYNAMIC_TREND_SLOPE_VARIANCE = \"dynamic_trend_slope_variance\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    INTERCEPT,\n    COEF_TREND,\n    EXPO_TREND,\n    SIGMA,\n    GAMMA_SEASONALITY,\n    WEEKDAY,\n    COEF_EXTRA_FEATURES,\n    COEF_SEASONALITY,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "from typing import Mapping\n\nimport immutabledict\nfrom numpyro import distributions as dist\n\n# Core model priors\nINTERCEPT = \"intercept\"\nCOEF_TREND = \"coef_trend\"\nEXPO_TREND = \"expo_trend\"\nSIGMA = \"sigma\"\nGAMMA_SEASONALITY = \"gamma_seasonality\"\nWEEKDAY = \"weekday\"\nCOEF_EXTRA_FEATURES = \"coef_extra_features\"\nCOEF_SEASONALITY = \"coef_seasonality\"\n\n# Lagging priors\nLAG_WEIGHT = \"lag_weight\"\nAD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\nPEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\n# Saturation priors\nEXPONENT = \"exponent\"\nHALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\nSLOPE = \"slope\"\n\n# Dynamic trend priors\nDYNAMIC_TREND_INITIAL_LEVEL = \"dynamic_trend_initial_level\"\nDYNAMIC_TREND_INITIAL_SLOPE = \"dynamic_trend_initial_slope\"\nDYNAMIC_TREND_LEVEL_VARIANCE = \"dynamic_trend_level_variance\"\nDYNAMIC_TREND_SLOPE_VARIANCE = \"dynamic_trend_slope_variance\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    INTERCEPT,\n    COEF_TREND,\n    EXPO_TREND,\n    SIGMA,\n    GAMMA_SEASONALITY,\n    WEEKDAY,\n    COEF_EXTRA_FEATURES,\n    COEF_SEASONALITY,\n    LAG_WEIGHT,\n    AD_EFFECT_RETENTION_RATE,\n    PEAK_EFFECT_DELAY,\n    EXPONENT,\n    HALF_MAX_EFFECTIVE_CONCENTRATION,\n    SLOPE,\n    DYNAMIC_TREND_INITIAL_LEVEL,\n    DYNAMIC_TREND_INITIAL_SLOPE,\n    DYNAMIC_TREND_LEVEL_VARIANCE,\n    DYNAMIC_TREND_SLOPE_VARIANCE,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "GAMMA_SEASONALITY = \"gamma_seasonality\"\nWEEKDAY = \"weekday\"\nCOEF_EXTRA_FEATURES = \"coef_extra_features\"\nCOEF_SEASONALITY = \"coef_seasonality\"\n\n# Lagging priors\nLAG_WEIGHT = \"lag_weight\"\nAD_EFFECT_RETENTION_RATE = \"ad_effect_retention_rate\"\nPEAK_EFFECT_DELAY = \"peak_effect_delay\"\n\n# Saturation priors\nEXPONENT = \"exponent\"\nHALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\nSLOPE = \"slope\"\n\n# Dynamic trend priors\nDYNAMIC_TREND_INITIAL_LEVEL = \"dynamic_trend_initial_level\"\nDYNAMIC_TREND_INITIAL_SLOPE = \"dynamic_trend_initial_slope\"\nDYNAMIC_TREND_LEVEL_VARIANCE = \"dynamic_trend_level_variance\"\nDYNAMIC_TREND_SLOPE_VARIANCE = \"dynamic_trend_slope_variance\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    INTERCEPT,\n    COEF_TREND,\n    EXPO_TREND,\n    SIGMA,\n    GAMMA_SEASONALITY,\n    WEEKDAY,\n    COEF_EXTRA_FEATURES,\n    COEF_SEASONALITY,\n    LAG_WEIGHT,\n    AD_EFFECT_RETENTION_RATE,\n    PEAK_EFFECT_DELAY,\n    EXPONENT,\n    HALF_MAX_EFFECTIVE_CONCENTRATION,\n    SLOPE,\n    DYNAMIC_TREND_INITIAL_LEVEL,\n    DYNAMIC_TREND_INITIAL_SLOPE,\n    DYNAMIC_TREND_LEVEL_VARIANCE,\n    DYNAMIC_TREND_SLOPE_VARIANCE,\n))\n\nGEO_ONLY_PRIORS = frozenset((COEF_SEASONALITY,))\n\n\ndef get_default_priors() -> Mapping[str, dist.Distribution]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      INTERCEPT: dist.HalfNormal(scale=2.),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "# Saturation priors\nEXPONENT = \"exponent\"\nHALF_MAX_EFFECTIVE_CONCENTRATION = \"half_max_effective_concentration\"\nSLOPE = \"slope\"\n\n# Dynamic trend priors\nDYNAMIC_TREND_INITIAL_LEVEL = \"dynamic_trend_initial_level\"\nDYNAMIC_TREND_INITIAL_SLOPE = \"dynamic_trend_initial_slope\"\nDYNAMIC_TREND_LEVEL_VARIANCE = \"dynamic_trend_level_variance\"\nDYNAMIC_TREND_SLOPE_VARIANCE = \"dynamic_trend_slope_variance\"\n\nMODEL_PRIORS_NAMES = frozenset((\n    INTERCEPT,\n    COEF_TREND,\n    EXPO_TREND,\n    SIGMA,\n    GAMMA_SEASONALITY,\n    WEEKDAY,\n    COEF_EXTRA_FEATURES,\n    COEF_SEASONALITY,\n    LAG_WEIGHT,\n    AD_EFFECT_RETENTION_RATE,\n    PEAK_EFFECT_DELAY,\n    EXPONENT,\n    HALF_MAX_EFFECTIVE_CONCENTRATION,\n    SLOPE,\n    DYNAMIC_TREND_INITIAL_LEVEL,\n    DYNAMIC_TREND_INITIAL_SLOPE,\n    DYNAMIC_TREND_LEVEL_VARIANCE,\n    DYNAMIC_TREND_SLOPE_VARIANCE,\n))\n\nGEO_ONLY_PRIORS = frozenset((COEF_SEASONALITY,))\n\n\ndef get_default_priors() -> Mapping[str, dist.Distribution]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      INTERCEPT: dist.HalfNormal(scale=2.),\n      COEF_TREND: dist.Normal(loc=0., scale=1.),\n      EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      SIGMA: dist.Gamma(concentration=1., rate=1.),\n      GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      WEEKDAY: dist.Normal(loc=0., scale=.5),\n      COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      COEF_SEASONALITY: dist.HalfNormal(scale=.5),\n      AD_EFFECT_RETENTION_RATE: dist.Beta(concentration1=1., concentration0=1.),\n      PEAK_EFFECT_DELAY: dist.HalfNormal(scale=2.),\n      EXPONENT: dist.Beta(concentration1=9., concentration0=1.),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_45-93", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "\nMODEL_PRIORS_NAMES = frozenset((\n    INTERCEPT,\n    COEF_TREND,\n    EXPO_TREND,\n    SIGMA,\n    GAMMA_SEASONALITY,\n    WEEKDAY,\n    COEF_EXTRA_FEATURES,\n    COEF_SEASONALITY,\n    LAG_WEIGHT,\n    AD_EFFECT_RETENTION_RATE,\n    PEAK_EFFECT_DELAY,\n    EXPONENT,\n    HALF_MAX_EFFECTIVE_CONCENTRATION,\n    SLOPE,\n    DYNAMIC_TREND_INITIAL_LEVEL,\n    DYNAMIC_TREND_INITIAL_SLOPE,\n    DYNAMIC_TREND_LEVEL_VARIANCE,\n    DYNAMIC_TREND_SLOPE_VARIANCE,\n))\n\nGEO_ONLY_PRIORS = frozenset((COEF_SEASONALITY,))\n\n\ndef get_default_priors() -> Mapping[str, dist.Distribution]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      INTERCEPT: dist.HalfNormal(scale=2.),\n      COEF_TREND: dist.Normal(loc=0., scale=1.),\n      EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      SIGMA: dist.Gamma(concentration=1., rate=1.),\n      GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      WEEKDAY: dist.Normal(loc=0., scale=.5),\n      COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      COEF_SEASONALITY: dist.HalfNormal(scale=.5),\n      AD_EFFECT_RETENTION_RATE: dist.Beta(concentration1=1., concentration0=1.),\n      PEAK_EFFECT_DELAY: dist.HalfNormal(scale=2.),\n      EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n      LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.),\n      HALF_MAX_EFFECTIVE_CONCENTRATION: dist.Gamma(concentration=1., rate=1.),\n      SLOPE: dist.Gamma(concentration=1., rate=1.),\n      DYNAMIC_TREND_INITIAL_LEVEL: dist.Normal(loc=.5, scale=2.5),\n      DYNAMIC_TREND_INITIAL_SLOPE: dist.Normal(loc=0., scale=.2),\n      DYNAMIC_TREND_LEVEL_VARIANCE: dist.Uniform(low=0., high=.1),\n      DYNAMIC_TREND_SLOPE_VARIANCE: dist.Uniform(low=0., high=.01),\n  })\n\nAST=Module(Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Tuple(Name(Load)Load)))FunctionDef(argumentsReturn(Call(Attribute(Name(Load)Load)Dict(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant)))))Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 93, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_55-93", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "    LAG_WEIGHT,\n    AD_EFFECT_RETENTION_RATE,\n    PEAK_EFFECT_DELAY,\n    EXPONENT,\n    HALF_MAX_EFFECTIVE_CONCENTRATION,\n    SLOPE,\n    DYNAMIC_TREND_INITIAL_LEVEL,\n    DYNAMIC_TREND_INITIAL_SLOPE,\n    DYNAMIC_TREND_LEVEL_VARIANCE,\n    DYNAMIC_TREND_SLOPE_VARIANCE,\n))\n\nGEO_ONLY_PRIORS = frozenset((COEF_SEASONALITY,))\n\n\ndef get_default_priors() -> Mapping[str, dist.Distribution]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      INTERCEPT: dist.HalfNormal(scale=2.),\n      COEF_TREND: dist.Normal(loc=0., scale=1.),\n      EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      SIGMA: dist.Gamma(concentration=1., rate=1.),\n      GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      WEEKDAY: dist.Normal(loc=0., scale=.5),\n      COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      COEF_SEASONALITY: dist.HalfNormal(scale=.5),\n      AD_EFFECT_RETENTION_RATE: dist.Beta(concentration1=1., concentration0=1.),\n      PEAK_EFFECT_DELAY: dist.HalfNormal(scale=2.),\n      EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n      LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.),\n      HALF_MAX_EFFECTIVE_CONCENTRATION: dist.Gamma(concentration=1., rate=1.),\n      SLOPE: dist.Gamma(concentration=1., rate=1.),\n      DYNAMIC_TREND_INITIAL_LEVEL: dist.Normal(loc=.5, scale=2.5),\n      DYNAMIC_TREND_INITIAL_SLOPE: dist.Normal(loc=0., scale=.2),\n      DYNAMIC_TREND_LEVEL_VARIANCE: dist.Uniform(low=0., high=.1),\n      DYNAMIC_TREND_SLOPE_VARIANCE: dist.Uniform(low=0., high=.01),\n  })", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 93, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-priors.py_65-93", "title": "google_lightweight_mmm-lightweight_mmm-core-priors.py", "text": "))\n\nGEO_ONLY_PRIORS = frozenset((COEF_SEASONALITY,))\n\n\ndef get_default_priors() -> Mapping[str, dist.Distribution]:\n  # Since JAX cannot be called before absl.app.run in tests we get default\n  # priors from a function.\n  return immutabledict.immutabledict({\n      INTERCEPT: dist.HalfNormal(scale=2.),\n      COEF_TREND: dist.Normal(loc=0., scale=1.),\n      EXPO_TREND: dist.Uniform(low=0.5, high=1.5),\n      SIGMA: dist.Gamma(concentration=1., rate=1.),\n      GAMMA_SEASONALITY: dist.Normal(loc=0., scale=1.),\n      WEEKDAY: dist.Normal(loc=0., scale=.5),\n      COEF_EXTRA_FEATURES: dist.Normal(loc=0., scale=1.),\n      COEF_SEASONALITY: dist.HalfNormal(scale=.5),\n      AD_EFFECT_RETENTION_RATE: dist.Beta(concentration1=1., concentration0=1.),\n      PEAK_EFFECT_DELAY: dist.HalfNormal(scale=2.),\n      EXPONENT: dist.Beta(concentration1=9., concentration0=1.),\n      LAG_WEIGHT: dist.Beta(concentration1=2., concentration0=1.),\n      HALF_MAX_EFFECTIVE_CONCENTRATION: dist.Gamma(concentration=1., rate=1.),\n      SLOPE: dist.Gamma(concentration=1., rate=1.),\n      DYNAMIC_TREND_INITIAL_LEVEL: dist.Normal(loc=.5, scale=2.5),\n      DYNAMIC_TREND_INITIAL_SLOPE: dist.Normal(loc=0., scale=.2),\n      DYNAMIC_TREND_LEVEL_VARIANCE: dist.Uniform(low=0., high=.1),\n      DYNAMIC_TREND_SLOPE_VARIANCE: dist.Uniform(low=0., high=.01),\n  })", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "priors.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 93, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-__init__.py_0-14", "title": "google_lightweight_mmm-lightweight_mmm-core-__init__.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nAST=Module", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for modeling the intercept.\"\"\"\n\nfrom typing import Mapping\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for modeling the intercept.\"\"\"\n\nfrom typing import Mapping\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\ndef simple_intercept(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str,\n                           dist.Distribution] = immutabledict.immutabledict(),\n) -> jnp.ndarray:\n  \"\"\"Calculates a national or geo incercept.\n  Note that this intercept is constant over time.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for modeling the intercept.\"\"\"\n\nfrom typing import Mapping\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\ndef simple_intercept(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str,\n                           dist.Distribution] = immutabledict.immutabledict(),\n) -> jnp.ndarray:\n  \"\"\"Calculates a national or geo incercept.\n  Note that this intercept is constant over time.\n\n  Args:\n    data: Media input data. Media data must have either 2 dims for national\n      model or 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. Refer to the full documentation on custom priors for\n      details.\n\n  Returns:\n    The values of the intercept.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Call(Attribute(Name(Load)Load)))Expr(Constant)Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for modeling the intercept.\"\"\"\n\nfrom typing import Mapping\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\ndef simple_intercept(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str,\n                           dist.Distribution] = immutabledict.immutabledict(),\n) -> jnp.ndarray:\n  \"\"\"Calculates a national or geo incercept.\n  Note that this intercept is constant over time.\n\n  Args:\n    data: Media input data. Media data must have either 2 dims for national\n      model or 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. Refer to the full documentation on custom priors for\n      details.\n\n  Returns:\n    The values of the intercept.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n\n  with numpyro.plate(name=f\"{priors.INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=priors.INTERCEPT,\n        fn=custom_priors.get(priors.INTERCEPT,\n                             default_priors[priors.INTERCEPT]),\n    )\n  return intercept\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Call(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py_15-55", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py", "text": "\nfrom typing import Mapping\n\nimport immutabledict\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\ndef simple_intercept(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str,\n                           dist.Distribution] = immutabledict.immutabledict(),\n) -> jnp.ndarray:\n  \"\"\"Calculates a national or geo incercept.\n  Note that this intercept is constant over time.\n\n  Args:\n    data: Media input data. Media data must have either 2 dims for national\n      model or 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. Refer to the full documentation on custom priors for\n      details.\n\n  Returns:\n    The values of the intercept.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n\n  with numpyro.plate(name=f\"{priors.INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=priors.INTERCEPT,\n        fn=custom_priors.get(priors.INTERCEPT,\n                             default_priors[priors.INTERCEPT]),\n    )\n  return intercept\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Call(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py_25-55", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept.py", "text": "\n\ndef simple_intercept(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str,\n                           dist.Distribution] = immutabledict.immutabledict(),\n) -> jnp.ndarray:\n  \"\"\"Calculates a national or geo incercept.\n  Note that this intercept is constant over time.\n\n  Args:\n    data: Media input data. Media data must have either 2 dims for national\n      model or 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. Refer to the full documentation on custom priors for\n      details.\n\n  Returns:\n    The values of the intercept.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n\n  with numpyro.plate(name=f\"{priors.INTERCEPT}_plate\", size=n_geos):\n    intercept = numpyro.sample(\n        name=priors.INTERCEPT,\n        fn=custom_priors.get(priors.INTERCEPT,\n                             default_priors[priors.INTERCEPT]),\n    )\n  return intercept\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))Call(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for intercept.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import core_utils\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for intercept.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.baseline import intercept\n\n\nclass InterceptTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for intercept.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.baseline import intercept\n\n\nclass InterceptTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_produces_output_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for intercept.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.baseline import intercept\n\n\nclass InterceptTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_produces_output_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    n_geos = core_utils.get_number_geos(data=data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)))))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load))keyword(Constant)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Constant))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.baseline import intercept\n\n\nclass InterceptTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_produces_output_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    n_geos = core_utils.get_number_geos(data=data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    intercept_values = mcmc.get_samples()[\"intercept_values\"]\n\n    self.assertEqual(intercept_values.shape, (num_samples, n_geos))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "from lightweight_mmm.core import priors\nfrom lightweight_mmm.core.baseline import intercept\n\n\nclass InterceptTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_produces_output_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    n_geos = core_utils.get_number_geos(data=data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    intercept_values = mcmc.get_samples()[\"intercept_values\"]\n\n    self.assertEqual(intercept_values.shape, (num_samples, n_geos))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_takes_custom_priors_correctly(self, data_shape):\n    prior_name = priors.INTERCEPT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_produces_output_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    n_geos = core_utils.get_number_geos(data=data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    intercept_values = mcmc.get_samples()[\"intercept_values\"]\n\n    self.assertEqual(intercept_values.shape, (num_samples, n_geos))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_takes_custom_priors_correctly(self, data_shape):\n    prior_name = priors.INTERCEPT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones(data_shape)\n\n    trace_handler = handlers.trace(\n        handlers.seed(intercept.simple_intercept, rng_seed=0))\n    trace = trace_handler.get_trace(data=media, custom_priors=custom_priors)\n    values_and_dists = {", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "          \"intercept_values\",\n          intercept.simple_intercept(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    n_geos = core_utils.get_number_geos(data=data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    intercept_values = mcmc.get_samples()[\"intercept_values\"]\n\n    self.assertEqual(intercept_values.shape, (num_samples, n_geos))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_takes_custom_priors_correctly(self, data_shape):\n    prior_name = priors.INTERCEPT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones(data_shape)\n\n    trace_handler = handlers.trace(\n        handlers.seed(intercept.simple_intercept, rng_seed=0))\n    trace = trace_handler.get_trace(data=media, custom_priors=custom_priors)\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name].base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_55-96", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "\n    mcmc.run(rng_key, data=data)\n    intercept_values = mcmc.get_samples()[\"intercept_values\"]\n\n    self.assertEqual(intercept_values.shape, (num_samples, n_geos))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_takes_custom_priors_correctly(self, data_shape):\n    prior_name = priors.INTERCEPT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones(data_shape)\n\n    trace_handler = handlers.trace(\n        handlers.seed(intercept.simple_intercept, rng_seed=0))\n    trace = trace_handler.get_trace(data=media, custom_priors=custom_priors)\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name].base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 96, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py_65-96", "title": "google_lightweight_mmm-lightweight_mmm-core-baseline-intercept_test.py", "text": "      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_simple_intercept_takes_custom_priors_correctly(self, data_shape):\n    prior_name = priors.INTERCEPT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones(data_shape)\n\n    trace_handler = handlers.trace(\n        handlers.seed(intercept.simple_intercept, rng_seed=0))\n    trace = trace_handler.get_trace(data=media, custom_priors=custom_priors)\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name].base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "baseline", "intercept_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 96, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for seasonality.\"\"\"\n\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core import core_utils\n\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "\n\n@jax.jit\ndef _sinusoidal_seasonality(\n    seasonality_arange: jnp.ndarray,\n    degrees_arange: jnp.ndarray,\n    gamma_seasonality: jnp.ndarray,\n    frequency: int,\n) -> jnp.ndarray:\n  \"\"\"Core calculation of cyclic variation seasonality.\n\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "\n  Args:\n    seasonality_arange: Array with range [0, N - 1] where N is the size of the\n      data for which the seasonality is modelled.\n    degrees_arange: Array with range [0, D - 1] where D is the number of degrees\n      to use. Must be greater or equal than 1.\n    gamma_seasonality: Factor to multiply to each degree calculation. Shape must\n      be aligned with the number of degrees.\n    frequency: Frecuency of the seasonality be in computed.\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  inner_value = seasonality_arange * 2 * jnp.pi * degrees_arange / frequency\n  season_matrix_sin = jnp.sin(inner_value)\n  season_matrix_cos = jnp.cos(inner_value)\n  season_matrix = jnp.concatenate([\n      jnp.expand_dims(a=season_matrix_sin, axis=-1),\n      jnp.expand_dims(a=season_matrix_cos, axis=-1)\n  ],\n                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "                                  axis=-1)\n  return jnp.einsum(\"tds, ds -> t\", season_matrix, gamma_seasonality)\n\n\ndef sinusoidal_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    degrees_seasonality: int = 2,\n    frequency: int = 52,\n) -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": ") -> jnp.ndarray:\n  \"\"\"Calculates cyclic variation seasonality.\n\n  For detailed info check:\n    https://en.wikipedia.org/wiki/Seasonality#Modeling\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n\n\ndef _intra_week_seasonality(\n    data: jnp.ndarray,\n    weekday: jnp.ndarray,\n) -> jnp.ndarray:\n  data_size = data.shape[0]\n  return weekday[jnp.arange(data_size) % 7]\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "      default ones.\n    degrees_seasonality: Number of degrees to use. Must be greater or equal than\n      1.\n    frequency: Frecuency of the seasonality be in computed. By default is 52 for\n      weekly data (52 weeks in a year).\n\n  Returns:\n    An array with the seasonality values.\n  \"\"\"\n  number_periods = data.shape[0]\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n\n\ndef _intra_week_seasonality(\n    data: jnp.ndarray,\n    weekday: jnp.ndarray,\n) -> jnp.ndarray:\n  data_size = data.shape[0]\n  return weekday[jnp.arange(data_size) % 7]\n\n\ndef intra_week_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Models intra week seasonality.\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  with numpyro.plate(name=f\"{priors.GAMMA_SEASONALITY}_sin_cos_plate\", size=2):\n    with numpyro.plate(\n        name=f\"{priors.GAMMA_SEASONALITY}_plate\", size=degrees_seasonality):\n      gamma_seasonality = numpyro.sample(\n          name=priors.GAMMA_SEASONALITY,\n          fn=custom_priors.get(priors.GAMMA_SEASONALITY,\n                               default_priors[priors.GAMMA_SEASONALITY]))\n  seasonality_arange = jnp.expand_dims(a=jnp.arange(number_periods), axis=-1)\n  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n\n\ndef _intra_week_seasonality(\n    data: jnp.ndarray,\n    weekday: jnp.ndarray,\n) -> jnp.ndarray:\n  data_size = data.shape[0]\n  return weekday[jnp.arange(data_size) % 7]\n\n\ndef intra_week_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Models intra week seasonality.\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n\n  Returns:\n    The contribution of the weekday seasonality.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(name=f\"{priors.WEEKDAY}_plate\", size=7):\n    weekday = numpyro.sample(\n        name=priors.WEEKDAY,\n        fn=custom_priors.get(priors.WEEKDAY, default_priors[priors.WEEKDAY]))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_95-142", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "  degrees_arange = jnp.arange(degrees_seasonality)\n  seasonality_values = _sinusoidal_seasonality(\n      seasonality_arange=seasonality_arange,\n      degrees_arange=degrees_arange,\n      frequency=frequency,\n      gamma_seasonality=gamma_seasonality,\n  )\n  if n_geos > 1:\n    seasonality_values = jnp.expand_dims(seasonality_values, axis=-1)\n  return seasonality_values\n\n\ndef _intra_week_seasonality(\n    data: jnp.ndarray,\n    weekday: jnp.ndarray,\n) -> jnp.ndarray:\n  data_size = data.shape[0]\n  return weekday[jnp.arange(data_size) % 7]\n\n\ndef intra_week_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Models intra week seasonality.\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n\n  Returns:\n    The contribution of the weekday seasonality.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(name=f\"{priors.WEEKDAY}_plate\", size=7):\n    weekday = numpyro.sample(\n        name=priors.WEEKDAY,\n        fn=custom_priors.get(priors.WEEKDAY, default_priors[priors.WEEKDAY]))\n\n  weekday_series = _intra_week_seasonality(data=data, weekday=weekday)\n\n  if data.ndim == 3:  # For geo model's case\n    weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n\n  return weekday_series", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 142, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_105-142", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "\n\ndef _intra_week_seasonality(\n    data: jnp.ndarray,\n    weekday: jnp.ndarray,\n) -> jnp.ndarray:\n  data_size = data.shape[0]\n  return weekday[jnp.arange(data_size) % 7]\n\n\ndef intra_week_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Models intra week seasonality.\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n\n  Returns:\n    The contribution of the weekday seasonality.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(name=f\"{priors.WEEKDAY}_plate\", size=7):\n    weekday = numpyro.sample(\n        name=priors.WEEKDAY,\n        fn=custom_priors.get(priors.WEEKDAY, default_priors[priors.WEEKDAY]))\n\n  weekday_series = _intra_week_seasonality(data=data, weekday=weekday)\n\n  if data.ndim == 3:  # For geo model's case\n    weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n\n  return weekday_series\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Return(Subscript(Name(Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load))ModConstant)Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 142, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py_115-142", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality.py", "text": "def intra_week_seasonality(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Models intra week seasonality.\n\n  Args:\n    data: Data for which the seasonality will be modelled for. It is used to\n      obtain the length of the time dimension, axis 0.\n    custom_priors: The custom priors we want the model to take instead of\n      default ones.\n\n  Returns:\n    The contribution of the weekday seasonality.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(name=f\"{priors.WEEKDAY}_plate\", size=7):\n    weekday = numpyro.sample(\n        name=priors.WEEKDAY,\n        fn=custom_priors.get(priors.WEEKDAY, default_priors[priors.WEEKDAY]))\n\n  weekday_series = _intra_week_seasonality(data=data, weekday=weekday)\n\n  if data.ndim == 3:  # For geo model's case\n    weekday_series = jnp.expand_dims(weekday_series, axis=-1)\n\n  return weekday_series\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 142, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for seasonality.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for seasonality.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for seasonality.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,\n          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for seasonality.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,\n          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",\n          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))keyword(UnaryOp(USubConstant))))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Load)))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,\n          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",\n          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(Name(Load)Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Load)))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "from lightweight_mmm.core.time import seasonality\n\n\nclass SeasonalityTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"2_degrees\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=5,\n          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",\n          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "          gamma_seasonality_shape=(5, 2),\n      ),\n      dict(\n          testcase_name=\"10_degree\",\n          seasonality_arange_value=150,\n          degrees_arange_shape=10,\n          gamma_seasonality_shape=(10, 2),\n      ),\n      dict(\n          testcase_name=\"1_degree\",\n          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "          seasonality_arange_value=200,\n          degrees_arange_shape=1,\n          gamma_seasonality_shape=(1, 2),\n      ),\n  ])\n  def test_core_sinusoidal_seasonality_produces_correct_shape(\n      self, seasonality_arange_value, degrees_arange_shape,\n      gamma_seasonality_shape):\n    seasonality_arange = jnp.expand_dims(\n        jnp.arange(seasonality_arange_value), axis=-1)\n    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    degrees_arange = jnp.arange(degrees_arange_shape)\n    gamma_seasonality = jnp.ones(gamma_seasonality_shape)\n\n    seasonality_values = seasonality._sinusoidal_seasonality(\n        seasonality_arange=seasonality_arange,\n        degrees_arange=degrees_arange,\n        gamma_seasonality=gamma_seasonality,\n        frequency=52,\n    )\n    self.assertEqual(seasonality_values.shape, (seasonality_arange_value,))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n\n    def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n\n    num_samples = 10", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"ten_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n\n    def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        data=data,\n        degrees_seasonality=degrees_seasonality,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "          data_shape=(500, 5, 5),\n          degrees_seasonality=10,\n          expected_shape=(10, 500, 1),\n      ),\n      dict(\n          testcase_name=\"one_degrees_national\",\n          data_shape=(500, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n\n    def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        data=data,\n        degrees_seasonality=degrees_seasonality,\n        frequency=52,\n    )\n    seasonality_values = mcmc.get_samples()[\"seasonality\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_sinusoidal_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.GAMMA_SEASONALITY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "      dict(\n          testcase_name=\"one_degrees_geo\",\n          data_shape=(500, 5, 5),\n          degrees_seasonality=1,\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_model_sinusoidal_seasonality_produces_correct_shape(\n      self, data_shape, degrees_seasonality, expected_shape):\n\n    def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        data=data,\n        degrees_seasonality=degrees_seasonality,\n        frequency=52,\n    )\n    seasonality_values = mcmc.get_samples()[\"seasonality\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_sinusoidal_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.GAMMA_SEASONALITY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    degrees_seasonality = 3\n    frequency = 365\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.sinusoidal_seasonality, rng_seed=0))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    def mock_model_function(data, degrees_seasonality, frequency):\n      numpyro.deterministic(\n          \"seasonality\",\n          seasonality.sinusoidal_seasonality(\n              data=data,\n              degrees_seasonality=degrees_seasonality,\n              custom_priors={},\n              frequency=frequency))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        data=data,\n        degrees_seasonality=degrees_seasonality,\n        frequency=52,\n    )\n    seasonality_values = mcmc.get_samples()[\"seasonality\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_sinusoidal_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.GAMMA_SEASONALITY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    degrees_seasonality = 3\n    frequency = 365\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.sinusoidal_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        degrees_seasonality=degrees_seasonality,\n        frequency=frequency,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(\n        rng_key,\n        data=data,\n        degrees_seasonality=degrees_seasonality,\n        frequency=52,\n    )\n    seasonality_values = mcmc.get_samples()[\"seasonality\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_sinusoidal_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.GAMMA_SEASONALITY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    degrees_seasonality = 3\n    frequency = 365\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.sinusoidal_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        degrees_seasonality=degrees_seasonality,\n        frequency=frequency,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "        frequency=52,\n    )\n    seasonality_values = mcmc.get_samples()[\"seasonality\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_sinusoidal_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.GAMMA_SEASONALITY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    degrees_seasonality = 3\n    frequency = 365\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.sinusoidal_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        degrees_seasonality=degrees_seasonality,\n        frequency=frequency,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees\",\n          data_shape=(500, 3),\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"five_degrees\",\n          data_shape=(500, 3, 5),\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_intra_week_seasonality_produces_correct_shape(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    degrees_seasonality = 3\n    frequency = 365\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.sinusoidal_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        degrees_seasonality=degrees_seasonality,\n        frequency=frequency,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees\",\n          data_shape=(500, 3),\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"five_degrees\",\n          data_shape=(500, 3, 5),\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_intra_week_seasonality_produces_correct_shape(\n      self, data_shape, expected_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        degrees_seasonality=degrees_seasonality,\n        frequency=frequency,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees\",\n          data_shape=(500, 3),\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"five_degrees\",\n          data_shape=(500, 3, 5),\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_intra_week_seasonality_produces_correct_shape(\n      self, data_shape, expected_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    seasonality_values = mcmc.get_samples()[\"intra_week\"]\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ten_degrees\",\n          data_shape=(500, 3),\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"five_degrees\",\n          data_shape=(500, 3, 5),\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_intra_week_seasonality_produces_correct_shape(\n      self, data_shape, expected_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    seasonality_values = mcmc.get_samples()[\"intra_week\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_intra_week_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.WEEKDAY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "          data_shape=(500, 3),\n          expected_shape=(10, 500),\n      ),\n      dict(\n          testcase_name=\"five_degrees\",\n          data_shape=(500, 3, 5),\n          expected_shape=(10, 500, 1),\n      ),\n  )\n  def test_intra_week_seasonality_produces_correct_shape(\n      self, data_shape, expected_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    seasonality_values = mcmc.get_samples()[\"intra_week\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_intra_week_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.WEEKDAY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.intra_week_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "      self, data_shape, expected_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"intra_week\",\n          seasonality.intra_week_seasonality(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    seasonality_values = mcmc.get_samples()[\"intra_week\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_intra_week_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.WEEKDAY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.intra_week_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_175-217", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    seasonality_values = mcmc.get_samples()[\"intra_week\"]\n\n    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_intra_week_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.WEEKDAY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.intra_week_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 217, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py_185-217", "title": "google_lightweight_mmm-lightweight_mmm-core-time-seasonality_test.py", "text": "    self.assertEqual(seasonality_values.shape, expected_shape)\n\n  def test_intra_week_seasonality_custom_priors_are_taken_correctly(self):\n    prior_name = priors.WEEKDAY\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(seasonality.intra_week_seasonality, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "seasonality_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 217, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for trend.\"\"\"\n\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for trend.\"\"\"\n\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for trend.\"\"\"\n\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.\n    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Core and modelling functions for trend.\"\"\"\n\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.\n    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "\nimport functools\nfrom typing import Mapping\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.\n    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "from lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _trend_with_exponent(coef_trend: jnp.ndarray, trend: jnp.ndarray,\n                         expo_trend: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies the coefficient and exponent to the trend to obtain trend values.\n\n  Args:\n    coef_trend: Coefficient to be multiplied by the trend.\n    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n\nAST=Module(ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Return(BinOp(Name(Load)MultBinOp(Name(Load)PowName(Load))))Attribute(Name(Load)Load)Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load)))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))If(Compare(Name(Load)GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "    trend: Initial trend values.\n    expo_trend: Exponent to be applied to the trend.\n\n  Returns:\n    The trend values generated.\n  \"\"\"\n  return coef_trend * trend**expo_trend\n\n\ndef trend_with_exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_periods\",))\ndef _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Trend with exponent for curvature.\n\n  Args:\n    data: Data for which trend will be created.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_periods\",))\ndef _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,\n    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "\n  Returns:\n    The values of the trend.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  n_geos = core_utils.get_number_geos(data=data)\n  # TODO(): Force all geos to have the same trend sign.\n  with numpyro.plate(name=f\"{priors.COEF_TREND}_plate\", size=n_geos):\n    coef_trend = numpyro.sample(\n        name=priors.COEF_TREND,\n        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_periods\",))\ndef _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,\n    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:\n    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "        fn=custom_priors.get(priors.COEF_TREND,\n                             default_priors[priors.COEF_TREND]))\n\n  expo_trend = numpyro.sample(\n      name=priors.EXPO_TREND,\n      fn=custom_priors.get(priors.EXPO_TREND,\n                           default_priors[priors.EXPO_TREND]))\n  linear_trend = jnp.arange(data.shape[0])\n  if n_geos > 1:  # For geo model's case\n    linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_periods\",))\ndef _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,\n    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:\n    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.\n  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "  return _trend_with_exponent(\n      coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_periods\",))\ndef _dynamic_trend(\n    number_periods: int,\n    random_walk_level: jnp.ndarray,\n    random_walk_slope: jnp.ndarray,\n    initial_level: jnp.ndarray,\n    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:\n    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.\n  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])\n  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "    initial_slope: jnp.ndarray,\n    variance_level: jnp.ndarray,\n    variance_slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates dynamic trend using local linear trend method.\n\n  More details about this function can be found in:\n  https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf\n\n  Args:\n    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.\n  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])\n  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n\n\ndef dynamic_trend(\n    geo_size: int,\n    data_size: int,\n    is_trend_prediction: bool,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Generates the dynamic trend to capture the baseline of kpi.\n\n  Args:\n    geo_size: Number of geos in the model.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "    number_periods: Number of time periods in the data.\n    random_walk_level: Random walk of level from sample.\n    random_walk_slope: Random walk of slope from sample.\n    initial_level: The initial value for level in local linear trend model.\n    initial_slope: The initial value for slope in local linear trend model.\n    variance_level: The variance of the expected increase in level between time.\n    variance_slope: The variance of the expected increase in slope between time.\n\n  Returns:\n    The dynamic trend values for the given data with the given parameters.\n  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])\n  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n\n\ndef dynamic_trend(\n    geo_size: int,\n    data_size: int,\n    is_trend_prediction: bool,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Generates the dynamic trend to capture the baseline of kpi.\n\n  Args:\n    geo_size: Number of geos in the model.\n    data_size: Number of time samples in the model.\n    is_trend_prediction: Whether it is used for prediction or fitting.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    Jax array with trend for each time t.\n  \"\"\"\n  default_priors = priors.get_default_priors()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "  \"\"\"\n  # Simulate gaussian random walk of level with initial level.\n  random_level = variance_level * random_walk_level\n  random_level_with_initial_level = jnp.concatenate(\n      [jnp.array([random_level[0] + initial_level]), random_level[1:]])\n  level_trend_t = jnp.cumsum(random_level_with_initial_level, axis=0)\n  # Simulate gaussian random walk of slope with initial slope.\n  random_slope = variance_slope * random_walk_slope\n  random_slope_with_initial_slope = jnp.concatenate(\n      [jnp.array([random_slope[0] + initial_slope]), random_slope[1:]])\n  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n\n\ndef dynamic_trend(\n    geo_size: int,\n    data_size: int,\n    is_trend_prediction: bool,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Generates the dynamic trend to capture the baseline of kpi.\n\n  Args:\n    geo_size: Number of geos in the model.\n    data_size: Number of time samples in the model.\n    is_trend_prediction: Whether it is used for prediction or fitting.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    Jax array with trend for each time t.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  if not is_trend_prediction:\n    random_walk_level = numpyro.sample(\"random_walk_level\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n    random_walk_slope = numpyro.sample(\"random_walk_slope\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n  else:\n    random_walk_level = numpyro.sample(\"random_walk_level_prediction\",\n                                       fn=dist.Normal(),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "  slope_trend_t = jnp.cumsum(random_slope_with_initial_slope, axis=0)\n  # Accumulate sum of slope series to address latent variable slope in function\n  # level_t = level_t-1 + slope_t-1.\n  initial_zero_shape = [(1, 0)] if slope_trend_t.ndim == 1 else [(1, 0), (0, 0)]\n  slope_trend_cumsum = jnp.pad(\n      jnp.cumsum(slope_trend_t, axis=0)[:number_periods - 1],\n      initial_zero_shape, mode=\"constant\", constant_values=0)\n  return level_trend_t + slope_trend_cumsum\n\n\ndef dynamic_trend(\n    geo_size: int,\n    data_size: int,\n    is_trend_prediction: bool,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Generates the dynamic trend to capture the baseline of kpi.\n\n  Args:\n    geo_size: Number of geos in the model.\n    data_size: Number of time samples in the model.\n    is_trend_prediction: Whether it is used for prediction or fitting.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    Jax array with trend for each time t.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  if not is_trend_prediction:\n    random_walk_level = numpyro.sample(\"random_walk_level\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n    random_walk_slope = numpyro.sample(\"random_walk_slope\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n  else:\n    random_walk_level = numpyro.sample(\"random_walk_level_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n    random_walk_slope = numpyro.sample(\"random_walk_slope_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_LEVEL}_plate\", size=geo_size):\n    trend_initial_level = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_LEVEL,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "def dynamic_trend(\n    geo_size: int,\n    data_size: int,\n    is_trend_prediction: bool,\n    custom_priors: Mapping[str, dist.Distribution],\n) -> jnp.ndarray:\n  \"\"\"Generates the dynamic trend to capture the baseline of kpi.\n\n  Args:\n    geo_size: Number of geos in the model.\n    data_size: Number of time samples in the model.\n    is_trend_prediction: Whether it is used for prediction or fitting.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    Jax array with trend for each time t.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  if not is_trend_prediction:\n    random_walk_level = numpyro.sample(\"random_walk_level\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n    random_walk_slope = numpyro.sample(\"random_walk_slope\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n  else:\n    random_walk_level = numpyro.sample(\"random_walk_level_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n    random_walk_slope = numpyro.sample(\"random_walk_slope_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_LEVEL}_plate\", size=geo_size):\n    trend_initial_level = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_LEVEL,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_LEVEL]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_SLOPE}_plate\", size=geo_size):\n    trend_initial_slope = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_SLOPE,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "    data_size: Number of time samples in the model.\n    is_trend_prediction: Whether it is used for prediction or fitting.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. See our custom_priors documentation for details about the\n      API and possible options.\n\n  Returns:\n    Jax array with trend for each time t.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  if not is_trend_prediction:\n    random_walk_level = numpyro.sample(\"random_walk_level\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n    random_walk_slope = numpyro.sample(\"random_walk_slope\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n  else:\n    random_walk_level = numpyro.sample(\"random_walk_level_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n    random_walk_slope = numpyro.sample(\"random_walk_slope_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_LEVEL}_plate\", size=geo_size):\n    trend_initial_level = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_LEVEL,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_LEVEL]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_SLOPE}_plate\", size=geo_size):\n    trend_initial_slope = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_SLOPE,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_SLOPE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_LEVEL_VARIANCE}_plate\", size=geo_size):\n    trend_level_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_LEVEL_VARIANCE]))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "  if not is_trend_prediction:\n    random_walk_level = numpyro.sample(\"random_walk_level\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n    random_walk_slope = numpyro.sample(\"random_walk_slope\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n  else:\n    random_walk_level = numpyro.sample(\"random_walk_level_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n    random_walk_slope = numpyro.sample(\"random_walk_slope_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_LEVEL}_plate\", size=geo_size):\n    trend_initial_level = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_LEVEL,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_LEVEL]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_SLOPE}_plate\", size=geo_size):\n    trend_initial_slope = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_SLOPE,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_SLOPE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_LEVEL_VARIANCE}_plate\", size=geo_size):\n    trend_level_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_LEVEL_VARIANCE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_SLOPE_VARIANCE}_plate\", size=geo_size):\n    trend_slope_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_SLOPE_VARIANCE]))\n\n  if geo_size == 1:  # National level model case.\n    random_walk_level = jnp.squeeze(random_walk_level)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "                                       sample_shape=(data_size, 1))\n\n    random_walk_slope = numpyro.sample(\"random_walk_slope_prediction\",\n                                       fn=dist.Normal(),\n                                       sample_shape=(data_size, 1))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_LEVEL}_plate\", size=geo_size):\n    trend_initial_level = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_LEVEL,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_LEVEL]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_SLOPE}_plate\", size=geo_size):\n    trend_initial_slope = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_SLOPE,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_SLOPE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_LEVEL_VARIANCE}_plate\", size=geo_size):\n    trend_level_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_LEVEL_VARIANCE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_SLOPE_VARIANCE}_plate\", size=geo_size):\n    trend_slope_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_SLOPE_VARIANCE]))\n\n  if geo_size == 1:  # National level model case.\n    random_walk_level = jnp.squeeze(random_walk_level)\n    random_walk_slope = jnp.squeeze(random_walk_slope)\n    trend_initial_level = jnp.squeeze(trend_initial_level)\n    trend_initial_slope = jnp.squeeze(trend_initial_slope)\n    trend_level_variance = jnp.squeeze(trend_level_variance)\n    trend_slope_variance = jnp.squeeze(trend_slope_variance)\n\n  return _dynamic_trend(\n      number_periods=data_size,\n      random_walk_level=random_walk_level,\n      random_walk_slope=random_walk_slope,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_165-209", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_LEVEL,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_LEVEL]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_INITIAL_SLOPE}_plate\", size=geo_size):\n    trend_initial_slope = numpyro.sample(\n        name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_INITIAL_SLOPE,\n            default_priors[priors.DYNAMIC_TREND_INITIAL_SLOPE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_LEVEL_VARIANCE}_plate\", size=geo_size):\n    trend_level_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_LEVEL_VARIANCE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_SLOPE_VARIANCE}_plate\", size=geo_size):\n    trend_slope_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_SLOPE_VARIANCE]))\n\n  if geo_size == 1:  # National level model case.\n    random_walk_level = jnp.squeeze(random_walk_level)\n    random_walk_slope = jnp.squeeze(random_walk_slope)\n    trend_initial_level = jnp.squeeze(trend_initial_level)\n    trend_initial_slope = jnp.squeeze(trend_initial_slope)\n    trend_level_variance = jnp.squeeze(trend_level_variance)\n    trend_slope_variance = jnp.squeeze(trend_slope_variance)\n\n  return _dynamic_trend(\n      number_periods=data_size,\n      random_walk_level=random_walk_level,\n      random_walk_slope=random_walk_slope,\n      initial_level=trend_initial_level,\n      initial_slope=trend_initial_slope,\n      variance_level=trend_level_variance,\n      variance_slope=trend_slope_variance)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 209, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend.py_175-209", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend.py", "text": "            default_priors[priors.DYNAMIC_TREND_INITIAL_SLOPE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_LEVEL_VARIANCE}_plate\", size=geo_size):\n    trend_level_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_LEVEL_VARIANCE]))\n\n  with numpyro.plate(\n      name=f\"{priors.DYNAMIC_TREND_SLOPE_VARIANCE}_plate\", size=geo_size):\n    trend_slope_variance = numpyro.sample(\n        name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n        fn=custom_priors.get(\n            priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n            default_priors[priors.DYNAMIC_TREND_SLOPE_VARIANCE]))\n\n  if geo_size == 1:  # National level model case.\n    random_walk_level = jnp.squeeze(random_walk_level)\n    random_walk_slope = jnp.squeeze(random_walk_slope)\n    trend_initial_level = jnp.squeeze(trend_initial_level)\n    trend_initial_slope = jnp.squeeze(trend_initial_slope)\n    trend_level_variance = jnp.squeeze(trend_level_variance)\n    trend_slope_variance = jnp.squeeze(trend_slope_variance)\n\n  return _dynamic_trend(\n      number_periods=data_size,\n      random_walk_level=random_walk_level,\n      random_walk_slope=random_walk_slope,\n      initial_level=trend_initial_level,\n      initial_slope=trend_initial_slope,\n      variance_level=trend_level_variance,\n      variance_slope=trend_slope_variance)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 209, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for trend.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for trend.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import trend\n\n\nclass TrendTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for trend.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import trend\n\n\nclass TrendTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          coef_trend_shape=(),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          coef_trend_shape=(5,),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for trend.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import trend\n\n\nclass TrendTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          coef_trend_shape=(),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          coef_trend_shape=(5,),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n  ])\n  def test_core_trend_with_exponent_produces_correct_shape(\n      self, coef_trend_shape, trend_length, expo_trend_shape):\n    coef_trend = jnp.ones(coef_trend_shape)\n    linear_trend = jnp.arange(trend_length)\n    if coef_trend.ndim == 1:  # For geo model's case\n      linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro import handlers\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import trend\n\n\nclass TrendTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          coef_trend_shape=(),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          coef_trend_shape=(5,),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n  ])\n  def test_core_trend_with_exponent_produces_correct_shape(\n      self, coef_trend_shape, trend_length, expo_trend_shape):\n    coef_trend = jnp.ones(coef_trend_shape)\n    linear_trend = jnp.arange(trend_length)\n    if coef_trend.ndim == 1:  # For geo model's case\n      linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(\n        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "from lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.time import trend\n\n\nclass TrendTest(parameterized.TestCase):\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national\",\n          coef_trend_shape=(),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          coef_trend_shape=(5,),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n  ])\n  def test_core_trend_with_exponent_produces_correct_shape(\n      self, coef_trend_shape, trend_length, expo_trend_shape):\n    coef_trend = jnp.ones(coef_trend_shape)\n    linear_trend = jnp.arange(trend_length)\n    if coef_trend.ndim == 1:  # For geo model's case\n      linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(\n        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(Subscript(Attribute(Name(Load)Load)ConstantLoad)Starred(Name(Load)Load)Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Tuple(Load))keyword(Constant)keyword(Tuple(Load)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantLoad))keyword(Constant)keyword(Tuple(Load)))Load)))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)))))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))Load)))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          coef_trend_shape=(),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          coef_trend_shape=(5,),\n          trend_length=150,\n          expo_trend_shape=(),\n      ),\n  ])\n  def test_core_trend_with_exponent_produces_correct_shape(\n      self, coef_trend_shape, trend_length, expo_trend_shape):\n    coef_trend = jnp.ones(coef_trend_shape)\n    linear_trend = jnp.arange(trend_length)\n    if coef_trend.ndim == 1:  # For geo model's case\n      linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(\n        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = () if data.ndim == 2 else (data.shape[2],)\n\n    mcmc.run(rng_key, data=data)\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "  ])\n  def test_core_trend_with_exponent_produces_correct_shape(\n      self, coef_trend_shape, trend_length, expo_trend_shape):\n    coef_trend = jnp.ones(coef_trend_shape)\n    linear_trend = jnp.arange(trend_length)\n    if coef_trend.ndim == 1:  # For geo model's case\n      linear_trend = jnp.expand_dims(linear_trend, axis=-1)\n    expo_trend = jnp.ones(expo_trend_shape)\n\n    trend_values = trend._trend_with_exponent(\n        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = () if data.ndim == 2 else (data.shape[2],)\n\n    mcmc.run(rng_key, data=data)\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.COEF_TREND}\",\n          prior_name=priors.COEF_TREND,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.EXPO_TREND}\",\n          prior_name=priors.EXPO_TREND,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "        coef_trend=coef_trend, trend=linear_trend, expo_trend=expo_trend)\n\n    self.assertEqual(trend_values.shape,\n                     (linear_trend.shape[0], *coef_trend_shape))\n\n  @parameterized.named_parameters([\n      dict(testcase_name=\"national\", data_shape=(150, 3)),\n      dict(testcase_name=\"geo\", data_shape=(150, 3, 5)),\n  ])\n  def test_trend_with_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = () if data.ndim == 2 else (data.shape[2],)\n\n    mcmc.run(rng_key, data=data)\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.COEF_TREND}\",\n          prior_name=priors.COEF_TREND,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.EXPO_TREND}\",\n          prior_name=priors.EXPO_TREND,\n      ),\n  )\n  def test_trend_with_exponent_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "\n    def mock_model_function(data):\n      numpyro.deterministic(\n          \"trend\", trend.trend_with_exponent(\n              data=data,\n              custom_priors={},\n          ))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = () if data.ndim == 2 else (data.shape[2],)\n\n    mcmc.run(rng_key, data=data)\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.COEF_TREND}\",\n          prior_name=priors.COEF_TREND,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.EXPO_TREND}\",\n          prior_name=priors.EXPO_TREND,\n      ),\n  )\n  def test_trend_with_exponent_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(trend.trend_with_exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = () if data.ndim == 2 else (data.shape[2],)\n\n    mcmc.run(rng_key, data=data)\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.COEF_TREND}\",\n          prior_name=priors.COEF_TREND,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.EXPO_TREND}\",\n          prior_name=priors.EXPO_TREND,\n      ),\n  )\n  def test_trend_with_exponent_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(trend.trend_with_exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.COEF_TREND}\",\n          prior_name=priors.COEF_TREND,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.EXPO_TREND}\",\n          prior_name=priors.EXPO_TREND,\n      ),\n  )\n  def test_trend_with_exponent_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(trend.trend_with_exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"dynamic_trend_national_shape\",\n          number_periods=100,\n          initial_level_shape=(),\n          initial_slope_shape=(),\n          variance_level_shape=(),\n          variance_slope_shape=(),\n      ),\n      dict(\n          testcase_name=\"dynamic_trend_geo_shape\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "      ),\n  )\n  def test_trend_with_exponent_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(trend.trend_with_exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"dynamic_trend_national_shape\",\n          number_periods=100,\n          initial_level_shape=(),\n          initial_slope_shape=(),\n          variance_level_shape=(),\n          variance_slope_shape=(),\n      ),\n      dict(\n          testcase_name=\"dynamic_trend_geo_shape\",\n          number_periods=100,\n          initial_level_shape=(2,),\n          initial_slope_shape=(2,),\n          variance_level_shape=(2,),\n          variance_slope_shape=(2,),\n      ),\n  ])\n  def test_core_dynamic_trend_produces_correct_shape(\n      self, number_periods, initial_level_shape, initial_slope_shape,\n      variance_level_shape, variance_slope_shape):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(trend.trend_with_exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"dynamic_trend_national_shape\",\n          number_periods=100,\n          initial_level_shape=(),\n          initial_slope_shape=(),\n          variance_level_shape=(),\n          variance_slope_shape=(),\n      ),\n      dict(\n          testcase_name=\"dynamic_trend_geo_shape\",\n          number_periods=100,\n          initial_level_shape=(2,),\n          initial_slope_shape=(2,),\n          variance_level_shape=(2,),\n          variance_slope_shape=(2,),\n      ),\n  ])\n  def test_core_dynamic_trend_produces_correct_shape(\n      self, number_periods, initial_level_shape, initial_slope_shape,\n      variance_level_shape, variance_slope_shape):\n    initial_level = jnp.ones(initial_level_shape)\n    initial_slope = jnp.ones(initial_slope_shape)\n    variance_level = jnp.ones(variance_level_shape)\n    variance_slope = jnp.ones(variance_slope_shape)\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    if initial_level.ndim == 1:  # For geo model's case\n      random_walk_level = jnp.expand_dims(random_walk_level, axis=-1)\n      random_walk_slope = jnp.expand_dims(random_walk_slope, axis=-1)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"dynamic_trend_national_shape\",\n          number_periods=100,\n          initial_level_shape=(),\n          initial_slope_shape=(),\n          variance_level_shape=(),\n          variance_slope_shape=(),\n      ),\n      dict(\n          testcase_name=\"dynamic_trend_geo_shape\",\n          number_periods=100,\n          initial_level_shape=(2,),\n          initial_slope_shape=(2,),\n          variance_level_shape=(2,),\n          variance_slope_shape=(2,),\n      ),\n  ])\n  def test_core_dynamic_trend_produces_correct_shape(\n      self, number_periods, initial_level_shape, initial_slope_shape,\n      variance_level_shape, variance_slope_shape):\n    initial_level = jnp.ones(initial_level_shape)\n    initial_slope = jnp.ones(initial_slope_shape)\n    variance_level = jnp.ones(variance_level_shape)\n    variance_slope = jnp.ones(variance_slope_shape)\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    if initial_level.ndim == 1:  # For geo model's case\n      random_walk_level = jnp.expand_dims(random_walk_level, axis=-1)\n      random_walk_slope = jnp.expand_dims(random_walk_slope, axis=-1)\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "      dict(\n          testcase_name=\"dynamic_trend_national_shape\",\n          number_periods=100,\n          initial_level_shape=(),\n          initial_slope_shape=(),\n          variance_level_shape=(),\n          variance_slope_shape=(),\n      ),\n      dict(\n          testcase_name=\"dynamic_trend_geo_shape\",\n          number_periods=100,\n          initial_level_shape=(2,),\n          initial_slope_shape=(2,),\n          variance_level_shape=(2,),\n          variance_slope_shape=(2,),\n      ),\n  ])\n  def test_core_dynamic_trend_produces_correct_shape(\n      self, number_periods, initial_level_shape, initial_slope_shape,\n      variance_level_shape, variance_slope_shape):\n    initial_level = jnp.ones(initial_level_shape)\n    initial_slope = jnp.ones(initial_slope_shape)\n    variance_level = jnp.ones(variance_level_shape)\n    variance_slope = jnp.ones(variance_slope_shape)\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    if initial_level.ndim == 1:  # For geo model's case\n      random_walk_level = jnp.expand_dims(random_walk_level, axis=-1)\n      random_walk_slope = jnp.expand_dims(random_walk_slope, axis=-1)\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    self.assertEqual(dynamic_trend_values.shape,\n                     (number_periods, *initial_level_shape))\n\n  def test_core_dynamic_trend_produces_correct_value(self):\n    number_periods = 5\n    initial_level = jnp.ones(())\n    initial_slope = jnp.ones(())\n    variance_level = jnp.ones(())\n    variance_slope = jnp.ones(())\n    random_walk_level = jnp.arange(number_periods)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          number_periods=100,\n          initial_level_shape=(2,),\n          initial_slope_shape=(2,),\n          variance_level_shape=(2,),\n          variance_slope_shape=(2,),\n      ),\n  ])\n  def test_core_dynamic_trend_produces_correct_shape(\n      self, number_periods, initial_level_shape, initial_slope_shape,\n      variance_level_shape, variance_slope_shape):\n    initial_level = jnp.ones(initial_level_shape)\n    initial_slope = jnp.ones(initial_slope_shape)\n    variance_level = jnp.ones(variance_level_shape)\n    variance_slope = jnp.ones(variance_slope_shape)\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    if initial_level.ndim == 1:  # For geo model's case\n      random_walk_level = jnp.expand_dims(random_walk_level, axis=-1)\n      random_walk_slope = jnp.expand_dims(random_walk_slope, axis=-1)\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    self.assertEqual(dynamic_trend_values.shape,\n                     (number_periods, *initial_level_shape))\n\n  def test_core_dynamic_trend_produces_correct_value(self):\n    number_periods = 5\n    initial_level = jnp.ones(())\n    initial_slope = jnp.ones(())\n    variance_level = jnp.ones(())\n    variance_slope = jnp.ones(())\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    dynamic_trend_expected_value = jnp.array([1, 3, 7, 14, 25])\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    initial_level = jnp.ones(initial_level_shape)\n    initial_slope = jnp.ones(initial_slope_shape)\n    variance_level = jnp.ones(variance_level_shape)\n    variance_slope = jnp.ones(variance_slope_shape)\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    if initial_level.ndim == 1:  # For geo model's case\n      random_walk_level = jnp.expand_dims(random_walk_level, axis=-1)\n      random_walk_slope = jnp.expand_dims(random_walk_slope, axis=-1)\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    self.assertEqual(dynamic_trend_values.shape,\n                     (number_periods, *initial_level_shape))\n\n  def test_core_dynamic_trend_produces_correct_value(self):\n    number_periods = 5\n    initial_level = jnp.ones(())\n    initial_slope = jnp.ones(())\n    variance_level = jnp.ones(())\n    variance_slope = jnp.ones(())\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    dynamic_trend_expected_value = jnp.array([1, 3, 7, 14, 25])\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    np.testing.assert_array_equal(x=dynamic_trend_values,\n                                  y=dynamic_trend_expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_with_prediction_is_true\",\n          data_shape=(100, 3),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    self.assertEqual(dynamic_trend_values.shape,\n                     (number_periods, *initial_level_shape))\n\n  def test_core_dynamic_trend_produces_correct_value(self):\n    number_periods = 5\n    initial_level = jnp.ones(())\n    initial_slope = jnp.ones(())\n    variance_level = jnp.ones(())\n    variance_slope = jnp.ones(())\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    dynamic_trend_expected_value = jnp.array([1, 3, 7, 14, 25])\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    np.testing.assert_array_equal(x=dynamic_trend_values,\n                                  y=dynamic_trend_expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_with_prediction_is_true\",\n          data_shape=(100, 3),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"geo_with_prediction_is_true\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"national_with_prediction_is_false\",\n          data_shape=(100, 3),\n          is_trend_prediction=False),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    self.assertEqual(dynamic_trend_values.shape,\n                     (number_periods, *initial_level_shape))\n\n  def test_core_dynamic_trend_produces_correct_value(self):\n    number_periods = 5\n    initial_level = jnp.ones(())\n    initial_slope = jnp.ones(())\n    variance_level = jnp.ones(())\n    variance_slope = jnp.ones(())\n    random_walk_level = jnp.arange(number_periods)\n    random_walk_slope = jnp.arange(number_periods)\n    dynamic_trend_expected_value = jnp.array([1, 3, 7, 14, 25])\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    np.testing.assert_array_equal(x=dynamic_trend_values,\n                                  y=dynamic_trend_expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_with_prediction_is_true\",\n          data_shape=(100, 3),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"geo_with_prediction_is_true\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"national_with_prediction_is_false\",\n          data_shape=(100, 3),\n          is_trend_prediction=False),\n      dict(\n          testcase_name=\"geo_with_prediction_is_false\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=False),\n  ])\n  def test_dynamic_trend_produces_correct_shape(\n      self, data_shape, is_trend_prediction):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    random_walk_slope = jnp.arange(number_periods)\n    dynamic_trend_expected_value = jnp.array([1, 3, 7, 14, 25])\n\n    dynamic_trend_values = trend._dynamic_trend(\n        number_periods=number_periods,\n        random_walk_level=random_walk_level,\n        random_walk_slope=random_walk_slope,\n        initial_level=initial_level,\n        initial_slope=initial_slope,\n        variance_level=variance_level,\n        variance_slope=variance_slope,\n    )\n\n    np.testing.assert_array_equal(x=dynamic_trend_values,\n                                  y=dynamic_trend_expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_with_prediction_is_true\",\n          data_shape=(100, 3),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"geo_with_prediction_is_true\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"national_with_prediction_is_false\",\n          data_shape=(100, 3),\n          is_trend_prediction=False),\n      dict(\n          testcase_name=\"geo_with_prediction_is_false\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=False),\n  ])\n  def test_dynamic_trend_produces_correct_shape(\n      self, data_shape, is_trend_prediction):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "        variance_slope=variance_slope,\n    )\n\n    np.testing.assert_array_equal(x=dynamic_trend_values,\n                                  y=dynamic_trend_expected_value)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"national_with_prediction_is_true\",\n          data_shape=(100, 3),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"geo_with_prediction_is_true\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"national_with_prediction_is_false\",\n          data_shape=(100, 3),\n          is_trend_prediction=False),\n      dict(\n          testcase_name=\"geo_with_prediction_is_false\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=False),\n  ])\n  def test_dynamic_trend_produces_correct_shape(\n      self, data_shape, is_trend_prediction):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = core_utils.get_geo_shape(data)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_195-245", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          is_trend_prediction=True),\n      dict(\n          testcase_name=\"geo_with_prediction_is_true\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=True),\n      dict(\n          testcase_name=\"national_with_prediction_is_false\",\n          data_shape=(100, 3),\n          is_trend_prediction=False),\n      dict(\n          testcase_name=\"geo_with_prediction_is_false\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=False),\n  ])\n  def test_dynamic_trend_produces_correct_shape(\n      self, data_shape, is_trend_prediction):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = core_utils.get_geo_shape(data)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_LEVEL}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_SLOPE}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n      ),\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_205-255", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          testcase_name=\"geo_with_prediction_is_false\",\n          data_shape=(150, 3, 5),\n          is_trend_prediction=False),\n  ])\n  def test_dynamic_trend_produces_correct_shape(\n      self, data_shape, is_trend_prediction):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = core_utils.get_geo_shape(data)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_LEVEL}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_SLOPE}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_LEVEL_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_SLOPE_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n      ),\n  )\n  def test_core_dynamic_trend_custom_priors_are_taken_correctly(\n      self, prior_name):", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_215-265", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = core_utils.get_geo_shape(data)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_LEVEL}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_SLOPE}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_LEVEL_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_SLOPE_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n      ),\n  )\n  def test_core_dynamic_trend_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    geo_size = 1\n    data_size = 10\n    trace_handler = handlers.trace(\n        handlers.seed(trend.dynamic_trend, rng_seed=0))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_225-275", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n    coef_expected_shape = core_utils.get_geo_shape(data)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_values = mcmc.get_samples()[\"trend\"]\n\n    self.assertEqual(trend_values.shape,\n                     (num_samples, data.shape[0], *coef_expected_shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_LEVEL}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_SLOPE}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_LEVEL_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_SLOPE_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n      ),\n  )\n  def test_core_dynamic_trend_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    geo_size = 1\n    data_size = 10\n    trace_handler = handlers.trace(\n        handlers.seed(trend.dynamic_trend, rng_seed=0))\n    trace = trace_handler.get_trace(\n        geo_size=geo_size,\n        data_size=data_size,\n        is_trend_prediction=False,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_235-285", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "  @parameterized.named_parameters(\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_LEVEL}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_LEVEL,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_INITIAL_SLOPE}\",\n          prior_name=priors.DYNAMIC_TREND_INITIAL_SLOPE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_LEVEL_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_SLOPE_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n      ),\n  )\n  def test_core_dynamic_trend_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    geo_size = 1\n    data_size = 10\n    trace_handler = handlers.trace(\n        handlers.seed(trend.dynamic_trend, rng_seed=0))\n    trace = trace_handler.get_trace(\n        geo_size=geo_size,\n        data_size=data_size,\n        is_trend_prediction=False,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_245-295", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          testcase_name=f\"model_{priors.DYNAMIC_TREND_LEVEL_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_LEVEL_VARIANCE,\n      ),\n      dict(\n          testcase_name=f\"model_{priors.DYNAMIC_TREND_SLOPE_VARIANCE}\",\n          prior_name=priors.DYNAMIC_TREND_SLOPE_VARIANCE,\n      ),\n  )\n  def test_core_dynamic_trend_custom_priors_are_taken_correctly(\n      self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    geo_size = 1\n    data_size = 10\n    trace_handler = handlers.trace(\n        handlers.seed(trend.dynamic_trend, rng_seed=0))\n    trace = trace_handler.get_trace(\n        geo_size=geo_size,\n        data_size=data_size,\n        is_trend_prediction=False,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"trend_prediction_is_true\",\n          is_trend_prediction=True,\n          expected_trend_parameter=[\n              \"random_walk_level_prediction\", \"random_walk_slope_prediction\"]\n          ),\n      dict(\n          testcase_name=\"trend_prediction_is_false\",\n          is_trend_prediction=False,\n          expected_trend_parameter=[\n              \"random_walk_level\", \"random_walk_slope\"]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_255-305", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    geo_size = 1\n    data_size = 10\n    trace_handler = handlers.trace(\n        handlers.seed(trend.dynamic_trend, rng_seed=0))\n    trace = trace_handler.get_trace(\n        geo_size=geo_size,\n        data_size=data_size,\n        is_trend_prediction=False,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"trend_prediction_is_true\",\n          is_trend_prediction=True,\n          expected_trend_parameter=[\n              \"random_walk_level_prediction\", \"random_walk_slope_prediction\"]\n          ),\n      dict(\n          testcase_name=\"trend_prediction_is_false\",\n          is_trend_prediction=False,\n          expected_trend_parameter=[\n              \"random_walk_level\", \"random_walk_slope\"]\n          ),\n  ])\n  def test_dynamic_trend_is_trend_prediction_produuce_correct_parameter_names(\n      self, is_trend_prediction, expected_trend_parameter):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_265-315", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    trace = trace_handler.get_trace(\n        geo_size=geo_size,\n        data_size=data_size,\n        is_trend_prediction=False,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"trend_prediction_is_true\",\n          is_trend_prediction=True,\n          expected_trend_parameter=[\n              \"random_walk_level_prediction\", \"random_walk_slope_prediction\"]\n          ),\n      dict(\n          testcase_name=\"trend_prediction_is_false\",\n          is_trend_prediction=False,\n          expected_trend_parameter=[\n              \"random_walk_level\", \"random_walk_slope\"]\n          ),\n  ])\n  def test_dynamic_trend_is_trend_prediction_produuce_correct_parameter_names(\n      self, is_trend_prediction, expected_trend_parameter):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data_shape = (10, 3)\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_275-325", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "    used_distribution = values_and_dists[prior_name]\n    if isinstance(used_distribution, dist.ExpandedDistribution):\n      used_distribution = used_distribution.base_dist\n\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name=\"trend_prediction_is_true\",\n          is_trend_prediction=True,\n          expected_trend_parameter=[\n              \"random_walk_level_prediction\", \"random_walk_slope_prediction\"]\n          ),\n      dict(\n          testcase_name=\"trend_prediction_is_false\",\n          is_trend_prediction=False,\n          expected_trend_parameter=[\n              \"random_walk_level\", \"random_walk_slope\"]\n          ),\n  ])\n  def test_dynamic_trend_is_trend_prediction_produuce_correct_parameter_names(\n      self, is_trend_prediction, expected_trend_parameter):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data_shape = (10, 3)\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_parameter = [\n        parameter for parameter, _ in mcmc.get_samples().items()\n        if parameter.startswith(\"random_walk\")]\n\n    self.assertEqual(trend_parameter, expected_trend_parameter)\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_285-326", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          testcase_name=\"trend_prediction_is_true\",\n          is_trend_prediction=True,\n          expected_trend_parameter=[\n              \"random_walk_level_prediction\", \"random_walk_slope_prediction\"]\n          ),\n      dict(\n          testcase_name=\"trend_prediction_is_false\",\n          is_trend_prediction=False,\n          expected_trend_parameter=[\n              \"random_walk_level\", \"random_walk_slope\"]\n          ),\n  ])\n  def test_dynamic_trend_is_trend_prediction_produuce_correct_parameter_names(\n      self, is_trend_prediction, expected_trend_parameter):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data_shape = (10, 3)\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_parameter = [\n        parameter for parameter, _ in mcmc.get_samples().items()\n        if parameter.startswith(\"random_walk\")]\n\n    self.assertEqual(trend_parameter, expected_trend_parameter)\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 326, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py_295-326", "title": "google_lightweight_mmm-lightweight_mmm-core-time-trend_test.py", "text": "          ),\n  ])\n  def test_dynamic_trend_is_trend_prediction_produuce_correct_parameter_names(\n      self, is_trend_prediction, expected_trend_parameter):\n\n    def mock_model_function(geo_size, data_size):\n      numpyro.deterministic(\n          \"trend\", trend.dynamic_trend(\n              geo_size=geo_size,\n              data_size=data_size,\n              is_trend_prediction=is_trend_prediction,\n              custom_priors={},\n          ))\n    num_samples = 10\n    data_shape = (10, 3)\n    data = jnp.ones(data_shape)\n    geo_size = core_utils.get_number_geos(data)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, geo_size=geo_size, data_size=data_shape[0])\n    trend_parameter = [\n        parameter for parameter, _ in mcmc.get_samples().items()\n        if parameter.startswith(\"random_walk\")]\n\n    self.assertEqual(trend_parameter, expected_trend_parameter)\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "trend_test.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 326, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-time-__init__.py_0-15", "title": "google_lightweight_mmm-lightweight_mmm-core-time-__init__.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\nAST=Module", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 15, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "time", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 15, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-identity.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-identity.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for identity transformations.\"\"\"\n\nfrom typing import Any\nimport jax.numpy as jnp\n\n\ndef identity_transform(\n    data: jnp.ndarray,  # pylint-ignore: unused-argument\n    *args: Any,\n    **kwargs: Any,\n) -> jnp.ndarray:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "identity.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-identity.py_0-27", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-identity.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module for identity transformations.\"\"\"\n\nfrom typing import Any\nimport jax.numpy as jnp\n\n\ndef identity_transform(\n    data: jnp.ndarray,  # pylint-ignore: unused-argument\n    *args: Any,\n    **kwargs: Any,\n) -> jnp.ndarray:\n  \"\"\"Identity transform. Returns the main input as is.\"\"\"\n  return data\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)Return(Name(Load))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "identity.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 27, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}, {"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "identity.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 27, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling lagging functions.\"\"\"\n\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "\nimport functools\nfrom typing import Mapping, Union\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom lightweight_mmm.core import priors\n\n\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "\n@functools.partial(jax.vmap, in_axes=(1, 1, None), out_axes=1)\ndef _carryover_convolve(data: jnp.ndarray, weights: jnp.ndarray,\n                        number_lags: int) -> jnp.ndarray:\n  \"\"\"Applies the convolution between the data and the weights for the carryover.\n\n  Args:\n    data: Input data.\n    weights: Window weights for the carryover.\n    number_lags: Number of lags the window has.\n\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "\n  Returns:\n    The result values from convolving the data and the weights with padding.\n  \"\"\"\n  window = jnp.concatenate([jnp.zeros(number_lags - 1), weights])\n  return jax.scipy.signal.convolve(data, window, mode=\"same\") / weights.sum()\n\n\n@functools.partial(jax.jit, static_argnames=(\"number_lags\",))\ndef _carryover(\n    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "    data: jnp.ndarray,\n    ad_effect_retention_rate: jnp.ndarray,\n    peak_effect_delay: jnp.ndarray,\n    number_lags: int,\n) -> jnp.ndarray:\n  \"\"\"Calculates media carryover.\n\n  More details about this function can be found in:\n  https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf\n\n  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "  Args:\n    data: Input data. It is expected that data has either 2 dimensions for\n      national models and 3 for geo models.\n    ad_effect_retention_rate: Retention rate of the advertisement effect.\n      Default is 0.5.\n    peak_effect_delay: Delay of the peak effect in the carryover function.\n      Default is 1.\n    number_lags: Number of lags to include in the carryover calculation. Default\n      is 13.\n\n  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "  Returns:\n    The carryover values for the given data with the given parameters.\n  \"\"\"\n  lags_arange = jnp.expand_dims(\n      jnp.arange(number_lags, dtype=jnp.float32), axis=-1)\n  convolve_func = _carryover_convolve\n  if data.ndim == 3:\n    # Since _carryover_convolve is already vmaped in the decorator we only need\n    # to vmap it once here to handle the geo level data. We keep the windows bi\n    # dimensional also for three dims data and vmap over only the extra data\n    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "    # dimension.\n    convolve_func = jax.vmap(\n        fun=_carryover_convolve, in_axes=(2, None, None), out_axes=2)\n  weights = ad_effect_retention_rate**((lags_arange - peak_effect_delay)**2)\n  return convolve_func(data, weights, number_lags)\n\n\ndef carryover(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "    *,\n    number_lags: int = 13,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n\n@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "      default ones.\n    number_lags: Number of lags for the carryover function.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}_plate\",\n      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n\n@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "      size=data.shape[1]):\n    ad_effect_retention_rate = numpyro.sample(\n        name=f\"{prefix}{priors.AD_EFFECT_RETENTION_RATE}\",\n        fn=custom_priors.get(priors.AD_EFFECT_RETENTION_RATE,\n                             default_priors[priors.AD_EFFECT_RETENTION_RATE]))\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}_plate\", size=data.shape[1]):\n    peak_effect_delay = numpyro.sample(\n        name=f\"{prefix}{priors.PEAK_EFFECT_DELAY}\",\n        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n\n@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "        fn=custom_priors.get(priors.PEAK_EFFECT_DELAY,\n                             default_priors[priors.PEAK_EFFECT_DELAY]))\n\n  return _carryover(\n      data=data,\n      ad_effect_retention_rate=ad_effect_retention_rate,\n      peak_effect_delay=peak_effect_delay,\n      number_lags=number_lags)\n\n\n@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\ndef adstock(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "@jax.jit\ndef _adstock(\n    data: jnp.ndarray,\n    lag_weight: Union[float, jnp.ndarray] = .9,\n    normalise: bool = True,\n) -> jnp.ndarray:\n  \"\"\"Calculates the adstock value of a given array.\n\n  To learn more about advertising lag:\n  https://en.wikipedia.org/wiki/Advertising_adstock\n\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\ndef adstock(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    normalise: bool = True,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "\n  Args:\n    data: Input array.\n    lag_weight: lag_weight effect of the adstock function. Default is 0.9.\n    normalise: Whether to normalise the output value. This normalization will\n      divide the output values by (1 / (1 - lag_weight)).\n\n  Returns:\n    The adstock output of the input array.\n  \"\"\"\n\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\ndef adstock(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    normalise: bool = True,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "\n  def adstock_internal(\n      prev_adstock: jnp.ndarray,\n      data: jnp.ndarray,\n      lag_weight: Union[float, jnp.ndarray] = lag_weight,\n  ) -> jnp.ndarray:\n    adstock_value = prev_adstock * lag_weight + data\n    return adstock_value, adstock_value# jax-ndarray\n\n  _, adstock_values = jax.lax.scan(\n      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\ndef adstock(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    normalise: bool = True,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.LAG_WEIGHT}_plate\", size=data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=f\"{prefix}{priors.LAG_WEIGHT}\",\n        fn=custom_priors.get(priors.LAG_WEIGHT,\n                             default_priors[priors.LAG_WEIGHT]))\n\n  if data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_155-197", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "      f=adstock_internal, init=data[0, ...], xs=data[1:, ...])\n  adstock_values = jnp.concatenate([jnp.array([data[0, ...]]), adstock_values])\n  return jax.lax.cond(\n      normalise,\n      lambda adstock_values: adstock_values / (1. / (1 - lag_weight)),\n      lambda adstock_values: adstock_values,\n      operand=adstock_values)\n\n\ndef adstock(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    normalise: bool = True,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.LAG_WEIGHT}_plate\", size=data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=f\"{prefix}{priors.LAG_WEIGHT}\",\n        fn=custom_priors.get(priors.LAG_WEIGHT,\n                             default_priors[priors.LAG_WEIGHT]))\n\n  if data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n\n  return _adstock(data=data, lag_weight=lag_weight, normalise=normalise)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 197, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py_165-197", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging.py", "text": "    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    normalise: bool = True,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for adstock and exponent\n      are \"lag_weight\" and \"exponent\".\n    normalise: Whether to normalise the output values.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n  with numpyro.plate(\n      name=f\"{prefix}{priors.LAG_WEIGHT}_plate\", size=data.shape[1]):\n    lag_weight = numpyro.sample(\n        name=f\"{prefix}{priors.LAG_WEIGHT}\",\n        fn=custom_priors.get(priors.LAG_WEIGHT,\n                             default_priors[priors.LAG_WEIGHT]))\n\n  if data.ndim == 3:\n    lag_weight = jnp.expand_dims(lag_weight, axis=-1)\n\n  return _adstock(data=data, lag_weight=lag_weight, normalise=normalise)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 197, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lagging.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lagging.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lagging.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for lagging.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad))keyword(Constant))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "from lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import lagging\n\n\nclass LaggingTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          ad_effect_retention_rate_shape=(3,),\n          peak_effect_delay_shape=(3,),\n          number_lags=13,\n      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "      ),\n  )\n  def test_core_carryover_produces_correct_shape(\n      self,\n      data_shape,\n      ad_effect_retention_rate_shape,\n      peak_effect_delay_shape,\n      number_lags,\n  ):\n    data = jnp.ones(data_shape)\n    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    number_lags = 15\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, number_lags=number_lags)\n    carryover_values = mcmc.get_samples()[\"carryover\"]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    ad_effect_retention_rate = jnp.ones(ad_effect_retention_rate_shape)\n    peak_effect_delay = jnp.ones(peak_effect_delay_shape)\n\n    output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=number_lags,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    number_lags = 15\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, number_lags=number_lags)\n    carryover_values = mcmc.get_samples()[\"carryover\"]\n\n    self.assertEqual(carryover_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ad_effect_retention_rate\",\n          prior_name=priors.AD_EFFECT_RETENTION_RATE,\n      ),\n      dict(\n          testcase_name=\"peak_effect_delay\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    number_lags = 15\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, number_lags=number_lags)\n    carryover_values = mcmc.get_samples()[\"carryover\"]\n\n    self.assertEqual(carryover_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ad_effect_retention_rate\",\n          prior_name=priors.AD_EFFECT_RETENTION_RATE,\n      ),\n      dict(\n          testcase_name=\"peak_effect_delay\",\n          prior_name=priors.PEAK_EFFECT_DELAY,\n      ),\n  )\n  def test_carryover_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "      ),\n  )\n  def test_carryover_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, number_lags):\n      numpyro.deterministic(\n          \"carryover\",\n          lagging.carryover(\n              data=data, custom_priors={}, number_lags=number_lags))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    number_lags = 15\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, number_lags=number_lags)\n    carryover_values = mcmc.get_samples()[\"carryover\"]\n\n    self.assertEqual(carryover_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ad_effect_retention_rate\",\n          prior_name=priors.AD_EFFECT_RETENTION_RATE,\n      ),\n      dict(\n          testcase_name=\"peak_effect_delay\",\n          prior_name=priors.PEAK_EFFECT_DELAY,\n      ),\n  )\n  def test_carryover_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    number_lags = 13\n\n    trace_handler = handlers.trace(handlers.seed(lagging.carryover, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        number_lags=number_lags,\n    )\n    values_and_dists = {", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    num_samples = 10\n    data = jnp.ones(data_shape)\n    number_lags = 15\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, number_lags=number_lags)\n    carryover_values = mcmc.get_samples()[\"carryover\"]\n\n    self.assertEqual(carryover_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ad_effect_retention_rate\",\n          prior_name=priors.AD_EFFECT_RETENTION_RATE,\n      ),\n      dict(\n          testcase_name=\"peak_effect_delay\",\n          prior_name=priors.PEAK_EFFECT_DELAY,\n      ),\n  )\n  def test_carryover_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    number_lags = 13\n\n    trace_handler = handlers.trace(handlers.seed(lagging.carryover, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        number_lags=number_lags,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "\n    self.assertEqual(carryover_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"ad_effect_retention_rate\",\n          prior_name=priors.AD_EFFECT_RETENTION_RATE,\n      ),\n      dict(\n          testcase_name=\"peak_effect_delay\",\n          prior_name=priors.PEAK_EFFECT_DELAY,\n      ),\n  )\n  def test_carryover_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    number_lags = 13\n\n    trace_handler = handlers.trace(handlers.seed(lagging.carryover, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        number_lags=number_lags,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          lag_weight_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          lag_weight_shape=(3, 1),\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "          prior_name=priors.PEAK_EFFECT_DELAY,\n      ),\n  )\n  def test_carryover_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n    number_lags = 13\n\n    trace_handler = handlers.trace(handlers.seed(lagging.carryover, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        number_lags=number_lags,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          lag_weight_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          lag_weight_shape=(3, 1),\n      ),\n  )\n  def test_core_adstock_produces_correct_shape(self, data_shape,\n                                               lag_weight_shape):\n    data = jnp.ones(data_shape)\n    lag_weight = jnp.ones(lag_weight_shape)\n\n    output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    self.assertEqual(output.shape, data_shape)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    media = jnp.ones((10, 5, 5))\n    number_lags = 13\n\n    trace_handler = handlers.trace(handlers.seed(lagging.carryover, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n        number_lags=number_lags,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          lag_weight_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          lag_weight_shape=(3, 1),\n      ),\n  )\n  def test_core_adstock_produces_correct_shape(self, data_shape,\n                                               lag_weight_shape):\n    data = jnp.ones(data_shape)\n    lag_weight = jnp.ones(lag_weight_shape)\n\n    output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          lag_weight_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          lag_weight_shape=(3, 1),\n      ),\n  )\n  def test_core_adstock_produces_correct_shape(self, data_shape,\n                                               lag_weight_shape):\n    data = jnp.ones(data_shape)\n    lag_weight = jnp.ones(lag_weight_shape)\n\n    output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_adstock_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, normalise):\n      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    normalise = True", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          lag_weight_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          lag_weight_shape=(3, 1),\n      ),\n  )\n  def test_core_adstock_produces_correct_shape(self, data_shape,\n                                               lag_weight_shape):\n    data = jnp.ones(data_shape)\n    lag_weight = jnp.ones(lag_weight_shape)\n\n    output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_adstock_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, normalise):\n      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    normalise = True\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, normalise=normalise)\n    adstock_values = mcmc.get_samples()[\"adstock\"]\n\n    self.assertEqual(adstock_values.shape, (num_samples, *data.shape))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "  )\n  def test_core_adstock_produces_correct_shape(self, data_shape,\n                                               lag_weight_shape):\n    data = jnp.ones(data_shape)\n    lag_weight = jnp.ones(lag_weight_shape)\n\n    output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_adstock_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, normalise):\n      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    normalise = True\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, normalise=normalise)\n    adstock_values = mcmc.get_samples()[\"adstock\"]\n\n    self.assertEqual(adstock_values.shape, (num_samples, *data.shape))\n\n  def test_adstock_custom_priors_are_taken_correctly(self):\n    prior_name = priors.LAG_WEIGHT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    data = jnp.ones((10, 5, 5))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_adstock_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, normalise):\n      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    normalise = True\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, normalise=normalise)\n    adstock_values = mcmc.get_samples()[\"adstock\"]\n\n    self.assertEqual(adstock_values.shape, (num_samples, *data.shape))\n\n  def test_adstock_custom_priors_are_taken_correctly(self):\n    prior_name = priors.LAG_WEIGHT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    data = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(lagging.adstock, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=data,\n        custom_priors=custom_priors,\n        normalise=True,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_165-215", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "  def test_adstock_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data, normalise):\n      numpyro.deterministic(\n          \"adstock\",\n          lagging.adstock(data=data, custom_priors={}, normalise=normalise))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    normalise = True\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, normalise=normalise)\n    adstock_values = mcmc.get_samples()[\"adstock\"]\n\n    self.assertEqual(adstock_values.shape, (num_samples, *data.shape))\n\n  def test_adstock_custom_priors_are_taken_correctly(self):\n    prior_name = priors.LAG_WEIGHT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    data = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(lagging.adstock, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=data,\n        custom_priors=custom_priors,\n        normalise=True,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_175-225", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data, normalise=normalise)\n    adstock_values = mcmc.get_samples()[\"adstock\"]\n\n    self.assertEqual(adstock_values.shape, (num_samples, *data.shape))\n\n  def test_adstock_custom_priors_are_taken_correctly(self):\n    prior_name = priors.LAG_WEIGHT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    data = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(lagging.adstock, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=data,\n        custom_priors=custom_priors,\n        normalise=True,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = lagging._carryover(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_185-235", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "  def test_adstock_custom_priors_are_taken_correctly(self):\n    prior_name = priors.LAG_WEIGHT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    data = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(lagging.adstock, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=data,\n        custom_priors=custom_priors,\n        normalise=True,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=7,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_195-236", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    trace_handler = handlers.trace(handlers.seed(lagging.adstock, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=data,\n        custom_priors=custom_priors,\n        normalise=True,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=7,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 236, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py_205-236", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-lagging_test.py", "text": "    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_adstock_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    lag_weight = jnp.full(5, 0.5)\n\n    generated_output = lagging._adstock(data=data, lag_weight=lag_weight)\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_carryover_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    ad_effect_retention_rate = jnp.full(5, 0.5)\n    peak_effect_delay = jnp.full(5, 0.5)\n\n    generated_output = lagging._carryover(\n        data=data,\n        ad_effect_retention_rate=ad_effect_retention_rate,\n        peak_effect_delay=peak_effect_delay,\n        number_lags=7,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "lagging_test.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 236, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n\nAST=Module(Expr(Constant)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Attribute(Name(Load)Load)Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Set of core and modelling saturation functions.\"\"\"\n\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "\nfrom typing import Mapping\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nfrom numpyro import distributions as dist\n\nfrom lightweight_mmm.core import core_utils\nfrom lightweight_mmm.core import priors\n\n\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "\n@jax.jit\ndef _hill(\n    data: jnp.ndarray,\n    half_max_effective_concentration: jnp.ndarray,\n    slope: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"Calculates the hill function for a given array of values.\n\n  Refer to the following link for detailed information on this equation:\n    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "    https://en.wikipedia.org/wiki/Hill_equation_(biochemistry)\n\n  Args:\n    data: Input data.\n    half_max_effective_concentration: ec50 value for the hill function.\n    slope: Slope of the hill function.\n\n  Returns:\n    The hill values for the respective input data.\n  \"\"\"\n  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "  save_transform = core_utils.apply_exponent_safe(\n      data=data / half_max_effective_concentration, exponent=-slope)\n  return jnp.where(save_transform == 0, x=0, y=1. / (1 + save_transform))\n\n\ndef hill(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": ") -> jnp.ndarray:\n  \"\"\"Transforms the input data with the adstock and hill functions.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones. The possible names of parameters for hill_adstock and\n      exponent are \"lag_weight\", \"half_max_effective_concentration\" and \"slope\".\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef _exponent(data: jnp.ndarray, exponent_values: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies exponent to the given data.\"\"\"\n  return core_utils.apply_exponent_safe(data=data, exponent=exponent_values)\n\n\ndef exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}_plate\",\n      size=data.shape[1]):\n    half_max_effective_concentration = numpyro.sample(\n        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef _exponent(data: jnp.ndarray, exponent_values: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies exponent to the given data.\"\"\"\n  return core_utils.apply_exponent_safe(data=data, exponent=exponent_values)\n\n\ndef exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    prefix: Prefix to use in the variable name for Numpyro.", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "        name=f\"{prefix}{priors.HALF_MAX_EFFECTIVE_CONCENTRATION}\",\n        fn=custom_priors.get(\n            priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n            default_priors[priors.HALF_MAX_EFFECTIVE_CONCENTRATION]))\n\n  with numpyro.plate(name=f\"{prefix}{priors.SLOPE}_plate\", size=data.shape[1]):\n    slope = numpyro.sample(\n        name=f\"{prefix}{priors.SLOPE}\",\n        fn=custom_priors.get(priors.SLOPE, default_priors[priors.SLOPE]))\n\n  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef _exponent(data: jnp.ndarray, exponent_values: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies exponent to the given data.\"\"\"\n  return core_utils.apply_exponent_safe(data=data, exponent=exponent_values)\n\n\ndef exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.EXPONENT}_plate\", size=data.shape[1]):\n    exponent_values = numpyro.sample(\n        name=f\"{prefix}{priors.EXPONENT}\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_85-130", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "  if data.ndim == 3:\n    half_max_effective_concentration = jnp.expand_dims(\n        half_max_effective_concentration, axis=-1)\n    slope = jnp.expand_dims(slope, axis=-1)\n\n  return _hill(\n      data=data,\n      half_max_effective_concentration=half_max_effective_concentration,\n      slope=slope)\n\n\ndef _exponent(data: jnp.ndarray, exponent_values: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies exponent to the given data.\"\"\"\n  return core_utils.apply_exponent_safe(data=data, exponent=exponent_values)\n\n\ndef exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.EXPONENT}_plate\", size=data.shape[1]):\n    exponent_values = numpyro.sample(\n        name=f\"{prefix}{priors.EXPONENT}\",\n        fn=custom_priors.get(priors.EXPONENT, default_priors[priors.EXPONENT]))\n\n  if data.ndim == 3:\n    exponent_values = jnp.expand_dims(exponent_values, axis=-1)\n  return _exponent(data=data, exponent_values=exponent_values)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 130, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py_95-130", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation.py", "text": "\ndef _exponent(data: jnp.ndarray, exponent_values: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Applies exponent to the given data.\"\"\"\n  return core_utils.apply_exponent_safe(data=data, exponent=exponent_values)\n\n\ndef exponent(\n    data: jnp.ndarray,\n    custom_priors: Mapping[str, dist.Distribution],\n    *,\n    prefix: str = \"\",\n) -> jnp.ndarray:\n  \"\"\"Transforms the input data with the carryover function and exponent.\n\n  Args:\n    data: Media data to be transformed. It is expected to have 2 dims for\n      national models and 3 for geo models.\n    custom_priors: The custom priors we want the model to take instead of the\n      default ones.\n    prefix: Prefix to use in the variable name for Numpyro.\n\n  Returns:\n    The transformed media data.\n  \"\"\"\n  default_priors = priors.get_default_priors()\n\n  with numpyro.plate(\n      name=f\"{prefix}{priors.EXPONENT}_plate\", size=data.shape[1]):\n    exponent_values = numpyro.sample(\n        name=f\"{prefix}{priors.EXPONENT}\",\n        fn=custom_priors.get(priors.EXPONENT, default_priors[priors.EXPONENT]))\n\n  if data.ndim == 3:\n    exponent_values = jnp.expand_dims(exponent_values, axis=-1)\n  return _exponent(data=data, exponent_values=exponent_values)\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Return(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Attribute(Name(Load)Load)Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))With(withitem(Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))Constant))keyword(Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(JoinedStr(FormattedValue(Name(Load))FormattedValue(Attribute(Name(Load)Load))))keyword(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load))))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))))Attribute(Name(Load)Load)))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 130, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_0-25", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for saturation.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\n\nAST=Module(Expr(Constant)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_0-35", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for saturation.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import saturation\n\n\nclass SaturationTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_0-45", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for saturation.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import saturation\n\n\nclass SaturationTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          half_max_effective_concentration_shape=(3,),\n          slope_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          half_max_effective_concentration_shape=(3, 1),\n          slope_shape=(3, 1),\n      ),\n  )", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_5-55", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for saturation.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import saturation\n\n\nclass SaturationTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          half_max_effective_concentration_shape=(3,),\n          slope_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          half_max_effective_concentration_shape=(3, 1),\n          slope_shape=(3, 1),\n      ),\n  )\n  def test_hill_core_produces_correct_shape(\n      self, data_shape, half_max_effective_concentration_shape, slope_shape):\n    data = jnp.ones(data_shape)\n    half_max_effective_concentration = jnp.ones(\n        half_max_effective_concentration_shape)\n    slope = jnp.ones(slope_shape)\n\n    output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_15-65", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\n\nfrom lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import saturation\n\n\nclass SaturationTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          half_max_effective_concentration_shape=(3,),\n          slope_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          half_max_effective_concentration_shape=(3, 1),\n          slope_shape=(3, 1),\n      ),\n  )\n  def test_hill_core_produces_correct_shape(\n      self, data_shape, half_max_effective_concentration_shape, slope_shape):\n    data = jnp.ones(data_shape)\n    half_max_effective_concentration = jnp.ones(\n        half_max_effective_concentration_shape)\n    slope = jnp.ones(slope_shape)\n\n    output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_25-75", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "from lightweight_mmm.core import priors\nfrom lightweight_mmm.core.transformations import saturation\n\n\nclass SaturationTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n          half_max_effective_concentration_shape=(3,),\n          slope_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          half_max_effective_concentration_shape=(3, 1),\n          slope_shape=(3, 1),\n      ),\n  )\n  def test_hill_core_produces_correct_shape(\n      self, data_shape, half_max_effective_concentration_shape, slope_shape):\n    data = jnp.ones(data_shape)\n    half_max_effective_concentration = jnp.ones(\n        half_max_effective_concentration_shape)\n    slope = jnp.ones(slope_shape)\n\n    output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_hill_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"hill\",\n                            saturation.hill(data=data, custom_priors={}))\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantLoad))keyword(Tuple(ConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Tuple(ConstantConstantLoad))keyword(Tuple(ConstantConstantLoad)))))FunctionDef(arguments(argarg)FunctionDef(arguments(arg)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Dict)))))Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantLoad)))Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad)))))))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_35-85", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "          half_max_effective_concentration_shape=(3,),\n          slope_shape=(3,),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n          half_max_effective_concentration_shape=(3, 1),\n          slope_shape=(3, 1),\n      ),\n  )\n  def test_hill_core_produces_correct_shape(\n      self, data_shape, half_max_effective_concentration_shape, slope_shape):\n    data = jnp.ones(data_shape)\n    half_max_effective_concentration = jnp.ones(\n        half_max_effective_concentration_shape)\n    slope = jnp.ones(slope_shape)\n\n    output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_hill_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"hill\",\n                            saturation.hill(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"hill\"]", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_45-95", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "  def test_hill_core_produces_correct_shape(\n      self, data_shape, half_max_effective_concentration_shape, slope_shape):\n    data = jnp.ones(data_shape)\n    half_max_effective_concentration = jnp.ones(\n        half_max_effective_concentration_shape)\n    slope = jnp.ones(slope_shape)\n\n    output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_hill_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"hill\",\n                            saturation.hill(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"hill\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"half_max_effective_concentration\",\n          prior_name=priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n      ),\n      dict(\n          testcase_name=\"slope\",", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_55-105", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "        slope=slope,\n    )\n\n    self.assertEqual(output.shape, data_shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_hill_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"hill\",\n                            saturation.hill(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"hill\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"half_max_effective_concentration\",\n          prior_name=priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n      ),\n      dict(\n          testcase_name=\"slope\",\n          prior_name=priors.SLOPE,\n      ),\n  )\n  def test_hill_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_65-115", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_hill_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"hill\",\n                            saturation.hill(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"hill\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"half_max_effective_concentration\",\n          prior_name=priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n      ),\n      dict(\n          testcase_name=\"slope\",\n          prior_name=priors.SLOPE,\n      ),\n  )\n  def test_hill_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(saturation.hill, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_75-125", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"hill\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"half_max_effective_concentration\",\n          prior_name=priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n      ),\n      dict(\n          testcase_name=\"slope\",\n          prior_name=priors.SLOPE,\n      ),\n  )\n  def test_hill_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(saturation.hill, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_exponent_core_produces_correct_shape(self):\n    pass\n", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_85-135", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"half_max_effective_concentration\",\n          prior_name=priors.HALF_MAX_EFFECTIVE_CONCENTRATION,\n      ),\n      dict(\n          testcase_name=\"slope\",\n          prior_name=priors.SLOPE,\n      ),\n  )\n  def test_hill_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(saturation.hill, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_exponent_core_produces_correct_shape(self):\n    pass\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_95-145", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "          prior_name=priors.SLOPE,\n      ),\n  )\n  def test_hill_custom_priors_are_taken_correctly(self, prior_name):\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(saturation.hill, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_exponent_core_produces_correct_shape(self):\n    pass\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"outer_exponent\",\n                            saturation.exponent(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_105-155", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(handlers.seed(saturation.hill, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_exponent_core_produces_correct_shape(self):\n    pass\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"outer_exponent\",\n                            saturation.exponent(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"outer_exponent\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  def test_exponent_custom_priors_are_taken_correctly(self):\n    prior_name = priors.EXPONENT", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_115-165", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_exponent_core_produces_correct_shape(self):\n    pass\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"outer_exponent\",\n                            saturation.exponent(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"outer_exponent\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  def test_exponent_custom_priors_are_taken_correctly(self):\n    prior_name = priors.EXPONENT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(saturation.exponent, rng_seed=0))", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_125-175", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"national\",\n          data_shape=(150, 3),\n      ),\n      dict(\n          testcase_name=\"geo\",\n          data_shape=(150, 3, 5),\n      ),\n  )\n  def test_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"outer_exponent\",\n                            saturation.exponent(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"outer_exponent\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  def test_exponent_custom_priors_are_taken_correctly(self):\n    prior_name = priors.EXPONENT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(saturation.exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_135-185", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "  def test_exponent_produces_correct_shape(self, data_shape):\n\n    def mock_model_function(data):\n      numpyro.deterministic(\"outer_exponent\",\n                            saturation.exponent(data=data, custom_priors={}))\n\n    num_samples = 10\n    data = jnp.ones(data_shape)\n    kernel = numpyro.infer.NUTS(model=mock_model_function)\n    mcmc = numpyro.infer.MCMC(\n        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"outer_exponent\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  def test_exponent_custom_priors_are_taken_correctly(self):\n    prior_name = priors.EXPONENT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(saturation.exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = saturation._hill(", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_145-195", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "        sampler=kernel, num_warmup=10, num_samples=num_samples, num_chains=1)\n    rng_key = jax.random.PRNGKey(0)\n\n    mcmc.run(rng_key, data=data)\n    output_values = mcmc.get_samples()[\"outer_exponent\"]\n\n    self.assertEqual(output_values.shape, (num_samples, *data.shape))\n\n  def test_exponent_custom_priors_are_taken_correctly(self):\n    prior_name = priors.EXPONENT\n    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(saturation.exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_exponent_zeros_stay_zero(self):\n    data = jnp.zeros((10, 5))\n    exponent_values = jnp.full(5, 0.5)", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_155-205", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "    expected_value1, expected_value2 = 5.2, 7.56\n    custom_priors = {\n        prior_name:\n            dist.Kumaraswamy(\n                concentration1=expected_value1, concentration0=expected_value2)\n    }\n    media = jnp.ones((10, 5, 5))\n\n    trace_handler = handlers.trace(\n        handlers.seed(saturation.exponent, rng_seed=0))\n    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_exponent_zeros_stay_zero(self):\n    data = jnp.zeros((10, 5))\n    exponent_values = jnp.full(5, 0.5)\n\n    generated_output = saturation._exponent(\n        data=data,\n        exponent_values=exponent_values,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\nif __name__ == \"__main__\":", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_165-206", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "    trace = trace_handler.get_trace(\n        data=media,\n        custom_priors=custom_priors,\n    )\n    values_and_dists = {\n        name: site[\"fn\"] for name, site in trace.items() if \"fn\" in site\n    }\n\n    used_distribution = values_and_dists[prior_name]\n    used_distribution = used_distribution.base_dist\n    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_exponent_zeros_stay_zero(self):\n    data = jnp.zeros((10, 5))\n    exponent_values = jnp.full(5, 0.5)\n\n    generated_output = saturation._exponent(\n        data=data,\n        exponent_values=exponent_values,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 206, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
{"_id": "google_lightweight_mmm_google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py_175-206", "title": "google_lightweight_mmm-lightweight_mmm-core-transformations-saturation_test.py", "text": "    self.assertIsInstance(used_distribution, dist.Kumaraswamy)\n    self.assertEqual(used_distribution.concentration0, expected_value2)\n    self.assertEqual(used_distribution.concentration1, expected_value1)\n\n  def test_hill_zeros_stay_zeros(self):\n    data = jnp.zeros((10, 5))\n    half_max_effective_concentration = jnp.full(5, 0.5)\n    slope = jnp.full(5, 0.5)\n\n    generated_output = saturation._hill(\n        data=data,\n        half_max_effective_concentration=half_max_effective_concentration,\n        slope=slope,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n  def test_exponent_zeros_stay_zero(self):\n    data = jnp.zeros((10, 5))\n    exponent_values = jnp.full(5, 0.5)\n\n    generated_output = saturation._exponent(\n        data=data,\n        exponent_values=exponent_values,\n    )\n\n    np.testing.assert_array_equal(x=generated_output, y=data)\n\n\nif __name__ == \"__main__\":\n  absltest.main()", "metadata": [{"fpath_tuple": ["google_lightweight_mmm", "lightweight_mmm", "core", "transformations", "saturation_test.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 206, "window_size": 50, "repo": "google_lightweight_mmm", "slice_size": 5}]}
