{"_id": "lucidrains_imagen-pytorch/0", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/0", "ground_truth": "    if len(arr) == 0:\n        return d\n    return arr[0]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "first", "line_no": 41}}
{"_id": "lucidrains_imagen-pytorch/1", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)", "metadata": {"task_id": "lucidrains_imagen-pytorch/1", "ground_truth": "    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "maybe", "line_no": 47}}
{"_id": "lucidrains_imagen-pytorch/2", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):", "metadata": {"task_id": "lucidrains_imagen-pytorch/2", "ground_truth": "        if not exists(x):\n            return x\n        return fn(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "inner", "line_no": 48}}
{"_id": "lucidrains_imagen-pytorch/3", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):", "metadata": {"task_id": "lucidrains_imagen-pytorch/3", "ground_truth": "    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 54, "function_name": "once", "line_no": 54}}
{"_id": "lucidrains_imagen-pytorch/4", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):", "metadata": {"task_id": "lucidrains_imagen-pytorch/4", "ground_truth": "        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 57, "function_name": "inner", "line_no": 57}}
{"_id": "lucidrains_imagen-pytorch/5", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):", "metadata": {"task_id": "lucidrains_imagen-pytorch/5", "ground_truth": "    if exists(val):\n        return val\n    return d() if callable(d) else d\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "default", "line_no": 67}}
{"_id": "lucidrains_imagen-pytorch/6", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/6", "ground_truth": "    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 72, "function_name": "cast_tuple", "line_no": 72}}
{"_id": "lucidrains_imagen-pytorch/7", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):", "metadata": {"task_id": "lucidrains_imagen-pytorch/7", "ground_truth": "    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "cast_uint8_images_to_float", "line_no": 94}}
{"_id": "lucidrains_imagen-pytorch/8", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):", "metadata": {"task_id": "lucidrains_imagen-pytorch/8", "ground_truth": "    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 102, "function_name": "zero_init_", "line_no": 102}}
{"_id": "lucidrains_imagen-pytorch/9", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/9", "ground_truth": "    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "pad_tuple_to_length", "line_no": 116}}
{"_id": "lucidrains_imagen-pytorch/10", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):", "metadata": {"task_id": "lucidrains_imagen-pytorch/10", "ground_truth": "    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 139, "function_name": "right_pad_dims_to", "line_no": 139}}
{"_id": "lucidrains_imagen-pytorch/11", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/11", "ground_truth": "    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 145, "function_name": "masked_mean", "line_no": 145}}
{"_id": "lucidrains_imagen-pytorch/12", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/12", "ground_truth": "    if len(tup) <= index:\n        return default\n    return tup[index]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 188, "function_name": "safe_get_tuple_index", "line_no": 188}}
{"_id": "lucidrains_imagen-pytorch/13", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):", "metadata": {"task_id": "lucidrains_imagen-pytorch/13", "ground_truth": "    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 204, "function_name": "prob_mask_like", "line_no": 204}}
{"_id": "lucidrains_imagen-pytorch/14", "text": " copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):", "metadata": {"task_id": "lucidrains_imagen-pytorch/14", "ground_truth": "        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1, "lineno": 227, "function_name": "__init__", "line_no": 227}}
{"_id": "lucidrains_imagen-pytorch/15", "text": "\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/15", "ground_truth": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 94, "lineno": 275, "function_name": "q_sample", "line_no": 275}}
{"_id": "lucidrains_imagen-pytorch/16", "text": "[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):", "metadata": {"task_id": "lucidrains_imagen-pytorch/16", "ground_truth": "        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 190, "lineno": 326, "function_name": "__init__", "line_no": 326}}
{"_id": "lucidrains_imagen-pytorch/17", "text": "to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):", "metadata": {"task_id": "lucidrains_imagen-pytorch/17", "ground_truth": "        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 198, "lineno": 333, "function_name": "forward", "line_no": 333}}
{"_id": "lucidrains_imagen-pytorch/18", "text": ", noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/18", "ground_truth": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 226, "lineno": 381, "function_name": "__init__", "line_no": 381}}
{"_id": "lucidrains_imagen-pytorch/19", "text": "batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/19", "ground_truth": "        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 279, "lineno": 453, "function_name": "__init__", "line_no": 453}}
{"_id": "lucidrains_imagen-pytorch/20", "text": " = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/20", "ground_truth": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 296, "lineno": 475, "function_name": "forward", "line_no": 475}}
{"_id": "lucidrains_imagen-pytorch/21", "text": "snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/21", "ground_truth": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 313, "lineno": 505, "function_name": "__init__", "line_no": 505}}
{"_id": "lucidrains_imagen-pytorch/22", "text": "\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/22", "ground_truth": "        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 415, "lineno": 602, "function_name": "__init__", "line_no": 602}}
{"_id": "lucidrains_imagen-pytorch/23", "text": "\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):", "metadata": {"task_id": "lucidrains_imagen-pytorch/23", "ground_truth": "        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 426, "lineno": 615, "function_name": "init_conv_", "line_no": 615}}
{"_id": "lucidrains_imagen-pytorch/24", "text": "init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle", "metadata": {"task_id": "lucidrains_imagen-pytorch/24", "ground_truth": "    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 441, "lineno": 629, "function_name": "Downsample", "line_no": 629}}
{"_id": "lucidrains_imagen-pytorch/25", "text": " FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):", "metadata": {"task_id": "lucidrains_imagen-pytorch/25", "ground_truth": "        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 471, "lineno": 652, "function_name": "__init__", "line_no": 652}}
{"_id": "lucidrains_imagen-pytorch/26", "text": "torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):", "metadata": {"task_id": "lucidrains_imagen-pytorch/26", "ground_truth": "        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 476, "lineno": 658, "function_name": "forward", "line_no": 658}}
{"_id": "lucidrains_imagen-pytorch/27", "text": "pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/27", "ground_truth": "        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 484, "lineno": 672, "function_name": "__init__", "line_no": 672}}
{"_id": "lucidrains_imagen-pytorch/28", "text": " latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/28", "ground_truth": "        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 489, "lineno": 678, "function_name": "forward", "line_no": 678}}
{"_id": "lucidrains_imagen-pytorch/29", "text": " bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/29", "ground_truth": "        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 514, "lineno": 701, "function_name": "__init__", "line_no": 701}}
{"_id": "lucidrains_imagen-pytorch/30", "text": " / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/30", "ground_truth": "        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 536, "lineno": 732, "function_name": "forward", "line_no": 732}}
{"_id": "lucidrains_imagen-pytorch/31", "text": " exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/31", "ground_truth": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 569, "lineno": 765, "function_name": "__init__", "line_no": 765}}
{"_id": "lucidrains_imagen-pytorch/32", "text": " inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/32", "ground_truth": "        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 778, "lineno": 949, "function_name": "__init__", "line_no": 949}}
{"_id": "lucidrains_imagen-pytorch/33", "text": "device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):", "metadata": {"task_id": "lucidrains_imagen-pytorch/33", "ground_truth": "        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 789, "lineno": 961, "function_name": "forward", "line_no": 961}}
{"_id": "lucidrains_imagen-pytorch/34", "text": " value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):", "metadata": {"task_id": "lucidrains_imagen-pytorch/34", "ground_truth": "    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 798, "lineno": 968, "function_name": "FeedForward", "line_no": 968}}
{"_id": "lucidrains_imagen-pytorch/35", "text": "float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/35", "ground_truth": "        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 824, "lineno": 998, "function_name": "__init__", "line_no": 998}}
{"_id": "lucidrains_imagen-pytorch/36", "text": "):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/36", "ground_truth": "        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 832, "lineno": 1008, "function_name": "forward", "line_no": 1008}}
{"_id": "lucidrains_imagen-pytorch/37", "text": " einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/37", "ground_truth": "        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 867, "lineno": 1054, "function_name": "__init__", "line_no": 1054}}
{"_id": "lucidrains_imagen-pytorch/38", "text": "),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/38", "ground_truth": "        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 897, "lineno": 1082, "function_name": "__init__", "line_no": 1082}}
{"_id": "lucidrains_imagen-pytorch/39", "text": " bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/39", "ground_truth": "        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 907, "lineno": 1096, "function_name": "forward", "line_no": 1096}}
{"_id": "lucidrains_imagen-pytorch/40", "text": " = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/40", "ground_truth": "        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1309, "lineno": 1452, "function_name": "cast_model_parameters", "line_no": 1452}}
{"_id": "lucidrains_imagen-pytorch/41", "text": "        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):", "metadata": {"task_id": "lucidrains_imagen-pytorch/41", "ground_truth": "        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1815, "lineno": 1985, "function_name": "get_unet", "line_no": 1985}}
{"_id": "lucidrains_imagen-pytorch/42", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):", "metadata": {"task_id": "lucidrains_imagen-pytorch/42", "ground_truth": "        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "check_image_sizes", "line_no": 80}}
{"_id": "lucidrains_imagen-pytorch/43", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):", "metadata": {"task_id": "lucidrains_imagen-pytorch/43", "ground_truth": "        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "create", "line_no": 86}}
{"_id": "lucidrains_imagen-pytorch/44", "text": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):", "metadata": {"task_id": "lucidrains_imagen-pytorch/44", "ground_truth": "    while True:\n        for data in dl:\n            yield data\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "cycle", "line_no": 25}}
{"_id": "lucidrains_imagen-pytorch/45", "text": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):", "metadata": {"task_id": "lucidrains_imagen-pytorch/45", "ground_truth": "    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "once", "line_no": 47}}
{"_id": "lucidrains_imagen-pytorch/46", "text": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):", "metadata": {"task_id": "lucidrains_imagen-pytorch/46", "ground_truth": "    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "get_encoded_dim", "line_no": 47}}
{"_id": "lucidrains_imagen-pytorch/47", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):", "metadata": {"task_id": "lucidrains_imagen-pytorch/47", "ground_truth": "    if exists(val):\n        return val\n    return d() if callable(d) else d\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 42, "function_name": "default", "line_no": 42}}
{"_id": "lucidrains_imagen-pytorch/48", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):", "metadata": {"task_id": "lucidrains_imagen-pytorch/48", "ground_truth": "    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "cast_tuple", "line_no": 47}}
{"_id": "lucidrains_imagen-pytorch/49", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):", "metadata": {"task_id": "lucidrains_imagen-pytorch/49", "ground_truth": "    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 63, "function_name": "group_dict_by_key", "line_no": 63}}
{"_id": "lucidrains_imagen-pytorch/50", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):", "metadata": {"task_id": "lucidrains_imagen-pytorch/50", "ground_truth": "    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "groupby_prefix_and_trim", "line_no": 77}}
{"_id": "lucidrains_imagen-pytorch/51", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):", "metadata": {"task_id": "lucidrains_imagen-pytorch/51", "ground_truth": "    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 82, "function_name": "num_to_groups", "line_no": 82}}
{"_id": "lucidrains_imagen-pytorch/52", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):", "metadata": {"task_id": "lucidrains_imagen-pytorch/52", "ground_truth": "        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "inner", "line_no": 116}}
{"_id": "lucidrains_imagen-pytorch/53", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):", "metadata": {"task_id": "lucidrains_imagen-pytorch/53", "ground_truth": "    for el in arr:\n        if cond(el):\n            return el\n    return None\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 161, "function_name": "find_first", "line_no": 161}}
{"_id": "lucidrains_imagen-pytorch/54", "text": "\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):", "metadata": {"task_id": "lucidrains_imagen-pytorch/54", "ground_truth": "        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 226, "lineno": 417, "function_name": "prepare", "line_no": 417}}
{"_id": "lucidrains_imagen-pytorch/55", "text": "et (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/55", "ground_truth": "        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 260, "lineno": 455, "function_name": "validate_and_set_unet_being_trained", "line_no": 455}}
{"_id": "lucidrains_imagen-pytorch/56", "text": "and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):", "metadata": {"task_id": "lucidrains_imagen-pytorch/56", "ground_truth": "        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 272, "lineno": 469, "function_name": "wrap_unet", "line_no": 469}}
{"_id": "lucidrains_imagen-pytorch/57", "text": "aler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):", "metadata": {"task_id": "lucidrains_imagen-pytorch/57", "ground_truth": "        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 299, "lineno": 494, "function_name": "set_accelerator_scaler", "line_no": 494}}
{"_id": "lucidrains_imagen-pytorch/58", "text": " None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/58", "ground_truth": "        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 317, "lineno": 515, "function_name": "validate_unet_number", "line_no": 515}}
{"_id": "lucidrains_imagen-pytorch/59", "text": " fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/59", "ground_truth": "        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 328, "lineno": 524, "function_name": "num_steps_taken", "line_no": 524}}
{"_id": "lucidrains_imagen-pytorch/60", "text": "r = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/60", "ground_truth": "        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 342, "lineno": 545, "function_name": "add_train_dataloader", "line_no": 545}}
{"_id": "lucidrains_imagen-pytorch/61", "text": "alingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):", "metadata": {"task_id": "lucidrains_imagen-pytorch/61", "ground_truth": "        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 363, "lineno": 561, "function_name": "add_train_dataset", "line_no": 561}}
{"_id": "lucidrains_imagen-pytorch/62", "text": "can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):", "metadata": {"task_id": "lucidrains_imagen-pytorch/62", "ground_truth": "        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 400, "lineno": 592, "function_name": "create_train_iter", "line_no": 592}}
{"_id": "lucidrains_imagen-pytorch/63", "text": " assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):", "metadata": {"task_id": "lucidrains_imagen-pytorch/63", "ground_truth": "        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 417, "lineno": 608, "function_name": "train_step", "line_no": 608}}
{"_id": "lucidrains_imagen-pytorch/64", "text": " # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):", "metadata": {"task_id": "lucidrains_imagen-pytorch/64", "ground_truth": "        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 442, "lineno": 627, "function_name": "step_with_dl_iter", "line_no": 627}}
{"_id": "lucidrains_imagen-pytorch/65", "text": "uple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):", "metadata": {"task_id": "lucidrains_imagen-pytorch/65", "ground_truth": "        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 628, "lineno": 819, "function_name": "get_ema_unet", "line_no": 819}}
{"_id": "lucidrains_imagen-pytorch/66", "text": "uler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):\n            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')\n\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:\n            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()\n\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return\n\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()\n\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):", "metadata": {"task_id": "lucidrains_imagen-pytorch/66", "ground_truth": "        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n\n            total_loss += loss.item()\n\n            if self.training:\n                self.accelerator.backward(loss)\n\n        return total_loss\n", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 779, "lineno": 972, "function_name": "forward", "line_no": 972}}
