{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):", "reference": "    if len(arr) == 0:\n        return d\n    return arr[0]\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/0", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 41, "function_name": "first", "line_no": 41}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)", "reference": "    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/1", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "maybe", "line_no": 47}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):", "reference": "        if not exists(x):\n            return x\n        return fn(x)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/2", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 48, "function_name": "inner", "line_no": 48}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):", "reference": "    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/3", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 54, "function_name": "once", "line_no": 54}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):", "reference": "        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/4", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 57, "function_name": "inner", "line_no": 57}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):", "reference": "    if exists(val):\n        return val\n    return d() if callable(d) else d\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/5", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 67, "function_name": "default", "line_no": 67}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):", "reference": "    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/6", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 72, "function_name": "cast_tuple", "line_no": 72}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):", "reference": "    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/7", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 94, "function_name": "cast_uint8_images_to_float", "line_no": 94}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):", "reference": "    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/8", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 102, "function_name": "zero_init_", "line_no": 102}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):", "reference": "    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/9", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "pad_tuple_to_length", "line_no": 116}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):", "reference": "    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/10", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 139, "function_name": "right_pad_dims_to", "line_no": 139}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):", "reference": "    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/11", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 145, "function_name": "masked_mean", "line_no": 145}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):", "reference": "    if len(tup) <= index:\n        return default\n    return tup[index]\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/12", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 188, "function_name": "safe_get_tuple_index", "line_no": 188}}
{"prompt": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):", "reference": "    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/13", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 0, "lineno": 204, "function_name": "prob_mask_like", "line_no": 204}}
{"prompt": " copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):", "reference": "        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-25", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-35", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/0-45", "text": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/15-65", "text": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/25-75", "text": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/35-85", "text": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/14", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1, "lineno": 227, "function_name": "__init__", "line_no": 227}}
{"prompt": "\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):", "reference": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/45-95", "text": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/55-105", "text": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/65-115", "text": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/75-125", "text": "    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/85-135", "text": "def maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/95-145", "text": "        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/105-155", "text": "\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to("}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/115-165", "text": "def pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/125-175", "text": "        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/135-185", "text": "def l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/15", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 94, "lineno": 275, "function_name": "q_sample", "line_no": 275}}
{"prompt": "[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):", "reference": "        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/145-195", "text": "    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/155-205", "text": "    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/165-215", "text": "    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/175-225", "text": "):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/185-235", "text": "    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/195-245", "text": "def normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/205-255", "text": "        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/215-265", "text": "def beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/225-275", "text": "class GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/235-285", "text": "\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/16", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 190, "lineno": 326, "function_name": "__init__", "line_no": 326}}
{"prompt": "to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):", "reference": "        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/155-205", "text": "    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/165-215", "text": "    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/175-225", "text": "):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/185-235", "text": "    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/195-245", "text": "def normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/205-255", "text": "        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/215-265", "text": "def beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/225-275", "text": "class GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/235-285", "text": "\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/245-295", "text": "        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/17", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 198, "lineno": 333, "function_name": "forward", "line_no": 333}}
{"prompt": ", noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):", "reference": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/185-235", "text": "    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/195-245", "text": "def normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/205-255", "text": "        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/215-265", "text": "def beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/225-275", "text": "class GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/235-285", "text": "\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/245-295", "text": "        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/255-305", "text": "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/265-315", "text": "        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/275-325", "text": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/18", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 226, "lineno": 381, "function_name": "__init__", "line_no": 381}}
{"prompt": "batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):", "reference": "        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/235-285", "text": "\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/245-295", "text": "        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/255-305", "text": "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/265-315", "text": "        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/275-325", "text": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/285-335", "text": "\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/295-345", "text": "        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/305-355", "text": "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/315-365", "text": "\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/325-375", "text": "    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/19", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 279, "lineno": 453, "function_name": "__init__", "line_no": 453}}
{"prompt": " = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):", "reference": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/255-305", "text": "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/265-315", "text": "        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/275-325", "text": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/285-335", "text": "\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/295-345", "text": "        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/305-355", "text": "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/315-365", "text": "\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/325-375", "text": "    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/335-385", "text": "        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/345-395", "text": "\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/20", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 296, "lineno": 475, "function_name": "forward", "line_no": 475}}
{"prompt": "snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):", "reference": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/265-315", "text": "        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/275-325", "text": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/285-335", "text": "\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/295-345", "text": "        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/305-355", "text": "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/315-365", "text": "\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/325-375", "text": "    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/335-385", "text": "        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/345-395", "text": "\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/355-405", "text": "        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/21", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 313, "lineno": 505, "function_name": "__init__", "line_no": 505}}
{"prompt": "\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):", "reference": "        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/375-425", "text": "        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/385-435", "text": "        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/395-445", "text": "\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/405-455", "text": "        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/415-465", "text": "        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/425-475", "text": "        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/435-485", "text": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/22", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 415, "lineno": 602, "function_name": "__init__", "line_no": 602}}
{"prompt": "\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):", "reference": "        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/385-435", "text": "        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/395-445", "text": "\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/405-455", "text": "        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/415-465", "text": "        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/425-475", "text": "        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/435-485", "text": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/23", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 426, "lineno": 615, "function_name": "init_conv_", "line_no": 615}}
{"prompt": "init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle", "reference": "    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/395-445", "text": "\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/405-455", "text": "        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/415-465", "text": "        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/425-475", "text": "        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/435-485", "text": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/485-535", "text": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/24", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 441, "lineno": 629, "function_name": "Downsample", "line_no": 629}}
{"prompt": " FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):", "reference": "        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/425-475", "text": "        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/435-485", "text": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/485-535", "text": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/495-545", "text": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/505-555", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/515-565", "text": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/25", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 471, "lineno": 652, "function_name": "__init__", "line_no": 652}}
{"prompt": "torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):", "reference": "        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/435-485", "text": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/485-535", "text": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/495-545", "text": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/505-555", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/515-565", "text": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/525-575", "text": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/26", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 476, "lineno": 658, "function_name": "forward", "line_no": 658}}
{"prompt": "pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):", "reference": "        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/435-485", "text": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/485-535", "text": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/495-545", "text": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/505-555", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/515-565", "text": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/525-575", "text": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/27", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 484, "lineno": 672, "function_name": "__init__", "line_no": 672}}
{"prompt": " latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):", "reference": "        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/445-495", "text": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/455-505", "text": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/485-535", "text": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/495-545", "text": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/505-555", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/515-565", "text": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/525-575", "text": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/535-585", "text": "\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/28", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 489, "lineno": 678, "function_name": "forward", "line_no": 678}}
{"prompt": " bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):", "reference": "        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/465-515", "text": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/475-525", "text": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/485-535", "text": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/495-545", "text": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/505-555", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/515-565", "text": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/525-575", "text": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/535-585", "text": "\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/545-595", "text": "            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/555-605", "text": "\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/29", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 514, "lineno": 701, "function_name": "__init__", "line_no": 701}}
{"prompt": " / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):", "reference": "        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/495-545", "text": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/505-555", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/515-565", "text": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/525-575", "text": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/535-585", "text": "\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/545-595", "text": "            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/555-605", "text": "\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/565-615", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/575-625", "text": "\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/585-635", "text": "\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/30", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 536, "lineno": 732, "function_name": "forward", "line_no": 732}}
{"prompt": " exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):", "reference": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/525-575", "text": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/535-585", "text": "\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/545-595", "text": "            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/555-605", "text": "\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/565-615", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/575-625", "text": "\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/585-635", "text": "\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/595-645", "text": "\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/605-655", "text": "\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/615-665", "text": "        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/31", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 569, "lineno": 765, "function_name": "__init__", "line_no": 765}}
{"prompt": " inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):", "reference": "        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/735-785", "text": "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/745-795", "text": "            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/755-805", "text": "    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/765-815", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/775-825", "text": "\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/785-835", "text": "            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/795-845", "text": "\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/805-855", "text": "        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/815-865", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/825-875", "text": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/32", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 778, "lineno": 949, "function_name": "__init__", "line_no": 949}}
{"prompt": "device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):", "reference": "        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/745-795", "text": "            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/755-805", "text": "    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/765-815", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/775-825", "text": "\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/785-835", "text": "            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/795-845", "text": "\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/805-855", "text": "        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/815-865", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/825-875", "text": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/835-885", "text": "        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/33", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 789, "lineno": 961, "function_name": "forward", "line_no": 961}}
{"prompt": " value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):", "reference": "    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/755-805", "text": "    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/765-815", "text": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/775-825", "text": "\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/785-835", "text": "            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/795-845", "text": "\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/805-855", "text": "        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/815-865", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/825-875", "text": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/835-885", "text": "        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/845-895", "text": "\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/34", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 798, "lineno": 968, "function_name": "FeedForward", "line_no": 968}}
{"prompt": "float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):", "reference": "        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/775-825", "text": "\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/785-835", "text": "            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/795-845", "text": "\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/805-855", "text": "        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/815-865", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/825-875", "text": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/835-885", "text": "        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/845-895", "text": "\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/855-905", "text": "            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/865-915", "text": "\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/35", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 824, "lineno": 998, "function_name": "__init__", "line_no": 998}}
{"prompt": "):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):", "reference": "        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/785-835", "text": "            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/795-845", "text": "\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/805-855", "text": "        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/815-865", "text": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/825-875", "text": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/835-885", "text": "        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/845-895", "text": "\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/855-905", "text": "            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/865-915", "text": "\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/875-925", "text": "        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/36", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 832, "lineno": 1008, "function_name": "forward", "line_no": 1008}}
{"prompt": " einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):", "reference": "        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/825-875", "text": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/835-885", "text": "        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/845-895", "text": "\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/855-905", "text": "            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/865-915", "text": "\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/875-925", "text": "        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/885-935", "text": "        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/895-945", "text": "        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/905-955", "text": "        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/915-965", "text": "        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/37", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 867, "lineno": 1054, "function_name": "__init__", "line_no": 1054}}
{"prompt": "),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):", "reference": "        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/855-905", "text": "            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/865-915", "text": "\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/875-925", "text": "        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/885-935", "text": "        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/895-945", "text": "        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/905-955", "text": "        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/915-965", "text": "        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/925-975", "text": "            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/935-985", "text": "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/945-995", "text": "        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/38", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 897, "lineno": 1082, "function_name": "__init__", "line_no": 1082}}
{"prompt": " bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):", "reference": "        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/865-915", "text": "\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/875-925", "text": "        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/885-935", "text": "        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/895-945", "text": "        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/905-955", "text": "        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/915-965", "text": "        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/925-975", "text": "            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/935-985", "text": "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/945-995", "text": "        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/955-1005", "text": "            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/39", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 907, "lineno": 1096, "function_name": "forward", "line_no": 1096}}
{"prompt": " = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):", "reference": "        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1265-1315", "text": "\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1275-1325", "text": "        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1285-1335", "text": "        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1295-1345", "text": "\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1305-1355", "text": "        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1315-1365", "text": "\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1325-1375", "text": "        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1335-1385", "text": "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1345-1395", "text": "                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1355-1405", "text": "                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/40", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1309, "lineno": 1452, "function_name": "cast_model_parameters", "line_no": 1452}}
{"prompt": "        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):", "reference": "        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1775-1825", "text": "            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\n# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n\nclass Imagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,\n        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1785-1835", "text": "    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,\n        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1795-1845", "text": "        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1805-1855", "text": "        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1815-1865", "text": "        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1825-1875", "text": "\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1835-1885", "text": "\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1845-1895", "text": "\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters("}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1855-1905", "text": "\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/pytorch.py/1865-1915", "text": "        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/41", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "context_start_lineno": 1815, "lineno": 1985, "function_name": "get_unet", "line_no": 1985}}
{"prompt": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):", "reference": "        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/0-25", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/0-35", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/0-45", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/5-55", "text": "from imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/15-65", "text": "def default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/25-75", "text": "\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/35-85", "text": "# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/45-95", "text": "    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/55-105", "text": "class Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/65-115", "text": "        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/42", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "context_start_lineno": 0, "lineno": 80, "function_name": "check_image_sizes", "line_no": 80}}
{"prompt": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):", "reference": "        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/0-25", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/0-35", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/0-45", "text": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/5-55", "text": "from imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/15-65", "text": "def default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/25-75", "text": "\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/35-85", "text": "# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/45-95", "text": "    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/55-105", "text": "class Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-configs.py/65-115", "text": "        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/43", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "context_start_lineno": 0, "lineno": 86, "function_name": "create", "line_no": 86}}
{"prompt": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):", "reference": "    while True:\n        for data in dl:\n            yield data\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-data.py/0-25", "text": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-data.py/0-35", "text": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-data.py/0-45", "text": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose(["}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-data.py/5-55", "text": "from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-data.py/15-65", "text": "import urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:\n                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-data.py/25-75", "text": "    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:\n                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))\n            images.append(image)\n\n        if len(texts) == 0:\n            return None\n        \n        texts = pad_sequence(texts, True)\n\n        newbatch = []\n        for i in range(len(texts)):\n            newbatch.append((images[i], texts[i]))"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/44", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "context_start_lineno": 0, "lineno": 25, "function_name": "cycle", "line_no": 25}}
{"prompt": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):", "reference": "    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/0-25", "text": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/0-35", "text": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/0-45", "text": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/5-55", "text": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/15-65", "text": "from einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/25-75", "text": "    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/35-85", "text": "def divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-imagen/video.py/45-95", "text": "\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/45", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "once", "line_no": 47}}
{"prompt": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):", "reference": "    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/0-25", "text": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/0-35", "text": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/0-45", "text": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/5-55", "text": "\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/15-65", "text": "\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/25-75", "text": "\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\","}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/35-85", "text": "    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",\n        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text("}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-t5.py/45-95", "text": "\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",\n        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text(\n    token_ids,\n    attn_mask = None,\n    pad_id = None,\n    name = DEFAULT_T5_NAME\n):\n    assert exists(attn_mask) or exists(pad_id)\n    t5, _ = get_model_and_tokenizer(name)\n\n    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/46", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "get_encoded_dim", "line_no": 47}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):", "reference": "    if exists(val):\n        return val\n    return d() if callable(d) else d\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/47", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 42, "function_name": "default", "line_no": 42}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):", "reference": "    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/45-95", "text": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/48", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 47, "function_name": "cast_tuple", "line_no": 47}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):", "reference": "    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/45-95", "text": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/55-105", "text": "            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/49", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 63, "function_name": "group_dict_by_key", "line_no": 63}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):", "reference": "    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/45-95", "text": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/55-105", "text": "            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/65-115", "text": "        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/50", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 77, "function_name": "groupby_prefix_and_trim", "line_no": 77}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):", "reference": "    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/45-95", "text": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/55-105", "text": "            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/65-115", "text": "        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/51", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 82, "function_name": "num_to_groups", "line_no": 82}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):", "reference": "        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/45-95", "text": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/55-105", "text": "            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/65-115", "text": "        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/52", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 116, "function_name": "inner", "line_no": 116}}
{"prompt": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):", "reference": "    for el in arr:\n        if cond(el):\n            return el\n    return None\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-25", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-35", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/0-45", "text": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/5-55", "text": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/15-65", "text": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/25-75", "text": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/35-85", "text": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/45-95", "text": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/55-105", "text": "            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/65-115", "text": "        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/53", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 0, "lineno": 161, "function_name": "find_first", "line_no": 161}}
{"prompt": "\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):", "reference": "        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/185-235", "text": "        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n\n# imagen trainer\n\ndef imagen_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):\n            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/195-245", "text": "            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):\n            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/205-255", "text": "            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/215-265", "text": "        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/225-275", "text": "\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/235-285", "text": "        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/245-295", "text": "        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/255-305", "text": "        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/265-315", "text": "        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/275-325", "text": "\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/54", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 226, "lineno": 417, "function_name": "prepare", "line_no": 417}}
{"prompt": "et (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):", "reference": "        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/215-265", "text": "        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/225-275", "text": "\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/235-285", "text": "        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/245-295", "text": "        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/255-305", "text": "        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/265-315", "text": "        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/275-325", "text": "\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/285-335", "text": "        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/295-345", "text": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/305-355", "text": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/55", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 260, "lineno": 455, "function_name": "validate_and_set_unet_being_trained", "line_no": 455}}
{"prompt": "and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):", "reference": "        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/225-275", "text": "\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/235-285", "text": "        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/245-295", "text": "        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/255-305", "text": "        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/265-315", "text": "        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/275-325", "text": "\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/285-335", "text": "        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/295-345", "text": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/305-355", "text": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/315-365", "text": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/56", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 272, "lineno": 469, "function_name": "wrap_unet", "line_no": 469}}
{"prompt": "aler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):", "reference": "        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/255-305", "text": "        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/265-315", "text": "        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/275-325", "text": "\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/285-335", "text": "        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/295-345", "text": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/305-355", "text": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/315-365", "text": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/325-375", "text": "\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/335-385", "text": "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/345-395", "text": "                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/57", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 299, "lineno": 494, "function_name": "set_accelerator_scaler", "line_no": 494}}
{"prompt": " None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):", "reference": "        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/275-325", "text": "\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/285-335", "text": "        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/295-345", "text": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/305-355", "text": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/315-365", "text": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/325-375", "text": "\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/335-385", "text": "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/345-395", "text": "                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/355-405", "text": "            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/365-415", "text": "            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/58", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 317, "lineno": 515, "function_name": "validate_unet_number", "line_no": 515}}
{"prompt": " fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):", "reference": "        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/285-335", "text": "        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/295-345", "text": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/305-355", "text": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/315-365", "text": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/325-375", "text": "\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/335-385", "text": "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/345-395", "text": "                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/355-405", "text": "            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/365-415", "text": "            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/375-425", "text": "            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/59", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 328, "lineno": 524, "function_name": "num_steps_taken", "line_no": 524}}
{"prompt": "r = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):", "reference": "        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/295-345", "text": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/305-355", "text": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/315-365", "text": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/325-375", "text": "\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/335-385", "text": "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/345-395", "text": "                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/355-405", "text": "            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/365-415", "text": "            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/375-425", "text": "            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/385-435", "text": "\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/60", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 342, "lineno": 545, "function_name": "add_train_dataloader", "line_no": 545}}
{"prompt": "alingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):", "reference": "        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/315-365", "text": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/325-375", "text": "\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/335-385", "text": "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/345-395", "text": "                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/355-405", "text": "            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/365-415", "text": "            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/375-425", "text": "            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/385-435", "text": "\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/395-445", "text": "        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/405-455", "text": "            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/61", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 363, "lineno": 561, "function_name": "add_train_dataset", "line_no": 561}}
{"prompt": "can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):", "reference": "        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/355-405", "text": "            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/365-415", "text": "            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/375-425", "text": "            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/385-435", "text": "\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/395-445", "text": "        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/405-455", "text": "            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/415-465", "text": "\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/425-475", "text": "\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/435-485", "text": "    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/445-495", "text": "        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/62", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 400, "lineno": 592, "function_name": "create_train_iter", "line_no": 592}}
{"prompt": " assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):", "reference": "        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/375-425", "text": "            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/385-435", "text": "\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/395-445", "text": "        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/405-455", "text": "            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/415-465", "text": "\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/425-475", "text": "\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/435-485", "text": "    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/445-495", "text": "        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/455-505", "text": "        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/465-515", "text": "\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/63", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 417, "lineno": 608, "function_name": "train_step", "line_no": 608}}
{"prompt": " # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):", "reference": "        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/395-445", "text": "        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/405-455", "text": "            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/415-465", "text": "\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/425-475", "text": "\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/435-485", "text": "    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/445-495", "text": "        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/455-505", "text": "        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/465-515", "text": "\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/475-525", "text": "        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/485-535", "text": "\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/64", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 442, "lineno": 627, "function_name": "step_with_dl_iter", "line_no": 627}}
{"prompt": "uple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):", "reference": "        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/585-635", "text": "\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/595-645", "text": "            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/605-655", "text": "        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/615-665", "text": "    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/625-675", "text": "\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/635-685", "text": "    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/645-695", "text": "            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/655-705", "text": "\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/665-715", "text": "        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/675-725", "text": "\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/65", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 628, "lineno": 819, "function_name": "get_ema_unet", "line_no": 819}}
{"prompt": "uler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):\n            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')\n\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:\n            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()\n\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return\n\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()\n\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):", "reference": "        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n\n            total_loss += loss.item()\n\n            if self.training:\n                self.accelerator.backward(loss)\n\n        return total_loss\n", "docs": [{"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/735-785", "text": "                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/745-795", "text": "    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/755-805", "text": "\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/765-815", "text": "            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/775-825", "text": "\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/785-835", "text": "            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/795-845", "text": "                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/805-855", "text": "                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/815-865", "text": "    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets"}, {"title": "lucidrains/imagen-pytorch/lucidrains/imagen-pytorch-imagen/pytorch-trainer.py/825-875", "text": "        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):"}], "metadata": {"task_id": "lucidrains_imagen-pytorch/66", "fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "context_start_lineno": 779, "lineno": 972, "function_name": "forward", "line_no": 972}}
