{"title": "facebookresearch_omnivore-hubconf.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "hubconf.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 19, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "hubconf.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 19, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\ndependencies = [\"torch\"]\n\nfrom omnivore.models import (  # noqa: F401, E402\n    omnivore_swinB,\n    omnivore_swinB_epic,\n    omnivore_swinB_imagenet21k,\n    omnivore_swinL_imagenet21k,\n    omnivore_swinL_kinetics600,\n    omnivore_swinS,\n    omnivore_swinT,\n)\n\nAST=Module(Assign(Name(Store)List(ConstantLoad))ImportFrom(aliasaliasaliasaliasaliasaliasalias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-hubconf.py_0-19"}
{"title": "facebookresearch_omnivore-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "setup.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"omnivore_fair\",\n    version=\"1.1\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-setup.py_0-25"}
{"title": "facebookresearch_omnivore-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "setup.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"omnivore_fair\",\n    version=\"1.1\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-setup.py_0-35"}
{"title": "facebookresearch_omnivore-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "setup.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 43, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"omnivore_fair\",\n    version=\"1.1\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",\n            \"ufmt==2.0.0b2\",\n            \"usort==1.0.2\",\n            \"libcst==0.4.1\",\n            ##################################\n        ],\n    },\n    packages=find_packages(exclude=(\"scripts\", \"tests\")),\n)\n\nAST=Module(ImportFrom(aliasalias)Expr(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))keyword(Constant)keyword(List(Load))keyword(Dict(ConstantList(ConstantConstantConstantConstantConstantLoad)))keyword(Call(Name(Load)keyword(Tuple(ConstantConstantLoad)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-setup.py_0-43"}
{"title": "facebookresearch_omnivore-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "setup.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 43, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "setup(\n    name=\"omnivore_fair\",\n    version=\"1.1\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",\n            \"ufmt==2.0.0b2\",\n            \"usort==1.0.2\",\n            \"libcst==0.4.1\",\n            ##################################\n        ],\n    },\n    packages=find_packages(exclude=(\"scripts\", \"tests\")),\n)\n\nAST=Module(Expr(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))keyword(Constant)keyword(List(Load))keyword(Dict(ConstantList(ConstantConstantConstantConstantConstantLoad)))keyword(Call(Name(Load)keyword(Tuple(ConstantConstantLoad)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-setup.py_5-43"}
{"title": "facebookresearch_omnivore-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "setup.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 43, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "temp_build", "setup.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 43, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",\n            \"ufmt==2.0.0b2\",\n            \"usort==1.0.2\",\n            \"libcst==0.4.1\",\n            ##################################\n        ],\n    },\n    packages=find_packages(exclude=(\"scripts\", \"tests\")),\n)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-setup.py_15-43"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom omnivision.models.vision_transformer import (\n    Attention,\n    Decoder,\n    PadIm2Video,\n    VisionTransformer,\n)\n\nfrom timm.models.layers import trunc_normal_\nfrom torch.hub import load_state_dict_from_url\n\n\nCHECKPOINT_PATHS = {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_0-25"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom omnivision.models.vision_transformer import (\n    Attention,\n    Decoder,\n    PadIm2Video,\n    VisionTransformer,\n)\n\nfrom timm.models.layers import trunc_normal_\nfrom torch.hub import load_state_dict_from_url\n\n\nCHECKPOINT_PATHS = {\n    \"omnimae_vitB_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_pretrain.torch\",\n    \"omnimae_vitB_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_in1k_ft.torch\",\n    \"omnimae_vitB_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_ssv2_ft.torch\",\n    \"omnimae_vitL_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_pretrain.torch\",\n    \"omnimae_vitL_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_in1k_ft.torch\",\n    \"omnimae_vitL_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_ssv2_ft.torch\",\n    \"omnimae_vitH_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_pretrain.torch\",\n    \"omnimae_vitH_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_in1k_ft.torch\",\n    \"omnimae_vitH_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_ssv2_ft.torch\",\n}\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_0-35"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom omnivision.models.vision_transformer import (\n    Attention,\n    Decoder,\n    PadIm2Video,\n    VisionTransformer,\n)\n\nfrom timm.models.layers import trunc_normal_\nfrom torch.hub import load_state_dict_from_url\n\n\nCHECKPOINT_PATHS = {\n    \"omnimae_vitB_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_pretrain.torch\",\n    \"omnimae_vitB_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_in1k_ft.torch\",\n    \"omnimae_vitB_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_ssv2_ft.torch\",\n    \"omnimae_vitL_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_pretrain.torch\",\n    \"omnimae_vitL_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_in1k_ft.torch\",\n    \"omnimae_vitL_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_ssv2_ft.torch\",\n    \"omnimae_vitH_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_pretrain.torch\",\n    \"omnimae_vitH_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_in1k_ft.torch\",\n    \"omnimae_vitH_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_ssv2_ft.torch\",\n}\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    if init_weight is not None:\n        init_weight(tensor=layer.weight.data)\n    if init_bias is not None:\n        init_bias(tensor=layer.bias.data)\n    return layer\n\n\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant))FunctionDef(arguments(argargargConstantConstant)If(Compare(Name(Load)IsNotConstant)Expr(Call(Name(Load)keyword(Attribute(Attribute(Name(Load)Load)Load)))))If(Compare(Name(Load)IsNotConstant)Expr(Call(Name(Load)keyword(Attribute(Attribute(Name(Load)Load)Load)))))Return(Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_0-45"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom omnivision.models.vision_transformer import (\n    Attention,\n    Decoder,\n    PadIm2Video,\n    VisionTransformer,\n)\n\nfrom timm.models.layers import trunc_normal_\nfrom torch.hub import load_state_dict_from_url\n\n\nCHECKPOINT_PATHS = {\n    \"omnimae_vitB_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_pretrain.torch\",\n    \"omnimae_vitB_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_in1k_ft.torch\",\n    \"omnimae_vitB_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_ssv2_ft.torch\",\n    \"omnimae_vitL_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_pretrain.torch\",\n    \"omnimae_vitL_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_in1k_ft.torch\",\n    \"omnimae_vitL_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_ssv2_ft.torch\",\n    \"omnimae_vitH_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_pretrain.torch\",\n    \"omnimae_vitH_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_in1k_ft.torch\",\n    \"omnimae_vitH_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_ssv2_ft.torch\",\n}\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    if init_weight is not None:\n        init_weight(tensor=layer.weight.data)\n    if init_bias is not None:\n        init_bias(tensor=layer.bias.data)\n    return layer\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    torch.nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass OmniMAE(nn.Module):\n    def __init__(self, trunk, head):\n        super().__init__()\n        self.trunk = trunk\n        self.head = head\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant))FunctionDef(arguments(argargargConstantConstant)If(Compare(Name(Load)IsNotConstant)Expr(Call(Name(Load)keyword(Attribute(Attribute(Name(Load)Load)Load)))))If(Compare(Name(Load)IsNotConstant)Expr(Call(Name(Load)keyword(Attribute(Attribute(Name(Load)Load)Load)))))Return(Name(Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Call(Attribute(Name(Load)Load)List(Subscript(Attribute(Name(Load)Load)ConstantLoad)UnaryOp(USubConstant)Load)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_5-55"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Decoder,\n    PadIm2Video,\n    VisionTransformer,\n)\n\nfrom timm.models.layers import trunc_normal_\nfrom torch.hub import load_state_dict_from_url\n\n\nCHECKPOINT_PATHS = {\n    \"omnimae_vitB_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_pretrain.torch\",\n    \"omnimae_vitB_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_in1k_ft.torch\",\n    \"omnimae_vitB_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_ssv2_ft.torch\",\n    \"omnimae_vitL_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_pretrain.torch\",\n    \"omnimae_vitL_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_in1k_ft.torch\",\n    \"omnimae_vitL_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_ssv2_ft.torch\",\n    \"omnimae_vitH_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_pretrain.torch\",\n    \"omnimae_vitH_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_in1k_ft.torch\",\n    \"omnimae_vitH_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_ssv2_ft.torch\",\n}\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    if init_weight is not None:\n        init_weight(tensor=layer.weight.data)\n    if init_bias is not None:\n        init_bias(tensor=layer.bias.data)\n    return layer\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    torch.nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass OmniMAE(nn.Module):\n    def __init__(self, trunk, head):\n        super().__init__()\n        self.trunk = trunk\n        self.head = head\n\n    def forward(self, imgOrVideo, mask=None):\n        # imgOrVideo: A tensor of shape [N,C,H,W] for images and [N,C,T,H,W] for videos\n        # mask: A boolean tensor of the shape [N, patch_layout's shpae]\n        outputs = self.trunk(imgOrVideo, mask=mask)\n        return self.head(outputs)\n  \n\ndef _load_checkpoint(model, checkpoint_name, pretrained, progress=True):\n    if pretrained:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_15-65"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"omnimae_vitB_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_pretrain.torch\",\n    \"omnimae_vitB_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_in1k_ft.torch\",\n    \"omnimae_vitB_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitb_ssv2_ft.torch\",\n    \"omnimae_vitL_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_pretrain.torch\",\n    \"omnimae_vitL_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_in1k_ft.torch\",\n    \"omnimae_vitL_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vitl_ssv2_ft.torch\",\n    \"omnimae_vitH_pretrain\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_pretrain.torch\",\n    \"omnimae_vitH_ft_in1k\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_in1k_ft.torch\",\n    \"omnimae_vitH_ft_ssv2\": \"https://dl.fbaipublicfiles.com/omnivore/omnimae_ckpts/vith_ssv2_ft.torch\",\n}\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    if init_weight is not None:\n        init_weight(tensor=layer.weight.data)\n    if init_bias is not None:\n        init_bias(tensor=layer.bias.data)\n    return layer\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    torch.nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass OmniMAE(nn.Module):\n    def __init__(self, trunk, head):\n        super().__init__()\n        self.trunk = trunk\n        self.head = head\n\n    def forward(self, imgOrVideo, mask=None):\n        # imgOrVideo: A tensor of shape [N,C,H,W] for images and [N,C,T,H,W] for videos\n        # mask: A boolean tensor of the shape [N, patch_layout's shpae]\n        outputs = self.trunk(imgOrVideo, mask=mask)\n        return self.head(outputs)\n  \n\ndef _load_checkpoint(model, checkpoint_name, pretrained, progress=True):\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n        print(f\"Loading {checkpoint_name} from {path}\")\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=True)\n        assert len(missing_keys) == 0 and len(unexpected_keys) == 0\n    return model\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_25-75"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    if init_weight is not None:\n        init_weight(tensor=layer.weight.data)\n    if init_bias is not None:\n        init_bias(tensor=layer.bias.data)\n    return layer\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    torch.nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass OmniMAE(nn.Module):\n    def __init__(self, trunk, head):\n        super().__init__()\n        self.trunk = trunk\n        self.head = head\n\n    def forward(self, imgOrVideo, mask=None):\n        # imgOrVideo: A tensor of shape [N,C,H,W] for images and [N,C,T,H,W] for videos\n        # mask: A boolean tensor of the shape [N, patch_layout's shpae]\n        outputs = self.trunk(imgOrVideo, mask=mask)\n        return self.head(outputs)\n  \n\ndef _load_checkpoint(model, checkpoint_name, pretrained, progress=True):\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n        print(f\"Loading {checkpoint_name} from {path}\")\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=True)\n        assert len(missing_keys) == 0 and len(unexpected_keys) == 0\n    return model\n\n\ndef vit_base_mae_pretraining(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_35-85"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    torch.nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass OmniMAE(nn.Module):\n    def __init__(self, trunk, head):\n        super().__init__()\n        self.trunk = trunk\n        self.head = head\n\n    def forward(self, imgOrVideo, mask=None):\n        # imgOrVideo: A tensor of shape [N,C,H,W] for images and [N,C,T,H,W] for videos\n        # mask: A boolean tensor of the shape [N, patch_layout's shpae]\n        outputs = self.trunk(imgOrVideo, mask=mask)\n        return self.head(outputs)\n  \n\ndef _load_checkpoint(model, checkpoint_name, pretrained, progress=True):\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n        print(f\"Loading {checkpoint_name} from {path}\")\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=True)\n        assert len(missing_keys) == 0 and len(unexpected_keys) == 0\n    return model\n\n\ndef vit_base_mae_pretraining(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_45-95"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def forward(self, imgOrVideo, mask=None):\n        # imgOrVideo: A tensor of shape [N,C,H,W] for images and [N,C,T,H,W] for videos\n        # mask: A boolean tensor of the shape [N, patch_layout's shpae]\n        outputs = self.trunk(imgOrVideo, mask=mask)\n        return self.head(outputs)\n  \n\ndef _load_checkpoint(model, checkpoint_name, pretrained, progress=True):\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n        print(f\"Loading {checkpoint_name} from {path}\")\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=True)\n        assert len(missing_keys) == 0 and len(unexpected_keys) == 0\n    return model\n\n\ndef vit_base_mae_pretraining(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_55-105"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        path = CHECKPOINT_PATHS[checkpoint_name]\n        print(f\"Loading {checkpoint_name} from {path}\")\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=True)\n        assert len(missing_keys) == 0 and len(unexpected_keys) == 0\n    return model\n\n\ndef vit_base_mae_pretraining(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_65-115"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def vit_base_mae_pretraining(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=384,\n            embed_dim=768,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_75-125"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=384,\n            embed_dim=768,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=384, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_85-135"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=384,\n            embed_dim=768,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=384, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_pretrain\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_ssv2(pretrained=True):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_95-145"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=384,\n            embed_dim=768,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=384, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_pretrain\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_105-155"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=384,\n            embed_dim=768,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=384, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_pretrain\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_115-165"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=384, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_pretrain\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_125-175"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_pretrain\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_135-185"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_145-195"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_ssv2\"\n    )\n    return model\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_155-205"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_165-215"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_175-225"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_185-235"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_base_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_195-245"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def vit_base_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=1000),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_205-255"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            Attention,\n            attn_drop=0,\n            num_heads=12,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_in1k\"\n    )\n    return model\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_215-265"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_in1k\"\n    )\n    return model\n\n\ndef vit_large_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_225-275"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=768,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_in1k\"\n    )\n    return model\n\n\ndef vit_large_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_235-285"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=768, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_in1k\"\n    )\n    return model\n\n\ndef vit_large_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_245-295"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitB_ft_in1k\"\n    )\n    return model\n\n\ndef vit_large_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_255-305"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def vit_large_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=512,\n            embed_dim=1024,\n            learnable_pos_embed=False,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_265-315"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=512,\n            embed_dim=1024,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_275-325"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=512,\n            embed_dim=1024,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_pretrain\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_ssv2(pretrained=True):\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_285-335"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=512,\n            embed_dim=1024,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_pretrain\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_295-345"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=4,\n            decoder_embed_dim=512,\n            embed_dim=1024,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_pretrain\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_305-355"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1536),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_pretrain\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_315-365"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_pretrain\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_325-375"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=174),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_335-385"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_ssv2\"\n    )\n    return model\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_345-395"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_355-405"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_365-415"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_375-425"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_large_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_385-435"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef vit_large_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 16, 16],\n        in_chans=3,\n        embed_dim=1024,\n        depth=24,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_395-445"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_in1k\"\n    )\n    return model\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_405-455"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_in1k\"\n    )\n    return model\n\n\ndef vit_huge_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_415-465"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 16, 16],\n                    out_channels=1024,\n                    stride=[2, 16, 16],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_in1k\"\n    )\n    return model\n\n\ndef vit_huge_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_425-475"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1024, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_in1k\"\n    )\n    return model\n\n\ndef vit_huge_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_435-485"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        layer=torch.nn.Linear(in_features=1024, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitL_ft_in1k\"\n    )\n    return model\n\n\ndef vit_huge_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_445-495"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef vit_huge_mae_pretraining(pretrained=True):\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=8,\n            decoder_embed_dim=512,\n            embed_dim=1280,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_455-505"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=8,\n            decoder_embed_dim=512,\n            embed_dim=1280,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1176),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_465-515"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=8,\n            decoder_embed_dim=512,\n            embed_dim=1280,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1176),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_pretrain\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_ssv2(pretrained=True):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_475-525"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=True,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=8,\n            decoder_embed_dim=512,\n            embed_dim=1280,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1176),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_pretrain\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_485-535"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=True,\n        post_encoder_params=None,\n        decoder=partial(\n            Decoder,\n            attn_target=partial(Attention, num_heads=16),\n            decoder_depth=8,\n            decoder_embed_dim=512,\n            embed_dim=1280,\n            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1176),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_pretrain\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_495-545"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            learnable_pos_embed=False,\n            qkv_bias=True,\n        ),\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=512, out_features=1176),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.02),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_pretrain\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_505-555"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_pretrain\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_ssv2(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_515-565"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_525-575"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_ssv2\"\n    )\n    return model", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_535-585"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_545-595"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_555-605"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    # NOTE: Head config for this model has funcky dropout in head\n    # ckpt loeading is different.\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_565-615"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        layer=torch.nn.Linear(in_features=1280, out_features=174),\n        init_bias=partial(torch.nn.init.zeros_),\n        init_weight=partial(trunc_normal_, mean=0.0, std=0.01),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_ssv2\"\n    )\n    return model\n\n\ndef vit_huge_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_575-625"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef vit_huge_mae_finetune_in1k(pretrained=True):\n\n    trunk = VisionTransformer(\n        img_size=[3, 16, 224, 224],\n        patch_size=[2, 14, 14],\n        in_chans=3,\n        embed_dim=1280,\n        depth=32,\n        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n\nAST=Module(FunctionDef(arguments(argConstant)Assign(Name(Store)Call(Name(Load)keyword(List(ConstantConstantConstantConstantLoad))keyword(List(ConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Call(Name(Load)Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(List(Call(Name(Load)keyword(Constant)keyword(Constant))Call(Name(Load)keyword(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Constant)keyword(List(ConstantConstantConstantLoad))keyword(Constant)keyword(List(ConstantConstantConstantLoad))))keyword(Call(Name(Load)Name(Load))))Load))keyword(Constant)keyword(Constant)keyword(UnaryOp(USubConstant))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_585-635"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mlp_ratio=4,\n        attn_target=partial(\n            Attention,\n            attn_drop=0,\n            num_heads=16,\n            proj_drop=0,\n            qk_scale=False,\n            qkv_bias=True,\n        ),\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_in1k\"\n    )\n    return model", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_595-645"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"global_pool\",\n        use_cls_token=False,\n        learnable_pos_embed=False,\n        layer_scale_type=None,\n        layer_scale_init_value=0.1,\n        patch_embed_type=\"generic\",\n        patch_embed_params_list=[\n            PadIm2Video(ntimes=2, pad_type=\"repeat\"),\n            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_in1k\"\n    )\n    return model", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_605-645"}
{"title": "facebookresearch_omnivore-omnimae-omni_mae_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "omni_mae_model.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            make_conv_or_linear(\n                layer=torch.nn.Conv3d(\n                    in_channels=3,\n                    kernel_size=[2, 14, 14],\n                    out_channels=1280,\n                    stride=[2, 14, 14],\n                ),\n                init_weight=partial(reshape_and_init_as_mlp),\n            ),\n        ],\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        patch_drop_max_patches=-1,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n    )\n\n    head = make_conv_or_linear(\n        layer=torch.nn.Linear(in_features=1280, out_features=1000),\n        init_weight=partial(trunc_normal_, mean=0.0, std=2.0e-05),\n    )\n\n    model = OmniMAE(trunk, head)\n    model = _load_checkpoint(\n        model=model, pretrained=pretrained, checkpoint_name=\"omnimae_vitH_ft_in1k\"\n    )\n    return model", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-omni_mae_model.py_615-645"}
{"title": "facebookresearch_omnivore-omnimae-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnimae", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "heads", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\nAST=Module", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnimae-__init__.py_0-1"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code borrowed from TnT - https://github.com/pytorch/tnt/blob/master/torchtnt/loggers/tensorboard.py\nimport atexit\nimport logging\nimport os\nimport uuid\nfrom typing import Any, Dict, Optional, Union\n\nfrom numpy import ndarray\nfrom omnivision.utils.train import get_machine_local_and_dist_rank, makedir\nfrom torch import Tensor\nfrom torch.utils.tensorboard import SummaryWriter\n\nScalar = Union[Tensor, ndarray, int, float]\n\n\ndef make_tensorboard_logger(log_dir: str, **writer_kwargs: Any):\n\n    makedir(log_dir)\n    return TensorBoardLogger(path=log_dir, **writer_kwargs)\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Call(Name(Load)Name(Load)))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code borrowed from TnT - https://github.com/pytorch/tnt/blob/master/torchtnt/loggers/tensorboard.py\nimport atexit\nimport logging\nimport os\nimport uuid\nfrom typing import Any, Dict, Optional, Union\n\nfrom numpy import ndarray\nfrom omnivision.utils.train import get_machine_local_and_dist_rank, makedir\nfrom torch import Tensor\nfrom torch.utils.tensorboard import SummaryWriter\n\nScalar = Union[Tensor, ndarray, int, float]\n\n\ndef make_tensorboard_logger(log_dir: str, **writer_kwargs: Any):\n\n    makedir(log_dir)\n    return TensorBoardLogger(path=log_dir, **writer_kwargs)\n\n\n# TODO: Expose writer building in configs.\nclass TensorBoardLogger(object):\n    \"\"\"\n    A simple logger for TensorBoard.\n    \"\"\"\n\n    def __init__(self, path: str, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Create a new TensorBoard logger.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code borrowed from TnT - https://github.com/pytorch/tnt/blob/master/torchtnt/loggers/tensorboard.py\nimport atexit\nimport logging\nimport os\nimport uuid\nfrom typing import Any, Dict, Optional, Union\n\nfrom numpy import ndarray\nfrom omnivision.utils.train import get_machine_local_and_dist_rank, makedir\nfrom torch import Tensor\nfrom torch.utils.tensorboard import SummaryWriter\n\nScalar = Union[Tensor, ndarray, int, float]\n\n\ndef make_tensorboard_logger(log_dir: str, **writer_kwargs: Any):\n\n    makedir(log_dir)\n    return TensorBoardLogger(path=log_dir, **writer_kwargs)\n\n\n# TODO: Expose writer building in configs.\nclass TensorBoardLogger(object):\n    \"\"\"\n    A simple logger for TensorBoard.\n    \"\"\"\n\n    def __init__(self, path: str, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Create a new TensorBoard logger.\n        On construction, the logger creates a new events file that logs\n        will be written to.  If the environment variable `RANK` is defined,\n        logger will only log if RANK = 0.\n\n        NOTE: If using the logger with distributed training:\n        - This logger can call collective operations\n        - Logs will be written on rank 0 only\n        - Logger must be constructed synchronously *after* initializing distributed process group.\n\n        Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Code borrowed from TnT - https://github.com/pytorch/tnt/blob/master/torchtnt/loggers/tensorboard.py\nimport atexit\nimport logging\nimport os\nimport uuid\nfrom typing import Any, Dict, Optional, Union\n\nfrom numpy import ndarray\nfrom omnivision.utils.train import get_machine_local_and_dist_rank, makedir\nfrom torch import Tensor\nfrom torch.utils.tensorboard import SummaryWriter\n\nScalar = Union[Tensor, ndarray, int, float]\n\n\ndef make_tensorboard_logger(log_dir: str, **writer_kwargs: Any):\n\n    makedir(log_dir)\n    return TensorBoardLogger(path=log_dir, **writer_kwargs)\n\n\n# TODO: Expose writer building in configs.\nclass TensorBoardLogger(object):\n    \"\"\"\n    A simple logger for TensorBoard.\n    \"\"\"\n\n    def __init__(self, path: str, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Create a new TensorBoard logger.\n        On construction, the logger creates a new events file that logs\n        will be written to.  If the environment variable `RANK` is defined,\n        logger will only log if RANK = 0.\n\n        NOTE: If using the logger with distributed training:\n        - This logger can call collective operations\n        - Logs will be written on rank 0 only\n        - Logger must be constructed synchronously *after* initializing distributed process group.\n\n        Args:\n            path (str): path to write logs to\n            *args, **kwargs: Extra arguments to pass to SummaryWriter\n        \"\"\"\n\n        self._writer: Optional[SummaryWriter] = None\n\n        _, self._rank = get_machine_local_and_dist_rank()\n        self._path: str = path\n\n        if self._rank == 0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from torch import Tensor\nfrom torch.utils.tensorboard import SummaryWriter\n\nScalar = Union[Tensor, ndarray, int, float]\n\n\ndef make_tensorboard_logger(log_dir: str, **writer_kwargs: Any):\n\n    makedir(log_dir)\n    return TensorBoardLogger(path=log_dir, **writer_kwargs)\n\n\n# TODO: Expose writer building in configs.\nclass TensorBoardLogger(object):\n    \"\"\"\n    A simple logger for TensorBoard.\n    \"\"\"\n\n    def __init__(self, path: str, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Create a new TensorBoard logger.\n        On construction, the logger creates a new events file that logs\n        will be written to.  If the environment variable `RANK` is defined,\n        logger will only log if RANK = 0.\n\n        NOTE: If using the logger with distributed training:\n        - This logger can call collective operations\n        - Logs will be written on rank 0 only\n        - Logger must be constructed synchronously *after* initializing distributed process group.\n\n        Args:\n            path (str): path to write logs to\n            *args, **kwargs: Extra arguments to pass to SummaryWriter\n        \"\"\"\n\n        self._writer: Optional[SummaryWriter] = None\n\n        _, self._rank = get_machine_local_and_dist_rank()\n        self._path: str = path\n\n        if self._rank == 0:\n            logging.info(\n                f\"TensorBoard SummaryWriter instantiated. Files will be stored in: {path}\"\n            )\n            self._writer = SummaryWriter(\n                log_dir=path, *args, filename_suffix=str(uuid.uuid4()), **kwargs\n            )\n        else:\n            logging.debug(\n                f\"Not logging metrics on this host because env RANK: {self._rank} != 0\"\n            )\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Name(Load)Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load)))Expr(Call(Name(Load)Name(Load)))Return(Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))))ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)AnnAssign(Attribute(Name(Load)Store)Subscript(Name(Load)Name(Load)Load)Constant)Assign(Tuple(Name(Store)Attribute(Name(Load)Store)Store)Call(Name(Load)))AnnAssign(Attribute(Name(Load)Store)Name(Load)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Assign(Attribute(Name(Load)Store)Call(Name(Load)Starred(Name(Load)Load)keyword(Name(Load))keyword(Call(Name(Load)Call(Attribute(Name(Load)Load))))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))Constant))))Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n# TODO: Expose writer building in configs.\nclass TensorBoardLogger(object):\n    \"\"\"\n    A simple logger for TensorBoard.\n    \"\"\"\n\n    def __init__(self, path: str, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Create a new TensorBoard logger.\n        On construction, the logger creates a new events file that logs\n        will be written to.  If the environment variable `RANK` is defined,\n        logger will only log if RANK = 0.\n\n        NOTE: If using the logger with distributed training:\n        - This logger can call collective operations\n        - Logs will be written on rank 0 only\n        - Logger must be constructed synchronously *after* initializing distributed process group.\n\n        Args:\n            path (str): path to write logs to\n            *args, **kwargs: Extra arguments to pass to SummaryWriter\n        \"\"\"\n\n        self._writer: Optional[SummaryWriter] = None\n\n        _, self._rank = get_machine_local_and_dist_rank()\n        self._path: str = path\n\n        if self._rank == 0:\n            logging.info(\n                f\"TensorBoard SummaryWriter instantiated. Files will be stored in: {path}\"\n            )\n            self._writer = SummaryWriter(\n                log_dir=path, *args, filename_suffix=str(uuid.uuid4()), **kwargs\n            )\n        else:\n            logging.debug(\n                f\"Not logging metrics on this host because env RANK: {self._rank} != 0\"\n            )\n\n        atexit.register(self.close)\n\n    @property\n    def writer(self) -> Optional[SummaryWriter]:\n        return self._writer\n\n    @property\n    def path(self) -> str:\n        return self._path\n\nAST=Module(ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)AnnAssign(Attribute(Name(Load)Store)Subscript(Name(Load)Name(Load)Load)Constant)Assign(Tuple(Name(Store)Attribute(Name(Load)Store)Store)Call(Name(Load)))AnnAssign(Attribute(Name(Load)Store)Name(Load)Name(Load))If(Compare(Attribute(Name(Load)Load)EqConstant)Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Assign(Attribute(Name(Load)Store)Call(Name(Load)Starred(Name(Load)Load)keyword(Name(Load))keyword(Call(Name(Load)Call(Attribute(Name(Load)Load))))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))Constant))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Constant)FunctionDef(arguments(arg)Return(Attribute(Name(Load)Load))Name(Load)Subscript(Name(Load)Name(Load)Load))FunctionDef(arguments(arg)Return(Attribute(Name(Load)Load))Name(Load)Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        On construction, the logger creates a new events file that logs\n        will be written to.  If the environment variable `RANK` is defined,\n        logger will only log if RANK = 0.\n\n        NOTE: If using the logger with distributed training:\n        - This logger can call collective operations\n        - Logs will be written on rank 0 only\n        - Logger must be constructed synchronously *after* initializing distributed process group.\n\n        Args:\n            path (str): path to write logs to\n            *args, **kwargs: Extra arguments to pass to SummaryWriter\n        \"\"\"\n\n        self._writer: Optional[SummaryWriter] = None\n\n        _, self._rank = get_machine_local_and_dist_rank()\n        self._path: str = path\n\n        if self._rank == 0:\n            logging.info(\n                f\"TensorBoard SummaryWriter instantiated. Files will be stored in: {path}\"\n            )\n            self._writer = SummaryWriter(\n                log_dir=path, *args, filename_suffix=str(uuid.uuid4()), **kwargs\n            )\n        else:\n            logging.debug(\n                f\"Not logging metrics on this host because env RANK: {self._rank} != 0\"\n            )\n\n        atexit.register(self.close)\n\n    @property\n    def writer(self) -> Optional[SummaryWriter]:\n        return self._writer\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def log_dict(self, payload: Dict[str, Scalar], step: int) -> None:\n        \"\"\"Add multiple scalar values to TensorBoard.\n\n        Args:\n            payload (dict): dictionary of tag name and scalar value\n            step (int, Optional): step value to record\n        \"\"\"\n\n        if not self._writer:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            path (str): path to write logs to\n            *args, **kwargs: Extra arguments to pass to SummaryWriter\n        \"\"\"\n\n        self._writer: Optional[SummaryWriter] = None\n\n        _, self._rank = get_machine_local_and_dist_rank()\n        self._path: str = path\n\n        if self._rank == 0:\n            logging.info(\n                f\"TensorBoard SummaryWriter instantiated. Files will be stored in: {path}\"\n            )\n            self._writer = SummaryWriter(\n                log_dir=path, *args, filename_suffix=str(uuid.uuid4()), **kwargs\n            )\n        else:\n            logging.debug(\n                f\"Not logging metrics on this host because env RANK: {self._rank} != 0\"\n            )\n\n        atexit.register(self.close)\n\n    @property\n    def writer(self) -> Optional[SummaryWriter]:\n        return self._writer\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def log_dict(self, payload: Dict[str, Scalar], step: int) -> None:\n        \"\"\"Add multiple scalar values to TensorBoard.\n\n        Args:\n            payload (dict): dictionary of tag name and scalar value\n            step (int, Optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        for k, v in payload.items():\n            self.log(k, v, step)\n\n    def log(self, name: str, data: Scalar, step: int) -> None:\n        \"\"\"Add scalar data to TensorBoard.\n\n        Args:\n            name (string): tag name used to group scalars", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            logging.info(\n                f\"TensorBoard SummaryWriter instantiated. Files will be stored in: {path}\"\n            )\n            self._writer = SummaryWriter(\n                log_dir=path, *args, filename_suffix=str(uuid.uuid4()), **kwargs\n            )\n        else:\n            logging.debug(\n                f\"Not logging metrics on this host because env RANK: {self._rank} != 0\"\n            )\n\n        atexit.register(self.close)\n\n    @property\n    def writer(self) -> Optional[SummaryWriter]:\n        return self._writer\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def log_dict(self, payload: Dict[str, Scalar], step: int) -> None:\n        \"\"\"Add multiple scalar values to TensorBoard.\n\n        Args:\n            payload (dict): dictionary of tag name and scalar value\n            step (int, Optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        for k, v in payload.items():\n            self.log(k, v, step)\n\n    def log(self, name: str, data: Scalar, step: int) -> None:\n        \"\"\"Add scalar data to TensorBoard.\n\n        Args:\n            name (string): tag name used to group scalars\n            data (float/int/Tensor): scalar data to log\n            step (int, optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_scalar(name, data, global_step=step, new_style=True)\n\n    def log_hparams(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        atexit.register(self.close)\n\n    @property\n    def writer(self) -> Optional[SummaryWriter]:\n        return self._writer\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def log_dict(self, payload: Dict[str, Scalar], step: int) -> None:\n        \"\"\"Add multiple scalar values to TensorBoard.\n\n        Args:\n            payload (dict): dictionary of tag name and scalar value\n            step (int, Optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        for k, v in payload.items():\n            self.log(k, v, step)\n\n    def log(self, name: str, data: Scalar, step: int) -> None:\n        \"\"\"Add scalar data to TensorBoard.\n\n        Args:\n            name (string): tag name used to group scalars\n            data (float/int/Tensor): scalar data to log\n            step (int, optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_scalar(name, data, global_step=step, new_style=True)\n\n    def log_hparams(\n        self, hparams: Dict[str, Scalar], metrics: Dict[str, Scalar]\n    ) -> None:\n        \"\"\"Add hyperparameter data to TensorBoard.\n\n        Args:\n            hparams (dict): dictionary of hyperparameter names and corresponding values\n            metrics (dict): dictionary of name of metric and corersponding values\n        \"\"\"\n\n        if not self._writer:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def log_dict(self, payload: Dict[str, Scalar], step: int) -> None:\n        \"\"\"Add multiple scalar values to TensorBoard.\n\n        Args:\n            payload (dict): dictionary of tag name and scalar value\n            step (int, Optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        for k, v in payload.items():\n            self.log(k, v, step)\n\n    def log(self, name: str, data: Scalar, step: int) -> None:\n        \"\"\"Add scalar data to TensorBoard.\n\n        Args:\n            name (string): tag name used to group scalars\n            data (float/int/Tensor): scalar data to log\n            step (int, optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_scalar(name, data, global_step=step, new_style=True)\n\n    def log_hparams(\n        self, hparams: Dict[str, Scalar], metrics: Dict[str, Scalar]\n    ) -> None:\n        \"\"\"Add hyperparameter data to TensorBoard.\n\n        Args:\n            hparams (dict): dictionary of hyperparameter names and corresponding values\n            metrics (dict): dictionary of name of metric and corersponding values\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_hparams(hparams, metrics)\n\n    def flush(self) -> None:\n        \"\"\"Writes pending logs to disk.\"\"\"\n\n        if not self._writer:\n            return\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            return\n\n        for k, v in payload.items():\n            self.log(k, v, step)\n\n    def log(self, name: str, data: Scalar, step: int) -> None:\n        \"\"\"Add scalar data to TensorBoard.\n\n        Args:\n            name (string): tag name used to group scalars\n            data (float/int/Tensor): scalar data to log\n            step (int, optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_scalar(name, data, global_step=step, new_style=True)\n\n    def log_hparams(\n        self, hparams: Dict[str, Scalar], metrics: Dict[str, Scalar]\n    ) -> None:\n        \"\"\"Add hyperparameter data to TensorBoard.\n\n        Args:\n            hparams (dict): dictionary of hyperparameter names and corresponding values\n            metrics (dict): dictionary of name of metric and corersponding values\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_hparams(hparams, metrics)\n\n    def flush(self) -> None:\n        \"\"\"Writes pending logs to disk.\"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.flush()\n\n    def close(self) -> None:\n        \"\"\"Close writer, flushing pending logs to disk.\n        Logs cannot be written after `close` is called.\n        \"\"\"\n\n        if not self._writer:\n            return\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 137, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            data (float/int/Tensor): scalar data to log\n            step (int, optional): step value to record\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_scalar(name, data, global_step=step, new_style=True)\n\n    def log_hparams(\n        self, hparams: Dict[str, Scalar], metrics: Dict[str, Scalar]\n    ) -> None:\n        \"\"\"Add hyperparameter data to TensorBoard.\n\n        Args:\n            hparams (dict): dictionary of hyperparameter names and corresponding values\n            metrics (dict): dictionary of name of metric and corersponding values\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_hparams(hparams, metrics)\n\n    def flush(self) -> None:\n        \"\"\"Writes pending logs to disk.\"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.flush()\n\n    def close(self) -> None:\n        \"\"\"Close writer, flushing pending logs to disk.\n        Logs cannot be written after `close` is called.\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.close()\n        self._writer = None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_95-137"}
{"title": "facebookresearch_omnivore-omnivision-logger.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "logger.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 137, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self, hparams: Dict[str, Scalar], metrics: Dict[str, Scalar]\n    ) -> None:\n        \"\"\"Add hyperparameter data to TensorBoard.\n\n        Args:\n            hparams (dict): dictionary of hyperparameter names and corresponding values\n            metrics (dict): dictionary of name of metric and corersponding values\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.add_hparams(hparams, metrics)\n\n    def flush(self) -> None:\n        \"\"\"Writes pending logs to disk.\"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.flush()\n\n    def close(self) -> None:\n        \"\"\"Close writer, flushing pending logs to disk.\n        Logs cannot be written after `close` is called.\n        \"\"\"\n\n        if not self._writer:\n            return\n\n        self._writer.close()\n        self._writer = None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-logger.py_105-137"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport random\n\nimport hydra\nimport submitit\nimport torch\nfrom hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nfrom omnivision.utils.train import makedir, register_omegaconf_resolvers\n\n# Make work w recent PyTorch versions (https://github.com/pytorch/pytorch/issues/37377)\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n\nregister_omegaconf_resolvers()\n\n\nclass SubmititRunner(submitit.helpers.Checkpointable):\n    \"\"\"A callable which is passed to submitit to launch the jobs.\"\"\"\n\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Constant)Expr(Call(Name(Load)))ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport random\n\nimport hydra\nimport submitit\nimport torch\nfrom hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nfrom omnivision.utils.train import makedir, register_omegaconf_resolvers\n\n# Make work w recent PyTorch versions (https://github.com/pytorch/pytorch/issues/37377)\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n\nregister_omegaconf_resolvers()\n\n\nclass SubmititRunner(submitit.helpers.Checkpointable):\n    \"\"\"A callable which is passed to submitit to launch the jobs.\"\"\"\n\n    def __init__(self, port, cfg):\n        self.cfg = cfg\n        self.port = port\n\n    def __call__(self):\n        register_omegaconf_resolvers()\n        job_env = submitit.JobEnvironment()\n        os.environ[\"MASTER_ADDR\"] = job_env.hostnames[0]\n        os.environ[\"MASTER_PORT\"] = str(self.port)\n        os.environ[\"RANK\"] = str(job_env.global_rank)\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Constant)Expr(Call(Name(Load)))ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)FunctionDef(arguments(argargarg)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(arg)Expr(Call(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport random\n\nimport hydra\nimport submitit\nimport torch\nfrom hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nfrom omnivision.utils.train import makedir, register_omegaconf_resolvers\n\n# Make work w recent PyTorch versions (https://github.com/pytorch/pytorch/issues/37377)\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n\nregister_omegaconf_resolvers()\n\n\nclass SubmititRunner(submitit.helpers.Checkpointable):\n    \"\"\"A callable which is passed to submitit to launch the jobs.\"\"\"\n\n    def __init__(self, port, cfg):\n        self.cfg = cfg\n        self.port = port\n\n    def __call__(self):\n        register_omegaconf_resolvers()\n        job_env = submitit.JobEnvironment()\n        os.environ[\"MASTER_ADDR\"] = job_env.hostnames[0]\n        os.environ[\"MASTER_PORT\"] = str(self.port)\n        os.environ[\"RANK\"] = str(job_env.global_rank)\n        os.environ[\"LOCAL_RANK\"] = str(job_env.local_rank)\n        os.environ[\"WORLD_SIZE\"] = str(job_env.num_tasks)\n\n        trainer = instantiate(self.cfg.trainer, _recursive_=False)\n        trainer.run()\n\n\ndef single_proc_run(local_rank, main_port, cfg, world_size):\n    \"\"\"Executes fun() on a single GPU in a multi-GPU setup.\"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Constant)Expr(Call(Name(Load)))ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)FunctionDef(arguments(argargarg)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(arg)Expr(Call(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)))))FunctionDef(arguments(argargargarg)Expr(Constant)Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport os\nimport random\n\nimport hydra\nimport submitit\nimport torch\nfrom hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nfrom omnivision.utils.train import makedir, register_omegaconf_resolvers\n\n# Make work w recent PyTorch versions (https://github.com/pytorch/pytorch/issues/37377)\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n\nregister_omegaconf_resolvers()\n\n\nclass SubmititRunner(submitit.helpers.Checkpointable):\n    \"\"\"A callable which is passed to submitit to launch the jobs.\"\"\"\n\n    def __init__(self, port, cfg):\n        self.cfg = cfg\n        self.port = port\n\n    def __call__(self):\n        register_omegaconf_resolvers()\n        job_env = submitit.JobEnvironment()\n        os.environ[\"MASTER_ADDR\"] = job_env.hostnames[0]\n        os.environ[\"MASTER_PORT\"] = str(self.port)\n        os.environ[\"RANK\"] = str(job_env.global_rank)\n        os.environ[\"LOCAL_RANK\"] = str(job_env.local_rank)\n        os.environ[\"WORLD_SIZE\"] = str(job_env.num_tasks)\n\n        trainer = instantiate(self.cfg.trainer, _recursive_=False)\n        trainer.run()\n\n\ndef single_proc_run(local_rank, main_port, cfg, world_size):\n    \"\"\"Executes fun() on a single GPU in a multi-GPU setup.\"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = str(main_port)\n    os.environ[\"RANK\"] = str(local_rank)\n    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n    os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n    trainer = instantiate(cfg.trainer, _recursive_=False)\n    trainer.run()\n\n\n@hydra.main(config_path=\"config\", config_name=None)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Make work w recent PyTorch versions (https://github.com/pytorch/pytorch/issues/37377)\nos.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n\nregister_omegaconf_resolvers()\n\n\nclass SubmititRunner(submitit.helpers.Checkpointable):\n    \"\"\"A callable which is passed to submitit to launch the jobs.\"\"\"\n\n    def __init__(self, port, cfg):\n        self.cfg = cfg\n        self.port = port\n\n    def __call__(self):\n        register_omegaconf_resolvers()\n        job_env = submitit.JobEnvironment()\n        os.environ[\"MASTER_ADDR\"] = job_env.hostnames[0]\n        os.environ[\"MASTER_PORT\"] = str(self.port)\n        os.environ[\"RANK\"] = str(job_env.global_rank)\n        os.environ[\"LOCAL_RANK\"] = str(job_env.local_rank)\n        os.environ[\"WORLD_SIZE\"] = str(job_env.num_tasks)\n\n        trainer = instantiate(self.cfg.trainer, _recursive_=False)\n        trainer.run()\n\n\ndef single_proc_run(local_rank, main_port, cfg, world_size):\n    \"\"\"Executes fun() on a single GPU in a multi-GPU setup.\"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = str(main_port)\n    os.environ[\"RANK\"] = str(local_rank)\n    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n    os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n    trainer = instantiate(cfg.trainer, _recursive_=False)\n    trainer.run()\n\n\n@hydra.main(config_path=\"config\", config_name=None)\ndef main(cfg) -> None:\n    print(\"###################### Train App Config ####################\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"############################################################\")\n\n    makedir(cfg.launcher.experiment_log_dir)\n\n    submitit_conf = cfg.get(\"submitit\", None)\n    assert submitit_conf is not None, \"Missing submitit config\"\n\n\nAST=Module(Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Constant)Expr(Call(Name(Load)))ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)FunctionDef(arguments(argargarg)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(arg)Expr(Call(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)))))FunctionDef(arguments(argargargarg)Expr(Constant)Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Constant)Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load)))Assign(Subscript(Attribute(Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Attribute(Name(Load)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load))))FunctionDef(arguments(arg)Expr(Call(Name(Load)Constant))Expr(Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))Expr(Call(Name(Load)Constant))Expr(Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assert(Compare(Name(Load)IsNotConstant)Constant)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Constant))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, port, cfg):\n        self.cfg = cfg\n        self.port = port\n\n    def __call__(self):\n        register_omegaconf_resolvers()\n        job_env = submitit.JobEnvironment()\n        os.environ[\"MASTER_ADDR\"] = job_env.hostnames[0]\n        os.environ[\"MASTER_PORT\"] = str(self.port)\n        os.environ[\"RANK\"] = str(job_env.global_rank)\n        os.environ[\"LOCAL_RANK\"] = str(job_env.local_rank)\n        os.environ[\"WORLD_SIZE\"] = str(job_env.num_tasks)\n\n        trainer = instantiate(self.cfg.trainer, _recursive_=False)\n        trainer.run()\n\n\ndef single_proc_run(local_rank, main_port, cfg, world_size):\n    \"\"\"Executes fun() on a single GPU in a multi-GPU setup.\"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = str(main_port)\n    os.environ[\"RANK\"] = str(local_rank)\n    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n    os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n    trainer = instantiate(cfg.trainer, _recursive_=False)\n    trainer.run()\n\n\n@hydra.main(config_path=\"config\", config_name=None)\ndef main(cfg) -> None:\n    print(\"###################### Train App Config ####################\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"############################################################\")\n\n    makedir(cfg.launcher.experiment_log_dir)\n\n    submitit_conf = cfg.get(\"submitit\", None)\n    assert submitit_conf is not None, \"Missing submitit config\"\n\n    if submitit_conf.get(\"log_save_dir\") is None:\n        submitit_dir = cfg.launcher.experiment_log_dir\n        submitit_dir = os.path.join(submitit_dir, \"submitit_logs\")\n    else:\n        submitit_dir = submitit_conf.log_save_dir\n\n    if submitit_conf.use_cluster:\n        executor = submitit.AutoExecutor(folder=submitit_dir)\n\n        job_kwargs = {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        os.environ[\"LOCAL_RANK\"] = str(job_env.local_rank)\n        os.environ[\"WORLD_SIZE\"] = str(job_env.num_tasks)\n\n        trainer = instantiate(self.cfg.trainer, _recursive_=False)\n        trainer.run()\n\n\ndef single_proc_run(local_rank, main_port, cfg, world_size):\n    \"\"\"Executes fun() on a single GPU in a multi-GPU setup.\"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = str(main_port)\n    os.environ[\"RANK\"] = str(local_rank)\n    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n    os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n    trainer = instantiate(cfg.trainer, _recursive_=False)\n    trainer.run()\n\n\n@hydra.main(config_path=\"config\", config_name=None)\ndef main(cfg) -> None:\n    print(\"###################### Train App Config ####################\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"############################################################\")\n\n    makedir(cfg.launcher.experiment_log_dir)\n\n    submitit_conf = cfg.get(\"submitit\", None)\n    assert submitit_conf is not None, \"Missing submitit config\"\n\n    if submitit_conf.get(\"log_save_dir\") is None:\n        submitit_dir = cfg.launcher.experiment_log_dir\n        submitit_dir = os.path.join(submitit_dir, \"submitit_logs\")\n    else:\n        submitit_dir = submitit_conf.log_save_dir\n\n    if submitit_conf.use_cluster:\n        executor = submitit.AutoExecutor(folder=submitit_dir)\n\n        job_kwargs = {\n            \"timeout_min\": 60 * submitit_conf.timeout_hour,\n            \"name\": submitit_conf.name,\n            \"slurm_partition\": submitit_conf.partition,\n            \"gpus_per_node\": cfg.launcher.gpus_per_node,\n            \"tasks_per_node\": cfg.launcher.gpus_per_node,  # one task per GPU\n            \"cpus_per_task\": submitit_conf.cpus_per_task,\n            \"nodes\": cfg.launcher.num_nodes,\n        }\n\n        if submitit_conf.get(\"mem_gb\", None) is not None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    os.environ[\"MASTER_PORT\"] = str(main_port)\n    os.environ[\"RANK\"] = str(local_rank)\n    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n    os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n    trainer = instantiate(cfg.trainer, _recursive_=False)\n    trainer.run()\n\n\n@hydra.main(config_path=\"config\", config_name=None)\ndef main(cfg) -> None:\n    print(\"###################### Train App Config ####################\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"############################################################\")\n\n    makedir(cfg.launcher.experiment_log_dir)\n\n    submitit_conf = cfg.get(\"submitit\", None)\n    assert submitit_conf is not None, \"Missing submitit config\"\n\n    if submitit_conf.get(\"log_save_dir\") is None:\n        submitit_dir = cfg.launcher.experiment_log_dir\n        submitit_dir = os.path.join(submitit_dir, \"submitit_logs\")\n    else:\n        submitit_dir = submitit_conf.log_save_dir\n\n    if submitit_conf.use_cluster:\n        executor = submitit.AutoExecutor(folder=submitit_dir)\n\n        job_kwargs = {\n            \"timeout_min\": 60 * submitit_conf.timeout_hour,\n            \"name\": submitit_conf.name,\n            \"slurm_partition\": submitit_conf.partition,\n            \"gpus_per_node\": cfg.launcher.gpus_per_node,\n            \"tasks_per_node\": cfg.launcher.gpus_per_node,  # one task per GPU\n            \"cpus_per_task\": submitit_conf.cpus_per_task,\n            \"nodes\": cfg.launcher.num_nodes,\n        }\n\n        if submitit_conf.get(\"mem_gb\", None) is not None:\n            job_kwargs[\"mem_gb\"] = submitit_conf.mem_gb\n        elif submitit_conf.get(\"mem\", None) is not None:\n            job_kwargs[\"slurm_mem\"] = submitit_conf.mem\n\n        if submitit_conf.get(\"constraints\", None) is not None:\n            job_kwargs[\"slurm_constraint\"] = submitit_conf.constraints\n\n        if submitit_conf.get(\"comment\", None) is not None:\n            job_kwargs[\"slurm_comment\"] = submitit_conf.comment\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def main(cfg) -> None:\n    print(\"###################### Train App Config ####################\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"############################################################\")\n\n    makedir(cfg.launcher.experiment_log_dir)\n\n    submitit_conf = cfg.get(\"submitit\", None)\n    assert submitit_conf is not None, \"Missing submitit config\"\n\n    if submitit_conf.get(\"log_save_dir\") is None:\n        submitit_dir = cfg.launcher.experiment_log_dir\n        submitit_dir = os.path.join(submitit_dir, \"submitit_logs\")\n    else:\n        submitit_dir = submitit_conf.log_save_dir\n\n    if submitit_conf.use_cluster:\n        executor = submitit.AutoExecutor(folder=submitit_dir)\n\n        job_kwargs = {\n            \"timeout_min\": 60 * submitit_conf.timeout_hour,\n            \"name\": submitit_conf.name,\n            \"slurm_partition\": submitit_conf.partition,\n            \"gpus_per_node\": cfg.launcher.gpus_per_node,\n            \"tasks_per_node\": cfg.launcher.gpus_per_node,  # one task per GPU\n            \"cpus_per_task\": submitit_conf.cpus_per_task,\n            \"nodes\": cfg.launcher.num_nodes,\n        }\n\n        if submitit_conf.get(\"mem_gb\", None) is not None:\n            job_kwargs[\"mem_gb\"] = submitit_conf.mem_gb\n        elif submitit_conf.get(\"mem\", None) is not None:\n            job_kwargs[\"slurm_mem\"] = submitit_conf.mem\n\n        if submitit_conf.get(\"constraints\", None) is not None:\n            job_kwargs[\"slurm_constraint\"] = submitit_conf.constraints\n\n        if submitit_conf.get(\"comment\", None) is not None:\n            job_kwargs[\"slurm_comment\"] = submitit_conf.comment\n\n        executor.update_parameters(**job_kwargs)\n\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        job = executor.submit(SubmititRunner(main_port, cfg))\n        print(\"Submitit Job ID:\", job.job_id)\n    else:\n        assert cfg.launcher.num_nodes == 1\n        num_proc = cfg.launcher.gpus_per_node\n\nAST=Module(FunctionDef(arguments(arg)Expr(Call(Name(Load)Constant))Expr(Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))Expr(Call(Name(Load)Constant))Expr(Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assert(Compare(Name(Load)IsNotConstant)Constant)If(Compare(Call(Attribute(Name(Load)Load)Constant)IsConstant)Assign(Name(Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Constant))Assign(Name(Store)Attribute(Name(Load)Load)))If(Attribute(Name(Load)Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantBinOp(ConstantMultAttribute(Name(Load)Load))Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Attribute(Attribute(Name(Load)Load)Load)))If(Compare(Call(Attribute(Name(Load)Load)ConstantConstant)IsNotConstant)Assign(Subscript(Name(Load)ConstantStore)Attribute(Name(Load)Load))If(Compare(Call(Attribute(Name(Load)Load)ConstantConstant)IsNotConstant)Assign(Subscript(Name(Load)ConstantStore)Attribute(Name(Load)Load))))If(Compare(Call(Attribute(Name(Load)Load)ConstantConstant)IsNotConstant)Assign(Subscript(Name(Load)ConstantStore)Attribute(Name(Load)Load)))If(Compare(Call(Attribute(Name(Load)Load)ConstantConstant)IsNotConstant)Assign(Subscript(Name(Load)ConstantStore)Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)Subscript(Attribute(Name(Load)Load)ConstantLoad)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)Name(Load))))Expr(Call(Name(Load)ConstantAttribute(Name(Load)Load)))Assert(Compare(Attribute(Attribute(Name(Load)Load)Load)EqConstant))Assign(Name(Store)Attribute(Attribute(Name(Load)Load)Load)))Constant))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    if submitit_conf.get(\"log_save_dir\") is None:\n        submitit_dir = cfg.launcher.experiment_log_dir\n        submitit_dir = os.path.join(submitit_dir, \"submitit_logs\")\n    else:\n        submitit_dir = submitit_conf.log_save_dir\n\n    if submitit_conf.use_cluster:\n        executor = submitit.AutoExecutor(folder=submitit_dir)\n\n        job_kwargs = {\n            \"timeout_min\": 60 * submitit_conf.timeout_hour,\n            \"name\": submitit_conf.name,\n            \"slurm_partition\": submitit_conf.partition,\n            \"gpus_per_node\": cfg.launcher.gpus_per_node,\n            \"tasks_per_node\": cfg.launcher.gpus_per_node,  # one task per GPU\n            \"cpus_per_task\": submitit_conf.cpus_per_task,\n            \"nodes\": cfg.launcher.num_nodes,\n        }\n\n        if submitit_conf.get(\"mem_gb\", None) is not None:\n            job_kwargs[\"mem_gb\"] = submitit_conf.mem_gb\n        elif submitit_conf.get(\"mem\", None) is not None:\n            job_kwargs[\"slurm_mem\"] = submitit_conf.mem\n\n        if submitit_conf.get(\"constraints\", None) is not None:\n            job_kwargs[\"slurm_constraint\"] = submitit_conf.constraints\n\n        if submitit_conf.get(\"comment\", None) is not None:\n            job_kwargs[\"slurm_comment\"] = submitit_conf.comment\n\n        executor.update_parameters(**job_kwargs)\n\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        job = executor.submit(SubmititRunner(main_port, cfg))\n        print(\"Submitit Job ID:\", job.job_id)\n    else:\n        assert cfg.launcher.num_nodes == 1\n        num_proc = cfg.launcher.gpus_per_node\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        if num_proc == 1:\n            # directly call single_proc so we can easily set breakpoints\n            # mp.spawn does not let us set breakpoints\n            single_proc_run(\n                local_rank=0, main_port=main_port, cfg=cfg, world_size=num_proc\n            )\n        else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            \"timeout_min\": 60 * submitit_conf.timeout_hour,\n            \"name\": submitit_conf.name,\n            \"slurm_partition\": submitit_conf.partition,\n            \"gpus_per_node\": cfg.launcher.gpus_per_node,\n            \"tasks_per_node\": cfg.launcher.gpus_per_node,  # one task per GPU\n            \"cpus_per_task\": submitit_conf.cpus_per_task,\n            \"nodes\": cfg.launcher.num_nodes,\n        }\n\n        if submitit_conf.get(\"mem_gb\", None) is not None:\n            job_kwargs[\"mem_gb\"] = submitit_conf.mem_gb\n        elif submitit_conf.get(\"mem\", None) is not None:\n            job_kwargs[\"slurm_mem\"] = submitit_conf.mem\n\n        if submitit_conf.get(\"constraints\", None) is not None:\n            job_kwargs[\"slurm_constraint\"] = submitit_conf.constraints\n\n        if submitit_conf.get(\"comment\", None) is not None:\n            job_kwargs[\"slurm_comment\"] = submitit_conf.comment\n\n        executor.update_parameters(**job_kwargs)\n\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        job = executor.submit(SubmititRunner(main_port, cfg))\n        print(\"Submitit Job ID:\", job.job_id)\n    else:\n        assert cfg.launcher.num_nodes == 1\n        num_proc = cfg.launcher.gpus_per_node\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        if num_proc == 1:\n            # directly call single_proc so we can easily set breakpoints\n            # mp.spawn does not let us set breakpoints\n            single_proc_run(\n                local_rank=0, main_port=main_port, cfg=cfg, world_size=num_proc\n            )\n        else:\n            mp_runner = torch.multiprocessing.start_processes\n            args = (main_port, cfg, num_proc)\n            # Note: using \"fork\" below, \"spawn\" causes time and error regressions. Using\n            # spawn changes the default multiprocessing context to spawn, which doesn't\n            # interact well with the dataloaders (likely due to the use of OpenCV).\n            mp_runner(single_proc_run, args=args, nprocs=num_proc, start_method=\"spawn\")\n\n\nif __name__ == \"__main__\":\n    main()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            job_kwargs[\"mem_gb\"] = submitit_conf.mem_gb\n        elif submitit_conf.get(\"mem\", None) is not None:\n            job_kwargs[\"slurm_mem\"] = submitit_conf.mem\n\n        if submitit_conf.get(\"constraints\", None) is not None:\n            job_kwargs[\"slurm_constraint\"] = submitit_conf.constraints\n\n        if submitit_conf.get(\"comment\", None) is not None:\n            job_kwargs[\"slurm_comment\"] = submitit_conf.comment\n\n        executor.update_parameters(**job_kwargs)\n\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        job = executor.submit(SubmititRunner(main_port, cfg))\n        print(\"Submitit Job ID:\", job.job_id)\n    else:\n        assert cfg.launcher.num_nodes == 1\n        num_proc = cfg.launcher.gpus_per_node\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        if num_proc == 1:\n            # directly call single_proc so we can easily set breakpoints\n            # mp.spawn does not let us set breakpoints\n            single_proc_run(\n                local_rank=0, main_port=main_port, cfg=cfg, world_size=num_proc\n            )\n        else:\n            mp_runner = torch.multiprocessing.start_processes\n            args = (main_port, cfg, num_proc)\n            # Note: using \"fork\" below, \"spawn\" causes time and error regressions. Using\n            # spawn changes the default multiprocessing context to spawn, which doesn't\n            # interact well with the dataloaders (likely due to the use of OpenCV).\n            mp_runner(single_proc_run, args=args, nprocs=num_proc, start_method=\"spawn\")\n\n\nif __name__ == \"__main__\":\n    main()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_85-125"}
{"title": "facebookresearch_omnivore-omnivision-train_app_submitit.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "train_app_submitit.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        executor.update_parameters(**job_kwargs)\n\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        job = executor.submit(SubmititRunner(main_port, cfg))\n        print(\"Submitit Job ID:\", job.job_id)\n    else:\n        assert cfg.launcher.num_nodes == 1\n        num_proc = cfg.launcher.gpus_per_node\n        main_port = random.randint(\n            submitit_conf.port_range[0], submitit_conf.port_range[1]\n        )\n        if num_proc == 1:\n            # directly call single_proc so we can easily set breakpoints\n            # mp.spawn does not let us set breakpoints\n            single_proc_run(\n                local_rank=0, main_port=main_port, cfg=cfg, world_size=num_proc\n            )\n        else:\n            mp_runner = torch.multiprocessing.start_processes\n            args = (main_port, cfg, num_proc)\n            # Note: using \"fork\" below, \"spawn\" causes time and error regressions. Using\n            # spawn changes the default multiprocessing context to spawn, which doesn't\n            # interact well with the dataloaders (likely due to the use of OpenCV).\n            mp_runner(single_proc_run, args=args, nprocs=num_proc, start_method=\"spawn\")\n\n\nif __name__ == \"__main__\":\n    main()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-train_app_submitit.py_95-125"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field, fields, is_dataclass, make_dataclass\nfrom typing import Any, Callable, Dict\n\nfrom torch.utils.data.dataloader import default_collate\n\n\n@dataclass\nclass Batch:\n    # the following are per batch args which are passed to the trainer\n    # and are set to reasonable defaults\n    model_fwd_kwargs: Dict = field(default_factory=dict)\n    accum_steps: int = 1\n\n\ndef create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"\n\nAST=Module(ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasaliasalias)ImportFrom(alias)ClassDef(AnnAssign(Name(Store)Name(Load)Call(Name(Load)keyword(Name(Load))))AnnAssign(Name(Store)Name(Load)Constant)Name(Load))FunctionDef(arguments(arg)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field, fields, is_dataclass, make_dataclass\nfrom typing import Any, Callable, Dict\n\nfrom torch.utils.data.dataloader import default_collate\n\n\n@dataclass\nclass Batch:\n    # the following are per batch args which are passed to the trainer\n    # and are set to reasonable defaults\n    model_fwd_kwargs: Dict = field(default_factory=dict)\n    accum_steps: int = 1\n\n\ndef create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"\n    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n\n\n@create_batch_sample_cls\n@dataclass\nclass Sample:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field, fields, is_dataclass, make_dataclass\nfrom typing import Any, Callable, Dict\n\nfrom torch.utils.data.dataloader import default_collate\n\n\n@dataclass\nclass Batch:\n    # the following are per batch args which are passed to the trainer\n    # and are set to reasonable defaults\n    model_fwd_kwargs: Dict = field(default_factory=dict)\n    accum_steps: int = 1\n\n\ndef create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"\n    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n\n\n@create_batch_sample_cls\n@dataclass\nclass Sample:\n    # NOTE: Up to Python 3.9, dataclasses don't support inheritance when there\n    # are both positional and default arguments present. See\n    # https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses\n    data_idx: int = None\n    data_valid: bool = None\n    label: Any = None\n\n    @classmethod\n    def get_batch_sample_class(cls):\n        return globals()[f\"{Batch.__name__}{cls.__name__}\"]\n\nAST=Module(ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasaliasalias)ImportFrom(alias)ClassDef(AnnAssign(Name(Store)Name(Load)Call(Name(Load)keyword(Name(Load))))AnnAssign(Name(Store)Name(Load)Constant)Name(Load))FunctionDef(arguments(arg)Expr(Constant)Assign(Name(Store)JoinedStr(FormattedValue(Attribute(Name(Load)Load))FormattedValue(Attribute(Name(Load)Load))))Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Tuple(Load))keyword(Tuple(Name(Load)Name(Load)Load))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Subscript(Call(Name(Load))Name(Load)Store)Name(Load))Return(Name(Load)))ClassDef(AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Subscript(Call(Name(Load))JoinedStr(FormattedValue(Attribute(Name(Load)Load))FormattedValue(Attribute(Name(Load)Load)))Load))Name(Load))Name(Load)Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom dataclasses import dataclass, field, fields, is_dataclass, make_dataclass\nfrom typing import Any, Callable, Dict\n\nfrom torch.utils.data.dataloader import default_collate\n\n\n@dataclass\nclass Batch:\n    # the following are per batch args which are passed to the trainer\n    # and are set to reasonable defaults\n    model_fwd_kwargs: Dict = field(default_factory=dict)\n    accum_steps: int = 1\n\n\ndef create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"\n    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n\n\n@create_batch_sample_cls\n@dataclass\nclass Sample:\n    # NOTE: Up to Python 3.9, dataclasses don't support inheritance when there\n    # are both positional and default arguments present. See\n    # https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses\n    data_idx: int = None\n    data_valid: bool = None\n    label: Any = None\n\n    @classmethod\n    def get_batch_sample_class(cls):\n        return globals()[f\"{Batch.__name__}{cls.__name__}\"]\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionSample(Sample):\n    vision: Any = None\n\n\n@create_batch_sample_cls\n@dataclass", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # and are set to reasonable defaults\n    model_fwd_kwargs: Dict = field(default_factory=dict)\n    accum_steps: int = 1\n\n\ndef create_batch_sample_cls(cls):\n    \"\"\"Dynamically creates a dataclass which is a `Batch` and a `Sample`.\n\n    This function also registers the class in globals() to make the class picklable.\n    \"\"\"\n    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n\n\n@create_batch_sample_cls\n@dataclass\nclass Sample:\n    # NOTE: Up to Python 3.9, dataclasses don't support inheritance when there\n    # are both positional and default arguments present. See\n    # https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses\n    data_idx: int = None\n    data_valid: bool = None\n    label: Any = None\n\n    @classmethod\n    def get_batch_sample_class(cls):\n        return globals()[f\"{Batch.__name__}{cls.__name__}\"]\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionSample(Sample):\n    vision: Any = None\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionMaskSample(VisionSample):\n    mask: Any = None\n\n\ndef dataclass_as_dict(obj):\n    # replacement for dataclasses.asdict which makes a deepcopy of everything\n    if is_dataclass(obj):\n        return {f.name: dataclass_as_dict(getattr(obj, f.name)) for f in fields(obj)}\n    return obj\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    cls_name = f\"{Batch.__name__}{cls.__name__}\"\n    batch_sample_cls = make_dataclass(cls_name, fields=(), bases=(cls, Batch))\n    batch_sample_cls.__module__ = __name__\n    globals()[cls_name] = batch_sample_cls\n    return cls\n\n\n@create_batch_sample_cls\n@dataclass\nclass Sample:\n    # NOTE: Up to Python 3.9, dataclasses don't support inheritance when there\n    # are both positional and default arguments present. See\n    # https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses\n    data_idx: int = None\n    data_valid: bool = None\n    label: Any = None\n\n    @classmethod\n    def get_batch_sample_class(cls):\n        return globals()[f\"{Batch.__name__}{cls.__name__}\"]\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionSample(Sample):\n    vision: Any = None\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionMaskSample(VisionSample):\n    mask: Any = None\n\n\ndef dataclass_as_dict(obj):\n    # replacement for dataclasses.asdict which makes a deepcopy of everything\n    if is_dataclass(obj):\n        return {f.name: dataclass_as_dict(getattr(obj, f.name)) for f in fields(obj)}\n    return obj\n\n\nclass DefaultOmnivoreCollator(Callable):\n    def __init__(\n        self,\n        output_key: str,\n        batch_kwargs=None,\n        batch_transforms=None,\n        input_batch_is_collated=False,\n    ) -> None:\n        self.output_key = output_key", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # NOTE: Up to Python 3.9, dataclasses don't support inheritance when there\n    # are both positional and default arguments present. See\n    # https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses\n    data_idx: int = None\n    data_valid: bool = None\n    label: Any = None\n\n    @classmethod\n    def get_batch_sample_class(cls):\n        return globals()[f\"{Batch.__name__}{cls.__name__}\"]\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionSample(Sample):\n    vision: Any = None\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionMaskSample(VisionSample):\n    mask: Any = None\n\n\ndef dataclass_as_dict(obj):\n    # replacement for dataclasses.asdict which makes a deepcopy of everything\n    if is_dataclass(obj):\n        return {f.name: dataclass_as_dict(getattr(obj, f.name)) for f in fields(obj)}\n    return obj\n\n\nclass DefaultOmnivoreCollator(Callable):\n    def __init__(\n        self,\n        output_key: str,\n        batch_kwargs=None,\n        batch_transforms=None,\n        input_batch_is_collated=False,\n    ) -> None:\n        self.output_key = output_key\n        self.batch_kwargs = batch_kwargs\n        self.batch_transforms = batch_transforms\n        self.input_batch_is_collated = input_batch_is_collated\n\n    @staticmethod\n    def collate_batch(batch_in):\n        batch = []\n        assert len(batch_in) > 0\n        for sample in batch_in:\n            assert isinstance(sample, Sample), f\"Found {type(sample)}\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n@create_batch_sample_cls\n@dataclass\nclass VisionSample(Sample):\n    vision: Any = None\n\n\n@create_batch_sample_cls\n@dataclass\nclass VisionMaskSample(VisionSample):\n    mask: Any = None\n\n\ndef dataclass_as_dict(obj):\n    # replacement for dataclasses.asdict which makes a deepcopy of everything\n    if is_dataclass(obj):\n        return {f.name: dataclass_as_dict(getattr(obj, f.name)) for f in fields(obj)}\n    return obj\n\n\nclass DefaultOmnivoreCollator(Callable):\n    def __init__(\n        self,\n        output_key: str,\n        batch_kwargs=None,\n        batch_transforms=None,\n        input_batch_is_collated=False,\n    ) -> None:\n        self.output_key = output_key\n        self.batch_kwargs = batch_kwargs\n        self.batch_transforms = batch_transforms\n        self.input_batch_is_collated = input_batch_is_collated\n\n    @staticmethod\n    def collate_batch(batch_in):\n        batch = []\n        assert len(batch_in) > 0\n        for sample in batch_in:\n            assert isinstance(sample, Sample), f\"Found {type(sample)}\"\n            batch.append(dataclass_as_dict(sample))\n        return batch, type(batch_in[0])\n\n    def __call__(self, batch_in):\n        if self.input_batch_is_collated:\n            batch = batch_in\n        else:\n            batch, sample_cls = self.collate_batch(batch_in)\n            batch_cls = sample_cls.get_batch_sample_class()\n            batch = batch_cls(**default_collate(batch))\n\nAST=Module(ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load)Constant)Name(Load)Name(Load))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load)Constant)Name(Load)Name(Load))FunctionDef(arguments(arg)If(Call(Name(Load)Name(Load))Return(DictComp(Attribute(Name(Load)Load)Call(Name(Load)Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Return(Name(Load)))ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))argargargConstantConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Constant)FunctionDef(arguments(arg)Assign(Name(Store)List(Load))Assert(Compare(Call(Name(Load)Name(Load))GtConstant))For(Name(Store)Name(Load)Assert(Call(Name(Load)Name(Load)Name(Load))JoinedStr(ConstantFormattedValue(Call(Name(Load)Name(Load)))))Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)))))Return(Tuple(Name(Load)Call(Name(Load)Subscript(Name(Load)ConstantLoad))Load))Name(Load))FunctionDef(arguments(argarg)If(Attribute(Name(Load)Load)Assign(Name(Store)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)keyword(Call(Name(Load)Name(Load)))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class VisionMaskSample(VisionSample):\n    mask: Any = None\n\n\ndef dataclass_as_dict(obj):\n    # replacement for dataclasses.asdict which makes a deepcopy of everything\n    if is_dataclass(obj):\n        return {f.name: dataclass_as_dict(getattr(obj, f.name)) for f in fields(obj)}\n    return obj\n\n\nclass DefaultOmnivoreCollator(Callable):\n    def __init__(\n        self,\n        output_key: str,\n        batch_kwargs=None,\n        batch_transforms=None,\n        input_batch_is_collated=False,\n    ) -> None:\n        self.output_key = output_key\n        self.batch_kwargs = batch_kwargs\n        self.batch_transforms = batch_transforms\n        self.input_batch_is_collated = input_batch_is_collated\n\n    @staticmethod\n    def collate_batch(batch_in):\n        batch = []\n        assert len(batch_in) > 0\n        for sample in batch_in:\n            assert isinstance(sample, Sample), f\"Found {type(sample)}\"\n            batch.append(dataclass_as_dict(sample))\n        return batch, type(batch_in[0])\n\n    def __call__(self, batch_in):\n        if self.input_batch_is_collated:\n            batch = batch_in\n        else:\n            batch, sample_cls = self.collate_batch(batch_in)\n            batch_cls = sample_cls.get_batch_sample_class()\n            batch = batch_cls(**default_collate(batch))\n        if self.batch_kwargs is not None:\n            batch_field_names = {f.name for f in fields(Batch)}\n            for key, value in self.batch_kwargs.items():\n                assert key in batch_field_names\n                setattr(batch, key, value)\n\n        if self.batch_transforms is not None:\n            for transform in self.batch_transforms:\n                batch = transform(batch)\n\n\nAST=Module(ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load)Constant))FunctionDef(arguments(arg)If(Call(Name(Load)Name(Load))Return(DictComp(Attribute(Name(Load)Load)Call(Name(Load)Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Return(Name(Load)))ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))argargargConstantConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Constant)FunctionDef(arguments(arg)Assign(Name(Store)List(Load))Assert(Compare(Call(Name(Load)Name(Load))GtConstant))For(Name(Store)Name(Load)Assert(Call(Name(Load)Name(Load)Name(Load))JoinedStr(ConstantFormattedValue(Call(Name(Load)Name(Load)))))Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)))))Return(Tuple(Name(Load)Call(Name(Load)Subscript(Name(Load)ConstantLoad))Load))Name(Load))FunctionDef(arguments(argarg)If(Attribute(Name(Load)Load)Assign(Name(Store)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)keyword(Call(Name(Load)Name(Load))))))If(Compare(Attribute(Name(Load)Load)IsNotConstant)Assign(Name(Store)SetComp(Attribute(Name(Load)Load)comprehension(Name(Store)Call(Name(Load)Name(Load)))))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load))Assert(Compare(Name(Load)InName(Load)))Expr(Call(Name(Load)Name(Load)Name(Load)Name(Load)))))If(Compare(Attribute(Name(Load)Load)IsNotConstant)For(Name(Store)Attribute(Name(Load)Load)Assign(Name(Store)Call(Name(Load)Name(Load))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass DefaultOmnivoreCollator(Callable):\n    def __init__(\n        self,\n        output_key: str,\n        batch_kwargs=None,\n        batch_transforms=None,\n        input_batch_is_collated=False,\n    ) -> None:\n        self.output_key = output_key\n        self.batch_kwargs = batch_kwargs\n        self.batch_transforms = batch_transforms\n        self.input_batch_is_collated = input_batch_is_collated\n\n    @staticmethod\n    def collate_batch(batch_in):\n        batch = []\n        assert len(batch_in) > 0\n        for sample in batch_in:\n            assert isinstance(sample, Sample), f\"Found {type(sample)}\"\n            batch.append(dataclass_as_dict(sample))\n        return batch, type(batch_in[0])\n\n    def __call__(self, batch_in):\n        if self.input_batch_is_collated:\n            batch = batch_in\n        else:\n            batch, sample_cls = self.collate_batch(batch_in)\n            batch_cls = sample_cls.get_batch_sample_class()\n            batch = batch_cls(**default_collate(batch))\n        if self.batch_kwargs is not None:\n            batch_field_names = {f.name for f in fields(Batch)}\n            for key, value in self.batch_kwargs.items():\n                assert key in batch_field_names\n                setattr(batch, key, value)\n\n        if self.batch_transforms is not None:\n            for transform in self.batch_transforms:\n                batch = transform(batch)\n\n        if self.output_key is not None:\n            batch = {self.output_key: batch}\n        return batch\n\n\nclass SampleListOmnivoreCollator(DefaultOmnivoreCollator):\n    @staticmethod\n    def collate_batch(batch_in):\n        \"\"\"\n        In this case each batch element is a list of Samples.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.batch_kwargs = batch_kwargs\n        self.batch_transforms = batch_transforms\n        self.input_batch_is_collated = input_batch_is_collated\n\n    @staticmethod\n    def collate_batch(batch_in):\n        batch = []\n        assert len(batch_in) > 0\n        for sample in batch_in:\n            assert isinstance(sample, Sample), f\"Found {type(sample)}\"\n            batch.append(dataclass_as_dict(sample))\n        return batch, type(batch_in[0])\n\n    def __call__(self, batch_in):\n        if self.input_batch_is_collated:\n            batch = batch_in\n        else:\n            batch, sample_cls = self.collate_batch(batch_in)\n            batch_cls = sample_cls.get_batch_sample_class()\n            batch = batch_cls(**default_collate(batch))\n        if self.batch_kwargs is not None:\n            batch_field_names = {f.name for f in fields(Batch)}\n            for key, value in self.batch_kwargs.items():\n                assert key in batch_field_names\n                setattr(batch, key, value)\n\n        if self.batch_transforms is not None:\n            for transform in self.batch_transforms:\n                batch = transform(batch)\n\n        if self.output_key is not None:\n            batch = {self.output_key: batch}\n        return batch\n\n\nclass SampleListOmnivoreCollator(DefaultOmnivoreCollator):\n    @staticmethod\n    def collate_batch(batch_in):\n        \"\"\"\n        In this case each batch element is a list of Samples.\n        This happens, for eg, when using replicate for MAE training where the same\n        sample is replicated N times and augmented those many times. Here we collate\n        the list into a single list.\n        \"\"\"\n        batch = []\n        assert len(batch_in) > 0\n        for samples in batch_in:\n            assert isinstance(samples, list), f\"Found {type(samples)}\"\n            assert all(\n                [isinstance(el, Sample) for el in samples]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 129, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            batch.append(dataclass_as_dict(sample))\n        return batch, type(batch_in[0])\n\n    def __call__(self, batch_in):\n        if self.input_batch_is_collated:\n            batch = batch_in\n        else:\n            batch, sample_cls = self.collate_batch(batch_in)\n            batch_cls = sample_cls.get_batch_sample_class()\n            batch = batch_cls(**default_collate(batch))\n        if self.batch_kwargs is not None:\n            batch_field_names = {f.name for f in fields(Batch)}\n            for key, value in self.batch_kwargs.items():\n                assert key in batch_field_names\n                setattr(batch, key, value)\n\n        if self.batch_transforms is not None:\n            for transform in self.batch_transforms:\n                batch = transform(batch)\n\n        if self.output_key is not None:\n            batch = {self.output_key: batch}\n        return batch\n\n\nclass SampleListOmnivoreCollator(DefaultOmnivoreCollator):\n    @staticmethod\n    def collate_batch(batch_in):\n        \"\"\"\n        In this case each batch element is a list of Samples.\n        This happens, for eg, when using replicate for MAE training where the same\n        sample is replicated N times and augmented those many times. Here we collate\n        the list into a single list.\n        \"\"\"\n        batch = []\n        assert len(batch_in) > 0\n        for samples in batch_in:\n            assert isinstance(samples, list), f\"Found {type(samples)}\"\n            assert all(\n                [isinstance(el, Sample) for el in samples]\n            ), f\"Found {[type(el) for el in samples]}\"\n            batch += [dataclass_as_dict(el) for el in samples]\n        sample_cls = type(batch_in[0][0])\n        return batch, sample_cls", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_85-129"}
{"title": "facebookresearch_omnivore-omnivision-data-api.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "api.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 129, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if self.batch_kwargs is not None:\n            batch_field_names = {f.name for f in fields(Batch)}\n            for key, value in self.batch_kwargs.items():\n                assert key in batch_field_names\n                setattr(batch, key, value)\n\n        if self.batch_transforms is not None:\n            for transform in self.batch_transforms:\n                batch = transform(batch)\n\n        if self.output_key is not None:\n            batch = {self.output_key: batch}\n        return batch\n\n\nclass SampleListOmnivoreCollator(DefaultOmnivoreCollator):\n    @staticmethod\n    def collate_batch(batch_in):\n        \"\"\"\n        In this case each batch element is a list of Samples.\n        This happens, for eg, when using replicate for MAE training where the same\n        sample is replicated N times and augmented those many times. Here we collate\n        the list into a single list.\n        \"\"\"\n        batch = []\n        assert len(batch_in) > 0\n        for samples in batch_in:\n            assert isinstance(samples, list), f\"Found {type(samples)}\"\n            assert all(\n                [isinstance(el, Sample) for el in samples]\n            ), f\"Found {[type(el) for el in samples]}\"\n            batch += [dataclass_as_dict(el) for el in samples]\n        sample_cls = type(batch_in[0][0])\n        return batch, sample_cls", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-api.py_95-129"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom typing import List, Optional, Sequence\n\nimport torch\n\nfrom omnivision.data.omni_dataset import OmniDataset\n\nfrom torch.utils.data import Dataset\n\n\nclass ConcatDataset(Dataset):\n    def __init__(\n        self,\n        datasets: List[OmniDataset],\n        max_steps: str,\n        batching_strategy: str = None,\n        repeat_factors: Optional[List[float]] = None,\n        dataset_weights=None,\n    ) -> None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom typing import List, Optional, Sequence\n\nimport torch\n\nfrom omnivision.data.omni_dataset import OmniDataset\n\nfrom torch.utils.data import Dataset\n\n\nclass ConcatDataset(Dataset):\n    def __init__(\n        self,\n        datasets: List[OmniDataset],\n        max_steps: str,\n        batching_strategy: str = None,\n        repeat_factors: Optional[List[float]] = None,\n        dataset_weights=None,\n    ) -> None:\n        \"\"\"\n        Creates an iterator that concatenates the list of datasets\n\n        Inputs\n        - dataloaders: List of dataloaders to concatenate\n        - epoch: Current epoch (used for shuffling)\n        - start_iter: Current iteration (unused for now)\n        - concat_iterator_params: Dict containing the following\n          - BATCHING_STRATEGY: String specifying batching strategy\n            - \"use_one\": each dataloader is picked individually. Batch contains output from only one of them.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom typing import List, Optional, Sequence\n\nimport torch\n\nfrom omnivision.data.omni_dataset import OmniDataset\n\nfrom torch.utils.data import Dataset\n\n\nclass ConcatDataset(Dataset):\n    def __init__(\n        self,\n        datasets: List[OmniDataset],\n        max_steps: str,\n        batching_strategy: str = None,\n        repeat_factors: Optional[List[float]] = None,\n        dataset_weights=None,\n    ) -> None:\n        \"\"\"\n        Creates an iterator that concatenates the list of datasets\n\n        Inputs\n        - dataloaders: List of dataloaders to concatenate\n        - epoch: Current epoch (used for shuffling)\n        - start_iter: Current iteration (unused for now)\n        - concat_iterator_params: Dict containing the following\n          - BATCHING_STRATEGY: String specifying batching strategy\n            - \"use_one\": each dataloader is picked individually. Batch contains output from only one of them.\n            - \"use_all\": outputs of all dataloaders per batch\n          - MAX_STEPS: String specifying how many steps to run for\n            - \"sum\": sum of the lengths of dataloaders. Typically used with \"use_one\" batching strategy\n            - \"max_dataset\": the dataloader with the maximum length. Typically used with \"use_all\".\n                             in this case, dataloaders with less samples can be replicated\n            MAX_STEPS can also be specified as a tuple of (string, multiplier) in which case the multiplier\n            scales the value specified via the string. This can control how likely it is for samples to get mixed.\n          - REPEAT_FACTORS: list of ints specifying how each dataset should be repeated\n                            -1 indicates that the dataset is padded to the maximum length of the underlying datasets\n                            Note that in this case maximum length is *AFTER* all the datasets have been replicated.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport logging\nfrom typing import List, Optional, Sequence\n\nimport torch\n\nfrom omnivision.data.omni_dataset import OmniDataset\n\nfrom torch.utils.data import Dataset\n\n\nclass ConcatDataset(Dataset):\n    def __init__(\n        self,\n        datasets: List[OmniDataset],\n        max_steps: str,\n        batching_strategy: str = None,\n        repeat_factors: Optional[List[float]] = None,\n        dataset_weights=None,\n    ) -> None:\n        \"\"\"\n        Creates an iterator that concatenates the list of datasets\n\n        Inputs\n        - dataloaders: List of dataloaders to concatenate\n        - epoch: Current epoch (used for shuffling)\n        - start_iter: Current iteration (unused for now)\n        - concat_iterator_params: Dict containing the following\n          - BATCHING_STRATEGY: String specifying batching strategy\n            - \"use_one\": each dataloader is picked individually. Batch contains output from only one of them.\n            - \"use_all\": outputs of all dataloaders per batch\n          - MAX_STEPS: String specifying how many steps to run for\n            - \"sum\": sum of the lengths of dataloaders. Typically used with \"use_one\" batching strategy\n            - \"max_dataset\": the dataloader with the maximum length. Typically used with \"use_all\".\n                             in this case, dataloaders with less samples can be replicated\n            MAX_STEPS can also be specified as a tuple of (string, multiplier) in which case the multiplier\n            scales the value specified via the string. This can control how likely it is for samples to get mixed.\n          - REPEAT_FACTORS: list of ints specifying how each dataset should be repeated\n                            -1 indicates that the dataset is padded to the maximum length of the underlying datasets\n                            Note that in this case maximum length is *AFTER* all the datasets have been replicated.\n        \"\"\"\n        super().__init__()\n        assert isinstance(datasets, Sequence)\n        self.datasets = datasets\n        num_data_sources = len(self.datasets)\n        self.batching_strategy = batching_strategy or \"use_one\"\n        self.max_steps = max_steps\n        self.repeat_factors = repeat_factors or [1 for _ in range(len(self.datasets))]\n        assert len(self.repeat_factors) == num_data_sources\n\n\nAST=Module(Import(alias)ImportFrom(aliasaliasalias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Subscript(Name(Load)Name(Load)Load)Load))argConstantConstantConstant)Expr(Constant)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Call(Name(Load)Name(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Name(Store)Call(Name(Load)Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)BoolOp(OrName(Load)Constant))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)BoolOp(OrName(Load)ListComp(Constantcomprehension(Name(Store)Call(Name(Load)Call(Name(Load)Attribute(Name(Load)Load)))))))Assert(Compare(Call(Name(Load)Attribute(Name(Load)Load))EqName(Load)))Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass ConcatDataset(Dataset):\n    def __init__(\n        self,\n        datasets: List[OmniDataset],\n        max_steps: str,\n        batching_strategy: str = None,\n        repeat_factors: Optional[List[float]] = None,\n        dataset_weights=None,\n    ) -> None:\n        \"\"\"\n        Creates an iterator that concatenates the list of datasets\n\n        Inputs\n        - dataloaders: List of dataloaders to concatenate\n        - epoch: Current epoch (used for shuffling)\n        - start_iter: Current iteration (unused for now)\n        - concat_iterator_params: Dict containing the following\n          - BATCHING_STRATEGY: String specifying batching strategy\n            - \"use_one\": each dataloader is picked individually. Batch contains output from only one of them.\n            - \"use_all\": outputs of all dataloaders per batch\n          - MAX_STEPS: String specifying how many steps to run for\n            - \"sum\": sum of the lengths of dataloaders. Typically used with \"use_one\" batching strategy\n            - \"max_dataset\": the dataloader with the maximum length. Typically used with \"use_all\".\n                             in this case, dataloaders with less samples can be replicated\n            MAX_STEPS can also be specified as a tuple of (string, multiplier) in which case the multiplier\n            scales the value specified via the string. This can control how likely it is for samples to get mixed.\n          - REPEAT_FACTORS: list of ints specifying how each dataset should be repeated\n                            -1 indicates that the dataset is padded to the maximum length of the underlying datasets\n                            Note that in this case maximum length is *AFTER* all the datasets have been replicated.\n        \"\"\"\n        super().__init__()\n        assert isinstance(datasets, Sequence)\n        self.datasets = datasets\n        num_data_sources = len(self.datasets)\n        self.batching_strategy = batching_strategy or \"use_one\"\n        self.max_steps = max_steps\n        self.repeat_factors = repeat_factors or [1 for _ in range(len(self.datasets))]\n        assert len(self.repeat_factors) == num_data_sources\n\n        self.dataset_weights = dataset_weights\n        if self.dataset_weights is not None:\n            assert sum(self.dataset_weights) == 1\n\n    def get_loader(self, **kwargs):\n        return ConcatIterator(self, **kwargs)\n\n\nclass ConcatIterator:\n    def __init__(self, concat_dataset, **kwargs) -> None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        Creates an iterator that concatenates the list of datasets\n\n        Inputs\n        - dataloaders: List of dataloaders to concatenate\n        - epoch: Current epoch (used for shuffling)\n        - start_iter: Current iteration (unused for now)\n        - concat_iterator_params: Dict containing the following\n          - BATCHING_STRATEGY: String specifying batching strategy\n            - \"use_one\": each dataloader is picked individually. Batch contains output from only one of them.\n            - \"use_all\": outputs of all dataloaders per batch\n          - MAX_STEPS: String specifying how many steps to run for\n            - \"sum\": sum of the lengths of dataloaders. Typically used with \"use_one\" batching strategy\n            - \"max_dataset\": the dataloader with the maximum length. Typically used with \"use_all\".\n                             in this case, dataloaders with less samples can be replicated\n            MAX_STEPS can also be specified as a tuple of (string, multiplier) in which case the multiplier\n            scales the value specified via the string. This can control how likely it is for samples to get mixed.\n          - REPEAT_FACTORS: list of ints specifying how each dataset should be repeated\n                            -1 indicates that the dataset is padded to the maximum length of the underlying datasets\n                            Note that in this case maximum length is *AFTER* all the datasets have been replicated.\n        \"\"\"\n        super().__init__()\n        assert isinstance(datasets, Sequence)\n        self.datasets = datasets\n        num_data_sources = len(self.datasets)\n        self.batching_strategy = batching_strategy or \"use_one\"\n        self.max_steps = max_steps\n        self.repeat_factors = repeat_factors or [1 for _ in range(len(self.datasets))]\n        assert len(self.repeat_factors) == num_data_sources\n\n        self.dataset_weights = dataset_weights\n        if self.dataset_weights is not None:\n            assert sum(self.dataset_weights) == 1\n\n    def get_loader(self, **kwargs):\n        return ConcatIterator(self, **kwargs)\n\n\nclass ConcatIterator:\n    def __init__(self, concat_dataset, **kwargs) -> None:\n        assert \"epoch\" in kwargs\n        epoch = kwargs[\"epoch\"]\n        # FIXME: this will create iters upon init instead of when iter(self) is called\n        dataloaders = [x.get_loader(**kwargs) for x in concat_dataset.datasets]\n        num_data_sources = len(dataloaders)\n        self.iterators = [iter(x) for x in dataloaders]\n        self.step_counter = 0\n        self.dataloaders = dataloaders\n        self.per_src_step_counter = [0 for _ in range(num_data_sources)]\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            - \"use_all\": outputs of all dataloaders per batch\n          - MAX_STEPS: String specifying how many steps to run for\n            - \"sum\": sum of the lengths of dataloaders. Typically used with \"use_one\" batching strategy\n            - \"max_dataset\": the dataloader with the maximum length. Typically used with \"use_all\".\n                             in this case, dataloaders with less samples can be replicated\n            MAX_STEPS can also be specified as a tuple of (string, multiplier) in which case the multiplier\n            scales the value specified via the string. This can control how likely it is for samples to get mixed.\n          - REPEAT_FACTORS: list of ints specifying how each dataset should be repeated\n                            -1 indicates that the dataset is padded to the maximum length of the underlying datasets\n                            Note that in this case maximum length is *AFTER* all the datasets have been replicated.\n        \"\"\"\n        super().__init__()\n        assert isinstance(datasets, Sequence)\n        self.datasets = datasets\n        num_data_sources = len(self.datasets)\n        self.batching_strategy = batching_strategy or \"use_one\"\n        self.max_steps = max_steps\n        self.repeat_factors = repeat_factors or [1 for _ in range(len(self.datasets))]\n        assert len(self.repeat_factors) == num_data_sources\n\n        self.dataset_weights = dataset_weights\n        if self.dataset_weights is not None:\n            assert sum(self.dataset_weights) == 1\n\n    def get_loader(self, **kwargs):\n        return ConcatIterator(self, **kwargs)\n\n\nclass ConcatIterator:\n    def __init__(self, concat_dataset, **kwargs) -> None:\n        assert \"epoch\" in kwargs\n        epoch = kwargs[\"epoch\"]\n        # FIXME: this will create iters upon init instead of when iter(self) is called\n        dataloaders = [x.get_loader(**kwargs) for x in concat_dataset.datasets]\n        num_data_sources = len(dataloaders)\n        self.iterators = [iter(x) for x in dataloaders]\n        self.step_counter = 0\n        self.dataloaders = dataloaders\n        self.per_src_step_counter = [0 for _ in range(num_data_sources)]\n\n        g = torch.Generator()\n        g.manual_seed(epoch)\n\n        iterator_lens = [len(x) for x in self.iterators]\n\n        # adjust iterator lengths based on repetitions\n        for idx in range(len(concat_dataset.repeat_factors)):\n            # assert isinstance(\n            #     concat_dataset.repeat_factors[idx], int\n            # ), f\"Only integer repeat factors are allowed. Found {type(concat_dataset.repeat_factors[idx])}\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        super().__init__()\n        assert isinstance(datasets, Sequence)\n        self.datasets = datasets\n        num_data_sources = len(self.datasets)\n        self.batching_strategy = batching_strategy or \"use_one\"\n        self.max_steps = max_steps\n        self.repeat_factors = repeat_factors or [1 for _ in range(len(self.datasets))]\n        assert len(self.repeat_factors) == num_data_sources\n\n        self.dataset_weights = dataset_weights\n        if self.dataset_weights is not None:\n            assert sum(self.dataset_weights) == 1\n\n    def get_loader(self, **kwargs):\n        return ConcatIterator(self, **kwargs)\n\n\nclass ConcatIterator:\n    def __init__(self, concat_dataset, **kwargs) -> None:\n        assert \"epoch\" in kwargs\n        epoch = kwargs[\"epoch\"]\n        # FIXME: this will create iters upon init instead of when iter(self) is called\n        dataloaders = [x.get_loader(**kwargs) for x in concat_dataset.datasets]\n        num_data_sources = len(dataloaders)\n        self.iterators = [iter(x) for x in dataloaders]\n        self.step_counter = 0\n        self.dataloaders = dataloaders\n        self.per_src_step_counter = [0 for _ in range(num_data_sources)]\n\n        g = torch.Generator()\n        g.manual_seed(epoch)\n\n        iterator_lens = [len(x) for x in self.iterators]\n\n        # adjust iterator lengths based on repetitions\n        for idx in range(len(concat_dataset.repeat_factors)):\n            # assert isinstance(\n            #     concat_dataset.repeat_factors[idx], int\n            # ), f\"Only integer repeat factors are allowed. Found {type(concat_dataset.repeat_factors[idx])}\"\n            if concat_dataset.repeat_factors[idx] > 0:\n                iterator_lens[idx] *= concat_dataset.repeat_factors[idx]\n                iterator_lens[idx] = int(iterator_lens[idx])\n            else:\n                assert (\n                    concat_dataset.repeat_factors[idx] == -1\n                ), \"repetition factor must be > 0 or -1\"\n\n        # repetition = -1\n        max_dataset_len = max(iterator_lens)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.dataset_weights = dataset_weights\n        if self.dataset_weights is not None:\n            assert sum(self.dataset_weights) == 1\n\n    def get_loader(self, **kwargs):\n        return ConcatIterator(self, **kwargs)\n\n\nclass ConcatIterator:\n    def __init__(self, concat_dataset, **kwargs) -> None:\n        assert \"epoch\" in kwargs\n        epoch = kwargs[\"epoch\"]\n        # FIXME: this will create iters upon init instead of when iter(self) is called\n        dataloaders = [x.get_loader(**kwargs) for x in concat_dataset.datasets]\n        num_data_sources = len(dataloaders)\n        self.iterators = [iter(x) for x in dataloaders]\n        self.step_counter = 0\n        self.dataloaders = dataloaders\n        self.per_src_step_counter = [0 for _ in range(num_data_sources)]\n\n        g = torch.Generator()\n        g.manual_seed(epoch)\n\n        iterator_lens = [len(x) for x in self.iterators]\n\n        # adjust iterator lengths based on repetitions\n        for idx in range(len(concat_dataset.repeat_factors)):\n            # assert isinstance(\n            #     concat_dataset.repeat_factors[idx], int\n            # ), f\"Only integer repeat factors are allowed. Found {type(concat_dataset.repeat_factors[idx])}\"\n            if concat_dataset.repeat_factors[idx] > 0:\n                iterator_lens[idx] *= concat_dataset.repeat_factors[idx]\n                iterator_lens[idx] = int(iterator_lens[idx])\n            else:\n                assert (\n                    concat_dataset.repeat_factors[idx] == -1\n                ), \"repetition factor must be > 0 or -1\"\n\n        # repetition = -1\n        max_dataset_len = max(iterator_lens)\n        for idx in range(len(concat_dataset.repeat_factors)):\n            if concat_dataset.repeat_factors[idx] == -1:\n                iterator_lens[idx] = max_dataset_len\n\n        self.iterator_lens = iterator_lens\n\n        if isinstance(concat_dataset.max_steps, (list, tuple)):\n            assert len(concat_dataset.max_steps) == 2\n            max_steps_method = concat_dataset.max_steps[0]\n            max_steps_mul = concat_dataset.max_steps[1]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert \"epoch\" in kwargs\n        epoch = kwargs[\"epoch\"]\n        # FIXME: this will create iters upon init instead of when iter(self) is called\n        dataloaders = [x.get_loader(**kwargs) for x in concat_dataset.datasets]\n        num_data_sources = len(dataloaders)\n        self.iterators = [iter(x) for x in dataloaders]\n        self.step_counter = 0\n        self.dataloaders = dataloaders\n        self.per_src_step_counter = [0 for _ in range(num_data_sources)]\n\n        g = torch.Generator()\n        g.manual_seed(epoch)\n\n        iterator_lens = [len(x) for x in self.iterators]\n\n        # adjust iterator lengths based on repetitions\n        for idx in range(len(concat_dataset.repeat_factors)):\n            # assert isinstance(\n            #     concat_dataset.repeat_factors[idx], int\n            # ), f\"Only integer repeat factors are allowed. Found {type(concat_dataset.repeat_factors[idx])}\"\n            if concat_dataset.repeat_factors[idx] > 0:\n                iterator_lens[idx] *= concat_dataset.repeat_factors[idx]\n                iterator_lens[idx] = int(iterator_lens[idx])\n            else:\n                assert (\n                    concat_dataset.repeat_factors[idx] == -1\n                ), \"repetition factor must be > 0 or -1\"\n\n        # repetition = -1\n        max_dataset_len = max(iterator_lens)\n        for idx in range(len(concat_dataset.repeat_factors)):\n            if concat_dataset.repeat_factors[idx] == -1:\n                iterator_lens[idx] = max_dataset_len\n\n        self.iterator_lens = iterator_lens\n\n        if isinstance(concat_dataset.max_steps, (list, tuple)):\n            assert len(concat_dataset.max_steps) == 2\n            max_steps_method = concat_dataset.max_steps[0]\n            max_steps_mul = concat_dataset.max_steps[1]\n        else:\n            max_steps_method = concat_dataset.max_steps\n            max_steps_mul = 1\n        if max_steps_method == \"sum\":\n            self.max_steps = int(sum(iterator_lens) * max_steps_mul)\n        elif max_steps_method == \"max_dataset\":\n            self.max_steps = int(max(iterator_lens) * max_steps_mul)\n        else:\n            raise ValueError(f\"max_steps_method: {max_steps_method} is not supported\")\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        g = torch.Generator()\n        g.manual_seed(epoch)\n\n        iterator_lens = [len(x) for x in self.iterators]\n\n        # adjust iterator lengths based on repetitions\n        for idx in range(len(concat_dataset.repeat_factors)):\n            # assert isinstance(\n            #     concat_dataset.repeat_factors[idx], int\n            # ), f\"Only integer repeat factors are allowed. Found {type(concat_dataset.repeat_factors[idx])}\"\n            if concat_dataset.repeat_factors[idx] > 0:\n                iterator_lens[idx] *= concat_dataset.repeat_factors[idx]\n                iterator_lens[idx] = int(iterator_lens[idx])\n            else:\n                assert (\n                    concat_dataset.repeat_factors[idx] == -1\n                ), \"repetition factor must be > 0 or -1\"\n\n        # repetition = -1\n        max_dataset_len = max(iterator_lens)\n        for idx in range(len(concat_dataset.repeat_factors)):\n            if concat_dataset.repeat_factors[idx] == -1:\n                iterator_lens[idx] = max_dataset_len\n\n        self.iterator_lens = iterator_lens\n\n        if isinstance(concat_dataset.max_steps, (list, tuple)):\n            assert len(concat_dataset.max_steps) == 2\n            max_steps_method = concat_dataset.max_steps[0]\n            max_steps_mul = concat_dataset.max_steps[1]\n        else:\n            max_steps_method = concat_dataset.max_steps\n            max_steps_mul = 1\n        if max_steps_method == \"sum\":\n            self.max_steps = int(sum(iterator_lens) * max_steps_mul)\n        elif max_steps_method == \"max_dataset\":\n            self.max_steps = int(max(iterator_lens) * max_steps_mul)\n        else:\n            raise ValueError(f\"max_steps_method: {max_steps_method} is not supported\")\n\n        # source_to_use is a binary array of shape data_sources x steps\n        # source_to_use[a, b] = 1 indicates that\n        # data source `a` will be used for creating a batch at step `b`\n        # this array defines how our batches are created (using one data source at a time or all or subset)\n\n        self.source_to_use = torch.zeros(\n            (num_data_sources, self.max_steps), dtype=torch.bool\n        )\n\n        if concat_dataset.batching_strategy == \"use_all\":", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if concat_dataset.repeat_factors[idx] > 0:\n                iterator_lens[idx] *= concat_dataset.repeat_factors[idx]\n                iterator_lens[idx] = int(iterator_lens[idx])\n            else:\n                assert (\n                    concat_dataset.repeat_factors[idx] == -1\n                ), \"repetition factor must be > 0 or -1\"\n\n        # repetition = -1\n        max_dataset_len = max(iterator_lens)\n        for idx in range(len(concat_dataset.repeat_factors)):\n            if concat_dataset.repeat_factors[idx] == -1:\n                iterator_lens[idx] = max_dataset_len\n\n        self.iterator_lens = iterator_lens\n\n        if isinstance(concat_dataset.max_steps, (list, tuple)):\n            assert len(concat_dataset.max_steps) == 2\n            max_steps_method = concat_dataset.max_steps[0]\n            max_steps_mul = concat_dataset.max_steps[1]\n        else:\n            max_steps_method = concat_dataset.max_steps\n            max_steps_mul = 1\n        if max_steps_method == \"sum\":\n            self.max_steps = int(sum(iterator_lens) * max_steps_mul)\n        elif max_steps_method == \"max_dataset\":\n            self.max_steps = int(max(iterator_lens) * max_steps_mul)\n        else:\n            raise ValueError(f\"max_steps_method: {max_steps_method} is not supported\")\n\n        # source_to_use is a binary array of shape data_sources x steps\n        # source_to_use[a, b] = 1 indicates that\n        # data source `a` will be used for creating a batch at step `b`\n        # this array defines how our batches are created (using one data source at a time or all or subset)\n\n        self.source_to_use = torch.zeros(\n            (num_data_sources, self.max_steps), dtype=torch.bool\n        )\n\n        if concat_dataset.batching_strategy == \"use_all\":\n            # we first assign to every index one data source to make sure every step\n            # has at least one data source\n            # then we assign remaining data source ids to remaining places randomly\n            for idx in range(num_data_sources):\n                assert iterator_lens[idx] <= self.max_steps\n            inds = torch.tensor(\n                [idx for i in range(num_data_sources) for idx in [i] * iterator_lens[i]]\n            )\n            assert len(inds) == sum(iterator_lens)\n            assert len(inds) >= self.max_steps", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for idx in range(len(concat_dataset.repeat_factors)):\n            if concat_dataset.repeat_factors[idx] == -1:\n                iterator_lens[idx] = max_dataset_len\n\n        self.iterator_lens = iterator_lens\n\n        if isinstance(concat_dataset.max_steps, (list, tuple)):\n            assert len(concat_dataset.max_steps) == 2\n            max_steps_method = concat_dataset.max_steps[0]\n            max_steps_mul = concat_dataset.max_steps[1]\n        else:\n            max_steps_method = concat_dataset.max_steps\n            max_steps_mul = 1\n        if max_steps_method == \"sum\":\n            self.max_steps = int(sum(iterator_lens) * max_steps_mul)\n        elif max_steps_method == \"max_dataset\":\n            self.max_steps = int(max(iterator_lens) * max_steps_mul)\n        else:\n            raise ValueError(f\"max_steps_method: {max_steps_method} is not supported\")\n\n        # source_to_use is a binary array of shape data_sources x steps\n        # source_to_use[a, b] = 1 indicates that\n        # data source `a` will be used for creating a batch at step `b`\n        # this array defines how our batches are created (using one data source at a time or all or subset)\n\n        self.source_to_use = torch.zeros(\n            (num_data_sources, self.max_steps), dtype=torch.bool\n        )\n\n        if concat_dataset.batching_strategy == \"use_all\":\n            # we first assign to every index one data source to make sure every step\n            # has at least one data source\n            # then we assign remaining data source ids to remaining places randomly\n            for idx in range(num_data_sources):\n                assert iterator_lens[idx] <= self.max_steps\n            inds = torch.tensor(\n                [idx for i in range(num_data_sources) for idx in [i] * iterator_lens[i]]\n            )\n            assert len(inds) == sum(iterator_lens)\n            assert len(inds) >= self.max_steps\n            inds = inds[torch.randperm(len(inds), generator=g)]\n            inds_init = inds[: self.max_steps]\n            for idx in range(num_data_sources):\n                inds_init_ds = inds_init == idx\n                self.source_to_use[idx, inds_init_ds] = 1\n                # get unused indices\n                inds_rem_ds = (self.source_to_use[idx] == 0).nonzero()\n                # permute the indices and pick as many as we need\n                rem_ds_size = iterator_lens[idx] - sum(inds_init_ds)\n                inds_rem_ds = inds_rem_ds[", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        else:\n            max_steps_method = concat_dataset.max_steps\n            max_steps_mul = 1\n        if max_steps_method == \"sum\":\n            self.max_steps = int(sum(iterator_lens) * max_steps_mul)\n        elif max_steps_method == \"max_dataset\":\n            self.max_steps = int(max(iterator_lens) * max_steps_mul)\n        else:\n            raise ValueError(f\"max_steps_method: {max_steps_method} is not supported\")\n\n        # source_to_use is a binary array of shape data_sources x steps\n        # source_to_use[a, b] = 1 indicates that\n        # data source `a` will be used for creating a batch at step `b`\n        # this array defines how our batches are created (using one data source at a time or all or subset)\n\n        self.source_to_use = torch.zeros(\n            (num_data_sources, self.max_steps), dtype=torch.bool\n        )\n\n        if concat_dataset.batching_strategy == \"use_all\":\n            # we first assign to every index one data source to make sure every step\n            # has at least one data source\n            # then we assign remaining data source ids to remaining places randomly\n            for idx in range(num_data_sources):\n                assert iterator_lens[idx] <= self.max_steps\n            inds = torch.tensor(\n                [idx for i in range(num_data_sources) for idx in [i] * iterator_lens[i]]\n            )\n            assert len(inds) == sum(iterator_lens)\n            assert len(inds) >= self.max_steps\n            inds = inds[torch.randperm(len(inds), generator=g)]\n            inds_init = inds[: self.max_steps]\n            for idx in range(num_data_sources):\n                inds_init_ds = inds_init == idx\n                self.source_to_use[idx, inds_init_ds] = 1\n                # get unused indices\n                inds_rem_ds = (self.source_to_use[idx] == 0).nonzero()\n                # permute the indices and pick as many as we need\n                rem_ds_size = iterator_lens[idx] - sum(inds_init_ds)\n                inds_rem_ds = inds_rem_ds[\n                    torch.randperm(len(inds_rem_ds), generator=g)\n                ][:rem_ds_size]\n                self.source_to_use[idx, inds_rem_ds] = 1\n                assert sum(self.source_to_use[idx]) == iterator_lens[idx]\n\n            # there should be no non-empty steps\n            assert sum(self.source_to_use.sum(dim=0) == 0) == 0\n        elif concat_dataset.batching_strategy == \"use_one\":\n            if max_steps_method != \"sum\" and max_steps_mul != 1:\n                raise NotImplementedError()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # source_to_use is a binary array of shape data_sources x steps\n        # source_to_use[a, b] = 1 indicates that\n        # data source `a` will be used for creating a batch at step `b`\n        # this array defines how our batches are created (using one data source at a time or all or subset)\n\n        self.source_to_use = torch.zeros(\n            (num_data_sources, self.max_steps), dtype=torch.bool\n        )\n\n        if concat_dataset.batching_strategy == \"use_all\":\n            # we first assign to every index one data source to make sure every step\n            # has at least one data source\n            # then we assign remaining data source ids to remaining places randomly\n            for idx in range(num_data_sources):\n                assert iterator_lens[idx] <= self.max_steps\n            inds = torch.tensor(\n                [idx for i in range(num_data_sources) for idx in [i] * iterator_lens[i]]\n            )\n            assert len(inds) == sum(iterator_lens)\n            assert len(inds) >= self.max_steps\n            inds = inds[torch.randperm(len(inds), generator=g)]\n            inds_init = inds[: self.max_steps]\n            for idx in range(num_data_sources):\n                inds_init_ds = inds_init == idx\n                self.source_to_use[idx, inds_init_ds] = 1\n                # get unused indices\n                inds_rem_ds = (self.source_to_use[idx] == 0).nonzero()\n                # permute the indices and pick as many as we need\n                rem_ds_size = iterator_lens[idx] - sum(inds_init_ds)\n                inds_rem_ds = inds_rem_ds[\n                    torch.randperm(len(inds_rem_ds), generator=g)\n                ][:rem_ds_size]\n                self.source_to_use[idx, inds_rem_ds] = 1\n                assert sum(self.source_to_use[idx]) == iterator_lens[idx]\n\n            # there should be no non-empty steps\n            assert sum(self.source_to_use.sum(dim=0) == 0) == 0\n        elif concat_dataset.batching_strategy == \"use_one\":\n            if max_steps_method != \"sum\" and max_steps_mul != 1:\n                raise NotImplementedError()\n\n            indices = []\n            for idx in range(num_data_sources):\n                indices.append(torch.ones(iterator_lens[idx], dtype=torch.int64) * idx)\n            indices = torch.cat(indices)\n            shuffle_indices = torch.randperm(len(indices), generator=g)\n            indices = indices[shuffle_indices]\n            assert (\n                len(indices) == self.max_steps\n            ), f\"Length of Indices {len(indices)} != steps {self.max_steps}\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            # we first assign to every index one data source to make sure every step\n            # has at least one data source\n            # then we assign remaining data source ids to remaining places randomly\n            for idx in range(num_data_sources):\n                assert iterator_lens[idx] <= self.max_steps\n            inds = torch.tensor(\n                [idx for i in range(num_data_sources) for idx in [i] * iterator_lens[i]]\n            )\n            assert len(inds) == sum(iterator_lens)\n            assert len(inds) >= self.max_steps\n            inds = inds[torch.randperm(len(inds), generator=g)]\n            inds_init = inds[: self.max_steps]\n            for idx in range(num_data_sources):\n                inds_init_ds = inds_init == idx\n                self.source_to_use[idx, inds_init_ds] = 1\n                # get unused indices\n                inds_rem_ds = (self.source_to_use[idx] == 0).nonzero()\n                # permute the indices and pick as many as we need\n                rem_ds_size = iterator_lens[idx] - sum(inds_init_ds)\n                inds_rem_ds = inds_rem_ds[\n                    torch.randperm(len(inds_rem_ds), generator=g)\n                ][:rem_ds_size]\n                self.source_to_use[idx, inds_rem_ds] = 1\n                assert sum(self.source_to_use[idx]) == iterator_lens[idx]\n\n            # there should be no non-empty steps\n            assert sum(self.source_to_use.sum(dim=0) == 0) == 0\n        elif concat_dataset.batching_strategy == \"use_one\":\n            if max_steps_method != \"sum\" and max_steps_mul != 1:\n                raise NotImplementedError()\n\n            indices = []\n            for idx in range(num_data_sources):\n                indices.append(torch.ones(iterator_lens[idx], dtype=torch.int64) * idx)\n            indices = torch.cat(indices)\n            shuffle_indices = torch.randperm(len(indices), generator=g)\n            indices = indices[shuffle_indices]\n            assert (\n                len(indices) == self.max_steps\n            ), f\"Length of Indices {len(indices)} != steps {self.max_steps}\"\n\n            for idx in range(num_data_sources):\n                sel_idx = torch.where(indices == idx)[0]\n                self.source_to_use[idx, sel_idx] = 1\n\n            # sanity check\n            # steps should have source_to_use = 1 at every step, and it should be set for only one dataset\n            assert torch.all(\n                self.source_to_use.sum(dim=0)\n            ).item(), \"use_one logic is incorrect\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            inds = inds[torch.randperm(len(inds), generator=g)]\n            inds_init = inds[: self.max_steps]\n            for idx in range(num_data_sources):\n                inds_init_ds = inds_init == idx\n                self.source_to_use[idx, inds_init_ds] = 1\n                # get unused indices\n                inds_rem_ds = (self.source_to_use[idx] == 0).nonzero()\n                # permute the indices and pick as many as we need\n                rem_ds_size = iterator_lens[idx] - sum(inds_init_ds)\n                inds_rem_ds = inds_rem_ds[\n                    torch.randperm(len(inds_rem_ds), generator=g)\n                ][:rem_ds_size]\n                self.source_to_use[idx, inds_rem_ds] = 1\n                assert sum(self.source_to_use[idx]) == iterator_lens[idx]\n\n            # there should be no non-empty steps\n            assert sum(self.source_to_use.sum(dim=0) == 0) == 0\n        elif concat_dataset.batching_strategy == \"use_one\":\n            if max_steps_method != \"sum\" and max_steps_mul != 1:\n                raise NotImplementedError()\n\n            indices = []\n            for idx in range(num_data_sources):\n                indices.append(torch.ones(iterator_lens[idx], dtype=torch.int64) * idx)\n            indices = torch.cat(indices)\n            shuffle_indices = torch.randperm(len(indices), generator=g)\n            indices = indices[shuffle_indices]\n            assert (\n                len(indices) == self.max_steps\n            ), f\"Length of Indices {len(indices)} != steps {self.max_steps}\"\n\n            for idx in range(num_data_sources):\n                sel_idx = torch.where(indices == idx)[0]\n                self.source_to_use[idx, sel_idx] = 1\n\n            # sanity check\n            # steps should have source_to_use = 1 at every step, and it should be set for only one dataset\n            assert torch.all(\n                self.source_to_use.sum(dim=0)\n            ).item(), \"use_one logic is incorrect\"\n            assert self.source_to_use.sum().item() == self.max_steps\n\n        # sanity check & logging\n        logging.info(\n            f\"Created a ConcatIterator with batching_strategy={concat_dataset.batching_strategy} and steps {concat_dataset.max_steps}. Steps per source:\"\n        )\n        for idx in range(num_data_sources):\n            assert (\n                self.source_to_use[idx].sum().item() == iterator_lens[idx]\n            ), \"source_to_use logic is incorrect\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    torch.randperm(len(inds_rem_ds), generator=g)\n                ][:rem_ds_size]\n                self.source_to_use[idx, inds_rem_ds] = 1\n                assert sum(self.source_to_use[idx]) == iterator_lens[idx]\n\n            # there should be no non-empty steps\n            assert sum(self.source_to_use.sum(dim=0) == 0) == 0\n        elif concat_dataset.batching_strategy == \"use_one\":\n            if max_steps_method != \"sum\" and max_steps_mul != 1:\n                raise NotImplementedError()\n\n            indices = []\n            for idx in range(num_data_sources):\n                indices.append(torch.ones(iterator_lens[idx], dtype=torch.int64) * idx)\n            indices = torch.cat(indices)\n            shuffle_indices = torch.randperm(len(indices), generator=g)\n            indices = indices[shuffle_indices]\n            assert (\n                len(indices) == self.max_steps\n            ), f\"Length of Indices {len(indices)} != steps {self.max_steps}\"\n\n            for idx in range(num_data_sources):\n                sel_idx = torch.where(indices == idx)[0]\n                self.source_to_use[idx, sel_idx] = 1\n\n            # sanity check\n            # steps should have source_to_use = 1 at every step, and it should be set for only one dataset\n            assert torch.all(\n                self.source_to_use.sum(dim=0)\n            ).item(), \"use_one logic is incorrect\"\n            assert self.source_to_use.sum().item() == self.max_steps\n\n        # sanity check & logging\n        logging.info(\n            f\"Created a ConcatIterator with batching_strategy={concat_dataset.batching_strategy} and steps {concat_dataset.max_steps}. Steps per source:\"\n        )\n        for idx in range(num_data_sources):\n            assert (\n                self.source_to_use[idx].sum().item() == iterator_lens[idx]\n            ), \"source_to_use logic is incorrect\"\n            if concat_dataset.repeat_factors[idx] == -1:\n                # make sure dataset is padded to max length\n                assert self.source_to_use[idx].sum().item() == self.max_steps\n            else:\n                assert self.source_to_use[idx].sum().item() == int(\n                    len(dataloaders[idx]) * concat_dataset.repeat_factors[idx]\n                )\n            logging.info(\n                f\"Dataset {idx}; len {iterator_lens[idx]}; Orig len {len(dataloaders[idx])}\"\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n            indices = []\n            for idx in range(num_data_sources):\n                indices.append(torch.ones(iterator_lens[idx], dtype=torch.int64) * idx)\n            indices = torch.cat(indices)\n            shuffle_indices = torch.randperm(len(indices), generator=g)\n            indices = indices[shuffle_indices]\n            assert (\n                len(indices) == self.max_steps\n            ), f\"Length of Indices {len(indices)} != steps {self.max_steps}\"\n\n            for idx in range(num_data_sources):\n                sel_idx = torch.where(indices == idx)[0]\n                self.source_to_use[idx, sel_idx] = 1\n\n            # sanity check\n            # steps should have source_to_use = 1 at every step, and it should be set for only one dataset\n            assert torch.all(\n                self.source_to_use.sum(dim=0)\n            ).item(), \"use_one logic is incorrect\"\n            assert self.source_to_use.sum().item() == self.max_steps\n\n        # sanity check & logging\n        logging.info(\n            f\"Created a ConcatIterator with batching_strategy={concat_dataset.batching_strategy} and steps {concat_dataset.max_steps}. Steps per source:\"\n        )\n        for idx in range(num_data_sources):\n            assert (\n                self.source_to_use[idx].sum().item() == iterator_lens[idx]\n            ), \"source_to_use logic is incorrect\"\n            if concat_dataset.repeat_factors[idx] == -1:\n                # make sure dataset is padded to max length\n                assert self.source_to_use[idx].sum().item() == self.max_steps\n            else:\n                assert self.source_to_use[idx].sum().item() == int(\n                    len(dataloaders[idx]) * concat_dataset.repeat_factors[idx]\n                )\n            logging.info(\n                f\"Dataset {idx}; len {iterator_lens[idx]}; Orig len {len(dataloaders[idx])}\"\n            )\n\n    def __iter__(self):\n        return self\n\n    def get_sample(self):\n        sample = {}\n        \"\"\"\n        grad_accum_sample = False\n        \"\"\"\n        for idx in range(len(self.iterators)):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n            for idx in range(num_data_sources):\n                sel_idx = torch.where(indices == idx)[0]\n                self.source_to_use[idx, sel_idx] = 1\n\n            # sanity check\n            # steps should have source_to_use = 1 at every step, and it should be set for only one dataset\n            assert torch.all(\n                self.source_to_use.sum(dim=0)\n            ).item(), \"use_one logic is incorrect\"\n            assert self.source_to_use.sum().item() == self.max_steps\n\n        # sanity check & logging\n        logging.info(\n            f\"Created a ConcatIterator with batching_strategy={concat_dataset.batching_strategy} and steps {concat_dataset.max_steps}. Steps per source:\"\n        )\n        for idx in range(num_data_sources):\n            assert (\n                self.source_to_use[idx].sum().item() == iterator_lens[idx]\n            ), \"source_to_use logic is incorrect\"\n            if concat_dataset.repeat_factors[idx] == -1:\n                # make sure dataset is padded to max length\n                assert self.source_to_use[idx].sum().item() == self.max_steps\n            else:\n                assert self.source_to_use[idx].sum().item() == int(\n                    len(dataloaders[idx]) * concat_dataset.repeat_factors[idx]\n                )\n            logging.info(\n                f\"Dataset {idx}; len {iterator_lens[idx]}; Orig len {len(dataloaders[idx])}\"\n            )\n\n    def __iter__(self):\n        return self\n\n    def get_sample(self):\n        sample = {}\n        \"\"\"\n        grad_accum_sample = False\n        \"\"\"\n        for idx in range(len(self.iterators)):\n            if not self.source_to_use[idx, self.step_counter]:\n                continue\n            try:\n                val = next(self.iterators[idx])\n            except StopIteration:\n                self.iterators[idx] = iter(self.dataloaders[idx])\n                val = next(self.iterators[idx])\n\n            self.per_src_step_counter[idx] += 1\n            if self.per_src_step_counter[idx] > self.iterator_lens[idx]:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            assert self.source_to_use.sum().item() == self.max_steps\n\n        # sanity check & logging\n        logging.info(\n            f\"Created a ConcatIterator with batching_strategy={concat_dataset.batching_strategy} and steps {concat_dataset.max_steps}. Steps per source:\"\n        )\n        for idx in range(num_data_sources):\n            assert (\n                self.source_to_use[idx].sum().item() == iterator_lens[idx]\n            ), \"source_to_use logic is incorrect\"\n            if concat_dataset.repeat_factors[idx] == -1:\n                # make sure dataset is padded to max length\n                assert self.source_to_use[idx].sum().item() == self.max_steps\n            else:\n                assert self.source_to_use[idx].sum().item() == int(\n                    len(dataloaders[idx]) * concat_dataset.repeat_factors[idx]\n                )\n            logging.info(\n                f\"Dataset {idx}; len {iterator_lens[idx]}; Orig len {len(dataloaders[idx])}\"\n            )\n\n    def __iter__(self):\n        return self\n\n    def get_sample(self):\n        sample = {}\n        \"\"\"\n        grad_accum_sample = False\n        \"\"\"\n        for idx in range(len(self.iterators)):\n            if not self.source_to_use[idx, self.step_counter]:\n                continue\n            try:\n                val = next(self.iterators[idx])\n            except StopIteration:\n                self.iterators[idx] = iter(self.dataloaders[idx])\n                val = next(self.iterators[idx])\n\n            self.per_src_step_counter[idx] += 1\n            if self.per_src_step_counter[idx] > self.iterator_lens[idx]:\n                raise ValueError(\n                    f\"Something is off. For iterator {idx} expected {self.iterator_lens[idx]} steps but currently at {self.per_src_step_counter[idx]}\"\n                )\n\n            \"\"\"\n            if GradAccumSampleHandler.is_grad_accum_sample(val):\n                vals = GradAccumSampleHandler.strip_grad_accum_sentinel(val)\n                if isinstance(sample, dict):\n                    # now `sample` is a dict which we want to make a list of dictionaries\n                    sample = [{x: sample[x]} for x in sample]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if concat_dataset.repeat_factors[idx] == -1:\n                # make sure dataset is padded to max length\n                assert self.source_to_use[idx].sum().item() == self.max_steps\n            else:\n                assert self.source_to_use[idx].sum().item() == int(\n                    len(dataloaders[idx]) * concat_dataset.repeat_factors[idx]\n                )\n            logging.info(\n                f\"Dataset {idx}; len {iterator_lens[idx]}; Orig len {len(dataloaders[idx])}\"\n            )\n\n    def __iter__(self):\n        return self\n\n    def get_sample(self):\n        sample = {}\n        \"\"\"\n        grad_accum_sample = False\n        \"\"\"\n        for idx in range(len(self.iterators)):\n            if not self.source_to_use[idx, self.step_counter]:\n                continue\n            try:\n                val = next(self.iterators[idx])\n            except StopIteration:\n                self.iterators[idx] = iter(self.dataloaders[idx])\n                val = next(self.iterators[idx])\n\n            self.per_src_step_counter[idx] += 1\n            if self.per_src_step_counter[idx] > self.iterator_lens[idx]:\n                raise ValueError(\n                    f\"Something is off. For iterator {idx} expected {self.iterator_lens[idx]} steps but currently at {self.per_src_step_counter[idx]}\"\n                )\n\n            \"\"\"\n            if GradAccumSampleHandler.is_grad_accum_sample(val):\n                vals = GradAccumSampleHandler.strip_grad_accum_sentinel(val)\n                if isinstance(sample, dict):\n                    # now `sample` is a dict which we want to make a list of dictionaries\n                    sample = [{x: sample[x]} for x in sample]\n                sample.extend([{output_key: val} for val in vals])\n                grad_accum_sample = True\n            elif grad_accum_sample:\n                # now `val` is a dict which we will append to the sample list\n                assert isinstance(sample, list)\n                sample.append({output_key: val})\n            else:\n                # grad accum isn't on, simply create a dictionary\n                sample[output_key] = val\n            \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __iter__(self):\n        return self\n\n    def get_sample(self):\n        sample = {}\n        \"\"\"\n        grad_accum_sample = False\n        \"\"\"\n        for idx in range(len(self.iterators)):\n            if not self.source_to_use[idx, self.step_counter]:\n                continue\n            try:\n                val = next(self.iterators[idx])\n            except StopIteration:\n                self.iterators[idx] = iter(self.dataloaders[idx])\n                val = next(self.iterators[idx])\n\n            self.per_src_step_counter[idx] += 1\n            if self.per_src_step_counter[idx] > self.iterator_lens[idx]:\n                raise ValueError(\n                    f\"Something is off. For iterator {idx} expected {self.iterator_lens[idx]} steps but currently at {self.per_src_step_counter[idx]}\"\n                )\n\n            \"\"\"\n            if GradAccumSampleHandler.is_grad_accum_sample(val):\n                vals = GradAccumSampleHandler.strip_grad_accum_sentinel(val)\n                if isinstance(sample, dict):\n                    # now `sample` is a dict which we want to make a list of dictionaries\n                    sample = [{x: sample[x]} for x in sample]\n                sample.extend([{output_key: val} for val in vals])\n                grad_accum_sample = True\n            elif grad_accum_sample:\n                # now `val` is a dict which we will append to the sample list\n                assert isinstance(sample, list)\n                sample.append({output_key: val})\n            else:\n                # grad accum isn't on, simply create a dictionary\n                sample[output_key] = val\n            \"\"\"\n            orig_keys = len(sample)\n            sample.update(val)\n            assert len(sample) == orig_keys + len(val)\n\n        \"\"\"\n        if grad_accum_sample:\n            return GradAccumSampleHandler.add_grad_accum_sentinel(sample)\n        \"\"\"\n        return sample\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if not self.source_to_use[idx, self.step_counter]:\n                continue\n            try:\n                val = next(self.iterators[idx])\n            except StopIteration:\n                self.iterators[idx] = iter(self.dataloaders[idx])\n                val = next(self.iterators[idx])\n\n            self.per_src_step_counter[idx] += 1\n            if self.per_src_step_counter[idx] > self.iterator_lens[idx]:\n                raise ValueError(\n                    f\"Something is off. For iterator {idx} expected {self.iterator_lens[idx]} steps but currently at {self.per_src_step_counter[idx]}\"\n                )\n\n            \"\"\"\n            if GradAccumSampleHandler.is_grad_accum_sample(val):\n                vals = GradAccumSampleHandler.strip_grad_accum_sentinel(val)\n                if isinstance(sample, dict):\n                    # now `sample` is a dict which we want to make a list of dictionaries\n                    sample = [{x: sample[x]} for x in sample]\n                sample.extend([{output_key: val} for val in vals])\n                grad_accum_sample = True\n            elif grad_accum_sample:\n                # now `val` is a dict which we will append to the sample list\n                assert isinstance(sample, list)\n                sample.append({output_key: val})\n            else:\n                # grad accum isn't on, simply create a dictionary\n                sample[output_key] = val\n            \"\"\"\n            orig_keys = len(sample)\n            sample.update(val)\n            assert len(sample) == orig_keys + len(val)\n\n        \"\"\"\n        if grad_accum_sample:\n            return GradAccumSampleHandler.add_grad_accum_sentinel(sample)\n        \"\"\"\n        return sample\n\n    def __next__(self):\n        if self.step_counter == self.max_steps:\n            raise StopIteration\n\n        sample = self.get_sample()\n\n        self.step_counter += 1\n        return sample\n\n    def __len__(self):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 256, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                raise ValueError(\n                    f\"Something is off. For iterator {idx} expected {self.iterator_lens[idx]} steps but currently at {self.per_src_step_counter[idx]}\"\n                )\n\n            \"\"\"\n            if GradAccumSampleHandler.is_grad_accum_sample(val):\n                vals = GradAccumSampleHandler.strip_grad_accum_sentinel(val)\n                if isinstance(sample, dict):\n                    # now `sample` is a dict which we want to make a list of dictionaries\n                    sample = [{x: sample[x]} for x in sample]\n                sample.extend([{output_key: val} for val in vals])\n                grad_accum_sample = True\n            elif grad_accum_sample:\n                # now `val` is a dict which we will append to the sample list\n                assert isinstance(sample, list)\n                sample.append({output_key: val})\n            else:\n                # grad accum isn't on, simply create a dictionary\n                sample[output_key] = val\n            \"\"\"\n            orig_keys = len(sample)\n            sample.update(val)\n            assert len(sample) == orig_keys + len(val)\n\n        \"\"\"\n        if grad_accum_sample:\n            return GradAccumSampleHandler.add_grad_accum_sentinel(sample)\n        \"\"\"\n        return sample\n\n    def __next__(self):\n        if self.step_counter == self.max_steps:\n            raise StopIteration\n\n        sample = self.get_sample()\n\n        self.step_counter += 1\n        return sample\n\n    def __len__(self):\n        return self.max_steps", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_215-256"}
{"title": "facebookresearch_omnivore-omnivision-data-concat_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "concat_dataset.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 256, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                sample.extend([{output_key: val} for val in vals])\n                grad_accum_sample = True\n            elif grad_accum_sample:\n                # now `val` is a dict which we will append to the sample list\n                assert isinstance(sample, list)\n                sample.append({output_key: val})\n            else:\n                # grad accum isn't on, simply create a dictionary\n                sample[output_key] = val\n            \"\"\"\n            orig_keys = len(sample)\n            sample.update(val)\n            assert len(sample) == orig_keys + len(val)\n\n        \"\"\"\n        if grad_accum_sample:\n            return GradAccumSampleHandler.add_grad_accum_sentinel(sample)\n        \"\"\"\n        return sample\n\n    def __next__(self):\n        if self.step_counter == self.max_steps:\n            raise StopIteration\n\n        sample = self.get_sample()\n\n        self.step_counter += 1\n        return sample\n\n    def __len__(self):\n        return self.max_steps", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-concat_dataset.py_225-256"}
{"title": "facebookresearch_omnivore-omnivision-data-omni_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "omni_dataset.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "omni_dataset.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 14, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom abc import ABC, abstractmethod\nfrom typing import Iterable\n\n\nclass OmniDataset(ABC):\n    @abstractmethod\n    def get_loader(self, *args, **kwargs) -> Iterable:\n        pass\n\nAST=Module(ImportFrom(aliasalias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argargarg)PassName(Load)Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-omni_dataset.py_0-14"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List\n\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as tvf\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.data import (\n    get_mean_image,\n    IdentityTransform,\n    SharedMemoryNumpyLoader,\n)\nfrom PIL import Image\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torch.utils.data import Dataset\n\n\n\nAST=Module(Import(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List\n\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as tvf\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.data import (\n    get_mean_image,\n    IdentityTransform,\n    SharedMemoryNumpyLoader,\n)\nfrom PIL import Image\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torch.utils.data import Dataset\n\n\nIDENTITY_TRANSFORM = IdentityTransform()\nDEFAULT_SPATIAL_SIZE = 224\n\n\nclass PathDataset(Dataset, ABC):\n    def __init__(\n        self,\n        path_file_list: List[str],\n        label_file_list: List[str],\n        remove_prefix=\"\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List\n\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as tvf\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.data import (\n    get_mean_image,\n    IdentityTransform,\n    SharedMemoryNumpyLoader,\n)\nfrom PIL import Image\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torch.utils.data import Dataset\n\n\nIDENTITY_TRANSFORM = IdentityTransform()\nDEFAULT_SPATIAL_SIZE = 224\n\n\nclass PathDataset(Dataset, ABC):\n    def __init__(\n        self,\n        path_file_list: List[str],\n        label_file_list: List[str],\n        remove_prefix=\"\",\n        new_prefix=\"\",\n        remove_suffix=\"\",\n        new_suffix=\"\",\n        transforms=None,\n    ):\n        \"\"\"Creates a dataset where the metadata is stored in a numpy file.\n\n        path_file_list: A list of paths which contain the path metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, List\n\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as tvf\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.data import (\n    get_mean_image,\n    IdentityTransform,\n    SharedMemoryNumpyLoader,\n)\nfrom PIL import Image\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torch.utils.data import Dataset\n\n\nIDENTITY_TRANSFORM = IdentityTransform()\nDEFAULT_SPATIAL_SIZE = 224\n\n\nclass PathDataset(Dataset, ABC):\n    def __init__(\n        self,\n        path_file_list: List[str],\n        label_file_list: List[str],\n        remove_prefix=\"\",\n        new_prefix=\"\",\n        remove_suffix=\"\",\n        new_suffix=\"\",\n        transforms=None,\n    ):\n        \"\"\"Creates a dataset where the metadata is stored in a numpy file.\n\n        path_file_list: A list of paths which contain the path metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        label_file_list: A list of paths which contain the label metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        \"\"\"\n        self.is_initialized = False\n        self.path_file_list = path_file_list\n        self.label_file_list = label_file_list\n        self.transforms = [] if transforms is None else transforms\n\n        self.remove_prefix = remove_prefix\n\nAST=Module(Import(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Call(Name(Load)))Assign(Name(Store)Constant)ClassDef(Name(Load)Name(Load)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))argargargargargConstantConstantConstantConstantConstant)Expr(Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)IfExp(Compare(Name(Load)IsConstant)List(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from omnivision.utils.data import (\n    get_mean_image,\n    IdentityTransform,\n    SharedMemoryNumpyLoader,\n)\nfrom PIL import Image\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torch.utils.data import Dataset\n\n\nIDENTITY_TRANSFORM = IdentityTransform()\nDEFAULT_SPATIAL_SIZE = 224\n\n\nclass PathDataset(Dataset, ABC):\n    def __init__(\n        self,\n        path_file_list: List[str],\n        label_file_list: List[str],\n        remove_prefix=\"\",\n        new_prefix=\"\",\n        remove_suffix=\"\",\n        new_suffix=\"\",\n        transforms=None,\n    ):\n        \"\"\"Creates a dataset where the metadata is stored in a numpy file.\n\n        path_file_list: A list of paths which contain the path metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        label_file_list: A list of paths which contain the label metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        \"\"\"\n        self.is_initialized = False\n        self.path_file_list = path_file_list\n        self.label_file_list = label_file_list\n        self.transforms = [] if transforms is None else transforms\n\n        self.remove_prefix = remove_prefix\n        self.new_prefix = new_prefix\n        self.remove_suffix = remove_suffix\n        self.new_suffix = new_suffix\n\n        self.paths = None\n        self.labels = None\n        self.file_idx = None\n\n        # used for shared memory\n        self.label_sm_loader = SharedMemoryNumpyLoader()\n\nAST=Module(ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Call(Name(Load)))Assign(Name(Store)Constant)ClassDef(Name(Load)Name(Load)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))argargargargargConstantConstantConstantConstantConstant)Expr(Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)IfExp(Compare(Name(Load)IsConstant)List(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Call(Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "IDENTITY_TRANSFORM = IdentityTransform()\nDEFAULT_SPATIAL_SIZE = 224\n\n\nclass PathDataset(Dataset, ABC):\n    def __init__(\n        self,\n        path_file_list: List[str],\n        label_file_list: List[str],\n        remove_prefix=\"\",\n        new_prefix=\"\",\n        remove_suffix=\"\",\n        new_suffix=\"\",\n        transforms=None,\n    ):\n        \"\"\"Creates a dataset where the metadata is stored in a numpy file.\n\n        path_file_list: A list of paths which contain the path metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        label_file_list: A list of paths which contain the label metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        \"\"\"\n        self.is_initialized = False\n        self.path_file_list = path_file_list\n        self.label_file_list = label_file_list\n        self.transforms = [] if transforms is None else transforms\n\n        self.remove_prefix = remove_prefix\n        self.new_prefix = new_prefix\n        self.remove_suffix = remove_suffix\n        self.new_suffix = new_suffix\n\n        self.paths = None\n        self.labels = None\n        self.file_idx = None\n\n        # used for shared memory\n        self.label_sm_loader = SharedMemoryNumpyLoader()\n        self.path_sm_loader = SharedMemoryNumpyLoader()\n\n        self._load_data()\n        self.num_samples = len(self.paths)\n        assert len(self.paths) == len(\n            self.labels\n        ), f\"Paths ({len(self.paths)}) != labels ({len(self.labels)})\"\n        logging.info(\n            f\"Created dataset from {self.path_file_list} of length: {self.num_samples}\"\n        )\n\nAST=Module(Assign(Name(Store)Call(Name(Load)))Assign(Name(Store)Constant)ClassDef(Name(Load)Name(Load)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))argargargargargConstantConstantConstantConstantConstant)Expr(Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)IfExp(Compare(Name(Load)IsConstant)List(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Call(Name(Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)))Expr(Call(Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Attribute(Name(Load)Load)))Assert(Compare(Call(Name(Load)Attribute(Name(Load)Load))EqCall(Name(Load)Attribute(Name(Load)Load)))JoinedStr(ConstantFormattedValue(Call(Name(Load)Attribute(Name(Load)Load)))ConstantFormattedValue(Call(Name(Load)Attribute(Name(Load)Load)))Constant))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Attribute(Name(Load)Load))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        new_prefix=\"\",\n        remove_suffix=\"\",\n        new_suffix=\"\",\n        transforms=None,\n    ):\n        \"\"\"Creates a dataset where the metadata is stored in a numpy file.\n\n        path_file_list: A list of paths which contain the path metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        label_file_list: A list of paths which contain the label metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        \"\"\"\n        self.is_initialized = False\n        self.path_file_list = path_file_list\n        self.label_file_list = label_file_list\n        self.transforms = [] if transforms is None else transforms\n\n        self.remove_prefix = remove_prefix\n        self.new_prefix = new_prefix\n        self.remove_suffix = remove_suffix\n        self.new_suffix = new_suffix\n\n        self.paths = None\n        self.labels = None\n        self.file_idx = None\n\n        # used for shared memory\n        self.label_sm_loader = SharedMemoryNumpyLoader()\n        self.path_sm_loader = SharedMemoryNumpyLoader()\n\n        self._load_data()\n        self.num_samples = len(self.paths)\n        assert len(self.paths) == len(\n            self.labels\n        ), f\"Paths ({len(self.paths)}) != labels ({len(self.labels)})\"\n        logging.info(\n            f\"Created dataset from {self.path_file_list} of length: {self.num_samples}\"\n        )\n\n    def _load_data(self):\n        logging.info(f\"Loading {self.label_file_list} with shared memory\")\n        self.labels, label_file_idx = self.label_sm_loader.load(self.label_file_list)\n        logging.info(f\"Loading {self.path_file_list} with shared memory\")\n        self.paths, path_file_idx = self.path_sm_loader.load(self.path_file_list)\n        assert (\n            label_file_idx == path_file_idx\n        ), \"Label file and path file were not found at the same index\"\n        self.is_initialized = True", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        label_file_list: A list of paths which contain the label metadata file. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the metadata.\n        \"\"\"\n        self.is_initialized = False\n        self.path_file_list = path_file_list\n        self.label_file_list = label_file_list\n        self.transforms = [] if transforms is None else transforms\n\n        self.remove_prefix = remove_prefix\n        self.new_prefix = new_prefix\n        self.remove_suffix = remove_suffix\n        self.new_suffix = new_suffix\n\n        self.paths = None\n        self.labels = None\n        self.file_idx = None\n\n        # used for shared memory\n        self.label_sm_loader = SharedMemoryNumpyLoader()\n        self.path_sm_loader = SharedMemoryNumpyLoader()\n\n        self._load_data()\n        self.num_samples = len(self.paths)\n        assert len(self.paths) == len(\n            self.labels\n        ), f\"Paths ({len(self.paths)}) != labels ({len(self.labels)})\"\n        logging.info(\n            f\"Created dataset from {self.path_file_list} of length: {self.num_samples}\"\n        )\n\n    def _load_data(self):\n        logging.info(f\"Loading {self.label_file_list} with shared memory\")\n        self.labels, label_file_idx = self.label_sm_loader.load(self.label_file_list)\n        logging.info(f\"Loading {self.path_file_list} with shared memory\")\n        self.paths, path_file_idx = self.path_sm_loader.load(self.path_file_list)\n        assert (\n            label_file_idx == path_file_idx\n        ), \"Label file and path file were not found at the same index\"\n        self.is_initialized = True\n        self.file_idx = path_file_idx\n\n    def _replace_path_prefix(self, path, replace_prefix, new_prefix):\n        if replace_prefix == \"\":\n            path = new_prefix + path\n        elif path.startswith(replace_prefix):\n            return new_prefix + path[len(replace_prefix) :]\n        else:\n            raise ValueError(f\"Cannot replace `{replace_prefix}`` prefix in `{path}`\")\n        return path", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.new_prefix = new_prefix\n        self.remove_suffix = remove_suffix\n        self.new_suffix = new_suffix\n\n        self.paths = None\n        self.labels = None\n        self.file_idx = None\n\n        # used for shared memory\n        self.label_sm_loader = SharedMemoryNumpyLoader()\n        self.path_sm_loader = SharedMemoryNumpyLoader()\n\n        self._load_data()\n        self.num_samples = len(self.paths)\n        assert len(self.paths) == len(\n            self.labels\n        ), f\"Paths ({len(self.paths)}) != labels ({len(self.labels)})\"\n        logging.info(\n            f\"Created dataset from {self.path_file_list} of length: {self.num_samples}\"\n        )\n\n    def _load_data(self):\n        logging.info(f\"Loading {self.label_file_list} with shared memory\")\n        self.labels, label_file_idx = self.label_sm_loader.load(self.label_file_list)\n        logging.info(f\"Loading {self.path_file_list} with shared memory\")\n        self.paths, path_file_idx = self.path_sm_loader.load(self.path_file_list)\n        assert (\n            label_file_idx == path_file_idx\n        ), \"Label file and path file were not found at the same index\"\n        self.is_initialized = True\n        self.file_idx = path_file_idx\n\n    def _replace_path_prefix(self, path, replace_prefix, new_prefix):\n        if replace_prefix == \"\":\n            path = new_prefix + path\n        elif path.startswith(replace_prefix):\n            return new_prefix + path[len(replace_prefix) :]\n        else:\n            raise ValueError(f\"Cannot replace `{replace_prefix}`` prefix in `{path}`\")\n        return path\n\n    def _replace_path_suffix(self, path, replace_suffix, new_suffix):\n        if replace_suffix == \"\":\n            path = path + new_suffix\n        elif path.endswith(replace_suffix):\n            return path[: -len(replace_suffix)] + new_suffix\n        else:\n            raise ValueError(f\"Cannot replace `{replace_suffix}`` suffix in `{path}`\")\n        return path\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.path_sm_loader = SharedMemoryNumpyLoader()\n\n        self._load_data()\n        self.num_samples = len(self.paths)\n        assert len(self.paths) == len(\n            self.labels\n        ), f\"Paths ({len(self.paths)}) != labels ({len(self.labels)})\"\n        logging.info(\n            f\"Created dataset from {self.path_file_list} of length: {self.num_samples}\"\n        )\n\n    def _load_data(self):\n        logging.info(f\"Loading {self.label_file_list} with shared memory\")\n        self.labels, label_file_idx = self.label_sm_loader.load(self.label_file_list)\n        logging.info(f\"Loading {self.path_file_list} with shared memory\")\n        self.paths, path_file_idx = self.path_sm_loader.load(self.path_file_list)\n        assert (\n            label_file_idx == path_file_idx\n        ), \"Label file and path file were not found at the same index\"\n        self.is_initialized = True\n        self.file_idx = path_file_idx\n\n    def _replace_path_prefix(self, path, replace_prefix, new_prefix):\n        if replace_prefix == \"\":\n            path = new_prefix + path\n        elif path.startswith(replace_prefix):\n            return new_prefix + path[len(replace_prefix) :]\n        else:\n            raise ValueError(f\"Cannot replace `{replace_prefix}`` prefix in `{path}`\")\n        return path\n\n    def _replace_path_suffix(self, path, replace_suffix, new_suffix):\n        if replace_suffix == \"\":\n            path = path + new_suffix\n        elif path.endswith(replace_suffix):\n            return path[: -len(replace_suffix)] + new_suffix\n        else:\n            raise ValueError(f\"Cannot replace `{replace_suffix}`` suffix in `{path}`\")\n        return path\n\n    def __len__(self):\n        return self.num_samples\n\n    @abstractmethod\n    def default_generator(self):\n        pass\n\n    @abstractmethod\n    def load_object(self, path):\n        pass", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _load_data(self):\n        logging.info(f\"Loading {self.label_file_list} with shared memory\")\n        self.labels, label_file_idx = self.label_sm_loader.load(self.label_file_list)\n        logging.info(f\"Loading {self.path_file_list} with shared memory\")\n        self.paths, path_file_idx = self.path_sm_loader.load(self.path_file_list)\n        assert (\n            label_file_idx == path_file_idx\n        ), \"Label file and path file were not found at the same index\"\n        self.is_initialized = True\n        self.file_idx = path_file_idx\n\n    def _replace_path_prefix(self, path, replace_prefix, new_prefix):\n        if replace_prefix == \"\":\n            path = new_prefix + path\n        elif path.startswith(replace_prefix):\n            return new_prefix + path[len(replace_prefix) :]\n        else:\n            raise ValueError(f\"Cannot replace `{replace_prefix}`` prefix in `{path}`\")\n        return path\n\n    def _replace_path_suffix(self, path, replace_suffix, new_suffix):\n        if replace_suffix == \"\":\n            path = path + new_suffix\n        elif path.endswith(replace_suffix):\n            return path[: -len(replace_suffix)] + new_suffix\n        else:\n            raise ValueError(f\"Cannot replace `{replace_suffix}`` suffix in `{path}`\")\n        return path\n\n    def __len__(self):\n        return self.num_samples\n\n    @abstractmethod\n    def default_generator(self):\n        pass\n\n    @abstractmethod\n    def load_object(self, path):\n        pass\n\n    def _get_path(self, idx):\n        path = self._replace_path_prefix(\n            self.paths[idx],\n            replace_prefix=self.remove_prefix,\n            new_prefix=self.new_prefix,\n        )\n        path = self._replace_path_suffix(\n            path, replace_suffix=self.remove_suffix, new_suffix=self.new_suffix\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.file_idx = path_file_idx\n\n    def _replace_path_prefix(self, path, replace_prefix, new_prefix):\n        if replace_prefix == \"\":\n            path = new_prefix + path\n        elif path.startswith(replace_prefix):\n            return new_prefix + path[len(replace_prefix) :]\n        else:\n            raise ValueError(f\"Cannot replace `{replace_prefix}`` prefix in `{path}`\")\n        return path\n\n    def _replace_path_suffix(self, path, replace_suffix, new_suffix):\n        if replace_suffix == \"\":\n            path = path + new_suffix\n        elif path.endswith(replace_suffix):\n            return path[: -len(replace_suffix)] + new_suffix\n        else:\n            raise ValueError(f\"Cannot replace `{replace_suffix}`` suffix in `{path}`\")\n        return path\n\n    def __len__(self):\n        return self.num_samples\n\n    @abstractmethod\n    def default_generator(self):\n        pass\n\n    @abstractmethod\n    def load_object(self, path):\n        pass\n\n    def _get_path(self, idx):\n        path = self._replace_path_prefix(\n            self.paths[idx],\n            replace_prefix=self.remove_prefix,\n            new_prefix=self.new_prefix,\n        )\n        path = self._replace_path_suffix(\n            path, replace_suffix=self.remove_suffix, new_suffix=self.new_suffix\n        )\n        return path\n\n    def try_load_object(self, idx):\n        is_success = True\n        try:\n            data = self.load_object(self._get_path(idx))\n        except Exception:\n            logging.warning(\n                f\"Couldn't load: {self.paths[idx]}. Exception:\", exc_info=True\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _replace_path_suffix(self, path, replace_suffix, new_suffix):\n        if replace_suffix == \"\":\n            path = path + new_suffix\n        elif path.endswith(replace_suffix):\n            return path[: -len(replace_suffix)] + new_suffix\n        else:\n            raise ValueError(f\"Cannot replace `{replace_suffix}`` suffix in `{path}`\")\n        return path\n\n    def __len__(self):\n        return self.num_samples\n\n    @abstractmethod\n    def default_generator(self):\n        pass\n\n    @abstractmethod\n    def load_object(self, path):\n        pass\n\n    def _get_path(self, idx):\n        path = self._replace_path_prefix(\n            self.paths[idx],\n            replace_prefix=self.remove_prefix,\n            new_prefix=self.new_prefix,\n        )\n        path = self._replace_path_suffix(\n            path, replace_suffix=self.remove_suffix, new_suffix=self.new_suffix\n        )\n        return path\n\n    def try_load_object(self, idx):\n        is_success = True\n        try:\n            data = self.load_object(self._get_path(idx))\n        except Exception:\n            logging.warning(\n                f\"Couldn't load: {self.paths[idx]}. Exception:\", exc_info=True\n            )\n            is_success = False\n            data = self.default_generator()\n        return data, is_success\n\n    def get_label(self, idx):\n        return None if self.labels is None else self.labels[idx]\n\n    @staticmethod\n    def create_sample(idx, data, label, is_success):\n        return VisionSample(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __len__(self):\n        return self.num_samples\n\n    @abstractmethod\n    def default_generator(self):\n        pass\n\n    @abstractmethod\n    def load_object(self, path):\n        pass\n\n    def _get_path(self, idx):\n        path = self._replace_path_prefix(\n            self.paths[idx],\n            replace_prefix=self.remove_prefix,\n            new_prefix=self.new_prefix,\n        )\n        path = self._replace_path_suffix(\n            path, replace_suffix=self.remove_suffix, new_suffix=self.new_suffix\n        )\n        return path\n\n    def try_load_object(self, idx):\n        is_success = True\n        try:\n            data = self.load_object(self._get_path(idx))\n        except Exception:\n            logging.warning(\n                f\"Couldn't load: {self.paths[idx]}. Exception:\", exc_info=True\n            )\n            is_success = False\n            data = self.default_generator()\n        return data, is_success\n\n    def get_label(self, idx):\n        return None if self.labels is None else self.labels[idx]\n\n    @staticmethod\n    def create_sample(idx, data, label, is_success):\n        return VisionSample(\n            vision=data, label=int(label), data_idx=idx, data_valid=is_success\n        )\n\n    def apply_transforms(self, sample):\n        for transform in self.transforms:\n            sample = transform(sample)\n        return sample\n\n    def __getitem__(self, idx):\n        data, is_success = self.try_load_object(idx)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _get_path(self, idx):\n        path = self._replace_path_prefix(\n            self.paths[idx],\n            replace_prefix=self.remove_prefix,\n            new_prefix=self.new_prefix,\n        )\n        path = self._replace_path_suffix(\n            path, replace_suffix=self.remove_suffix, new_suffix=self.new_suffix\n        )\n        return path\n\n    def try_load_object(self, idx):\n        is_success = True\n        try:\n            data = self.load_object(self._get_path(idx))\n        except Exception:\n            logging.warning(\n                f\"Couldn't load: {self.paths[idx]}. Exception:\", exc_info=True\n            )\n            is_success = False\n            data = self.default_generator()\n        return data, is_success\n\n    def get_label(self, idx):\n        return None if self.labels is None else self.labels[idx]\n\n    @staticmethod\n    def create_sample(idx, data, label, is_success):\n        return VisionSample(\n            vision=data, label=int(label), data_idx=idx, data_valid=is_success\n        )\n\n    def apply_transforms(self, sample):\n        for transform in self.transforms:\n            sample = transform(sample)\n        return sample\n\n    def __getitem__(self, idx):\n        data, is_success = self.try_load_object(idx)\n        label = self.get_label(idx)\n        sample = self.create_sample(idx, data, label, is_success)\n        sample = self.apply_transforms(sample)\n        return sample\n\n\nclass ImagePathDataset(PathDataset):\n    def default_generator(self):\n        return get_mean_image(DEFAULT_SPATIAL_SIZE)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return path\n\n    def try_load_object(self, idx):\n        is_success = True\n        try:\n            data = self.load_object(self._get_path(idx))\n        except Exception:\n            logging.warning(\n                f\"Couldn't load: {self.paths[idx]}. Exception:\", exc_info=True\n            )\n            is_success = False\n            data = self.default_generator()\n        return data, is_success\n\n    def get_label(self, idx):\n        return None if self.labels is None else self.labels[idx]\n\n    @staticmethod\n    def create_sample(idx, data, label, is_success):\n        return VisionSample(\n            vision=data, label=int(label), data_idx=idx, data_valid=is_success\n        )\n\n    def apply_transforms(self, sample):\n        for transform in self.transforms:\n            sample = transform(sample)\n        return sample\n\n    def __getitem__(self, idx):\n        data, is_success = self.try_load_object(idx)\n        label = self.get_label(idx)\n        sample = self.create_sample(idx, data, label, is_success)\n        sample = self.apply_transforms(sample)\n        return sample\n\n\nclass ImagePathDataset(PathDataset):\n    def default_generator(self):\n        return get_mean_image(DEFAULT_SPATIAL_SIZE)\n\n    def load_object(self, path) -> Image:\n        with g_pathmgr.open(path, \"rb\") as fopen:\n            return Image.open(fopen).convert(\"RGB\")\n\n\nclass ImageWithDepthPathDataset(ImagePathDataset):\n    def __init__(\n        self,\n        depth_path_file_list: List[str],\n        *args,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            is_success = False\n            data = self.default_generator()\n        return data, is_success\n\n    def get_label(self, idx):\n        return None if self.labels is None else self.labels[idx]\n\n    @staticmethod\n    def create_sample(idx, data, label, is_success):\n        return VisionSample(\n            vision=data, label=int(label), data_idx=idx, data_valid=is_success\n        )\n\n    def apply_transforms(self, sample):\n        for transform in self.transforms:\n            sample = transform(sample)\n        return sample\n\n    def __getitem__(self, idx):\n        data, is_success = self.try_load_object(idx)\n        label = self.get_label(idx)\n        sample = self.create_sample(idx, data, label, is_success)\n        sample = self.apply_transforms(sample)\n        return sample\n\n\nclass ImagePathDataset(PathDataset):\n    def default_generator(self):\n        return get_mean_image(DEFAULT_SPATIAL_SIZE)\n\n    def load_object(self, path) -> Image:\n        with g_pathmgr.open(path, \"rb\") as fopen:\n            return Image.open(fopen).convert(\"RGB\")\n\n\nclass ImageWithDepthPathDataset(ImagePathDataset):\n    def __init__(\n        self,\n        depth_path_file_list: List[str],\n        *args,\n        remove_depth_prefix=\"\",\n        new_depth_prefix=\"\",\n        remove_depth_suffix=\"\",\n        new_depth_suffix=\"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Shared Memory dataloader for RGB+Depth datasets.\n        \"\"\"\n        super().__init__(*args, **kwargs)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            vision=data, label=int(label), data_idx=idx, data_valid=is_success\n        )\n\n    def apply_transforms(self, sample):\n        for transform in self.transforms:\n            sample = transform(sample)\n        return sample\n\n    def __getitem__(self, idx):\n        data, is_success = self.try_load_object(idx)\n        label = self.get_label(idx)\n        sample = self.create_sample(idx, data, label, is_success)\n        sample = self.apply_transforms(sample)\n        return sample\n\n\nclass ImagePathDataset(PathDataset):\n    def default_generator(self):\n        return get_mean_image(DEFAULT_SPATIAL_SIZE)\n\n    def load_object(self, path) -> Image:\n        with g_pathmgr.open(path, \"rb\") as fopen:\n            return Image.open(fopen).convert(\"RGB\")\n\n\nclass ImageWithDepthPathDataset(ImagePathDataset):\n    def __init__(\n        self,\n        depth_path_file_list: List[str],\n        *args,\n        remove_depth_prefix=\"\",\n        new_depth_prefix=\"\",\n        remove_depth_suffix=\"\",\n        new_depth_suffix=\"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Shared Memory dataloader for RGB+Depth datasets.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        self.depth_path_file_list = depth_path_file_list\n\n        self.remove_depth_prefix = remove_depth_prefix\n        self.new_depth_prefix = new_depth_prefix\n        self.remove_depth_suffix = remove_depth_suffix\n        self.new_depth_suffix = new_depth_suffix\n\n        self.depth_path_sm_loader = SharedMemoryNumpyLoader()\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        label = self.get_label(idx)\n        sample = self.create_sample(idx, data, label, is_success)\n        sample = self.apply_transforms(sample)\n        return sample\n\n\nclass ImagePathDataset(PathDataset):\n    def default_generator(self):\n        return get_mean_image(DEFAULT_SPATIAL_SIZE)\n\n    def load_object(self, path) -> Image:\n        with g_pathmgr.open(path, \"rb\") as fopen:\n            return Image.open(fopen).convert(\"RGB\")\n\n\nclass ImageWithDepthPathDataset(ImagePathDataset):\n    def __init__(\n        self,\n        depth_path_file_list: List[str],\n        *args,\n        remove_depth_prefix=\"\",\n        new_depth_prefix=\"\",\n        remove_depth_suffix=\"\",\n        new_depth_suffix=\"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Shared Memory dataloader for RGB+Depth datasets.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        self.depth_path_file_list = depth_path_file_list\n\n        self.remove_depth_prefix = remove_depth_prefix\n        self.new_depth_prefix = new_depth_prefix\n        self.remove_depth_suffix = remove_depth_suffix\n        self.new_depth_suffix = new_depth_suffix\n\n        self.depth_path_sm_loader = SharedMemoryNumpyLoader()\n\n        logging.info(f\"Loading {self.depth_path_file_list} with shared memory\")\n        self.depth_paths, depth_file_idx = self.depth_path_sm_loader.load(\n            self.depth_path_file_list\n        )\n\n        assert (\n            depth_file_idx == self.file_idx\n        ), \"Depth file and path file were not found at the same index\"\n\n    def _load_depth(self, image_path):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def load_object(self, path) -> Image:\n        with g_pathmgr.open(path, \"rb\") as fopen:\n            return Image.open(fopen).convert(\"RGB\")\n\n\nclass ImageWithDepthPathDataset(ImagePathDataset):\n    def __init__(\n        self,\n        depth_path_file_list: List[str],\n        *args,\n        remove_depth_prefix=\"\",\n        new_depth_prefix=\"\",\n        remove_depth_suffix=\"\",\n        new_depth_suffix=\"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Shared Memory dataloader for RGB+Depth datasets.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        self.depth_path_file_list = depth_path_file_list\n\n        self.remove_depth_prefix = remove_depth_prefix\n        self.new_depth_prefix = new_depth_prefix\n        self.remove_depth_suffix = remove_depth_suffix\n        self.new_depth_suffix = new_depth_suffix\n\n        self.depth_path_sm_loader = SharedMemoryNumpyLoader()\n\n        logging.info(f\"Loading {self.depth_path_file_list} with shared memory\")\n        self.depth_paths, depth_file_idx = self.depth_path_sm_loader.load(\n            self.depth_path_file_list\n        )\n\n        assert (\n            depth_file_idx == self.file_idx\n        ), \"Depth file and path file were not found at the same index\"\n\n    def _load_depth(self, image_path):\n        \"\"\"\n        Returns:\n            A (H, W, 1) tensor\n        \"\"\"\n        with g_pathmgr.open(image_path, \"rb\") as fopen:\n            # Depth is being saved as a .pt file instead\n            # of as an image\n            return torch.load(fopen)\n\n    def _get_depth_path(self, idx):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        remove_depth_prefix=\"\",\n        new_depth_prefix=\"\",\n        remove_depth_suffix=\"\",\n        new_depth_suffix=\"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Shared Memory dataloader for RGB+Depth datasets.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        self.depth_path_file_list = depth_path_file_list\n\n        self.remove_depth_prefix = remove_depth_prefix\n        self.new_depth_prefix = new_depth_prefix\n        self.remove_depth_suffix = remove_depth_suffix\n        self.new_depth_suffix = new_depth_suffix\n\n        self.depth_path_sm_loader = SharedMemoryNumpyLoader()\n\n        logging.info(f\"Loading {self.depth_path_file_list} with shared memory\")\n        self.depth_paths, depth_file_idx = self.depth_path_sm_loader.load(\n            self.depth_path_file_list\n        )\n\n        assert (\n            depth_file_idx == self.file_idx\n        ), \"Depth file and path file were not found at the same index\"\n\n    def _load_depth(self, image_path):\n        \"\"\"\n        Returns:\n            A (H, W, 1) tensor\n        \"\"\"\n        with g_pathmgr.open(image_path, \"rb\") as fopen:\n            # Depth is being saved as a .pt file instead\n            # of as an image\n            return torch.load(fopen)\n\n    def _get_depth_path(self, idx):\n        path = self._replace_path_prefix(\n            self.depth_paths[idx],\n            replace_prefix=self.remove_depth_prefix,\n            new_prefix=self.new_depth_prefix,\n        )\n        path = self._replace_path_suffix(\n            path,\n            replace_suffix=self.remove_depth_suffix,\n            new_suffix=self.new_depth_suffix,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        self.depth_path_file_list = depth_path_file_list\n\n        self.remove_depth_prefix = remove_depth_prefix\n        self.new_depth_prefix = new_depth_prefix\n        self.remove_depth_suffix = remove_depth_suffix\n        self.new_depth_suffix = new_depth_suffix\n\n        self.depth_path_sm_loader = SharedMemoryNumpyLoader()\n\n        logging.info(f\"Loading {self.depth_path_file_list} with shared memory\")\n        self.depth_paths, depth_file_idx = self.depth_path_sm_loader.load(\n            self.depth_path_file_list\n        )\n\n        assert (\n            depth_file_idx == self.file_idx\n        ), \"Depth file and path file were not found at the same index\"\n\n    def _load_depth(self, image_path):\n        \"\"\"\n        Returns:\n            A (H, W, 1) tensor\n        \"\"\"\n        with g_pathmgr.open(image_path, \"rb\") as fopen:\n            # Depth is being saved as a .pt file instead\n            # of as an image\n            return torch.load(fopen)\n\n    def _get_depth_path(self, idx):\n        path = self._replace_path_prefix(\n            self.depth_paths[idx],\n            replace_prefix=self.remove_depth_prefix,\n            new_prefix=self.new_depth_prefix,\n        )\n        path = self._replace_path_suffix(\n            path,\n            replace_suffix=self.remove_depth_suffix,\n            new_suffix=self.new_depth_suffix,\n        )\n        return path\n\n    def default_generator(self):\n        image = get_mean_image(DEFAULT_SPATIAL_SIZE)\n        depth = torch.zeros(\n            (1, DEFAULT_SPATIAL_SIZE, DEFAULT_SPATIAL_SIZE), dtype=torch.float32\n        )\n        return torch.cat([tvf.to_tensor(image), depth], dim=0)\n\n    def try_load_object(self, idx):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        logging.info(f\"Loading {self.depth_path_file_list} with shared memory\")\n        self.depth_paths, depth_file_idx = self.depth_path_sm_loader.load(\n            self.depth_path_file_list\n        )\n\n        assert (\n            depth_file_idx == self.file_idx\n        ), \"Depth file and path file were not found at the same index\"\n\n    def _load_depth(self, image_path):\n        \"\"\"\n        Returns:\n            A (H, W, 1) tensor\n        \"\"\"\n        with g_pathmgr.open(image_path, \"rb\") as fopen:\n            # Depth is being saved as a .pt file instead\n            # of as an image\n            return torch.load(fopen)\n\n    def _get_depth_path(self, idx):\n        path = self._replace_path_prefix(\n            self.depth_paths[idx],\n            replace_prefix=self.remove_depth_prefix,\n            new_prefix=self.new_depth_prefix,\n        )\n        path = self._replace_path_suffix(\n            path,\n            replace_suffix=self.remove_depth_suffix,\n            new_suffix=self.new_depth_suffix,\n        )\n        return path\n\n    def default_generator(self):\n        image = get_mean_image(DEFAULT_SPATIAL_SIZE)\n        depth = torch.zeros(\n            (1, DEFAULT_SPATIAL_SIZE, DEFAULT_SPATIAL_SIZE), dtype=torch.float32\n        )\n        return torch.cat([tvf.to_tensor(image), depth], dim=0)\n\n    def try_load_object(self, idx):\n        image, is_success = super().try_load_object(idx)\n        if is_success:\n            try:\n                depth = self._load_depth(self._get_depth_path(idx))\n                if depth.ndim == 2:\n                    depth = depth[None, ...]  # (1, H, W)\n                image_with_depth = torch.cat(\n                    [tvf.to_tensor(image), depth], dim=0\n                )  # (4, H, W)\n            except Exception:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        Returns:\n            A (H, W, 1) tensor\n        \"\"\"\n        with g_pathmgr.open(image_path, \"rb\") as fopen:\n            # Depth is being saved as a .pt file instead\n            # of as an image\n            return torch.load(fopen)\n\n    def _get_depth_path(self, idx):\n        path = self._replace_path_prefix(\n            self.depth_paths[idx],\n            replace_prefix=self.remove_depth_prefix,\n            new_prefix=self.new_depth_prefix,\n        )\n        path = self._replace_path_suffix(\n            path,\n            replace_suffix=self.remove_depth_suffix,\n            new_suffix=self.new_depth_suffix,\n        )\n        return path\n\n    def default_generator(self):\n        image = get_mean_image(DEFAULT_SPATIAL_SIZE)\n        depth = torch.zeros(\n            (1, DEFAULT_SPATIAL_SIZE, DEFAULT_SPATIAL_SIZE), dtype=torch.float32\n        )\n        return torch.cat([tvf.to_tensor(image), depth], dim=0)\n\n    def try_load_object(self, idx):\n        image, is_success = super().try_load_object(idx)\n        if is_success:\n            try:\n                depth = self._load_depth(self._get_depth_path(idx))\n                if depth.ndim == 2:\n                    depth = depth[None, ...]  # (1, H, W)\n                image_with_depth = torch.cat(\n                    [tvf.to_tensor(image), depth], dim=0\n                )  # (4, H, W)\n            except Exception:\n                logging.warning(\n                    f\"Couldn't load depth image: {self.depth_paths[idx]}. Exception:\",\n                    exc_info=True,\n                )\n                is_success = False\n\n        if not is_success:\n            image_with_depth = self.default_generator()\n\n        return image_with_depth, is_success", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        path = self._replace_path_prefix(\n            self.depth_paths[idx],\n            replace_prefix=self.remove_depth_prefix,\n            new_prefix=self.new_depth_prefix,\n        )\n        path = self._replace_path_suffix(\n            path,\n            replace_suffix=self.remove_depth_suffix,\n            new_suffix=self.new_depth_suffix,\n        )\n        return path\n\n    def default_generator(self):\n        image = get_mean_image(DEFAULT_SPATIAL_SIZE)\n        depth = torch.zeros(\n            (1, DEFAULT_SPATIAL_SIZE, DEFAULT_SPATIAL_SIZE), dtype=torch.float32\n        )\n        return torch.cat([tvf.to_tensor(image), depth], dim=0)\n\n    def try_load_object(self, idx):\n        image, is_success = super().try_load_object(idx)\n        if is_success:\n            try:\n                depth = self._load_depth(self._get_depth_path(idx))\n                if depth.ndim == 2:\n                    depth = depth[None, ...]  # (1, H, W)\n                image_with_depth = torch.cat(\n                    [tvf.to_tensor(image), depth], dim=0\n                )  # (4, H, W)\n            except Exception:\n                logging.warning(\n                    f\"Couldn't load depth image: {self.depth_paths[idx]}. Exception:\",\n                    exc_info=True,\n                )\n                is_success = False\n\n        if not is_success:\n            image_with_depth = self.default_generator()\n\n        return image_with_depth, is_success\n\n\nclass VideoPathDataset(PathDataset):\n    def __init__(\n        self,\n        clip_sampler,\n        frame_sampler,\n        decoder,\n        normalize_to_0_1,\n        *args,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return path\n\n    def default_generator(self):\n        image = get_mean_image(DEFAULT_SPATIAL_SIZE)\n        depth = torch.zeros(\n            (1, DEFAULT_SPATIAL_SIZE, DEFAULT_SPATIAL_SIZE), dtype=torch.float32\n        )\n        return torch.cat([tvf.to_tensor(image), depth], dim=0)\n\n    def try_load_object(self, idx):\n        image, is_success = super().try_load_object(idx)\n        if is_success:\n            try:\n                depth = self._load_depth(self._get_depth_path(idx))\n                if depth.ndim == 2:\n                    depth = depth[None, ...]  # (1, H, W)\n                image_with_depth = torch.cat(\n                    [tvf.to_tensor(image), depth], dim=0\n                )  # (4, H, W)\n            except Exception:\n                logging.warning(\n                    f\"Couldn't load depth image: {self.depth_paths[idx]}. Exception:\",\n                    exc_info=True,\n                )\n                is_success = False\n\n        if not is_success:\n            image_with_depth = self.default_generator()\n\n        return image_with_depth, is_success\n\n\nclass VideoPathDataset(PathDataset):\n    def __init__(\n        self,\n        clip_sampler,\n        frame_sampler,\n        decoder,\n        normalize_to_0_1,\n        *args,\n        decoder_kwargs=None,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.clip_sampler = clip_sampler\n        self.frame_sampler = frame_sampler\n        self.decoder = decoder\n        self.normalize_to_0_1 = normalize_to_0_1\n        self.decoder_kwargs = {} if decoder_kwargs is None else decoder_kwargs\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        image, is_success = super().try_load_object(idx)\n        if is_success:\n            try:\n                depth = self._load_depth(self._get_depth_path(idx))\n                if depth.ndim == 2:\n                    depth = depth[None, ...]  # (1, H, W)\n                image_with_depth = torch.cat(\n                    [tvf.to_tensor(image), depth], dim=0\n                )  # (4, H, W)\n            except Exception:\n                logging.warning(\n                    f\"Couldn't load depth image: {self.depth_paths[idx]}. Exception:\",\n                    exc_info=True,\n                )\n                is_success = False\n\n        if not is_success:\n            image_with_depth = self.default_generator()\n\n        return image_with_depth, is_success\n\n\nclass VideoPathDataset(PathDataset):\n    def __init__(\n        self,\n        clip_sampler,\n        frame_sampler,\n        decoder,\n        normalize_to_0_1,\n        *args,\n        decoder_kwargs=None,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.clip_sampler = clip_sampler\n        self.frame_sampler = frame_sampler\n        self.decoder = decoder\n        self.normalize_to_0_1 = normalize_to_0_1\n        self.decoder_kwargs = {} if decoder_kwargs is None else decoder_kwargs\n\n    def _get_video_object(self, path):\n        return EncodedVideo.from_path(\n            path, decoder=self.decoder, decode_audio=False, **self.decoder_kwargs\n        )\n\n    def load_object(self, path) -> List[torch.Tensor]:\n        \"\"\"\n        Returns:\n            A (C, T, H, W) tensor.\n        \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                logging.warning(\n                    f\"Couldn't load depth image: {self.depth_paths[idx]}. Exception:\",\n                    exc_info=True,\n                )\n                is_success = False\n\n        if not is_success:\n            image_with_depth = self.default_generator()\n\n        return image_with_depth, is_success\n\n\nclass VideoPathDataset(PathDataset):\n    def __init__(\n        self,\n        clip_sampler,\n        frame_sampler,\n        decoder,\n        normalize_to_0_1,\n        *args,\n        decoder_kwargs=None,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.clip_sampler = clip_sampler\n        self.frame_sampler = frame_sampler\n        self.decoder = decoder\n        self.normalize_to_0_1 = normalize_to_0_1\n        self.decoder_kwargs = {} if decoder_kwargs is None else decoder_kwargs\n\n    def _get_video_object(self, path):\n        return EncodedVideo.from_path(\n            path, decoder=self.decoder, decode_audio=False, **self.decoder_kwargs\n        )\n\n    def load_object(self, path) -> List[torch.Tensor]:\n        \"\"\"\n        Returns:\n            A (C, T, H, W) tensor.\n        \"\"\"\n        video = self._get_video_object(path)\n        # Read out all clips in this video\n        all_clips_timepoints = []\n        is_last_clip = False\n        end = 0.0\n        while not is_last_clip:\n            start, end, _, _, is_last_clip = self.clip_sampler(\n                end, video.duration, annotation=None\n            )\n            all_clips_timepoints.append((start, end))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass VideoPathDataset(PathDataset):\n    def __init__(\n        self,\n        clip_sampler,\n        frame_sampler,\n        decoder,\n        normalize_to_0_1,\n        *args,\n        decoder_kwargs=None,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.clip_sampler = clip_sampler\n        self.frame_sampler = frame_sampler\n        self.decoder = decoder\n        self.normalize_to_0_1 = normalize_to_0_1\n        self.decoder_kwargs = {} if decoder_kwargs is None else decoder_kwargs\n\n    def _get_video_object(self, path):\n        return EncodedVideo.from_path(\n            path, decoder=self.decoder, decode_audio=False, **self.decoder_kwargs\n        )\n\n    def load_object(self, path) -> List[torch.Tensor]:\n        \"\"\"\n        Returns:\n            A (C, T, H, W) tensor.\n        \"\"\"\n        video = self._get_video_object(path)\n        # Read out all clips in this video\n        all_clips_timepoints = []\n        is_last_clip = False\n        end = 0.0\n        while not is_last_clip:\n            start, end, _, _, is_last_clip = self.clip_sampler(\n                end, video.duration, annotation=None\n            )\n            all_clips_timepoints.append((start, end))\n        all_frames = []\n        for clip_timepoints in all_clips_timepoints:\n            # Read the clip, get frames\n            clip = video.get_clip(clip_timepoints[0], clip_timepoints[1])[\"video\"]\n            if clip is None:\n                logging.error(\n                    \"Got a None clip. Make sure the clip timepoints \"\n                    \"are long enough: %s\",\n                    clip_timepoints,\n                )\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argargargargargargargConstantarg)Expr(Call(Attribute(Call(Name(Load))Load)Starred(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)IfExp(Compare(Name(Load)IsConstant)DictName(Load))))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load))keyword(Constant)keyword(Attribute(Name(Load)Load)))))FunctionDef(arguments(argarg)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)List(Load))Assign(Name(Store)Constant)Assign(Name(Store)Constant)While(UnaryOp(NotName(Load))Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)Attribute(Name(Load)Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Load))))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Assign(Name(Store)Subscript(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)Subscript(Name(Load)ConstantLoad))ConstantLoad))If(Compare(Name(Load)IsConstant)Expr(Call(Attribute(Name(Load)Load)ConstantName(Load)))))Subscript(Name(Load)Attribute(Name(Load)Load)Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        decoder_kwargs=None,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.clip_sampler = clip_sampler\n        self.frame_sampler = frame_sampler\n        self.decoder = decoder\n        self.normalize_to_0_1 = normalize_to_0_1\n        self.decoder_kwargs = {} if decoder_kwargs is None else decoder_kwargs\n\n    def _get_video_object(self, path):\n        return EncodedVideo.from_path(\n            path, decoder=self.decoder, decode_audio=False, **self.decoder_kwargs\n        )\n\n    def load_object(self, path) -> List[torch.Tensor]:\n        \"\"\"\n        Returns:\n            A (C, T, H, W) tensor.\n        \"\"\"\n        video = self._get_video_object(path)\n        # Read out all clips in this video\n        all_clips_timepoints = []\n        is_last_clip = False\n        end = 0.0\n        while not is_last_clip:\n            start, end, _, _, is_last_clip = self.clip_sampler(\n                end, video.duration, annotation=None\n            )\n            all_clips_timepoints.append((start, end))\n        all_frames = []\n        for clip_timepoints in all_clips_timepoints:\n            # Read the clip, get frames\n            clip = video.get_clip(clip_timepoints[0], clip_timepoints[1])[\"video\"]\n            if clip is None:\n                logging.error(\n                    \"Got a None clip. Make sure the clip timepoints \"\n                    \"are long enough: %s\",\n                    clip_timepoints,\n                )\n            frames = self.frame_sampler(clip)\n            if self.normalize_to_0_1:\n                frames = frames / 255.0  # since this is float, need 0-1\n            all_frames.append(frames)\n        if len(all_frames) == 1:\n            # When only one clip is sampled (eg at training time), remove the\n            # outermost list object so it can work with default collators etc.\n            all_frames = all_frames[0]\n        return all_frames\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def _get_video_object(self, path):\n        return EncodedVideo.from_path(\n            path, decoder=self.decoder, decode_audio=False, **self.decoder_kwargs\n        )\n\n    def load_object(self, path) -> List[torch.Tensor]:\n        \"\"\"\n        Returns:\n            A (C, T, H, W) tensor.\n        \"\"\"\n        video = self._get_video_object(path)\n        # Read out all clips in this video\n        all_clips_timepoints = []\n        is_last_clip = False\n        end = 0.0\n        while not is_last_clip:\n            start, end, _, _, is_last_clip = self.clip_sampler(\n                end, video.duration, annotation=None\n            )\n            all_clips_timepoints.append((start, end))\n        all_frames = []\n        for clip_timepoints in all_clips_timepoints:\n            # Read the clip, get frames\n            clip = video.get_clip(clip_timepoints[0], clip_timepoints[1])[\"video\"]\n            if clip is None:\n                logging.error(\n                    \"Got a None clip. Make sure the clip timepoints \"\n                    \"are long enough: %s\",\n                    clip_timepoints,\n                )\n            frames = self.frame_sampler(clip)\n            if self.normalize_to_0_1:\n                frames = frames / 255.0  # since this is float, need 0-1\n            all_frames.append(frames)\n        if len(all_frames) == 1:\n            # When only one clip is sampled (eg at training time), remove the\n            # outermost list object so it can work with default collators etc.\n            all_frames = all_frames[0]\n        return all_frames\n\n    def default_generator(self):\n        dummy = (\n            torch.ones(\n                (\n                    3,\n                    self.frame_sampler._num_samples,\n                    DEFAULT_SPATIAL_SIZE,\n                    DEFAULT_SPATIAL_SIZE,\n                )\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 330, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        video = self._get_video_object(path)\n        # Read out all clips in this video\n        all_clips_timepoints = []\n        is_last_clip = False\n        end = 0.0\n        while not is_last_clip:\n            start, end, _, _, is_last_clip = self.clip_sampler(\n                end, video.duration, annotation=None\n            )\n            all_clips_timepoints.append((start, end))\n        all_frames = []\n        for clip_timepoints in all_clips_timepoints:\n            # Read the clip, get frames\n            clip = video.get_clip(clip_timepoints[0], clip_timepoints[1])[\"video\"]\n            if clip is None:\n                logging.error(\n                    \"Got a None clip. Make sure the clip timepoints \"\n                    \"are long enough: %s\",\n                    clip_timepoints,\n                )\n            frames = self.frame_sampler(clip)\n            if self.normalize_to_0_1:\n                frames = frames / 255.0  # since this is float, need 0-1\n            all_frames.append(frames)\n        if len(all_frames) == 1:\n            # When only one clip is sampled (eg at training time), remove the\n            # outermost list object so it can work with default collators etc.\n            all_frames = all_frames[0]\n        return all_frames\n\n    def default_generator(self):\n        dummy = (\n            torch.ones(\n                (\n                    3,\n                    self.frame_sampler._num_samples,\n                    DEFAULT_SPATIAL_SIZE,\n                    DEFAULT_SPATIAL_SIZE,\n                )\n            )\n            * 0.5\n        )\n        if hasattr(self.clip_sampler, \"_clips_per_video\"):\n            return [dummy] * self.clip_sampler._clips_per_video\n        return dummy", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_285-330"}
{"title": "facebookresearch_omnivore-omnivision-data-path_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "path_dataset.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 330, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        all_frames = []\n        for clip_timepoints in all_clips_timepoints:\n            # Read the clip, get frames\n            clip = video.get_clip(clip_timepoints[0], clip_timepoints[1])[\"video\"]\n            if clip is None:\n                logging.error(\n                    \"Got a None clip. Make sure the clip timepoints \"\n                    \"are long enough: %s\",\n                    clip_timepoints,\n                )\n            frames = self.frame_sampler(clip)\n            if self.normalize_to_0_1:\n                frames = frames / 255.0  # since this is float, need 0-1\n            all_frames.append(frames)\n        if len(all_frames) == 1:\n            # When only one clip is sampled (eg at training time), remove the\n            # outermost list object so it can work with default collators etc.\n            all_frames = all_frames[0]\n        return all_frames\n\n    def default_generator(self):\n        dummy = (\n            torch.ones(\n                (\n                    3,\n                    self.frame_sampler._num_samples,\n                    DEFAULT_SPATIAL_SIZE,\n                    DEFAULT_SPATIAL_SIZE,\n                )\n            )\n            * 0.5\n        )\n        if hasattr(self.clip_sampler, \"_clips_per_video\"):\n            return [dummy] * self.clip_sampler._clips_per_video\n        return dummy", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-path_dataset.py_295-330"}
{"title": "facebookresearch_omnivore-omnivision-data-synthetic_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "synthetic_dataset.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom omnivision.data.api import VisionSample\nfrom torch.utils.data import Dataset\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, tensor_shape, length, label=0, value=1) -> None:\n        self.tensor = torch.full(tuple(tensor_shape), float(value))\n        self.label = label\n        self.length = length\n\n    def __len__(self) -> int:\n        return self.length\n\n    def __getitem__(self, idx) -> torch.Tensor:\n        return VisionSample(\n            vision=self.tensor,\n            label=self.label,\n            data_idx=idx,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-synthetic_dataset.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-synthetic_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "synthetic_dataset.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 27, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "synthetic_dataset.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 27, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom omnivision.data.api import VisionSample\nfrom torch.utils.data import Dataset\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, tensor_shape, length, label=0, value=1) -> None:\n        self.tensor = torch.full(tuple(tensor_shape), float(value))\n        self.label = label\n        self.length = length\n\n    def __len__(self) -> int:\n        return self.length\n\n    def __getitem__(self, idx) -> torch.Tensor:\n        return VisionSample(\n            vision=self.tensor,\n            label=self.label,\n            data_idx=idx,\n            data_valid=True,\n        )\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argargargargargConstantConstant)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))Call(Name(Load)Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Constant)FunctionDef(arguments(arg)Return(Attribute(Name(Load)Load))Name(Load))FunctionDef(arguments(argarg)Return(Call(Name(Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Name(Load))keyword(Constant)))Attribute(Name(Load)Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-synthetic_dataset.py_0-27"}
{"title": "facebookresearch_omnivore-omnivision-data-torch_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "torch_dataset.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Callable, Iterable, Optional\n\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler, IterableDataset\n\nfrom .omni_dataset import OmniDataset\n\n\nclass TorchDataset(OmniDataset):\n    def __init__(\n        self,\n        dataset: Dataset,\n        batch_size: int,\n        num_workers: int,\n        shuffle: bool,\n        pin_memory: bool,\n        drop_last: bool,\n        collate_fn: Optional[Callable] = None,\n        worker_init_fn: Optional[Callable] = None,\n    ) -> None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-torch_dataset.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-torch_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "torch_dataset.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Callable, Iterable, Optional\n\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler, IterableDataset\n\nfrom .omni_dataset import OmniDataset\n\n\nclass TorchDataset(OmniDataset):\n    def __init__(\n        self,\n        dataset: Dataset,\n        batch_size: int,\n        num_workers: int,\n        shuffle: bool,\n        pin_memory: bool,\n        drop_last: bool,\n        collate_fn: Optional[Callable] = None,\n        worker_init_fn: Optional[Callable] = None,\n    ) -> None:\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.shuffle = shuffle\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.collate_fn = collate_fn\n        self.worker_init_fn = worker_init_fn\n        assert not isinstance(self.dataset, IterableDataset), \"Not supported yet\"\n        self.sampler = DistributedSampler(self.dataset, shuffle=self.shuffle)\n\nAST=Module(ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))ConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assert(UnaryOp(NotCall(Name(Load)Attribute(Name(Load)Load)Name(Load)))Constant)Assign(Attribute(Name(Load)Store)Call(Name(Load)Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))))Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-torch_dataset.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-torch_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "torch_dataset.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Callable, Iterable, Optional\n\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler, IterableDataset\n\nfrom .omni_dataset import OmniDataset\n\n\nclass TorchDataset(OmniDataset):\n    def __init__(\n        self,\n        dataset: Dataset,\n        batch_size: int,\n        num_workers: int,\n        shuffle: bool,\n        pin_memory: bool,\n        drop_last: bool,\n        collate_fn: Optional[Callable] = None,\n        worker_init_fn: Optional[Callable] = None,\n    ) -> None:\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.shuffle = shuffle\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.collate_fn = collate_fn\n        self.worker_init_fn = worker_init_fn\n        assert not isinstance(self.dataset, IterableDataset), \"Not supported yet\"\n        self.sampler = DistributedSampler(self.dataset, shuffle=self.shuffle)\n\n    def get_loader(self, epoch) -> Iterable:\n        self.sampler.set_epoch(epoch)\n\n        return DataLoader(\n            self.dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            drop_last=self.drop_last,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-torch_dataset.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-torch_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "torch_dataset.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 49, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom typing import Callable, Iterable, Optional\n\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler, IterableDataset\n\nfrom .omni_dataset import OmniDataset\n\n\nclass TorchDataset(OmniDataset):\n    def __init__(\n        self,\n        dataset: Dataset,\n        batch_size: int,\n        num_workers: int,\n        shuffle: bool,\n        pin_memory: bool,\n        drop_last: bool,\n        collate_fn: Optional[Callable] = None,\n        worker_init_fn: Optional[Callable] = None,\n    ) -> None:\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.shuffle = shuffle\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.collate_fn = collate_fn\n        self.worker_init_fn = worker_init_fn\n        assert not isinstance(self.dataset, IterableDataset), \"Not supported yet\"\n        self.sampler = DistributedSampler(self.dataset, shuffle=self.shuffle)\n\n    def get_loader(self, epoch) -> Iterable:\n        self.sampler.set_epoch(epoch)\n\n        return DataLoader(\n            self.dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            drop_last=self.drop_last,\n            sampler=self.sampler,\n            collate_fn=self.collate_fn,\n            worker_init_fn=self.worker_init_fn,\n        )\n\nAST=Module(ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))ConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assert(UnaryOp(NotCall(Name(Load)Attribute(Name(Load)Load)Name(Load)))Constant)Assign(Attribute(Name(Load)Store)Call(Name(Load)Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))))Constant)FunctionDef(arguments(argarg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Return(Call(Name(Load)Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-torch_dataset.py_5-49"}
{"title": "facebookresearch_omnivore-omnivision-data-torch_dataset.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "torch_dataset.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 49, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self,\n        dataset: Dataset,\n        batch_size: int,\n        num_workers: int,\n        shuffle: bool,\n        pin_memory: bool,\n        drop_last: bool,\n        collate_fn: Optional[Callable] = None,\n        worker_init_fn: Optional[Callable] = None,\n    ) -> None:\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.shuffle = shuffle\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.collate_fn = collate_fn\n        self.worker_init_fn = worker_init_fn\n        assert not isinstance(self.dataset, IterableDataset), \"Not supported yet\"\n        self.sampler = DistributedSampler(self.dataset, shuffle=self.shuffle)\n\n    def get_loader(self, epoch) -> Iterable:\n        self.sampler.set_epoch(epoch)\n\n        return DataLoader(\n            self.dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            drop_last=self.drop_last,\n            sampler=self.sampler,\n            collate_fn=self.collate_fn,\n            worker_init_fn=self.worker_init_fn,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-torch_dataset.py_15-49"}
{"title": "facebookresearch_omnivore-omnivision-data-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\nAST=Module", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-__init__.py_0-1"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.transforms.pytorchvideo import uniform_crop\nfrom PIL import Image\nfrom torchvision import transforms\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = tuple(ordering)\n\n    def forward(self, frames):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.transforms.pytorchvideo import uniform_crop\nfrom PIL import Image\nfrom torchvision import transforms\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = tuple(ordering)\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass PILToRGB(nn.Module):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.transforms.pytorchvideo import uniform_crop\nfrom PIL import Image\nfrom torchvision import transforms\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = tuple(ordering)\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass PILToRGB(nn.Module):\n    \"\"\"\n    PIL Image to RGB\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, image: Image) -> Image:\n        return image.convert(\"RGB\")\n\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Name(Load))))FunctionDef(arguments(argarg)Expr(Constant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load)))Constant)FunctionDef(arguments(argarg(Name(Load)))Return(Call(Attribute(Name(Load)Load)Constant))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.transforms.pytorchvideo import uniform_crop\nfrom PIL import Image\nfrom torchvision import transforms\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = tuple(ordering)\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass PILToRGB(nn.Module):\n    \"\"\"\n    PIL Image to RGB\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, image: Image) -> Image:\n        return image.convert(\"RGB\")\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Name(Load))))FunctionDef(arguments(argarg)Expr(Constant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load)))Constant)FunctionDef(arguments(argarg(Name(Load)))Return(Call(Attribute(Name(Load)Load)Constant))Name(Load)))ClassDef(Attribute(Name(Load)Load)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = tuple(ordering)\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass PILToRGB(nn.Module):\n    \"\"\"\n    PIL Image to RGB\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, image: Image) -> Image:\n        return image.convert(\"RGB\")\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Name(Load))))FunctionDef(arguments(argarg)Expr(Constant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load)))Constant)FunctionDef(arguments(argarg(Name(Load)))Return(Call(Attribute(Name(Load)Load)Constant))Name(Load)))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))Assign(Attribute(Name(Load)Store)List(Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass PILToRGB(nn.Module):\n    \"\"\"\n    PIL Image to RGB\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, image: Image) -> Image:\n        return image.convert(\"RGB\")\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, video: torch.Tensor) -> Sequence[torch.Tensor]:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    PIL Image to RGB\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, image: Image) -> Image:\n        return image.convert(\"RGB\")\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, video: torch.Tensor) -> Sequence[torch.Tensor]:\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert video.ndim == 4, \"Must be (C,T,H,W)\"\n        res = []\n        for spatial_idx in self.crops_to_ext:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 92, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, video: torch.Tensor) -> Sequence[torch.Tensor]:\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert video.ndim == 4, \"Must be (C,T,H,W)\"\n        res = []\n        for spatial_idx in self.crops_to_ext:\n            res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        if not self.flipped_crops_to_ext:\n            return res\n        flipped_video = transforms.functional.hflip(video)\n        for spatial_idx in self.flipped_crops_to_ext:\n            res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))Assign(Attribute(Name(Load)Store)List(Load))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantLoad))Assign(Attribute(Name(Load)Store)List(Load))Raise(Call(Name(Load)Constant))))))FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Expr(Constant)Assert(Compare(Attribute(Name(Load)Load)EqConstant)Constant)Assign(Name(Store)List(Load))For(Name(Store)Attribute(Name(Load)Load)Expr(Call(Attribute(Name(Load)Load)Subscript(Call(Name(Load)Name(Load)Attribute(Name(Load)Load)Name(Load))ConstantLoad))))If(UnaryOp(NotAttribute(Name(Load)Load))Return(Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))For(Name(Store)Attribute(Name(Load)Load)Expr(Call(Attribute(Name(Load)Load)Subscript(Call(Name(Load)Name(Load)Attribute(Name(Load)Load)Name(Load))ConstantLoad))))Return(Name(Load))Subscript(Name(Load)Attribute(Name(Load)Load)Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_45-92"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 92, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, video: torch.Tensor) -> Sequence[torch.Tensor]:\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert video.ndim == 4, \"Must be (C,T,H,W)\"\n        res = []\n        for spatial_idx in self.crops_to_ext:\n            res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        if not self.flipped_crops_to_ext:\n            return res\n        flipped_video = transforms.functional.hflip(video)\n        for spatial_idx in self.flipped_crops_to_ext:\n            res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_55-92"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-basic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "basic.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 92, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, video: torch.Tensor) -> Sequence[torch.Tensor]:\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert video.ndim == 4, \"Must be (C,T,H,W)\"\n        res = []\n        for spatial_idx in self.crops_to_ext:\n            res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        if not self.flipped_crops_to_ext:\n            return res\n        flipped_video = transforms.functional.hflip(video)\n        for spatial_idx in self.flipped_crops_to_ext:\n            res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-basic.py_65-92"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/mixup.py,\npublished under an Apache License 2.0, with modifications by Matthew Leavitt\n(ito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n- _mix_batch.__call__() now checks device of data its passed, and passes\ndevice argument accordingly. Previous behavior allowed called functions to\ndefault to using cuda, which caused an error when using CPU-based data.\n\nCOMMENT FROM ORIGINAL:\nMixup and Cutmix\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899) # NOQA\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/mixup.py,\npublished under an Apache License 2.0, with modifications by Matthew Leavitt\n(ito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n- _mix_batch.__call__() now checks device of data its passed, and passes\ndevice argument accordingly. Previous behavior allowed called functions to\ndefault to using cuda, which caused an error when using CPU-based data.\n\nCOMMENT FROM ORIGINAL:\nMixup and Cutmix\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899) # NOQA\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport collections.abc as abc\nimport logging\nfrom typing import Any, Callable\n\nimport numpy as np\nimport torch\nfrom omnivision.data.api import VisionSample\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)ImportFrom(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/mixup.py,\npublished under an Apache License 2.0, with modifications by Matthew Leavitt\n(ito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n- _mix_batch.__call__() now checks device of data its passed, and passes\ndevice argument accordingly. Previous behavior allowed called functions to\ndefault to using cuda, which caused an error when using CPU-based data.\n\nCOMMENT FROM ORIGINAL:\nMixup and Cutmix\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899) # NOQA\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport collections.abc as abc\nimport logging\nfrom typing import Any, Callable\n\nimport numpy as np\nimport torch\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.generic import convert_to_one_hot\n\n\nclass CutMixUp(Callable):\n    def __init__(self, **kwargs) -> None:\n        self.kwargs = kwargs\n\n    def __call__(self, batch):\n        \"\"\"\n        This collator implements CutMix (https://arxiv.org/abs/1905.04899) and/or", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/mixup.py,\npublished under an Apache License 2.0, with modifications by Matthew Leavitt\n(ito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n- _mix_batch.__call__() now checks device of data its passed, and passes\ndevice argument accordingly. Previous behavior allowed called functions to\ndefault to using cuda, which caused an error when using CPU-based data.\n\nCOMMENT FROM ORIGINAL:\nMixup and Cutmix\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899) # NOQA\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport collections.abc as abc\nimport logging\nfrom typing import Any, Callable\n\nimport numpy as np\nimport torch\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.generic import convert_to_one_hot\n\n\nclass CutMixUp(Callable):\n    def __init__(self, **kwargs) -> None:\n        self.kwargs = kwargs\n\n    def __call__(self, batch):\n        \"\"\"\n        This collator implements CutMix (https://arxiv.org/abs/1905.04899) and/or\n        MixUp (https://arxiv.org/abs/1710.09412) via ClassyVision's\n        implementation (link when publicly available).\n\n        kwargs:\n        :mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        :cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        :cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active\n        and uses this vs alpha if not None.\n        :prob (float): probability of applying mixup or cutmix per batch or element\n        :switch_prob (float): probability of switching to cutmix instead of mixup", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "device argument accordingly. Previous behavior allowed called functions to\ndefault to using cuda, which caused an error when using CPU-based data.\n\nCOMMENT FROM ORIGINAL:\nMixup and Cutmix\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899) # NOQA\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport collections.abc as abc\nimport logging\nfrom typing import Any, Callable\n\nimport numpy as np\nimport torch\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.generic import convert_to_one_hot\n\n\nclass CutMixUp(Callable):\n    def __init__(self, **kwargs) -> None:\n        self.kwargs = kwargs\n\n    def __call__(self, batch):\n        \"\"\"\n        This collator implements CutMix (https://arxiv.org/abs/1905.04899) and/or\n        MixUp (https://arxiv.org/abs/1710.09412) via ClassyVision's\n        implementation (link when publicly available).\n\n        kwargs:\n        :mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        :cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        :cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active\n        and uses this vs alpha if not None.\n        :prob (float): probability of applying mixup or cutmix per batch or element\n        :switch_prob (float): probability of switching to cutmix instead of mixup\n        when both are active\n        :mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of\n        elements), 'elem' (element)\n        :correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        :label_smoothing (float): apply label smoothing to the mixed target tensor\n        :num_classes (int): number of classes for target\n\n\n        The collators collates the batch for the following input (assuming k-copies of image):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "Hacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport collections.abc as abc\nimport logging\nfrom typing import Any, Callable\n\nimport numpy as np\nimport torch\nfrom omnivision.data.api import VisionSample\nfrom omnivision.utils.generic import convert_to_one_hot\n\n\nclass CutMixUp(Callable):\n    def __init__(self, **kwargs) -> None:\n        self.kwargs = kwargs\n\n    def __call__(self, batch):\n        \"\"\"\n        This collator implements CutMix (https://arxiv.org/abs/1905.04899) and/or\n        MixUp (https://arxiv.org/abs/1710.09412) via ClassyVision's\n        implementation (link when publicly available).\n\n        kwargs:\n        :mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        :cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        :cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active\n        and uses this vs alpha if not None.\n        :prob (float): probability of applying mixup or cutmix per batch or element\n        :switch_prob (float): probability of switching to cutmix instead of mixup\n        when both are active\n        :mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of\n        elements), 'elem' (element)\n        :correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        :label_smoothing (float): apply label smoothing to the mixed target tensor\n        :num_classes (int): number of classes for target\n\n\n        The collators collates the batch for the following input (assuming k-copies of image):\n\n        Input:\n            batch: Example\n                    batch = [\n                        {\"data\" : [img1_0, ..., img1_k], ..},\n                        {\"data\" : [img2_0, ..., img2_k], ...},\n                        ...\n                    ]\n\n        Returns: Example output:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from omnivision.utils.generic import convert_to_one_hot\n\n\nclass CutMixUp(Callable):\n    def __init__(self, **kwargs) -> None:\n        self.kwargs = kwargs\n\n    def __call__(self, batch):\n        \"\"\"\n        This collator implements CutMix (https://arxiv.org/abs/1905.04899) and/or\n        MixUp (https://arxiv.org/abs/1710.09412) via ClassyVision's\n        implementation (link when publicly available).\n\n        kwargs:\n        :mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        :cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        :cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active\n        and uses this vs alpha if not None.\n        :prob (float): probability of applying mixup or cutmix per batch or element\n        :switch_prob (float): probability of switching to cutmix instead of mixup\n        when both are active\n        :mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of\n        elements), 'elem' (element)\n        :correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        :label_smoothing (float): apply label smoothing to the mixed target tensor\n        :num_classes (int): number of classes for target\n\n\n        The collators collates the batch for the following input (assuming k-copies of image):\n\n        Input:\n            batch: Example\n                    batch = [\n                        {\"data\" : [img1_0, ..., img1_k], ..},\n                        {\"data\" : [img2_0, ..., img2_k], ...},\n                        ...\n                    ]\n\n        Returns: Example output:\n                    output = {\n                                \"data\": torch.tensor([img1_0, ..., imgN_0],\n                                    [img1_k, ..., imgN_k]) ..\n                            }\n        \"\"\"\n        assert isinstance(batch, VisionSample)\n\n        # Instantiate CutMix + Mixup (CutMixUp!) object\n        cutmixup_transform_obj = Mixup(**self.kwargs)\n\n\nAST=Module(ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg)Assign(Attribute(Name(Load)Store)Name(Load))Constant)FunctionDef(arguments(argarg)Expr(Constant)Assert(Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)keyword(Attribute(Name(Load)Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        MixUp (https://arxiv.org/abs/1710.09412) via ClassyVision's\n        implementation (link when publicly available).\n\n        kwargs:\n        :mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        :cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        :cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active\n        and uses this vs alpha if not None.\n        :prob (float): probability of applying mixup or cutmix per batch or element\n        :switch_prob (float): probability of switching to cutmix instead of mixup\n        when both are active\n        :mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of\n        elements), 'elem' (element)\n        :correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        :label_smoothing (float): apply label smoothing to the mixed target tensor\n        :num_classes (int): number of classes for target\n\n\n        The collators collates the batch for the following input (assuming k-copies of image):\n\n        Input:\n            batch: Example\n                    batch = [\n                        {\"data\" : [img1_0, ..., img1_k], ..},\n                        {\"data\" : [img2_0, ..., img2_k], ...},\n                        ...\n                    ]\n\n        Returns: Example output:\n                    output = {\n                                \"data\": torch.tensor([img1_0, ..., imgN_0],\n                                    [img1_k, ..., imgN_k]) ..\n                            }\n        \"\"\"\n        assert isinstance(batch, VisionSample)\n\n        # Instantiate CutMix + Mixup (CutMixUp!) object\n        cutmixup_transform_obj = Mixup(**self.kwargs)\n\n        # Get data and labels into format accepted by Mixup\n        cutmixup_output = cutmixup_transform_obj(\n            {\"input\": batch.vision, \"target\": batch.label}\n        )\n        batch.vision = cutmixup_output[\"input\"]\n        batch.label = cutmixup_output[\"target\"]\n\n        return batch\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        when both are active\n        :mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of\n        elements), 'elem' (element)\n        :correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        :label_smoothing (float): apply label smoothing to the mixed target tensor\n        :num_classes (int): number of classes for target\n\n\n        The collators collates the batch for the following input (assuming k-copies of image):\n\n        Input:\n            batch: Example\n                    batch = [\n                        {\"data\" : [img1_0, ..., img1_k], ..},\n                        {\"data\" : [img2_0, ..., img2_k], ...},\n                        ...\n                    ]\n\n        Returns: Example output:\n                    output = {\n                                \"data\": torch.tensor([img1_0, ..., imgN_0],\n                                    [img1_k, ..., imgN_k]) ..\n                            }\n        \"\"\"\n        assert isinstance(batch, VisionSample)\n\n        # Instantiate CutMix + Mixup (CutMixUp!) object\n        cutmixup_transform_obj = Mixup(**self.kwargs)\n\n        # Get data and labels into format accepted by Mixup\n        cutmixup_output = cutmixup_transform_obj(\n            {\"input\": batch.vision, \"target\": batch.label}\n        )\n        batch.vision = cutmixup_output[\"input\"]\n        batch.label = cutmixup_output[\"target\"]\n\n        return batch\n\n\n# Modification/addition\ndef data_back_to_input_form(data, labels, data_valid, data_idx):\n    \"\"\"\n    \"De\"-collates data back into their form when originally passed.\n    \"\"\"\n    assert len(data) == len(labels)\n    assert len(data_idx) == len(data_valid)\n    data_input_form = []\n    num_duplicates, num_images = len(data), len(data[0])\n    for sample_i in range(num_images):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        Input:\n            batch: Example\n                    batch = [\n                        {\"data\" : [img1_0, ..., img1_k], ..},\n                        {\"data\" : [img2_0, ..., img2_k], ...},\n                        ...\n                    ]\n\n        Returns: Example output:\n                    output = {\n                                \"data\": torch.tensor([img1_0, ..., imgN_0],\n                                    [img1_k, ..., imgN_k]) ..\n                            }\n        \"\"\"\n        assert isinstance(batch, VisionSample)\n\n        # Instantiate CutMix + Mixup (CutMixUp!) object\n        cutmixup_transform_obj = Mixup(**self.kwargs)\n\n        # Get data and labels into format accepted by Mixup\n        cutmixup_output = cutmixup_transform_obj(\n            {\"input\": batch.vision, \"target\": batch.label}\n        )\n        batch.vision = cutmixup_output[\"input\"]\n        batch.label = cutmixup_output[\"target\"]\n\n        return batch\n\n\n# Modification/addition\ndef data_back_to_input_form(data, labels, data_valid, data_idx):\n    \"\"\"\n    \"De\"-collates data back into their form when originally passed.\n    \"\"\"\n    assert len(data) == len(labels)\n    assert len(data_idx) == len(data_valid)\n    data_input_form = []\n    num_duplicates, num_images = len(data), len(data[0])\n    for sample_i in range(num_images):\n        sample_input_form = {\"data\": [], \"data_valid\": [], \"data_idx\": [], \"label\": []}\n        for duplicate_i in range(num_duplicates):\n            valid_and_idx_i = sample_i + (num_duplicates * duplicate_i)\n            sample_input_form[\"data\"].append(data[duplicate_i][sample_i])\n            sample_input_form[\"label\"].append(labels[duplicate_i][sample_i].tolist())\n            sample_input_form[\"data_idx\"].append(data_idx[valid_and_idx_i].item())\n            sample_input_form[\"data_valid\"].append(data_valid[valid_and_idx_i].item())\n        data_input_form.append(sample_input_form)\n    return data_input_form\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    output = {\n                                \"data\": torch.tensor([img1_0, ..., imgN_0],\n                                    [img1_k, ..., imgN_k]) ..\n                            }\n        \"\"\"\n        assert isinstance(batch, VisionSample)\n\n        # Instantiate CutMix + Mixup (CutMixUp!) object\n        cutmixup_transform_obj = Mixup(**self.kwargs)\n\n        # Get data and labels into format accepted by Mixup\n        cutmixup_output = cutmixup_transform_obj(\n            {\"input\": batch.vision, \"target\": batch.label}\n        )\n        batch.vision = cutmixup_output[\"input\"]\n        batch.label = cutmixup_output[\"target\"]\n\n        return batch\n\n\n# Modification/addition\ndef data_back_to_input_form(data, labels, data_valid, data_idx):\n    \"\"\"\n    \"De\"-collates data back into their form when originally passed.\n    \"\"\"\n    assert len(data) == len(labels)\n    assert len(data_idx) == len(data_valid)\n    data_input_form = []\n    num_duplicates, num_images = len(data), len(data[0])\n    for sample_i in range(num_images):\n        sample_input_form = {\"data\": [], \"data_valid\": [], \"data_idx\": [], \"label\": []}\n        for duplicate_i in range(num_duplicates):\n            valid_and_idx_i = sample_i + (num_duplicates * duplicate_i)\n            sample_input_form[\"data\"].append(data[duplicate_i][sample_i])\n            sample_input_form[\"label\"].append(labels[duplicate_i][sample_i].tolist())\n            sample_input_form[\"data_idx\"].append(data_idx[valid_and_idx_i].item())\n            sample_input_form[\"data_valid\"].append(data_valid[valid_and_idx_i].item())\n        data_input_form.append(sample_input_form)\n    return data_input_form\n\n\ndef _recursive_mixup(sample: Any, permuted_indices: torch.Tensor, coeff: float):\n    if isinstance(sample, (tuple, list)):\n        mixed_sample = []\n        for s in sample:\n            mixed_sample.append(_recursive_mixup(s, permuted_indices, coeff))\n\n        return mixed_sample if isinstance(sample, list) else tuple(mixed_sample)\n    elif isinstance(sample, abc.Mapping):\n        mixed_sample = {}", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # Get data and labels into format accepted by Mixup\n        cutmixup_output = cutmixup_transform_obj(\n            {\"input\": batch.vision, \"target\": batch.label}\n        )\n        batch.vision = cutmixup_output[\"input\"]\n        batch.label = cutmixup_output[\"target\"]\n\n        return batch\n\n\n# Modification/addition\ndef data_back_to_input_form(data, labels, data_valid, data_idx):\n    \"\"\"\n    \"De\"-collates data back into their form when originally passed.\n    \"\"\"\n    assert len(data) == len(labels)\n    assert len(data_idx) == len(data_valid)\n    data_input_form = []\n    num_duplicates, num_images = len(data), len(data[0])\n    for sample_i in range(num_images):\n        sample_input_form = {\"data\": [], \"data_valid\": [], \"data_idx\": [], \"label\": []}\n        for duplicate_i in range(num_duplicates):\n            valid_and_idx_i = sample_i + (num_duplicates * duplicate_i)\n            sample_input_form[\"data\"].append(data[duplicate_i][sample_i])\n            sample_input_form[\"label\"].append(labels[duplicate_i][sample_i].tolist())\n            sample_input_form[\"data_idx\"].append(data_idx[valid_and_idx_i].item())\n            sample_input_form[\"data_valid\"].append(data_valid[valid_and_idx_i].item())\n        data_input_form.append(sample_input_form)\n    return data_input_form\n\n\ndef _recursive_mixup(sample: Any, permuted_indices: torch.Tensor, coeff: float):\n    if isinstance(sample, (tuple, list)):\n        mixed_sample = []\n        for s in sample:\n            mixed_sample.append(_recursive_mixup(s, permuted_indices, coeff))\n\n        return mixed_sample if isinstance(sample, list) else tuple(mixed_sample)\n    elif isinstance(sample, abc.Mapping):\n        mixed_sample = {}\n        for key, val in sample.items():\n            mixed_sample[key] = _recursive_mixup(val, permuted_indices, coeff)\n\n        return mixed_sample\n    else:\n        assert torch.is_tensor(sample), \"sample is expected to be a pytorch tensor\"\n        # Assume training data is at least 3D tensor (i.e. 1D data). We only\n        # mixup content data tensor (e.g. video clip), and skip\n        # other tensors, such as frame_idx and timestamp in video clip samples.\n        if sample.ndim >= 3:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Modification/addition\ndef data_back_to_input_form(data, labels, data_valid, data_idx):\n    \"\"\"\n    \"De\"-collates data back into their form when originally passed.\n    \"\"\"\n    assert len(data) == len(labels)\n    assert len(data_idx) == len(data_valid)\n    data_input_form = []\n    num_duplicates, num_images = len(data), len(data[0])\n    for sample_i in range(num_images):\n        sample_input_form = {\"data\": [], \"data_valid\": [], \"data_idx\": [], \"label\": []}\n        for duplicate_i in range(num_duplicates):\n            valid_and_idx_i = sample_i + (num_duplicates * duplicate_i)\n            sample_input_form[\"data\"].append(data[duplicate_i][sample_i])\n            sample_input_form[\"label\"].append(labels[duplicate_i][sample_i].tolist())\n            sample_input_form[\"data_idx\"].append(data_idx[valid_and_idx_i].item())\n            sample_input_form[\"data_valid\"].append(data_valid[valid_and_idx_i].item())\n        data_input_form.append(sample_input_form)\n    return data_input_form\n\n\ndef _recursive_mixup(sample: Any, permuted_indices: torch.Tensor, coeff: float):\n    if isinstance(sample, (tuple, list)):\n        mixed_sample = []\n        for s in sample:\n            mixed_sample.append(_recursive_mixup(s, permuted_indices, coeff))\n\n        return mixed_sample if isinstance(sample, list) else tuple(mixed_sample)\n    elif isinstance(sample, abc.Mapping):\n        mixed_sample = {}\n        for key, val in sample.items():\n            mixed_sample[key] = _recursive_mixup(val, permuted_indices, coeff)\n\n        return mixed_sample\n    else:\n        assert torch.is_tensor(sample), \"sample is expected to be a pytorch tensor\"\n        # Assume training data is at least 3D tensor (i.e. 1D data). We only\n        # mixup content data tensor (e.g. video clip), and skip\n        # other tensors, such as frame_idx and timestamp in video clip samples.\n        if sample.ndim >= 3:\n            sample = coeff * sample + (1.0 - coeff) * sample[permuted_indices, :]\n\n        return sample\n\n\ndef one_hot(x, num_classes, on_value=1.0, off_value=0.0, device=\"cuda\"):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(\n        1, x, on_value\n    )\n\nAST=Module(FunctionDef(arguments(argargargarg)Expr(Constant)Assert(Compare(Call(Name(Load)Name(Load))EqCall(Name(Load)Name(Load))))Assert(Compare(Call(Name(Load)Name(Load))EqCall(Name(Load)Name(Load))))Assign(Name(Store)List(Load))Assign(Tuple(Name(Store)Name(Store)Store)Tuple(Call(Name(Load)Name(Load))Call(Name(Load)Subscript(Name(Load)ConstantLoad))Load))For(Name(Store)Call(Name(Load)Name(Load))Assign(Name(Store)Dict(ConstantConstantConstantConstantList(Load)List(Load)List(Load)List(Load)))For(Name(Store)Call(Name(Load)Name(Load))Assign(Name(Store)BinOp(Name(Load)AddBinOp(Name(Load)MultName(Load))))Expr(Call(Attribute(Subscript(Name(Load)ConstantLoad)Load)Subscript(Subscript(Name(Load)Name(Load)Load)Name(Load)Load)))Expr(Call(Attribute(Subscript(Name(Load)ConstantLoad)Load)Call(Attribute(Subscript(Subscript(Name(Load)Name(Load)Load)Name(Load)Load)Load))))Expr(Call(Attribute(Subscript(Name(Load)ConstantLoad)Load)Call(Attribute(Subscript(Name(Load)Name(Load)Load)Load))))Expr(Call(Attribute(Subscript(Name(Load)ConstantLoad)Load)Call(Attribute(Subscript(Name(Load)Name(Load)Load)Load)))))Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load)))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load))arg(Name(Load)))If(Call(Name(Load)Name(Load)Tuple(Name(Load)Name(Load)Load))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)Name(Load)Name(Load)))))Return(IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)Call(Name(Load)Name(Load))))If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Assign(Name(Store)Dict)For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load)Name(Load)Name(Load)Name(Load))))Return(Name(Load))Assert(Call(Attribute(Name(Load)Load)Name(Load))Constant)If(Compare(Attribute(Name(Load)Load)GtEConstant)Assign(Name(Store)BinOp(BinOp(Name(Load)MultName(Load))AddBinOp(BinOp(ConstantSubName(Load))MultSubscript(Name(Load)Tuple(Name(Load)SliceLoad)Load)))))Return(Name(Load)))))FunctionDef(arguments(argargargargargConstantConstantConstant)Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load))Load)UnaryOp(USubConstant)Constant))Return(Call(Attribute(Call(Attribute(Name(Load)Load)Tuple(Subscript(Call(Attribute(Name(Load)Load))ConstantLoad)Name(Load)Load)Name(Load)keyword(Name(Load)))Load)ConstantName(Load)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        sample_input_form = {\"data\": [], \"data_valid\": [], \"data_idx\": [], \"label\": []}\n        for duplicate_i in range(num_duplicates):\n            valid_and_idx_i = sample_i + (num_duplicates * duplicate_i)\n            sample_input_form[\"data\"].append(data[duplicate_i][sample_i])\n            sample_input_form[\"label\"].append(labels[duplicate_i][sample_i].tolist())\n            sample_input_form[\"data_idx\"].append(data_idx[valid_and_idx_i].item())\n            sample_input_form[\"data_valid\"].append(data_valid[valid_and_idx_i].item())\n        data_input_form.append(sample_input_form)\n    return data_input_form\n\n\ndef _recursive_mixup(sample: Any, permuted_indices: torch.Tensor, coeff: float):\n    if isinstance(sample, (tuple, list)):\n        mixed_sample = []\n        for s in sample:\n            mixed_sample.append(_recursive_mixup(s, permuted_indices, coeff))\n\n        return mixed_sample if isinstance(sample, list) else tuple(mixed_sample)\n    elif isinstance(sample, abc.Mapping):\n        mixed_sample = {}\n        for key, val in sample.items():\n            mixed_sample[key] = _recursive_mixup(val, permuted_indices, coeff)\n\n        return mixed_sample\n    else:\n        assert torch.is_tensor(sample), \"sample is expected to be a pytorch tensor\"\n        # Assume training data is at least 3D tensor (i.e. 1D data). We only\n        # mixup content data tensor (e.g. video clip), and skip\n        # other tensors, such as frame_idx and timestamp in video clip samples.\n        if sample.ndim >= 3:\n            sample = coeff * sample + (1.0 - coeff) * sample[permuted_indices, :]\n\n        return sample\n\n\ndef one_hot(x, num_classes, on_value=1.0, off_value=0.0, device=\"cuda\"):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(\n        1, x, on_value\n    )\n\n\ndef mixup_target(target, num_classes, lam=1.0, smoothing=0.0, device=\"cuda\"):\n    off_value = smoothing / num_classes\n    on_value = 1.0 - smoothing + off_value\n    y1 = one_hot(\n        target, num_classes, on_value=on_value, off_value=off_value, device=device\n    )\n    y2 = one_hot(\n        target.flip(0),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef _recursive_mixup(sample: Any, permuted_indices: torch.Tensor, coeff: float):\n    if isinstance(sample, (tuple, list)):\n        mixed_sample = []\n        for s in sample:\n            mixed_sample.append(_recursive_mixup(s, permuted_indices, coeff))\n\n        return mixed_sample if isinstance(sample, list) else tuple(mixed_sample)\n    elif isinstance(sample, abc.Mapping):\n        mixed_sample = {}\n        for key, val in sample.items():\n            mixed_sample[key] = _recursive_mixup(val, permuted_indices, coeff)\n\n        return mixed_sample\n    else:\n        assert torch.is_tensor(sample), \"sample is expected to be a pytorch tensor\"\n        # Assume training data is at least 3D tensor (i.e. 1D data). We only\n        # mixup content data tensor (e.g. video clip), and skip\n        # other tensors, such as frame_idx and timestamp in video clip samples.\n        if sample.ndim >= 3:\n            sample = coeff * sample + (1.0 - coeff) * sample[permuted_indices, :]\n\n        return sample\n\n\ndef one_hot(x, num_classes, on_value=1.0, off_value=0.0, device=\"cuda\"):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(\n        1, x, on_value\n    )\n\n\ndef mixup_target(target, num_classes, lam=1.0, smoothing=0.0, device=\"cuda\"):\n    off_value = smoothing / num_classes\n    on_value = 1.0 - smoothing + off_value\n    y1 = one_hot(\n        target, num_classes, on_value=on_value, off_value=off_value, device=device\n    )\n    y2 = one_hot(\n        target.flip(0),\n        num_classes,\n        on_value=on_value,\n        off_value=off_value,\n        device=device,\n    )\n    return y1 * lam + y2 * (1.0 - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"Standard CutMix bounding-box", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for key, val in sample.items():\n            mixed_sample[key] = _recursive_mixup(val, permuted_indices, coeff)\n\n        return mixed_sample\n    else:\n        assert torch.is_tensor(sample), \"sample is expected to be a pytorch tensor\"\n        # Assume training data is at least 3D tensor (i.e. 1D data). We only\n        # mixup content data tensor (e.g. video clip), and skip\n        # other tensors, such as frame_idx and timestamp in video clip samples.\n        if sample.ndim >= 3:\n            sample = coeff * sample + (1.0 - coeff) * sample[permuted_indices, :]\n\n        return sample\n\n\ndef one_hot(x, num_classes, on_value=1.0, off_value=0.0, device=\"cuda\"):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(\n        1, x, on_value\n    )\n\n\ndef mixup_target(target, num_classes, lam=1.0, smoothing=0.0, device=\"cuda\"):\n    off_value = smoothing / num_classes\n    on_value = 1.0 - smoothing + off_value\n    y1 = one_hot(\n        target, num_classes, on_value=on_value, off_value=off_value, device=device\n    )\n    y2 = one_hot(\n        target.flip(0),\n        num_classes,\n        on_value=on_value,\n        off_value=off_value,\n        device=device,\n    )\n    return y1 * lam + y2 * (1.0 - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin\n            (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            sample = coeff * sample + (1.0 - coeff) * sample[permuted_indices, :]\n\n        return sample\n\n\ndef one_hot(x, num_classes, on_value=1.0, off_value=0.0, device=\"cuda\"):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(\n        1, x, on_value\n    )\n\n\ndef mixup_target(target, num_classes, lam=1.0, smoothing=0.0, device=\"cuda\"):\n    off_value = smoothing / num_classes\n    on_value = 1.0 - smoothing + off_value\n    y1 = one_hot(\n        target, num_classes, on_value=on_value, off_value=off_value, device=device\n    )\n    y2 = one_hot(\n        target.flip(0),\n        num_classes,\n        on_value=on_value,\n        off_value=off_value,\n        device=device,\n    )\n    return y1 * lam + y2 * (1.0 - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin\n            (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)\n    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef mixup_target(target, num_classes, lam=1.0, smoothing=0.0, device=\"cuda\"):\n    off_value = smoothing / num_classes\n    on_value = 1.0 - smoothing + off_value\n    y1 = one_hot(\n        target, num_classes, on_value=on_value, off_value=off_value, device=device\n    )\n    y2 = one_hot(\n        target.flip(0),\n        num_classes,\n        on_value=on_value,\n        off_value=off_value,\n        device=device,\n    )\n    return y1 * lam + y2 * (1.0 - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin\n            (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)\n    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh\n\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\"Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9\n    range for max.\n    Args:\n        img_shape (tuple): Image shape as tuple", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        num_classes,\n        on_value=on_value,\n        off_value=off_value,\n        device=device,\n    )\n    return y1 * lam + y2 * (1.0 - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin\n            (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)\n    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh\n\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\"Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9\n    range for max.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image\n        size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np.random.randint(\n        int(img_h * minmax[0]), int(img_h * minmax[1]), size=count\n    )\n    cut_w = np.random.randint(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin\n            (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)\n    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh\n\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\"Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9\n    range for max.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image\n        size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np.random.randint(\n        int(img_h * minmax[0]), int(img_h * minmax[1]), size=count\n    )\n    cut_w = np.random.randint(\n        int(img_w * minmax[0]), int(img_w * minmax[1]), size=count\n    )\n    yl = np.random.randint(0, img_h - cut_h, size=count)\n    xl = np.random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh\n\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\"Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9\n    range for max.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image\n        size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np.random.randint(\n        int(img_h * minmax[0]), int(img_h * minmax[1]), size=count\n    )\n    cut_w = np.random.randint(\n        int(img_w * minmax[0]), int(img_w * minmax[1]), size=count\n    )\n    yl = np.random.randint(0, img_h - cut_h, size=count)\n    xl = np.random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(\n    img_shape, lam, ratio_minmax=None, correct_lam=True, count=None\n):\n    \"\"\"Generate bbox and apply lambda correction.\"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1.0 - bbox_area / float(img_shape[-2] * img_shape[-1])", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\"Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9\n    range for max.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image\n        size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np.random.randint(\n        int(img_h * minmax[0]), int(img_h * minmax[1]), size=count\n    )\n    cut_w = np.random.randint(\n        int(img_w * minmax[0]), int(img_w * minmax[1]), size=count\n    )\n    yl = np.random.randint(0, img_h - cut_h, size=count)\n    xl = np.random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(\n    img_shape, lam, ratio_minmax=None, correct_lam=True, count=None\n):\n    \"\"\"Generate bbox and apply lambda correction.\"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1.0 - bbox_area / float(img_shape[-2] * img_shape[-1])\n    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is\n        active and uses this vs alpha if not None.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        minmax (tuple or list): Min and max bbox ratios (as percent of image\n        size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np.random.randint(\n        int(img_h * minmax[0]), int(img_h * minmax[1]), size=count\n    )\n    cut_w = np.random.randint(\n        int(img_w * minmax[0]), int(img_w * minmax[1]), size=count\n    )\n    yl = np.random.randint(0, img_h - cut_h, size=count)\n    xl = np.random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(\n    img_shape, lam, ratio_minmax=None, correct_lam=True, count=None\n):\n    \"\"\"Generate bbox and apply lambda correction.\"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1.0 - bbox_area / float(img_shape[-2] * img_shape[-1])\n    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is\n        active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or\n        element\n        switch_prob (float): probability of switching to cutmix instead of\n        mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair\n        of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        label_smoothing (float): apply label smoothing to the mixed target\n        tensor", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        int(img_w * minmax[0]), int(img_w * minmax[1]), size=count\n    )\n    yl = np.random.randint(0, img_h - cut_h, size=count)\n    xl = np.random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(\n    img_shape, lam, ratio_minmax=None, correct_lam=True, count=None\n):\n    \"\"\"Generate bbox and apply lambda correction.\"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1.0 - bbox_area / float(img_shape[-2] * img_shape[-1])\n    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is\n        active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or\n        element\n        switch_prob (float): probability of switching to cutmix instead of\n        mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair\n        of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        label_smoothing (float): apply label smoothing to the mixed target\n        tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    img_shape, lam, ratio_minmax=None, correct_lam=True, count=None\n):\n    \"\"\"Generate bbox and apply lambda correction.\"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1.0 - bbox_area / float(img_shape[-2] * img_shape[-1])\n    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is\n        active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or\n        element\n        switch_prob (float): probability of switching to cutmix instead of\n        mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair\n        of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        label_smoothing (float): apply label smoothing to the mixed target\n        tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,\n        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is\n        active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or\n        element\n        switch_prob (float): probability of switching to cutmix instead of\n        mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair\n        of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        label_smoothing (float): apply label smoothing to the mixed target\n        tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,\n        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = (\n            correct_lam  # correct lambda based on clipped area for cutmix\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        prob (float): probability of applying mixup or cutmix per batch or\n        element\n        switch_prob (float): probability of switching to cutmix instead of\n        mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair\n        of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by\n        image borders\n        label_smoothing (float): apply label smoothing to the mixed target\n        tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,\n        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = (\n            correct_lam  # correct lambda based on clipped area for cutmix\n        )\n        self.mixup_enabled = (\n            True  # set to false to disable mixing (intended tp be set by train loop)\n        )\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand(batch_size) < self.switch_prob", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,\n        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = (\n            correct_lam  # correct lambda based on clipped area for cutmix\n        )\n        self.mixup_enabled = (\n            True  # set to false to disable mixing (intended tp be set by train loop)\n        )\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(\n                        self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                    ),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(\n                    self.mixup_alpha, self.mixup_alpha, size=batch_size", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = (\n            correct_lam  # correct lambda based on clipped area for cutmix\n        )\n        self.mixup_enabled = (\n            True  # set to false to disable mixing (intended tp be set by train loop)\n        )\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(\n                        self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                    ),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(\n                    self.mixup_alpha, self.mixup_alpha, size=batch_size\n                )\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool)\n                lam_mix = np.random.beta(\n                    self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                )\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = (\n            correct_lam  # correct lambda based on clipped area for cutmix\n        )\n        self.mixup_enabled = (\n            True  # set to false to disable mixing (intended tp be set by train loop)\n        )\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(\n                        self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                    ),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(\n                    self.mixup_alpha, self.mixup_alpha, size=batch_size\n                )\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool)\n                lam_mix = np.random.beta(\n                    self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                )\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = np.where(\n                np.random.rand(batch_size) < self.mix_prob,\n                lam_mix.astype(np.float32),\n                lam,\n            )\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.0", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.mixup_enabled = (\n            True  # set to false to disable mixing (intended tp be set by train loop)\n        )\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(\n                        self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                    ),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(\n                    self.mixup_alpha, self.mixup_alpha, size=batch_size\n                )\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool)\n                lam_mix = np.random.beta(\n                    self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                )\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = np.where(\n                np.random.rand(batch_size) < self.mix_prob,\n                lam_mix.astype(np.float32),\n                lam,\n            )\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.0\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand() < self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha > 0.0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(\n                        self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                    ),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(\n                    self.mixup_alpha, self.mixup_alpha, size=batch_size\n                )\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool)\n                lam_mix = np.random.beta(\n                    self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                )\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = np.where(\n                np.random.rand(batch_size) < self.mix_prob,\n                lam_mix.astype(np.float32),\n                lam,\n            )\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.0\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand() < self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = float(lam_mix)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                )\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool)\n                lam_mix = np.random.beta(\n                    self.cutmix_alpha, self.cutmix_alpha, size=batch_size\n                )\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = np.where(\n                np.random.rand(batch_size) < self.mix_prob,\n                lam_mix.astype(np.float32),\n                lam,\n            )\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.0\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand() < self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                )\n            lam = np.where(\n                np.random.rand(batch_size) < self.mix_prob,\n                lam_mix.astype(np.float32),\n                lam,\n            )\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.0\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand() < self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0:\n                use_cutmix = np.random.rand() < self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha > 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert AssertionError, (\n                    \"One of mixup_alpha > 0., cutmix_alpha > 0.,\"\n                    \"cutmix_minmax not None should be true.\"\n                )\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:\n            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape,\n                lam,\n                ratio_minmax=self.cutmix_minmax,\n                correct_lam=self.correct_lam,\n            )\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:\n            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape,\n                lam,\n                ratio_minmax=self.cutmix_minmax,\n                correct_lam=self.correct_lam,\n            )\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = x.flip(0).mul_(1.0 - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def __call__(self, sample):\n        x = sample[\"input\"]\n        orig_shape = x.shape\n        if len(orig_shape) == 5:\n            # Video case, convert to image by moving time to channels\n            x = x.flatten(1, 2)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape,\n                        lam,\n                        ratio_minmax=self.cutmix_minmax,\n                        correct_lam=self.correct_lam,\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:\n            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape,\n                lam,\n                ratio_minmax=self.cutmix_minmax,\n                correct_lam=self.correct_lam,\n            )\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = x.flip(0).mul_(1.0 - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def __call__(self, sample):\n        x = sample[\"input\"]\n        orig_shape = x.shape\n        if len(orig_shape) == 5:\n            # Video case, convert to image by moving time to channels\n            x = x.flatten(1, 2)\n        target = sample[\"target\"]\n        len_to_mix = len(x)\n        if len(x) % 2 != 0:\n            len_to_mix = len_to_mix - 1\n            logging.warning(\n                \"Batch size should be even when using this. \"\n                \"Got %d. Applying cutmix to first %d elements.\",\n                len(x),\n                len_to_mix,\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_365-415"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:\n            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape,\n                lam,\n                ratio_minmax=self.cutmix_minmax,\n                correct_lam=self.correct_lam,\n            )\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = x.flip(0).mul_(1.0 - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def __call__(self, sample):\n        x = sample[\"input\"]\n        orig_shape = x.shape\n        if len(orig_shape) == 5:\n            # Video case, convert to image by moving time to channels\n            x = x.flatten(1, 2)\n        target = sample[\"target\"]\n        len_to_mix = len(x)\n        if len(x) % 2 != 0:\n            len_to_mix = len_to_mix - 1\n            logging.warning(\n                \"Batch size should be even when using this. \"\n                \"Got %d. Applying cutmix to first %d elements.\",\n                len(x),\n                len_to_mix,\n            )\n        # rgirdhar: using torch.narrow to make sure it is a view and shares\n        # the same storage (since the function modifies it inplace)\n        if self.mode == \"elem\":\n            lam = self._mix_elem(torch.narrow(x, 0, 0, len_to_mix))\n        elif self.mode == \"pair\":\n            lam = self._mix_pair(torch.narrow(x, 0, 0, len_to_mix))\n        else:\n            lam = self._mix_batch(torch.narrow(x, 0, 0, len_to_mix))\n        # Modified to pass device argument based on target.device to prevent\n        # failure on CPU-based data.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_375-425"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape,\n                lam,\n                ratio_minmax=self.cutmix_minmax,\n                correct_lam=self.correct_lam,\n            )\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = x.flip(0).mul_(1.0 - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def __call__(self, sample):\n        x = sample[\"input\"]\n        orig_shape = x.shape\n        if len(orig_shape) == 5:\n            # Video case, convert to image by moving time to channels\n            x = x.flatten(1, 2)\n        target = sample[\"target\"]\n        len_to_mix = len(x)\n        if len(x) % 2 != 0:\n            len_to_mix = len_to_mix - 1\n            logging.warning(\n                \"Batch size should be even when using this. \"\n                \"Got %d. Applying cutmix to first %d elements.\",\n                len(x),\n                len_to_mix,\n            )\n        # rgirdhar: using torch.narrow to make sure it is a view and shares\n        # the same storage (since the function modifies it inplace)\n        if self.mode == \"elem\":\n            lam = self._mix_elem(torch.narrow(x, 0, 0, len_to_mix))\n        elif self.mode == \"pair\":\n            lam = self._mix_pair(torch.narrow(x, 0, 0, len_to_mix))\n        else:\n            lam = self._mix_batch(torch.narrow(x, 0, 0, len_to_mix))\n        # Modified to pass device argument based on target.device to prevent\n        # failure on CPU-based data.\n        target_1hot = convert_to_one_hot(target.view(-1, 1), self.num_classes).to(\n            torch.float\n        )\n        target_1hot[:len_to_mix] = mixup_target(\n            target[:len_to_mix],\n            self.num_classes,\n            lam,\n            self.label_smoothing,\n            device=target.device,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_385-435"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 439, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            x_flipped = x.flip(0).mul_(1.0 - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def __call__(self, sample):\n        x = sample[\"input\"]\n        orig_shape = x.shape\n        if len(orig_shape) == 5:\n            # Video case, convert to image by moving time to channels\n            x = x.flatten(1, 2)\n        target = sample[\"target\"]\n        len_to_mix = len(x)\n        if len(x) % 2 != 0:\n            len_to_mix = len_to_mix - 1\n            logging.warning(\n                \"Batch size should be even when using this. \"\n                \"Got %d. Applying cutmix to first %d elements.\",\n                len(x),\n                len_to_mix,\n            )\n        # rgirdhar: using torch.narrow to make sure it is a view and shares\n        # the same storage (since the function modifies it inplace)\n        if self.mode == \"elem\":\n            lam = self._mix_elem(torch.narrow(x, 0, 0, len_to_mix))\n        elif self.mode == \"pair\":\n            lam = self._mix_pair(torch.narrow(x, 0, 0, len_to_mix))\n        else:\n            lam = self._mix_batch(torch.narrow(x, 0, 0, len_to_mix))\n        # Modified to pass device argument based on target.device to prevent\n        # failure on CPU-based data.\n        target_1hot = convert_to_one_hot(target.view(-1, 1), self.num_classes).to(\n            torch.float\n        )\n        target_1hot[:len_to_mix] = mixup_target(\n            target[:len_to_mix],\n            self.num_classes,\n            lam,\n            self.label_smoothing,\n            device=target.device,\n        )\n        if len(orig_shape) == 5:\n            # Video case, convert back to video\n            x = x.view(orig_shape)\n        return {\"input\": x, \"target\": target_1hot}", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_395-439"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "cutmixup.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 439, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        target = sample[\"target\"]\n        len_to_mix = len(x)\n        if len(x) % 2 != 0:\n            len_to_mix = len_to_mix - 1\n            logging.warning(\n                \"Batch size should be even when using this. \"\n                \"Got %d. Applying cutmix to first %d elements.\",\n                len(x),\n                len_to_mix,\n            )\n        # rgirdhar: using torch.narrow to make sure it is a view and shares\n        # the same storage (since the function modifies it inplace)\n        if self.mode == \"elem\":\n            lam = self._mix_elem(torch.narrow(x, 0, 0, len_to_mix))\n        elif self.mode == \"pair\":\n            lam = self._mix_pair(torch.narrow(x, 0, 0, len_to_mix))\n        else:\n            lam = self._mix_batch(torch.narrow(x, 0, 0, len_to_mix))\n        # Modified to pass device argument based on target.device to prevent\n        # failure on CPU-based data.\n        target_1hot = convert_to_one_hot(target.view(-1, 1), self.num_classes).to(\n            torch.float\n        )\n        target_1hot[:len_to_mix] = mixup_target(\n            target[:len_to_mix],\n            self.num_classes,\n            lam,\n            self.label_smoothing,\n            device=target.device,\n        )\n        if len(orig_shape) == 5:\n            # Video case, convert back to video\n            x = x.view(orig_shape)\n        return {\"input\": x, \"target\": target_1hot}", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-cutmixup.py_405-439"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copied from torchvision file:\n# https://github.com/pytorch/vision/blob/eb2fb25304dd3180c092231bbe83cd88d25f7cb3/torchvision/transforms/autoaugment.py\n\nimport math\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as pth_transforms\nimport torchvision.transforms.functional as F\n\n\n# Ops which can be used on depth\nDEPTH_OPS = [\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateX\",\n    \"TranslateY\",\n    \"Rotate\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copied from torchvision file:\n# https://github.com/pytorch/vision/blob/eb2fb25304dd3180c092231bbe83cd88d25f7cb3/torchvision/transforms/autoaugment.py\n\nimport math\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as pth_transforms\nimport torchvision.transforms.functional as F\n\n\n# Ops which can be used on depth\nDEPTH_OPS = [\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateX\",\n    \"TranslateY\",\n    \"Rotate\",\n    \"Invert\",\n    \"Identity\",\n]\n\n\ndef _apply_op(\n    img: torch.Tensor,\n    op_name: str,\n    magnitude: float,\n    interpolation: F.InterpolationMode,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copied from torchvision file:\n# https://github.com/pytorch/vision/blob/eb2fb25304dd3180c092231bbe83cd88d25f7cb3/torchvision/transforms/autoaugment.py\n\nimport math\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as pth_transforms\nimport torchvision.transforms.functional as F\n\n\n# Ops which can be used on depth\nDEPTH_OPS = [\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateX\",\n    \"TranslateY\",\n    \"Rotate\",\n    \"Invert\",\n    \"Identity\",\n]\n\n\ndef _apply_op(\n    img: torch.Tensor,\n    op_name: str,\n    magnitude: float,\n    interpolation: F.InterpolationMode,\n    fill: Optional[List[float]],\n):\n    if op_name == \"ShearX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[math.degrees(magnitude), 0.0],\n            interpolation=interpolation,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Copied from torchvision file:\n# https://github.com/pytorch/vision/blob/eb2fb25304dd3180c092231bbe83cd88d25f7cb3/torchvision/transforms/autoaugment.py\n\nimport math\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as pth_transforms\nimport torchvision.transforms.functional as F\n\n\n# Ops which can be used on depth\nDEPTH_OPS = [\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateX\",\n    \"TranslateY\",\n    \"Rotate\",\n    \"Invert\",\n    \"Identity\",\n]\n\n\ndef _apply_op(\n    img: torch.Tensor,\n    op_name: str,\n    magnitude: float,\n    interpolation: F.InterpolationMode,\n    fill: Optional[List[float]],\n):\n    if op_name == \"ShearX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[math.degrees(magnitude), 0.0],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"ShearY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[0.0, math.degrees(magnitude)],\n            interpolation=interpolation,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import torchvision.transforms.functional as F\n\n\n# Ops which can be used on depth\nDEPTH_OPS = [\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateX\",\n    \"TranslateY\",\n    \"Rotate\",\n    \"Invert\",\n    \"Identity\",\n]\n\n\ndef _apply_op(\n    img: torch.Tensor,\n    op_name: str,\n    magnitude: float,\n    interpolation: F.InterpolationMode,\n    fill: Optional[List[float]],\n):\n    if op_name == \"ShearX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[math.degrees(magnitude), 0.0],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"ShearY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[0.0, math.degrees(magnitude)],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"TranslateX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[int(magnitude), 0],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"Invert\",\n    \"Identity\",\n]\n\n\ndef _apply_op(\n    img: torch.Tensor,\n    op_name: str,\n    magnitude: float,\n    interpolation: F.InterpolationMode,\n    fill: Optional[List[float]],\n):\n    if op_name == \"ShearX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[math.degrees(magnitude), 0.0],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"ShearY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[0.0, math.degrees(magnitude)],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"TranslateX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[int(magnitude), 0],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"TranslateY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, int(magnitude)],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    fill: Optional[List[float]],\n):\n    if op_name == \"ShearX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[math.degrees(magnitude), 0.0],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"ShearY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[0.0, math.degrees(magnitude)],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"TranslateX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[int(magnitude), 0],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"TranslateY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, int(magnitude)],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"Rotate\":\n        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)\n    elif op_name == \"Brightness\":\n        img = F.adjust_brightness(img, 1.0 + magnitude)\n    elif op_name == \"Color\":\n        img = F.adjust_saturation(img, 1.0 + magnitude)\n    elif op_name == \"Contrast\":\n        img = F.adjust_contrast(img, 1.0 + magnitude)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            fill=fill,\n        )\n    elif op_name == \"ShearY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, 0],\n            scale=1.0,\n            shear=[0.0, math.degrees(magnitude)],\n            interpolation=interpolation,\n            fill=fill,\n        )\n    elif op_name == \"TranslateX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[int(magnitude), 0],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"TranslateY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, int(magnitude)],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"Rotate\":\n        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)\n    elif op_name == \"Brightness\":\n        img = F.adjust_brightness(img, 1.0 + magnitude)\n    elif op_name == \"Color\":\n        img = F.adjust_saturation(img, 1.0 + magnitude)\n    elif op_name == \"Contrast\":\n        img = F.adjust_contrast(img, 1.0 + magnitude)\n    elif op_name == \"Sharpness\":\n        img = F.adjust_sharpness(img, 1.0 + magnitude)\n    elif op_name == \"Posterize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.posterize(img, int(magnitude))\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Solarize\":\n        img = F.solarize(img, magnitude)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            fill=fill,\n        )\n    elif op_name == \"TranslateX\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[int(magnitude), 0],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"TranslateY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, int(magnitude)],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"Rotate\":\n        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)\n    elif op_name == \"Brightness\":\n        img = F.adjust_brightness(img, 1.0 + magnitude)\n    elif op_name == \"Color\":\n        img = F.adjust_saturation(img, 1.0 + magnitude)\n    elif op_name == \"Contrast\":\n        img = F.adjust_contrast(img, 1.0 + magnitude)\n    elif op_name == \"Sharpness\":\n        img = F.adjust_sharpness(img, 1.0 + magnitude)\n    elif op_name == \"Posterize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.posterize(img, int(magnitude))\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Solarize\":\n        img = F.solarize(img, magnitude)\n    elif op_name == \"AutoContrast\":\n        img = F.autocontrast(img)\n    elif op_name == \"Equalize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.equalize(img)\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Invert\":\n        img = F.invert(img)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            fill=fill,\n        )\n    elif op_name == \"TranslateY\":\n        img = F.affine(\n            img,\n            angle=0.0,\n            translate=[0, int(magnitude)],\n            scale=1.0,\n            interpolation=interpolation,\n            shear=[0.0, 0.0],\n            fill=fill,\n        )\n    elif op_name == \"Rotate\":\n        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)\n    elif op_name == \"Brightness\":\n        img = F.adjust_brightness(img, 1.0 + magnitude)\n    elif op_name == \"Color\":\n        img = F.adjust_saturation(img, 1.0 + magnitude)\n    elif op_name == \"Contrast\":\n        img = F.adjust_contrast(img, 1.0 + magnitude)\n    elif op_name == \"Sharpness\":\n        img = F.adjust_sharpness(img, 1.0 + magnitude)\n    elif op_name == \"Posterize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.posterize(img, int(magnitude))\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Solarize\":\n        img = F.solarize(img, magnitude)\n    elif op_name == \"AutoContrast\":\n        img = F.autocontrast(img)\n    elif op_name == \"Equalize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.equalize(img)\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Invert\":\n        img = F.invert(img)\n    elif op_name == \"Identity\":\n        pass\n    else:\n        raise ValueError(\"The provided operator {} is not recognized.\".format(op_name))\n    return img\n\n\nclass RandAugment3d(torch.nn.Module):\n    \"\"\"\n    Wrapper around torchvision RandAugment transform", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            fill=fill,\n        )\n    elif op_name == \"Rotate\":\n        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)\n    elif op_name == \"Brightness\":\n        img = F.adjust_brightness(img, 1.0 + magnitude)\n    elif op_name == \"Color\":\n        img = F.adjust_saturation(img, 1.0 + magnitude)\n    elif op_name == \"Contrast\":\n        img = F.adjust_contrast(img, 1.0 + magnitude)\n    elif op_name == \"Sharpness\":\n        img = F.adjust_sharpness(img, 1.0 + magnitude)\n    elif op_name == \"Posterize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.posterize(img, int(magnitude))\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Solarize\":\n        img = F.solarize(img, magnitude)\n    elif op_name == \"AutoContrast\":\n        img = F.autocontrast(img)\n    elif op_name == \"Equalize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.equalize(img)\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Invert\":\n        img = F.invert(img)\n    elif op_name == \"Identity\":\n        pass\n    else:\n        raise ValueError(\"The provided operator {} is not recognized.\".format(op_name))\n    return img\n\n\nclass RandAugment3d(torch.nn.Module):\n    \"\"\"\n    Wrapper around torchvision RandAugment transform\n    to support 4 channel input for RGBD data\n\n    Args:\n        num_ops (int): Number of augmentation transformations to apply sequentially.\n        magnitude (int): Magnitude for all the transformations.\n        num_magnitude_bins (int): The number of different magnitude values.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif op_name == \"Sharpness\":\n        img = F.adjust_sharpness(img, 1.0 + magnitude)\n    elif op_name == \"Posterize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.posterize(img, int(magnitude))\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Solarize\":\n        img = F.solarize(img, magnitude)\n    elif op_name == \"AutoContrast\":\n        img = F.autocontrast(img)\n    elif op_name == \"Equalize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.equalize(img)\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Invert\":\n        img = F.invert(img)\n    elif op_name == \"Identity\":\n        pass\n    else:\n        raise ValueError(\"The provided operator {} is not recognized.\".format(op_name))\n    return img\n\n\nclass RandAugment3d(torch.nn.Module):\n    \"\"\"\n    Wrapper around torchvision RandAugment transform\n    to support 4 channel input for RGBD data\n\n    Args:\n        num_ops (int): Number of augmentation transformations to apply sequentially.\n        magnitude (int): Magnitude for all the transformations.\n        num_magnitude_bins (int): The number of different magnitude values.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_ops: int = 2,\n        magnitude: int = 9,\n        num_magnitude_bins: int = 31,\n        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n        fill: Optional[List[float]] = None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif op_name == \"AutoContrast\":\n        img = F.autocontrast(img)\n    elif op_name == \"Equalize\":\n        # The tensor dtype must be torch.uint8\n        # and values are expected to be in [0, 255]\n        img = (img * 255).to(dtype=torch.uint8)\n        img = F.equalize(img)\n        img = (img / 255.0).to(dtype=torch.float32)\n    elif op_name == \"Invert\":\n        img = F.invert(img)\n    elif op_name == \"Identity\":\n        pass\n    else:\n        raise ValueError(\"The provided operator {} is not recognized.\".format(op_name))\n    return img\n\n\nclass RandAugment3d(torch.nn.Module):\n    \"\"\"\n    Wrapper around torchvision RandAugment transform\n    to support 4 channel input for RGBD data\n\n    Args:\n        num_ops (int): Number of augmentation transformations to apply sequentially.\n        magnitude (int): Magnitude for all the transformations.\n        num_magnitude_bins (int): The number of different magnitude values.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_ops: int = 2,\n        magnitude: int = 9,\n        num_magnitude_bins: int = 31,\n        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n        fill: Optional[List[float]] = None,\n    ) -> None:\n        super().__init__()\n        self.num_ops = num_ops\n        self.magnitude = magnitude\n        self.num_magnitude_bins = num_magnitude_bins\n        self.interpolation = interpolation\n        self.fill = fill\n\n    def _augmentation_space(\n        self, num_bins: int, image_size: List[int]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif op_name == \"Identity\":\n        pass\n    else:\n        raise ValueError(\"The provided operator {} is not recognized.\".format(op_name))\n    return img\n\n\nclass RandAugment3d(torch.nn.Module):\n    \"\"\"\n    Wrapper around torchvision RandAugment transform\n    to support 4 channel input for RGBD data\n\n    Args:\n        num_ops (int): Number of augmentation transformations to apply sequentially.\n        magnitude (int): Magnitude for all the transformations.\n        num_magnitude_bins (int): The number of different magnitude values.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_ops: int = 2,\n        magnitude: int = 9,\n        num_magnitude_bins: int = 31,\n        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n        fill: Optional[List[float]] = None,\n    ) -> None:\n        super().__init__()\n        self.num_ops = num_ops\n        self.magnitude = magnitude\n        self.num_magnitude_bins = num_magnitude_bins\n        self.interpolation = interpolation\n        self.fill = fill\n\n    def _augmentation_space(\n        self, num_bins: int, image_size: List[int]\n    ) -> Dict[str, Tuple[torch.Tensor, bool]]:\n        return {\n            # op_name: (magnitudes, signed)\n            \"Identity\": (torch.tensor(0.0), False),\n            \"ShearX\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"ShearY\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"TranslateX\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[0], num_bins),\n                True,\n            ),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    to support 4 channel input for RGBD data\n\n    Args:\n        num_ops (int): Number of augmentation transformations to apply sequentially.\n        magnitude (int): Magnitude for all the transformations.\n        num_magnitude_bins (int): The number of different magnitude values.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_ops: int = 2,\n        magnitude: int = 9,\n        num_magnitude_bins: int = 31,\n        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n        fill: Optional[List[float]] = None,\n    ) -> None:\n        super().__init__()\n        self.num_ops = num_ops\n        self.magnitude = magnitude\n        self.num_magnitude_bins = num_magnitude_bins\n        self.interpolation = interpolation\n        self.fill = fill\n\n    def _augmentation_space(\n        self, num_bins: int, image_size: List[int]\n    ) -> Dict[str, Tuple[torch.Tensor, bool]]:\n        return {\n            # op_name: (magnitudes, signed)\n            \"Identity\": (torch.tensor(0.0), False),\n            \"ShearX\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"ShearY\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"TranslateX\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[0], num_bins),\n                True,\n            ),\n            \"TranslateY\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[1], num_bins),\n                True,\n            ),\n            \"Rotate\": (torch.linspace(0.0, 30.0, num_bins), True),\n            \"Brightness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Color\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Contrast\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Sharpness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Posterize\": (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            image. If given a number, the value is used for all bands respectively.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_ops: int = 2,\n        magnitude: int = 9,\n        num_magnitude_bins: int = 31,\n        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n        fill: Optional[List[float]] = None,\n    ) -> None:\n        super().__init__()\n        self.num_ops = num_ops\n        self.magnitude = magnitude\n        self.num_magnitude_bins = num_magnitude_bins\n        self.interpolation = interpolation\n        self.fill = fill\n\n    def _augmentation_space(\n        self, num_bins: int, image_size: List[int]\n    ) -> Dict[str, Tuple[torch.Tensor, bool]]:\n        return {\n            # op_name: (magnitudes, signed)\n            \"Identity\": (torch.tensor(0.0), False),\n            \"ShearX\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"ShearY\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"TranslateX\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[0], num_bins),\n                True,\n            ),\n            \"TranslateY\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[1], num_bins),\n                True,\n            ),\n            \"Rotate\": (torch.linspace(0.0, 30.0, num_bins), True),\n            \"Brightness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Color\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Contrast\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Sharpness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Posterize\": (\n                8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(),\n                False,\n            ),\n            \"Solarize\": (torch.linspace(256.0, 0.0, num_bins), False),\n            \"AutoContrast\": (torch.tensor(0.0), False),\n            \"Equalize\": (torch.tensor(0.0), False),\n        }\n\n    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n        \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ) -> None:\n        super().__init__()\n        self.num_ops = num_ops\n        self.magnitude = magnitude\n        self.num_magnitude_bins = num_magnitude_bins\n        self.interpolation = interpolation\n        self.fill = fill\n\n    def _augmentation_space(\n        self, num_bins: int, image_size: List[int]\n    ) -> Dict[str, Tuple[torch.Tensor, bool]]:\n        return {\n            # op_name: (magnitudes, signed)\n            \"Identity\": (torch.tensor(0.0), False),\n            \"ShearX\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"ShearY\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"TranslateX\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[0], num_bins),\n                True,\n            ),\n            \"TranslateY\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[1], num_bins),\n                True,\n            ),\n            \"Rotate\": (torch.linspace(0.0, 30.0, num_bins), True),\n            \"Brightness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Color\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Contrast\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Sharpness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Posterize\": (\n                8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(),\n                False,\n            ),\n            \"Solarize\": (torch.linspace(256.0, 0.0, num_bins), False),\n            \"AutoContrast\": (torch.tensor(0.0), False),\n            \"Equalize\": (torch.tensor(0.0), False),\n        }\n\n    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n            img (PIL Image or Tensor): Image to be transformed.\n        Returns:\n            PIL Image or Tensor: Transformed image.\n        \"\"\"\n        assert isinstance(img, torch.Tensor)\n\n        C, H, W = img.shape\n        images = [img[:3, ...]]  # RGB\n\n        if C == 4:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ) -> Dict[str, Tuple[torch.Tensor, bool]]:\n        return {\n            # op_name: (magnitudes, signed)\n            \"Identity\": (torch.tensor(0.0), False),\n            \"ShearX\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"ShearY\": (torch.linspace(0.0, 0.3, num_bins), True),\n            \"TranslateX\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[0], num_bins),\n                True,\n            ),\n            \"TranslateY\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[1], num_bins),\n                True,\n            ),\n            \"Rotate\": (torch.linspace(0.0, 30.0, num_bins), True),\n            \"Brightness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Color\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Contrast\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Sharpness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Posterize\": (\n                8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(),\n                False,\n            ),\n            \"Solarize\": (torch.linspace(256.0, 0.0, num_bins), False),\n            \"AutoContrast\": (torch.tensor(0.0), False),\n            \"Equalize\": (torch.tensor(0.0), False),\n        }\n\n    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n            img (PIL Image or Tensor): Image to be transformed.\n        Returns:\n            PIL Image or Tensor: Transformed image.\n        \"\"\"\n        assert isinstance(img, torch.Tensor)\n\n        C, H, W = img.shape\n        images = [img[:3, ...]]  # RGB\n\n        if C == 4:\n            depth = img[3:4, ...]  # (1, H, W)\n            images.append(depth)\n\n        # Select ops\n        # We sample an op and its metadata so that the same op\n        # is applied to both RGB and D where relevant\n        selected_ops = []\n        for _ in range(self.num_ops):\n            op_meta = self._augmentation_space(self.num_magnitude_bins, (H, W))\n            op_index = int(torch.randint(len(op_meta), (1,)).item())", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            \"TranslateY\": (\n                torch.linspace(0.0, 150.0 / 331.0 * image_size[1], num_bins),\n                True,\n            ),\n            \"Rotate\": (torch.linspace(0.0, 30.0, num_bins), True),\n            \"Brightness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Color\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Contrast\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Sharpness\": (torch.linspace(0.0, 0.9, num_bins), True),\n            \"Posterize\": (\n                8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(),\n                False,\n            ),\n            \"Solarize\": (torch.linspace(256.0, 0.0, num_bins), False),\n            \"AutoContrast\": (torch.tensor(0.0), False),\n            \"Equalize\": (torch.tensor(0.0), False),\n        }\n\n    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n            img (PIL Image or Tensor): Image to be transformed.\n        Returns:\n            PIL Image or Tensor: Transformed image.\n        \"\"\"\n        assert isinstance(img, torch.Tensor)\n\n        C, H, W = img.shape\n        images = [img[:3, ...]]  # RGB\n\n        if C == 4:\n            depth = img[3:4, ...]  # (1, H, W)\n            images.append(depth)\n\n        # Select ops\n        # We sample an op and its metadata so that the same op\n        # is applied to both RGB and D where relevant\n        selected_ops = []\n        for _ in range(self.num_ops):\n            op_meta = self._augmentation_space(self.num_magnitude_bins, (H, W))\n            op_index = int(torch.randint(len(op_meta), (1,)).item())\n            op_name = list(op_meta.keys())[op_index]\n            selected_ops.append(op_name)\n\n        # Apply on both images and depth\n        images_out = []\n        for im in images:\n            # Only apply some ops for depth if\n            # they are part of DEPTH_OPS\n            run_on_depth = C == 1 and op_name in DEPTH_OPS\n            if C == 3 or run_on_depth:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(),\n                False,\n            ),\n            \"Solarize\": (torch.linspace(256.0, 0.0, num_bins), False),\n            \"AutoContrast\": (torch.tensor(0.0), False),\n            \"Equalize\": (torch.tensor(0.0), False),\n        }\n\n    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n            img (PIL Image or Tensor): Image to be transformed.\n        Returns:\n            PIL Image or Tensor: Transformed image.\n        \"\"\"\n        assert isinstance(img, torch.Tensor)\n\n        C, H, W = img.shape\n        images = [img[:3, ...]]  # RGB\n\n        if C == 4:\n            depth = img[3:4, ...]  # (1, H, W)\n            images.append(depth)\n\n        # Select ops\n        # We sample an op and its metadata so that the same op\n        # is applied to both RGB and D where relevant\n        selected_ops = []\n        for _ in range(self.num_ops):\n            op_meta = self._augmentation_space(self.num_magnitude_bins, (H, W))\n            op_index = int(torch.randint(len(op_meta), (1,)).item())\n            op_name = list(op_meta.keys())[op_index]\n            selected_ops.append(op_name)\n\n        # Apply on both images and depth\n        images_out = []\n        for im in images:\n            # Only apply some ops for depth if\n            # they are part of DEPTH_OPS\n            run_on_depth = C == 1 and op_name in DEPTH_OPS\n            if C == 3 or run_on_depth:\n                fill = self.fill\n                if isinstance(im, torch.Tensor):\n                    if isinstance(fill, (int, float)):\n                        fill = [float(fill)] * C\n                    elif fill is not None:\n                        fill = [float(f) for f in fill]\n\n                for op_name in selected_ops:\n                    magnitudes, signed = op_meta[op_name]\n                    magnitude = (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            img (PIL Image or Tensor): Image to be transformed.\n        Returns:\n            PIL Image or Tensor: Transformed image.\n        \"\"\"\n        assert isinstance(img, torch.Tensor)\n\n        C, H, W = img.shape\n        images = [img[:3, ...]]  # RGB\n\n        if C == 4:\n            depth = img[3:4, ...]  # (1, H, W)\n            images.append(depth)\n\n        # Select ops\n        # We sample an op and its metadata so that the same op\n        # is applied to both RGB and D where relevant\n        selected_ops = []\n        for _ in range(self.num_ops):\n            op_meta = self._augmentation_space(self.num_magnitude_bins, (H, W))\n            op_index = int(torch.randint(len(op_meta), (1,)).item())\n            op_name = list(op_meta.keys())[op_index]\n            selected_ops.append(op_name)\n\n        # Apply on both images and depth\n        images_out = []\n        for im in images:\n            # Only apply some ops for depth if\n            # they are part of DEPTH_OPS\n            run_on_depth = C == 1 and op_name in DEPTH_OPS\n            if C == 3 or run_on_depth:\n                fill = self.fill\n                if isinstance(im, torch.Tensor):\n                    if isinstance(fill, (int, float)):\n                        fill = [float(fill)] * C\n                    elif fill is not None:\n                        fill = [float(f) for f in fill]\n\n                for op_name in selected_ops:\n                    magnitudes, signed = op_meta[op_name]\n                    magnitude = (\n                        float(magnitudes[self.magnitude].item())\n                        if magnitudes.ndim > 0\n                        else 0.0\n                    )\n                    if signed and torch.randint(2, (1,)):\n                        magnitude *= -1.0\n\n                    im = _apply_op(\n                        im,\n                        op_name,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            depth = img[3:4, ...]  # (1, H, W)\n            images.append(depth)\n\n        # Select ops\n        # We sample an op and its metadata so that the same op\n        # is applied to both RGB and D where relevant\n        selected_ops = []\n        for _ in range(self.num_ops):\n            op_meta = self._augmentation_space(self.num_magnitude_bins, (H, W))\n            op_index = int(torch.randint(len(op_meta), (1,)).item())\n            op_name = list(op_meta.keys())[op_index]\n            selected_ops.append(op_name)\n\n        # Apply on both images and depth\n        images_out = []\n        for im in images:\n            # Only apply some ops for depth if\n            # they are part of DEPTH_OPS\n            run_on_depth = C == 1 and op_name in DEPTH_OPS\n            if C == 3 or run_on_depth:\n                fill = self.fill\n                if isinstance(im, torch.Tensor):\n                    if isinstance(fill, (int, float)):\n                        fill = [float(fill)] * C\n                    elif fill is not None:\n                        fill = [float(f) for f in fill]\n\n                for op_name in selected_ops:\n                    magnitudes, signed = op_meta[op_name]\n                    magnitude = (\n                        float(magnitudes[self.magnitude].item())\n                        if magnitudes.ndim > 0\n                        else 0.0\n                    )\n                    if signed and torch.randint(2, (1,)):\n                        magnitude *= -1.0\n\n                    im = _apply_op(\n                        im,\n                        op_name,\n                        magnitude,\n                        interpolation=self.interpolation,\n                        fill=fill,\n                    )\n\n            # Save modified image\n            images_out.append(im)\n\n        # Concat the img and depth back if present\n        images_out = torch.cat(images_out, dim=0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            op_name = list(op_meta.keys())[op_index]\n            selected_ops.append(op_name)\n\n        # Apply on both images and depth\n        images_out = []\n        for im in images:\n            # Only apply some ops for depth if\n            # they are part of DEPTH_OPS\n            run_on_depth = C == 1 and op_name in DEPTH_OPS\n            if C == 3 or run_on_depth:\n                fill = self.fill\n                if isinstance(im, torch.Tensor):\n                    if isinstance(fill, (int, float)):\n                        fill = [float(fill)] * C\n                    elif fill is not None:\n                        fill = [float(f) for f in fill]\n\n                for op_name in selected_ops:\n                    magnitudes, signed = op_meta[op_name]\n                    magnitude = (\n                        float(magnitudes[self.magnitude].item())\n                        if magnitudes.ndim > 0\n                        else 0.0\n                    )\n                    if signed and torch.randint(2, (1,)):\n                        magnitude *= -1.0\n\n                    im = _apply_op(\n                        im,\n                        op_name,\n                        magnitude,\n                        interpolation=self.interpolation,\n                        fill=fill,\n                    )\n\n            # Save modified image\n            images_out.append(im)\n\n        # Concat the img and depth back if present\n        images_out = torch.cat(images_out, dim=0)\n\n        return images_out\n\n    def __repr__(self) -> str:\n        s = self.__class__.__name__ + \"(\"\n        s += \"num_ops={num_ops}\"\n        s += \", magnitude={magnitude}\"\n        s += \", num_magnitude_bins={num_magnitude_bins}\"\n        s += \", interpolation={interpolation}\"\n        s += \", fill={fill}\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                fill = self.fill\n                if isinstance(im, torch.Tensor):\n                    if isinstance(fill, (int, float)):\n                        fill = [float(fill)] * C\n                    elif fill is not None:\n                        fill = [float(f) for f in fill]\n\n                for op_name in selected_ops:\n                    magnitudes, signed = op_meta[op_name]\n                    magnitude = (\n                        float(magnitudes[self.magnitude].item())\n                        if magnitudes.ndim > 0\n                        else 0.0\n                    )\n                    if signed and torch.randint(2, (1,)):\n                        magnitude *= -1.0\n\n                    im = _apply_op(\n                        im,\n                        op_name,\n                        magnitude,\n                        interpolation=self.interpolation,\n                        fill=fill,\n                    )\n\n            # Save modified image\n            images_out.append(im)\n\n        # Concat the img and depth back if present\n        images_out = torch.cat(images_out, dim=0)\n\n        return images_out\n\n    def __repr__(self) -> str:\n        s = self.__class__.__name__ + \"(\"\n        s += \"num_ops={num_ops}\"\n        s += \", magnitude={magnitude}\"\n        s += \", num_magnitude_bins={num_magnitude_bins}\"\n        s += \", interpolation={interpolation}\"\n        s += \", fill={fill}\"\n        s += \")\"\n        return s.format(**self.__dict__)\n\n\nclass ColorJitter3d(pth_transforms.ColorJitter):\n    \"\"\"\n    Apply ColorJitter on an image of shape (4, H, W)\n    \"\"\"\n\n    def __init__(self, brightness, contrast, saturation, hue):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                        float(magnitudes[self.magnitude].item())\n                        if magnitudes.ndim > 0\n                        else 0.0\n                    )\n                    if signed and torch.randint(2, (1,)):\n                        magnitude *= -1.0\n\n                    im = _apply_op(\n                        im,\n                        op_name,\n                        magnitude,\n                        interpolation=self.interpolation,\n                        fill=fill,\n                    )\n\n            # Save modified image\n            images_out.append(im)\n\n        # Concat the img and depth back if present\n        images_out = torch.cat(images_out, dim=0)\n\n        return images_out\n\n    def __repr__(self) -> str:\n        s = self.__class__.__name__ + \"(\"\n        s += \"num_ops={num_ops}\"\n        s += \", magnitude={magnitude}\"\n        s += \", num_magnitude_bins={num_magnitude_bins}\"\n        s += \", interpolation={interpolation}\"\n        s += \", fill={fill}\"\n        s += \")\"\n        return s.format(**self.__dict__)\n\n\nclass ColorJitter3d(pth_transforms.ColorJitter):\n    \"\"\"\n    Apply ColorJitter on an image of shape (4, H, W)\n    \"\"\"\n\n    def __init__(self, brightness, contrast, saturation, hue):\n        \"\"\"\n        Args:\n            strength (float): A number used to quantify the strength of the\n                              color distortion.\n        \"\"\"\n        super().__init__(\n            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n        )\n\n    def __call__(self, image: torch.Tensor):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                        magnitude,\n                        interpolation=self.interpolation,\n                        fill=fill,\n                    )\n\n            # Save modified image\n            images_out.append(im)\n\n        # Concat the img and depth back if present\n        images_out = torch.cat(images_out, dim=0)\n\n        return images_out\n\n    def __repr__(self) -> str:\n        s = self.__class__.__name__ + \"(\"\n        s += \"num_ops={num_ops}\"\n        s += \", magnitude={magnitude}\"\n        s += \", num_magnitude_bins={num_magnitude_bins}\"\n        s += \", interpolation={interpolation}\"\n        s += \", fill={fill}\"\n        s += \")\"\n        return s.format(**self.__dict__)\n\n\nclass ColorJitter3d(pth_transforms.ColorJitter):\n    \"\"\"\n    Apply ColorJitter on an image of shape (4, H, W)\n    \"\"\"\n\n    def __init__(self, brightness, contrast, saturation, hue):\n        \"\"\"\n        Args:\n            strength (float): A number used to quantify the strength of the\n                              color distortion.\n        \"\"\"\n        super().__init__(\n            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n        )\n\n    def __call__(self, image: torch.Tensor):\n        if not isinstance(image, torch.Tensor):\n            raise ValueError(\"Expected tensor input\")\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = \"This transform is for 4 channel RGBD input only; got %d\" % C\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n        color_img_jitter = super().__call__(color_img)\n        img = torch.cat([color_img_jitter, depth_img], dim=0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        return images_out\n\n    def __repr__(self) -> str:\n        s = self.__class__.__name__ + \"(\"\n        s += \"num_ops={num_ops}\"\n        s += \", magnitude={magnitude}\"\n        s += \", num_magnitude_bins={num_magnitude_bins}\"\n        s += \", interpolation={interpolation}\"\n        s += \", fill={fill}\"\n        s += \")\"\n        return s.format(**self.__dict__)\n\n\nclass ColorJitter3d(pth_transforms.ColorJitter):\n    \"\"\"\n    Apply ColorJitter on an image of shape (4, H, W)\n    \"\"\"\n\n    def __init__(self, brightness, contrast, saturation, hue):\n        \"\"\"\n        Args:\n            strength (float): A number used to quantify the strength of the\n                              color distortion.\n        \"\"\"\n        super().__init__(\n            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n        )\n\n    def __call__(self, image: torch.Tensor):\n        if not isinstance(image, torch.Tensor):\n            raise ValueError(\"Expected tensor input\")\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = \"This transform is for 4 channel RGBD input only; got %d\" % C\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n        color_img_jitter = super().__call__(color_img)\n        img = torch.cat([color_img_jitter, depth_img], dim=0)\n\n        return img\n\n\nclass DropChannels(torch.nn.Module):\n    \"\"\"\n    Drops Channels with predefined probability values.\n    Pads the dropped channels with `pad_value`.\n    Channels can be tied using `tie_channels`\n    For example, for RGBD input, RGB can be tied by using `tie_channels=[0,1,2]`.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        s += \")\"\n        return s.format(**self.__dict__)\n\n\nclass ColorJitter3d(pth_transforms.ColorJitter):\n    \"\"\"\n    Apply ColorJitter on an image of shape (4, H, W)\n    \"\"\"\n\n    def __init__(self, brightness, contrast, saturation, hue):\n        \"\"\"\n        Args:\n            strength (float): A number used to quantify the strength of the\n                              color distortion.\n        \"\"\"\n        super().__init__(\n            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n        )\n\n    def __call__(self, image: torch.Tensor):\n        if not isinstance(image, torch.Tensor):\n            raise ValueError(\"Expected tensor input\")\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = \"This transform is for 4 channel RGBD input only; got %d\" % C\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n        color_img_jitter = super().__call__(color_img)\n        img = torch.cat([color_img_jitter, depth_img], dim=0)\n\n        return img\n\n\nclass DropChannels(torch.nn.Module):\n    \"\"\"\n    Drops Channels with predefined probability values.\n    Pads the dropped channels with `pad_value`.\n    Channels can be tied using `tie_channels`\n    For example, for RGBD input, RGB can be tied by using `tie_channels=[0,1,2]`.\n    In this case, channels [0,1,2] will be dropped all at once or not at all.\n    Assumes input is of the form CxHxW or TxCxHxW\n    \"\"\"\n\n    def __init__(\n        self, channel_probs, fill_values, tie_channels=None, all_channel_drop=False\n    ):\n        \"\"\"\n        channel_probs: List of probabilities\n        fill_values: List of values to fill the dropped channels with", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        Args:\n            strength (float): A number used to quantify the strength of the\n                              color distortion.\n        \"\"\"\n        super().__init__(\n            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n        )\n\n    def __call__(self, image: torch.Tensor):\n        if not isinstance(image, torch.Tensor):\n            raise ValueError(\"Expected tensor input\")\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = \"This transform is for 4 channel RGBD input only; got %d\" % C\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n        color_img_jitter = super().__call__(color_img)\n        img = torch.cat([color_img_jitter, depth_img], dim=0)\n\n        return img\n\n\nclass DropChannels(torch.nn.Module):\n    \"\"\"\n    Drops Channels with predefined probability values.\n    Pads the dropped channels with `pad_value`.\n    Channels can be tied using `tie_channels`\n    For example, for RGBD input, RGB can be tied by using `tie_channels=[0,1,2]`.\n    In this case, channels [0,1,2] will be dropped all at once or not at all.\n    Assumes input is of the form CxHxW or TxCxHxW\n    \"\"\"\n\n    def __init__(\n        self, channel_probs, fill_values, tie_channels=None, all_channel_drop=False\n    ):\n        \"\"\"\n        channel_probs: List of probabilities\n        fill_values: List of values to fill the dropped channels with\n        tie_channels: List of indices. Tie dropping of certain channels.\n        all_channel_drop: Bool variable to prevent cases where all channels are dropped.\n        \"\"\"\n        super().__init__()\n        assert len(channel_probs) == len(\n            fill_values\n        ), f\"Mismatch in length of channel_probs and fill_values: {len(channel_probs)} vs. {len(fill_values)}\"\n\n        assert len(channel_probs) in [\n            3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if not isinstance(image, torch.Tensor):\n            raise ValueError(\"Expected tensor input\")\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = \"This transform is for 4 channel RGBD input only; got %d\" % C\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n        color_img_jitter = super().__call__(color_img)\n        img = torch.cat([color_img_jitter, depth_img], dim=0)\n\n        return img\n\n\nclass DropChannels(torch.nn.Module):\n    \"\"\"\n    Drops Channels with predefined probability values.\n    Pads the dropped channels with `pad_value`.\n    Channels can be tied using `tie_channels`\n    For example, for RGBD input, RGB can be tied by using `tie_channels=[0,1,2]`.\n    In this case, channels [0,1,2] will be dropped all at once or not at all.\n    Assumes input is of the form CxHxW or TxCxHxW\n    \"\"\"\n\n    def __init__(\n        self, channel_probs, fill_values, tie_channels=None, all_channel_drop=False\n    ):\n        \"\"\"\n        channel_probs: List of probabilities\n        fill_values: List of values to fill the dropped channels with\n        tie_channels: List of indices. Tie dropping of certain channels.\n        all_channel_drop: Bool variable to prevent cases where all channels are dropped.\n        \"\"\"\n        super().__init__()\n        assert len(channel_probs) == len(\n            fill_values\n        ), f\"Mismatch in length of channel_probs and fill_values: {len(channel_probs)} vs. {len(fill_values)}\"\n\n        assert len(channel_probs) in [\n            3,\n            4,\n        ], f\"channel_probs length is {len(channel_probs)}. Should be 3 or 4\"\n\n        channel_probs = np.array(channel_probs, dtype=np.float32)\n        assert np.all(channel_probs >= 0)\n        assert np.all(channel_probs <= 1)\n\n        self.channel_probs = channel_probs\n        self.fill_values = fill_values\n        self.tie_channels = tie_channels", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        return img\n\n\nclass DropChannels(torch.nn.Module):\n    \"\"\"\n    Drops Channels with predefined probability values.\n    Pads the dropped channels with `pad_value`.\n    Channels can be tied using `tie_channels`\n    For example, for RGBD input, RGB can be tied by using `tie_channels=[0,1,2]`.\n    In this case, channels [0,1,2] will be dropped all at once or not at all.\n    Assumes input is of the form CxHxW or TxCxHxW\n    \"\"\"\n\n    def __init__(\n        self, channel_probs, fill_values, tie_channels=None, all_channel_drop=False\n    ):\n        \"\"\"\n        channel_probs: List of probabilities\n        fill_values: List of values to fill the dropped channels with\n        tie_channels: List of indices. Tie dropping of certain channels.\n        all_channel_drop: Bool variable to prevent cases where all channels are dropped.\n        \"\"\"\n        super().__init__()\n        assert len(channel_probs) == len(\n            fill_values\n        ), f\"Mismatch in length of channel_probs and fill_values: {len(channel_probs)} vs. {len(fill_values)}\"\n\n        assert len(channel_probs) in [\n            3,\n            4,\n        ], f\"channel_probs length is {len(channel_probs)}. Should be 3 or 4\"\n\n        channel_probs = np.array(channel_probs, dtype=np.float32)\n        assert np.all(channel_probs >= 0)\n        assert np.all(channel_probs <= 1)\n\n        self.channel_probs = channel_probs\n        self.fill_values = fill_values\n        self.tie_channels = tie_channels\n        self.all_channel_drop = all_channel_drop\n\n        if tie_channels is not None:\n            assert len(tie_channels) <= len(channel_probs)\n            assert max(tie_channels) < len(channel_probs)\n            assert min(tie_channels) >= 0\n            tie_probs = [channel_probs[x] for x in tie_channels]\n            assert len(set(tie_probs)) == 1, \"All tie_channel probs must be equal\"\n\n    def __call__(self, x):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    In this case, channels [0,1,2] will be dropped all at once or not at all.\n    Assumes input is of the form CxHxW or TxCxHxW\n    \"\"\"\n\n    def __init__(\n        self, channel_probs, fill_values, tie_channels=None, all_channel_drop=False\n    ):\n        \"\"\"\n        channel_probs: List of probabilities\n        fill_values: List of values to fill the dropped channels with\n        tie_channels: List of indices. Tie dropping of certain channels.\n        all_channel_drop: Bool variable to prevent cases where all channels are dropped.\n        \"\"\"\n        super().__init__()\n        assert len(channel_probs) == len(\n            fill_values\n        ), f\"Mismatch in length of channel_probs and fill_values: {len(channel_probs)} vs. {len(fill_values)}\"\n\n        assert len(channel_probs) in [\n            3,\n            4,\n        ], f\"channel_probs length is {len(channel_probs)}. Should be 3 or 4\"\n\n        channel_probs = np.array(channel_probs, dtype=np.float32)\n        assert np.all(channel_probs >= 0)\n        assert np.all(channel_probs <= 1)\n\n        self.channel_probs = channel_probs\n        self.fill_values = fill_values\n        self.tie_channels = tie_channels\n        self.all_channel_drop = all_channel_drop\n\n        if tie_channels is not None:\n            assert len(tie_channels) <= len(channel_probs)\n            assert max(tie_channels) < len(channel_probs)\n            assert min(tie_channels) >= 0\n            tie_probs = [channel_probs[x] for x in tie_channels]\n            assert len(set(tie_probs)) == 1, \"All tie_channel probs must be equal\"\n\n    def __call__(self, x):\n        assert isinstance(x, torch.Tensor)\n        if x.ndim == 3:\n            # CxHxW\n            num_channels = x.shape[0]\n            channel_index = 0\n        elif x.ndim == 4:\n            # TxCxHxW\n            num_channels = x.shape[1]\n            channel_index = 1\n        else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        tie_channels: List of indices. Tie dropping of certain channels.\n        all_channel_drop: Bool variable to prevent cases where all channels are dropped.\n        \"\"\"\n        super().__init__()\n        assert len(channel_probs) == len(\n            fill_values\n        ), f\"Mismatch in length of channel_probs and fill_values: {len(channel_probs)} vs. {len(fill_values)}\"\n\n        assert len(channel_probs) in [\n            3,\n            4,\n        ], f\"channel_probs length is {len(channel_probs)}. Should be 3 or 4\"\n\n        channel_probs = np.array(channel_probs, dtype=np.float32)\n        assert np.all(channel_probs >= 0)\n        assert np.all(channel_probs <= 1)\n\n        self.channel_probs = channel_probs\n        self.fill_values = fill_values\n        self.tie_channels = tie_channels\n        self.all_channel_drop = all_channel_drop\n\n        if tie_channels is not None:\n            assert len(tie_channels) <= len(channel_probs)\n            assert max(tie_channels) < len(channel_probs)\n            assert min(tie_channels) >= 0\n            tie_probs = [channel_probs[x] for x in tie_channels]\n            assert len(set(tie_probs)) == 1, \"All tie_channel probs must be equal\"\n\n    def __call__(self, x):\n        assert isinstance(x, torch.Tensor)\n        if x.ndim == 3:\n            # CxHxW\n            num_channels = x.shape[0]\n            channel_index = 0\n        elif x.ndim == 4:\n            # TxCxHxW\n            num_channels = x.shape[1]\n            channel_index = 1\n        else:\n            raise ValueError(f\"Unexpected number of dims {x.ndim}. Expected 3 or 4.\")\n\n        assert num_channels == len(\n            self.channel_probs\n        ), f\"channel_probs is {len(self.channel_probs)} but got {num_channels} channels\"\n\n        to_drop = [\n            np.random.random() < self.channel_probs[c] for c in range(num_channels)\n        ]\n        if self.tie_channels is not None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            4,\n        ], f\"channel_probs length is {len(channel_probs)}. Should be 3 or 4\"\n\n        channel_probs = np.array(channel_probs, dtype=np.float32)\n        assert np.all(channel_probs >= 0)\n        assert np.all(channel_probs <= 1)\n\n        self.channel_probs = channel_probs\n        self.fill_values = fill_values\n        self.tie_channels = tie_channels\n        self.all_channel_drop = all_channel_drop\n\n        if tie_channels is not None:\n            assert len(tie_channels) <= len(channel_probs)\n            assert max(tie_channels) < len(channel_probs)\n            assert min(tie_channels) >= 0\n            tie_probs = [channel_probs[x] for x in tie_channels]\n            assert len(set(tie_probs)) == 1, \"All tie_channel probs must be equal\"\n\n    def __call__(self, x):\n        assert isinstance(x, torch.Tensor)\n        if x.ndim == 3:\n            # CxHxW\n            num_channels = x.shape[0]\n            channel_index = 0\n        elif x.ndim == 4:\n            # TxCxHxW\n            num_channels = x.shape[1]\n            channel_index = 1\n        else:\n            raise ValueError(f\"Unexpected number of dims {x.ndim}. Expected 3 or 4.\")\n\n        assert num_channels == len(\n            self.channel_probs\n        ), f\"channel_probs is {len(self.channel_probs)} but got {num_channels} channels\"\n\n        to_drop = [\n            np.random.random() < self.channel_probs[c] for c in range(num_channels)\n        ]\n        if self.tie_channels is not None:\n            first_drop = to_drop[self.tie_channels[0]]\n            for idx in self.tie_channels[1:]:\n                to_drop[idx] = first_drop\n\n        if all(to_drop) and self.all_channel_drop is False:\n            # all channels will be dropped, prevent it\n            to_drop = [False for _ in range(num_channels)]\n\n        for c in range(num_channels):\n            if not to_drop[c]:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.all_channel_drop = all_channel_drop\n\n        if tie_channels is not None:\n            assert len(tie_channels) <= len(channel_probs)\n            assert max(tie_channels) < len(channel_probs)\n            assert min(tie_channels) >= 0\n            tie_probs = [channel_probs[x] for x in tie_channels]\n            assert len(set(tie_probs)) == 1, \"All tie_channel probs must be equal\"\n\n    def __call__(self, x):\n        assert isinstance(x, torch.Tensor)\n        if x.ndim == 3:\n            # CxHxW\n            num_channels = x.shape[0]\n            channel_index = 0\n        elif x.ndim == 4:\n            # TxCxHxW\n            num_channels = x.shape[1]\n            channel_index = 1\n        else:\n            raise ValueError(f\"Unexpected number of dims {x.ndim}. Expected 3 or 4.\")\n\n        assert num_channels == len(\n            self.channel_probs\n        ), f\"channel_probs is {len(self.channel_probs)} but got {num_channels} channels\"\n\n        to_drop = [\n            np.random.random() < self.channel_probs[c] for c in range(num_channels)\n        ]\n        if self.tie_channels is not None:\n            first_drop = to_drop[self.tie_channels[0]]\n            for idx in self.tie_channels[1:]:\n                to_drop[idx] = first_drop\n\n        if all(to_drop) and self.all_channel_drop is False:\n            # all channels will be dropped, prevent it\n            to_drop = [False for _ in range(num_channels)]\n\n        for c in range(num_channels):\n            if not to_drop[c]:\n                continue\n            if channel_index == 0:\n                x[c, ...] = self.fill_values[c]\n            elif channel_index == 1:\n                x[:, c, ...] = self.fill_values[c]\n            else:\n                raise NotImplementedError()\n        return x\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert isinstance(x, torch.Tensor)\n        if x.ndim == 3:\n            # CxHxW\n            num_channels = x.shape[0]\n            channel_index = 0\n        elif x.ndim == 4:\n            # TxCxHxW\n            num_channels = x.shape[1]\n            channel_index = 1\n        else:\n            raise ValueError(f\"Unexpected number of dims {x.ndim}. Expected 3 or 4.\")\n\n        assert num_channels == len(\n            self.channel_probs\n        ), f\"channel_probs is {len(self.channel_probs)} but got {num_channels} channels\"\n\n        to_drop = [\n            np.random.random() < self.channel_probs[c] for c in range(num_channels)\n        ]\n        if self.tie_channels is not None:\n            first_drop = to_drop[self.tie_channels[0]]\n            for idx in self.tie_channels[1:]:\n                to_drop[idx] = first_drop\n\n        if all(to_drop) and self.all_channel_drop is False:\n            # all channels will be dropped, prevent it\n            to_drop = [False for _ in range(num_channels)]\n\n        for c in range(num_channels):\n            if not to_drop[c]:\n                continue\n            if channel_index == 0:\n                x[c, ...] = self.fill_values[c]\n            elif channel_index == 1:\n                x[:, c, ...] = self.fill_values[c]\n            else:\n                raise NotImplementedError()\n        return x\n\n\nclass DepthNorm(torch.nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            raise ValueError(f\"Unexpected number of dims {x.ndim}. Expected 3 or 4.\")\n\n        assert num_channels == len(\n            self.channel_probs\n        ), f\"channel_probs is {len(self.channel_probs)} but got {num_channels} channels\"\n\n        to_drop = [\n            np.random.random() < self.channel_probs[c] for c in range(num_channels)\n        ]\n        if self.tie_channels is not None:\n            first_drop = to_drop[self.tie_channels[0]]\n            for idx in self.tie_channels[1:]:\n                to_drop[idx] = first_drop\n\n        if all(to_drop) and self.all_channel_drop is False:\n            # all channels will be dropped, prevent it\n            to_drop = [False for _ in range(num_channels)]\n\n        for c in range(num_channels):\n            if not to_drop[c]:\n                continue\n            if channel_index == 0:\n                x[c, ...] = self.fill_values[c]\n            elif channel_index == 1:\n                x[:, c, ...] = self.fill_values[c]\n            else:\n                raise NotImplementedError()\n        return x\n\n\nclass DepthNorm(torch.nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            first_drop = to_drop[self.tie_channels[0]]\n            for idx in self.tie_channels[1:]:\n                to_drop[idx] = first_drop\n\n        if all(to_drop) and self.all_channel_drop is False:\n            # all channels will be dropped, prevent it\n            to_drop = [False for _ in range(num_channels)]\n\n        for c in range(num_channels):\n            if not to_drop[c]:\n                continue\n            if channel_index == 0:\n                x[c, ...] = self.fill_values[c]\n            elif channel_index == 1:\n                x[:, c, ...] = self.fill_values[c]\n            else:\n                raise NotImplementedError()\n        return x\n\n\nclass DepthNorm(torch.nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def __call__(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                continue\n            if channel_index == 0:\n                x[c, ...] = self.fill_values[c]\n            elif channel_index == 1:\n                x[:, c, ...] = self.fill_values[c]\n            else:\n                raise NotImplementedError()\n        return x\n\n\nclass DepthNorm(torch.nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def __call__(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 413, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class DepthNorm(torch.nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def __call__(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img\n\nAST=Module(ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Expr(Call(Attribute(Call(Name(Load))Load)))If(Compare(Name(Load)LtConstant)Raise(Call(Name(Load)BinOp(ConstantModName(Load)))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Attribute(Name(Load)Load))If(Compare(Name(Load)NotEqConstant)Assign(Name(Store)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))))Raise(Call(Name(Load)Name(Load))))Assign(Name(Store)Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))Assign(Name(Store)Subscript(Name(Load)Tuple(Slice(ConstantConstant)ConstantLoad)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))))If(Attribute(Name(Load)Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load)))))AugAssign(Name(Store)DivAttribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Name(Load)Name(Load)Load)keyword(Constant)))Return(Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_365-413"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 413, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def __call__(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_375-413"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_rgbd.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 413, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def __call__(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_rgbd.py_385-413"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_video.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_video.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Callable\n\nimport torch\n\n\nclass ImageToSingleFrameVideo(Callable):\n    def __call__(self, image_tensor):\n        \"\"\"Converts (N x) C x H X W image to a (N x) C x T x H x W (T = 1) video frame.\"\"\"\n        if image_tensor.ndim == 3:\n            return image_tensor[:, None, ...]\n        assert image_tensor.ndim == 4\n        return image_tensor[:, :, None, ...]\n\n\nclass RepeatedPadIm2VideoSingleImage(torch.nn.Module):\n    def __init__(self, ntimes, time_dim=1):\n        super().__init__()\n        assert ntimes > 0\n        self.ntimes = ntimes\n\nAST=Module(ImportFrom(aliasalias)Import(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg)Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Subscript(Name(Load)Tuple(SliceConstantConstantLoad)Load)))Assert(Compare(Attribute(Name(Load)Load)EqConstant))Return(Subscript(Name(Load)Tuple(SliceSliceConstantConstantLoad)Load))))ClassDef(Attribute(Attribute(Name(Load)Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Compare(Name(Load)GtConstant))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_video.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_video.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_video.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 34, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_video.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 34, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Callable\n\nimport torch\n\n\nclass ImageToSingleFrameVideo(Callable):\n    def __call__(self, image_tensor):\n        \"\"\"Converts (N x) C x H X W image to a (N x) C x T x H x W (T = 1) video frame.\"\"\"\n        if image_tensor.ndim == 3:\n            return image_tensor[:, None, ...]\n        assert image_tensor.ndim == 4\n        return image_tensor[:, :, None, ...]\n\n\nclass RepeatedPadIm2VideoSingleImage(torch.nn.Module):\n    def __init__(self, ntimes, time_dim=1):\n        super().__init__()\n        assert ntimes > 0\n        self.ntimes = ntimes\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        # C x H x W -> C x T x H x W\n        x = x.unsqueeze(self.time_dim)\n        new_shape = [1] * len(x.shape)\n        new_shape[self.time_dim] = self.ntimes\n        x = x.repeat(new_shape)\n        return x\n\nAST=Module(ImportFrom(aliasalias)Import(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg)Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Subscript(Name(Load)Tuple(SliceConstantConstantLoad)Load)))Assert(Compare(Attribute(Name(Load)Load)EqConstant))Return(Subscript(Name(Load)Tuple(SliceSliceConstantConstantLoad)Load))))ClassDef(Attribute(Attribute(Name(Load)Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Compare(Name(Load)GtConstant))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Assign(Name(Store)BinOp(List(ConstantLoad)MultCall(Name(Load)Attribute(Name(Load)Load))))Assign(Subscript(Name(Load)Attribute(Name(Load)Load)Store)Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_video.py_0-34"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-image_video.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "image_video.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 34, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom typing import Any, Callable\n\nimport torch\n\n\nclass ImageToSingleFrameVideo(Callable):\n    def __call__(self, image_tensor):\n        \"\"\"Converts (N x) C x H X W image to a (N x) C x T x H x W (T = 1) video frame.\"\"\"\n        if image_tensor.ndim == 3:\n            return image_tensor[:, None, ...]\n        assert image_tensor.ndim == 4\n        return image_tensor[:, :, None, ...]\n\n\nclass RepeatedPadIm2VideoSingleImage(torch.nn.Module):\n    def __init__(self, ntimes, time_dim=1):\n        super().__init__()\n        assert ntimes > 0\n        self.ntimes = ntimes\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        # C x H x W -> C x T x H x W\n        x = x.unsqueeze(self.time_dim)\n        new_shape = [1] * len(x.shape)\n        new_shape[self.time_dim] = self.ntimes\n        x = x.repeat(new_shape)\n        return x\n\nAST=Module(ImportFrom(aliasalias)Import(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg)Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Subscript(Name(Load)Tuple(SliceConstantConstantLoad)Load)))Assert(Compare(Attribute(Name(Load)Load)EqConstant))Return(Subscript(Name(Load)Tuple(SliceSliceConstantConstantLoad)Load))))ClassDef(Attribute(Attribute(Name(Load)Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Compare(Name(Load)GtConstant))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Assign(Name(Store)BinOp(List(ConstantLoad)MultCall(Name(Load)Attribute(Name(Load)Load))))Assign(Subscript(Name(Load)Attribute(Name(Load)Load)Store)Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-image_video.py_5-34"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Parts of code are modified from https://github.com/microsoft/unilm/blob/b94ec76c36f02fb2b0bf0dcb0b8554a2185173cd/beit/masking_generator.py\n\nimport math\nimport random\nfrom abc import ABC\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\n\ndef get_image_dims(img):\n    # If an image, convert to singleton video\n    if img.ndim == 3:\n        time_dim = 1\n        squeeze_dim = True\n    else:\n        time_dim = img.shape[-3]\n        squeeze_dim = False\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)Import(alias)Import(alias)FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))Assign(Name(Store)Constant))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Parts of code are modified from https://github.com/microsoft/unilm/blob/b94ec76c36f02fb2b0bf0dcb0b8554a2185173cd/beit/masking_generator.py\n\nimport math\nimport random\nfrom abc import ABC\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\n\ndef get_image_dims(img):\n    # If an image, convert to singleton video\n    if img.ndim == 3:\n        time_dim = 1\n        squeeze_dim = True\n    else:\n        time_dim = img.shape[-3]\n        squeeze_dim = False\n    return squeeze_dim, time_dim, img.shape[-2], img.shape[-1]\n\n\ndef get_pred_ratio(pred_ratio, pred_ratio_var):\n    if isinstance(pred_ratio, list):\n        curr_pred_ratio = []\n        for prm, prv in zip(pred_ratio, pred_ratio_var):\n            assert prm >= prv\n            pr = random.uniform(prm - prv, prm + prv) if prv > 0 else prm\n            curr_pred_ratio.append(pr)\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)Import(alias)Import(alias)FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))Assign(Name(Store)Constant))Return(Tuple(Name(Load)Name(Load)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load)Load)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)List(Load))For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)Name(Load))Assert(Compare(Name(Load)GtEName(Load)))Assign(Name(Store)IfExp(Compare(Name(Load)GtConstant)Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubName(Load))BinOp(Name(Load)AddName(Load)))Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Parts of code are modified from https://github.com/microsoft/unilm/blob/b94ec76c36f02fb2b0bf0dcb0b8554a2185173cd/beit/masking_generator.py\n\nimport math\nimport random\nfrom abc import ABC\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\n\ndef get_image_dims(img):\n    # If an image, convert to singleton video\n    if img.ndim == 3:\n        time_dim = 1\n        squeeze_dim = True\n    else:\n        time_dim = img.shape[-3]\n        squeeze_dim = False\n    return squeeze_dim, time_dim, img.shape[-2], img.shape[-1]\n\n\ndef get_pred_ratio(pred_ratio, pred_ratio_var):\n    if isinstance(pred_ratio, list):\n        curr_pred_ratio = []\n        for prm, prv in zip(pred_ratio, pred_ratio_var):\n            assert prm >= prv\n            pr = random.uniform(prm - prv, prm + prv) if prv > 0 else prm\n            curr_pred_ratio.append(pr)\n        curr_pred_ratio = random.choice(curr_pred_ratio)\n    else:\n        assert pred_ratio >= pred_ratio_var\n        curr_pred_ratio = (\n            random.uniform(\n                pred_ratio - pred_ratio_var,\n                pred_ratio + pred_ratio_var,\n            )\n            if pred_ratio_var > 0\n            else pred_ratio", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Parts of code are modified from https://github.com/microsoft/unilm/blob/b94ec76c36f02fb2b0bf0dcb0b8554a2185173cd/beit/masking_generator.py\n\nimport math\nimport random\nfrom abc import ABC\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\n\ndef get_image_dims(img):\n    # If an image, convert to singleton video\n    if img.ndim == 3:\n        time_dim = 1\n        squeeze_dim = True\n    else:\n        time_dim = img.shape[-3]\n        squeeze_dim = False\n    return squeeze_dim, time_dim, img.shape[-2], img.shape[-1]\n\n\ndef get_pred_ratio(pred_ratio, pred_ratio_var):\n    if isinstance(pred_ratio, list):\n        curr_pred_ratio = []\n        for prm, prv in zip(pred_ratio, pred_ratio_var):\n            assert prm >= prv\n            pr = random.uniform(prm - prv, prm + prv) if prv > 0 else prm\n            curr_pred_ratio.append(pr)\n        curr_pred_ratio = random.choice(curr_pred_ratio)\n    else:\n        assert pred_ratio >= pred_ratio_var\n        curr_pred_ratio = (\n            random.uniform(\n                pred_ratio - pred_ratio_var,\n                pred_ratio + pred_ratio_var,\n            )\n            if pred_ratio_var > 0\n            else pred_ratio\n        )\n    return curr_pred_ratio\n\n\nclass Masking(ABC):\n    pass\n\n\nclass BlockMasking(Masking):\n    def __init__(self, pred_aspect_ratio: Tuple[float] = (0.3, 1 / 0.3)):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_image_dims(img):\n    # If an image, convert to singleton video\n    if img.ndim == 3:\n        time_dim = 1\n        squeeze_dim = True\n    else:\n        time_dim = img.shape[-3]\n        squeeze_dim = False\n    return squeeze_dim, time_dim, img.shape[-2], img.shape[-1]\n\n\ndef get_pred_ratio(pred_ratio, pred_ratio_var):\n    if isinstance(pred_ratio, list):\n        curr_pred_ratio = []\n        for prm, prv in zip(pred_ratio, pred_ratio_var):\n            assert prm >= prv\n            pr = random.uniform(prm - prv, prm + prv) if prv > 0 else prm\n            curr_pred_ratio.append(pr)\n        curr_pred_ratio = random.choice(curr_pred_ratio)\n    else:\n        assert pred_ratio >= pred_ratio_var\n        curr_pred_ratio = (\n            random.uniform(\n                pred_ratio - pred_ratio_var,\n                pred_ratio + pred_ratio_var,\n            )\n            if pred_ratio_var > 0\n            else pred_ratio\n        )\n    return curr_pred_ratio\n\n\nclass Masking(ABC):\n    pass\n\n\nclass BlockMasking(Masking):\n    def __init__(self, pred_aspect_ratio: Tuple[float] = (0.3, 1 / 0.3)):\n        self.pred_aspect_ratio = pred_aspect_ratio\n\n    def __call__(self, T: int, H: int, W: int, high: float) -> np.ndarray:\n        assert T == 1, \"Does not support videos yet\"\n        mask = np.zeros((T, H, W), dtype=bool)\n        mask_count = 0\n        log_aspect_ratio = tuple(map(lambda x: math.log(x), self.pred_aspect_ratio))\n        while mask_count < high:\n            max_mask_patches = high - mask_count\n\n\nAST=Module(FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))Assign(Name(Store)Constant))Return(Tuple(Name(Load)Name(Load)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load)Load)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)List(Load))For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)Name(Load))Assert(Compare(Name(Load)GtEName(Load)))Assign(Name(Store)IfExp(Compare(Name(Load)GtConstant)Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubName(Load))BinOp(Name(Load)AddName(Load)))Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assert(Compare(Name(Load)GtEName(Load)))Assign(Name(Store)IfExp(Compare(Name(Load)GtConstant)Call(Attribute(Name(Load)Load)BinOp(Name(Load)SubName(Load))BinOp(Name(Load)AddName(Load)))Name(Load))))Return(Name(Load)))ClassDef(Name(Load)Pass)ClassDef(Name(Load)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))Tuple(ConstantBinOp(ConstantDivConstant)Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load)))Assert(Compare(Name(Load)EqConstant)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Name(Load)Load)keyword(Name(Load))))Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Call(Attribute(Name(Load)Load)Name(Load)))Attribute(Name(Load)Load))))While(Compare(Name(Load)LtName(Load))Assign(Name(Store)BinOp(Name(Load)SubName(Load))))Attribute(Name(Load)Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return squeeze_dim, time_dim, img.shape[-2], img.shape[-1]\n\n\ndef get_pred_ratio(pred_ratio, pred_ratio_var):\n    if isinstance(pred_ratio, list):\n        curr_pred_ratio = []\n        for prm, prv in zip(pred_ratio, pred_ratio_var):\n            assert prm >= prv\n            pr = random.uniform(prm - prv, prm + prv) if prv > 0 else prm\n            curr_pred_ratio.append(pr)\n        curr_pred_ratio = random.choice(curr_pred_ratio)\n    else:\n        assert pred_ratio >= pred_ratio_var\n        curr_pred_ratio = (\n            random.uniform(\n                pred_ratio - pred_ratio_var,\n                pred_ratio + pred_ratio_var,\n            )\n            if pred_ratio_var > 0\n            else pred_ratio\n        )\n    return curr_pred_ratio\n\n\nclass Masking(ABC):\n    pass\n\n\nclass BlockMasking(Masking):\n    def __init__(self, pred_aspect_ratio: Tuple[float] = (0.3, 1 / 0.3)):\n        self.pred_aspect_ratio = pred_aspect_ratio\n\n    def __call__(self, T: int, H: int, W: int, high: float) -> np.ndarray:\n        assert T == 1, \"Does not support videos yet\"\n        mask = np.zeros((T, H, W), dtype=bool)\n        mask_count = 0\n        log_aspect_ratio = tuple(map(lambda x: math.log(x), self.pred_aspect_ratio))\n        while mask_count < high:\n            max_mask_patches = high - mask_count\n\n            delta = 0\n            for _ in range(10):\n                low = (min(H, W) // 3) ** 2\n                target_area = random.uniform(low, max_mask_patches)\n                aspect_ratio = math.exp(random.uniform(*log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < W and h < H:\n                    top = random.randint(0, H - h)\n                    left = random.randint(0, W - w)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        curr_pred_ratio = random.choice(curr_pred_ratio)\n    else:\n        assert pred_ratio >= pred_ratio_var\n        curr_pred_ratio = (\n            random.uniform(\n                pred_ratio - pred_ratio_var,\n                pred_ratio + pred_ratio_var,\n            )\n            if pred_ratio_var > 0\n            else pred_ratio\n        )\n    return curr_pred_ratio\n\n\nclass Masking(ABC):\n    pass\n\n\nclass BlockMasking(Masking):\n    def __init__(self, pred_aspect_ratio: Tuple[float] = (0.3, 1 / 0.3)):\n        self.pred_aspect_ratio = pred_aspect_ratio\n\n    def __call__(self, T: int, H: int, W: int, high: float) -> np.ndarray:\n        assert T == 1, \"Does not support videos yet\"\n        mask = np.zeros((T, H, W), dtype=bool)\n        mask_count = 0\n        log_aspect_ratio = tuple(map(lambda x: math.log(x), self.pred_aspect_ratio))\n        while mask_count < high:\n            max_mask_patches = high - mask_count\n\n            delta = 0\n            for _ in range(10):\n                low = (min(H, W) // 3) ** 2\n                target_area = random.uniform(low, max_mask_patches)\n                aspect_ratio = math.exp(random.uniform(*log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < W and h < H:\n                    top = random.randint(0, H - h)\n                    left = random.randint(0, W - w)\n                    num_masked = mask[top : top + h, left : left + w].sum()\n                    if 0 < h * w - num_masked <= max_mask_patches:\n                        for i in range(top, top + h):\n                            for j in range(left, left + w):\n                                if mask[0, i, j] == 0:\n                                    mask[0, i, j] = 1\n                                    delta += 1\n\n                if delta > 0:\n                    break", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n    return curr_pred_ratio\n\n\nclass Masking(ABC):\n    pass\n\n\nclass BlockMasking(Masking):\n    def __init__(self, pred_aspect_ratio: Tuple[float] = (0.3, 1 / 0.3)):\n        self.pred_aspect_ratio = pred_aspect_ratio\n\n    def __call__(self, T: int, H: int, W: int, high: float) -> np.ndarray:\n        assert T == 1, \"Does not support videos yet\"\n        mask = np.zeros((T, H, W), dtype=bool)\n        mask_count = 0\n        log_aspect_ratio = tuple(map(lambda x: math.log(x), self.pred_aspect_ratio))\n        while mask_count < high:\n            max_mask_patches = high - mask_count\n\n            delta = 0\n            for _ in range(10):\n                low = (min(H, W) // 3) ** 2\n                target_area = random.uniform(low, max_mask_patches)\n                aspect_ratio = math.exp(random.uniform(*log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < W and h < H:\n                    top = random.randint(0, H - h)\n                    left = random.randint(0, W - w)\n                    num_masked = mask[top : top + h, left : left + w].sum()\n                    if 0 < h * w - num_masked <= max_mask_patches:\n                        for i in range(top, top + h):\n                            for j in range(left, left + w):\n                                if mask[0, i, j] == 0:\n                                    mask[0, i, j] = 1\n                                    delta += 1\n\n                if delta > 0:\n                    break\n\n            if delta == 0:\n                break\n            else:\n                mask_count += delta\n        return mask\n\n\nclass RandMasking(Masking):\n    @staticmethod", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.pred_aspect_ratio = pred_aspect_ratio\n\n    def __call__(self, T: int, H: int, W: int, high: float) -> np.ndarray:\n        assert T == 1, \"Does not support videos yet\"\n        mask = np.zeros((T, H, W), dtype=bool)\n        mask_count = 0\n        log_aspect_ratio = tuple(map(lambda x: math.log(x), self.pred_aspect_ratio))\n        while mask_count < high:\n            max_mask_patches = high - mask_count\n\n            delta = 0\n            for _ in range(10):\n                low = (min(H, W) // 3) ** 2\n                target_area = random.uniform(low, max_mask_patches)\n                aspect_ratio = math.exp(random.uniform(*log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < W and h < H:\n                    top = random.randint(0, H - h)\n                    left = random.randint(0, W - w)\n                    num_masked = mask[top : top + h, left : left + w].sum()\n                    if 0 < h * w - num_masked <= max_mask_patches:\n                        for i in range(top, top + h):\n                            for j in range(left, left + w):\n                                if mask[0, i, j] == 0:\n                                    mask[0, i, j] = 1\n                                    delta += 1\n\n                if delta > 0:\n                    break\n\n            if delta == 0:\n                break\n            else:\n                mask_count += delta\n        return mask\n\n\nclass RandMasking(Masking):\n    @staticmethod\n    def __call__(\n        T: int, H: int, W: int, high: float, shuffle: bool = True\n    ) -> np.ndarray:\n        mask = np.hstack(\n            [\n                np.zeros(T * H * W - int(high)),\n                np.ones(int(high)),\n            ]\n        ).astype(bool)\n        if shuffle:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            delta = 0\n            for _ in range(10):\n                low = (min(H, W) // 3) ** 2\n                target_area = random.uniform(low, max_mask_patches)\n                aspect_ratio = math.exp(random.uniform(*log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < W and h < H:\n                    top = random.randint(0, H - h)\n                    left = random.randint(0, W - w)\n                    num_masked = mask[top : top + h, left : left + w].sum()\n                    if 0 < h * w - num_masked <= max_mask_patches:\n                        for i in range(top, top + h):\n                            for j in range(left, left + w):\n                                if mask[0, i, j] == 0:\n                                    mask[0, i, j] = 1\n                                    delta += 1\n\n                if delta > 0:\n                    break\n\n            if delta == 0:\n                break\n            else:\n                mask_count += delta\n        return mask\n\n\nclass RandMasking(Masking):\n    @staticmethod\n    def __call__(\n        T: int, H: int, W: int, high: float, shuffle: bool = True\n    ) -> np.ndarray:\n        mask = np.hstack(\n            [\n                np.zeros(T * H * W - int(high)),\n                np.ones(int(high)),\n            ]\n        ).astype(bool)\n        if shuffle:\n            np.random.shuffle(mask)\n        mask = mask.reshape(T, H, W)\n        return mask\n\n\nclass TubeMasking(Masking):\n    \"\"\"\n    Extends any frame mask to the temporal extent of a video.\n    \"\"\"\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    num_masked = mask[top : top + h, left : left + w].sum()\n                    if 0 < h * w - num_masked <= max_mask_patches:\n                        for i in range(top, top + h):\n                            for j in range(left, left + w):\n                                if mask[0, i, j] == 0:\n                                    mask[0, i, j] = 1\n                                    delta += 1\n\n                if delta > 0:\n                    break\n\n            if delta == 0:\n                break\n            else:\n                mask_count += delta\n        return mask\n\n\nclass RandMasking(Masking):\n    @staticmethod\n    def __call__(\n        T: int, H: int, W: int, high: float, shuffle: bool = True\n    ) -> np.ndarray:\n        mask = np.hstack(\n            [\n                np.zeros(T * H * W - int(high)),\n                np.ones(int(high)),\n            ]\n        ).astype(bool)\n        if shuffle:\n            np.random.shuffle(mask)\n        mask = mask.reshape(T, H, W)\n        return mask\n\n\nclass TubeMasking(Masking):\n    \"\"\"\n    Extends any frame mask to the temporal extent of a video.\n    \"\"\"\n\n    def __init__(self, frame_masking: Masking):\n        self.frame_masking = frame_masking\n\n    def __call__(self, T: int, H: int, W: int, high: float):\n        # Get a frame level mask with 1/T the masking\n        frame_mask = self.frame_masking(1, H, W, high / T)\n        # replicate the frame mask for all the T frames\n        return frame_mask.repeat(T, 0)\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n            if delta == 0:\n                break\n            else:\n                mask_count += delta\n        return mask\n\n\nclass RandMasking(Masking):\n    @staticmethod\n    def __call__(\n        T: int, H: int, W: int, high: float, shuffle: bool = True\n    ) -> np.ndarray:\n        mask = np.hstack(\n            [\n                np.zeros(T * H * W - int(high)),\n                np.ones(int(high)),\n            ]\n        ).astype(bool)\n        if shuffle:\n            np.random.shuffle(mask)\n        mask = mask.reshape(T, H, W)\n        return mask\n\n\nclass TubeMasking(Masking):\n    \"\"\"\n    Extends any frame mask to the temporal extent of a video.\n    \"\"\"\n\n    def __init__(self, frame_masking: Masking):\n        self.frame_masking = frame_masking\n\n    def __call__(self, T: int, H: int, W: int, high: float):\n        # Get a frame level mask with 1/T the masking\n        frame_mask = self.frame_masking(1, H, W, high / T)\n        # replicate the frame mask for all the T frames\n        return frame_mask.repeat(T, 0)\n\n\nclass CausalMasking(RandMasking):\n    \"\"\"\n    Masks out the first N tokens in a raster order.\n    \"\"\"\n\n    @classmethod\n    def __call__(cls, *args, **kwargs) -> np.ndarray:\n        kwargs[\"shuffle\"] = False\n        T = super().__call__(*args, **kwargs)\n        return T", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __call__(\n        T: int, H: int, W: int, high: float, shuffle: bool = True\n    ) -> np.ndarray:\n        mask = np.hstack(\n            [\n                np.zeros(T * H * W - int(high)),\n                np.ones(int(high)),\n            ]\n        ).astype(bool)\n        if shuffle:\n            np.random.shuffle(mask)\n        mask = mask.reshape(T, H, W)\n        return mask\n\n\nclass TubeMasking(Masking):\n    \"\"\"\n    Extends any frame mask to the temporal extent of a video.\n    \"\"\"\n\n    def __init__(self, frame_masking: Masking):\n        self.frame_masking = frame_masking\n\n    def __call__(self, T: int, H: int, W: int, high: float):\n        # Get a frame level mask with 1/T the masking\n        frame_mask = self.frame_masking(1, H, W, high / T)\n        # replicate the frame mask for all the T frames\n        return frame_mask.repeat(T, 0)\n\n\nclass CausalMasking(RandMasking):\n    \"\"\"\n    Masks out the first N tokens in a raster order.\n    \"\"\"\n\n    @classmethod\n    def __call__(cls, *args, **kwargs) -> np.ndarray:\n        kwargs[\"shuffle\"] = False\n        T = super().__call__(*args, **kwargs)\n        return T\n\n\nclass RandomFrameMasking(Masking):\n    \"\"\"\n    Masks out random frames from the clip.\n    \"\"\"\n\n    @staticmethod\n    def __call__(T: int, H: int, W: int, high: float) -> np.ndarray:\n        mask = np.zeros((T, H, W), dtype=np.bool)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            np.random.shuffle(mask)\n        mask = mask.reshape(T, H, W)\n        return mask\n\n\nclass TubeMasking(Masking):\n    \"\"\"\n    Extends any frame mask to the temporal extent of a video.\n    \"\"\"\n\n    def __init__(self, frame_masking: Masking):\n        self.frame_masking = frame_masking\n\n    def __call__(self, T: int, H: int, W: int, high: float):\n        # Get a frame level mask with 1/T the masking\n        frame_mask = self.frame_masking(1, H, W, high / T)\n        # replicate the frame mask for all the T frames\n        return frame_mask.repeat(T, 0)\n\n\nclass CausalMasking(RandMasking):\n    \"\"\"\n    Masks out the first N tokens in a raster order.\n    \"\"\"\n\n    @classmethod\n    def __call__(cls, *args, **kwargs) -> np.ndarray:\n        kwargs[\"shuffle\"] = False\n        T = super().__call__(*args, **kwargs)\n        return T\n\n\nclass RandomFrameMasking(Masking):\n    \"\"\"\n    Masks out random frames from the clip.\n    \"\"\"\n\n    @staticmethod\n    def __call__(T: int, H: int, W: int, high: float) -> np.ndarray:\n        mask = np.zeros((T, H, W), dtype=np.bool)\n        # Can't be exact to high since need to mask the full frame, but round it\n        # high is the max number of tokens to mask in the input\n        pred_ratio = high * 1.0 / (T * H * W)\n        frame_ids = np.arange(T, dtype=int)\n        np.random.shuffle(frame_ids)\n        frame_ids_to_mask = frame_ids[: math.floor(T * pred_ratio)]\n        mask[frame_ids_to_mask, ...] = True\n        return mask\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, frame_masking: Masking):\n        self.frame_masking = frame_masking\n\n    def __call__(self, T: int, H: int, W: int, high: float):\n        # Get a frame level mask with 1/T the masking\n        frame_mask = self.frame_masking(1, H, W, high / T)\n        # replicate the frame mask for all the T frames\n        return frame_mask.repeat(T, 0)\n\n\nclass CausalMasking(RandMasking):\n    \"\"\"\n    Masks out the first N tokens in a raster order.\n    \"\"\"\n\n    @classmethod\n    def __call__(cls, *args, **kwargs) -> np.ndarray:\n        kwargs[\"shuffle\"] = False\n        T = super().__call__(*args, **kwargs)\n        return T\n\n\nclass RandomFrameMasking(Masking):\n    \"\"\"\n    Masks out random frames from the clip.\n    \"\"\"\n\n    @staticmethod\n    def __call__(T: int, H: int, W: int, high: float) -> np.ndarray:\n        mask = np.zeros((T, H, W), dtype=np.bool)\n        # Can't be exact to high since need to mask the full frame, but round it\n        # high is the max number of tokens to mask in the input\n        pred_ratio = high * 1.0 / (T * H * W)\n        frame_ids = np.arange(T, dtype=int)\n        np.random.shuffle(frame_ids)\n        frame_ids_to_mask = frame_ids[: math.floor(T * pred_ratio)]\n        mask[frame_ids_to_mask, ...] = True\n        return mask\n\n\ndef ibot_style_mask_image(\n    image: torch.Tensor,\n    patch_size: List[int],  # [patch_t, patch_h, patch_w]\n    pred_ratio: Tuple[float],\n    pred_ratio_var: Tuple[float],\n    pred_shape: Masking = RandMasking,\n    precomputed_pred_ratio: float = None,\n):\n    squeeze_dim, img_t, img_h, img_w = get_image_dims(image)\n    T = max(img_t // patch_size[0], 1)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class CausalMasking(RandMasking):\n    \"\"\"\n    Masks out the first N tokens in a raster order.\n    \"\"\"\n\n    @classmethod\n    def __call__(cls, *args, **kwargs) -> np.ndarray:\n        kwargs[\"shuffle\"] = False\n        T = super().__call__(*args, **kwargs)\n        return T\n\n\nclass RandomFrameMasking(Masking):\n    \"\"\"\n    Masks out random frames from the clip.\n    \"\"\"\n\n    @staticmethod\n    def __call__(T: int, H: int, W: int, high: float) -> np.ndarray:\n        mask = np.zeros((T, H, W), dtype=np.bool)\n        # Can't be exact to high since need to mask the full frame, but round it\n        # high is the max number of tokens to mask in the input\n        pred_ratio = high * 1.0 / (T * H * W)\n        frame_ids = np.arange(T, dtype=int)\n        np.random.shuffle(frame_ids)\n        frame_ids_to_mask = frame_ids[: math.floor(T * pred_ratio)]\n        mask[frame_ids_to_mask, ...] = True\n        return mask\n\n\ndef ibot_style_mask_image(\n    image: torch.Tensor,\n    patch_size: List[int],  # [patch_t, patch_h, patch_w]\n    pred_ratio: Tuple[float],\n    pred_ratio_var: Tuple[float],\n    pred_shape: Masking = RandMasking,\n    precomputed_pred_ratio: float = None,\n):\n    squeeze_dim, img_t, img_h, img_w = get_image_dims(image)\n    T = max(img_t // patch_size[0], 1)\n    H = img_h // patch_size[1]\n    W = img_w // patch_size[2]\n    if precomputed_pred_ratio is None:\n        precomputed_pred_ratio = get_pred_ratio(\n            pred_ratio=pred_ratio, pred_ratio_var=pred_ratio_var\n        )\n    # high is the max number of tokens to mask in the input\n    high = precomputed_pred_ratio * T * H * W\n    mask = pred_shape(T, H, W, high)\n\n\nAST=Module(ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argargarg)Assign(Subscript(Name(Load)ConstantStore)Constant)Assign(Name(Store)Call(Attribute(Call(Name(Load))Load)Starred(Name(Load)Load)keyword(Name(Load))))Return(Name(Load))Name(Load)Attribute(Name(Load)Load)))ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Name(Load)Load)keyword(Attribute(Name(Load)Load))))Assign(Name(Store)BinOp(BinOp(Name(Load)MultConstant)DivBinOp(BinOp(Name(Load)MultName(Load))MultName(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Subscript(Name(Load)Slice(Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load))))Load))Assign(Subscript(Name(Load)Tuple(Name(Load)ConstantLoad)Store)Constant)Return(Name(Load))Name(Load)Attribute(Name(Load)Load)))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))Constant))Assign(Name(Store)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad)))Assign(Name(Store)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad)))If(Compare(Name(Load)IsConstant)Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))))Assign(Name(Store)BinOp(BinOp(BinOp(Name(Load)MultName(Load))MultName(Load))MultName(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass RandomFrameMasking(Masking):\n    \"\"\"\n    Masks out random frames from the clip.\n    \"\"\"\n\n    @staticmethod\n    def __call__(T: int, H: int, W: int, high: float) -> np.ndarray:\n        mask = np.zeros((T, H, W), dtype=np.bool)\n        # Can't be exact to high since need to mask the full frame, but round it\n        # high is the max number of tokens to mask in the input\n        pred_ratio = high * 1.0 / (T * H * W)\n        frame_ids = np.arange(T, dtype=int)\n        np.random.shuffle(frame_ids)\n        frame_ids_to_mask = frame_ids[: math.floor(T * pred_ratio)]\n        mask[frame_ids_to_mask, ...] = True\n        return mask\n\n\ndef ibot_style_mask_image(\n    image: torch.Tensor,\n    patch_size: List[int],  # [patch_t, patch_h, patch_w]\n    pred_ratio: Tuple[float],\n    pred_ratio_var: Tuple[float],\n    pred_shape: Masking = RandMasking,\n    precomputed_pred_ratio: float = None,\n):\n    squeeze_dim, img_t, img_h, img_w = get_image_dims(image)\n    T = max(img_t // patch_size[0], 1)\n    H = img_h // patch_size[1]\n    W = img_w // patch_size[2]\n    if precomputed_pred_ratio is None:\n        precomputed_pred_ratio = get_pred_ratio(\n            pred_ratio=pred_ratio, pred_ratio_var=pred_ratio_var\n        )\n    # high is the max number of tokens to mask in the input\n    high = precomputed_pred_ratio * T * H * W\n    mask = pred_shape(T, H, W, high)\n\n    if squeeze_dim:\n        # Remove the time dim from the mask since the image doesn't have it\n        mask = np.squeeze(mask, axis=0)\n    ret_dict = {\n        \"data\": image,\n        \"mask\": torch.from_numpy(mask),\n    }\n    return ret_dict\n\n\n\nAST=Module(ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Name(Load)Load)keyword(Attribute(Name(Load)Load))))Assign(Name(Store)BinOp(BinOp(Name(Load)MultConstant)DivBinOp(BinOp(Name(Load)MultName(Load))MultName(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Subscript(Name(Load)Slice(Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load))))Load))Assign(Subscript(Name(Load)Tuple(Name(Load)ConstantLoad)Store)Constant)Return(Name(Load))Name(Load)Attribute(Name(Load)Load)))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))Constant))Assign(Name(Store)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad)))Assign(Name(Store)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad)))If(Compare(Name(Load)IsConstant)Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))))Assign(Name(Store)BinOp(BinOp(BinOp(Name(Load)MultName(Load))MultName(Load))MultName(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant))))Assign(Name(Store)Dict(ConstantConstantName(Load)Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # Can't be exact to high since need to mask the full frame, but round it\n        # high is the max number of tokens to mask in the input\n        pred_ratio = high * 1.0 / (T * H * W)\n        frame_ids = np.arange(T, dtype=int)\n        np.random.shuffle(frame_ids)\n        frame_ids_to_mask = frame_ids[: math.floor(T * pred_ratio)]\n        mask[frame_ids_to_mask, ...] = True\n        return mask\n\n\ndef ibot_style_mask_image(\n    image: torch.Tensor,\n    patch_size: List[int],  # [patch_t, patch_h, patch_w]\n    pred_ratio: Tuple[float],\n    pred_ratio_var: Tuple[float],\n    pred_shape: Masking = RandMasking,\n    precomputed_pred_ratio: float = None,\n):\n    squeeze_dim, img_t, img_h, img_w = get_image_dims(image)\n    T = max(img_t // patch_size[0], 1)\n    H = img_h // patch_size[1]\n    W = img_w // patch_size[2]\n    if precomputed_pred_ratio is None:\n        precomputed_pred_ratio = get_pred_ratio(\n            pred_ratio=pred_ratio, pred_ratio_var=pred_ratio_var\n        )\n    # high is the max number of tokens to mask in the input\n    high = precomputed_pred_ratio * T * H * W\n    mask = pred_shape(T, H, W, high)\n\n    if squeeze_dim:\n        # Remove the time dim from the mask since the image doesn't have it\n        mask = np.squeeze(mask, axis=0)\n    ret_dict = {\n        \"data\": image,\n        \"mask\": torch.from_numpy(mask),\n    }\n    return ret_dict\n\n\nclass MaskImageModeling:\n    def __init__(\n        self,\n        pred_ratio: Tuple[float],\n        pred_ratio_var: Tuple[float],\n        patch_size: List[int],\n        pred_shape: Masking = RandMasking,\n        mim_start_epochs: float = 0,\n    ):\n        self.pred_ratio = pred_ratio", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def ibot_style_mask_image(\n    image: torch.Tensor,\n    patch_size: List[int],  # [patch_t, patch_h, patch_w]\n    pred_ratio: Tuple[float],\n    pred_ratio_var: Tuple[float],\n    pred_shape: Masking = RandMasking,\n    precomputed_pred_ratio: float = None,\n):\n    squeeze_dim, img_t, img_h, img_w = get_image_dims(image)\n    T = max(img_t // patch_size[0], 1)\n    H = img_h // patch_size[1]\n    W = img_w // patch_size[2]\n    if precomputed_pred_ratio is None:\n        precomputed_pred_ratio = get_pred_ratio(\n            pred_ratio=pred_ratio, pred_ratio_var=pred_ratio_var\n        )\n    # high is the max number of tokens to mask in the input\n    high = precomputed_pred_ratio * T * H * W\n    mask = pred_shape(T, H, W, high)\n\n    if squeeze_dim:\n        # Remove the time dim from the mask since the image doesn't have it\n        mask = np.squeeze(mask, axis=0)\n    ret_dict = {\n        \"data\": image,\n        \"mask\": torch.from_numpy(mask),\n    }\n    return ret_dict\n\n\nclass MaskImageModeling:\n    def __init__(\n        self,\n        pred_ratio: Tuple[float],\n        pred_ratio_var: Tuple[float],\n        patch_size: List[int],\n        pred_shape: Masking = RandMasking,\n        mim_start_epochs: float = 0,\n    ):\n        self.pred_ratio = pred_ratio\n        self.pred_ratio_var = pred_ratio_var\n        self.patch_size = patch_size\n        self.pred_shape = pred_shape\n        self.mim_start_epochs = int(math.floor(mim_start_epochs))\n        if self.mim_start_epochs != mim_start_epochs:\n            raise NotImplementedError(\n                \"The mim_start_epochs must be integer. Fractional not supported yet.\"\n            )\n        self.current_epoch = None\n\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))Constant))Assign(Name(Store)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad)))Assign(Name(Store)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad)))If(Compare(Name(Load)IsConstant)Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))))Assign(Name(Store)BinOp(BinOp(BinOp(Name(Load)MultName(Load))MultName(Load))MultName(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)Name(Load)Name(Load)))If(Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant))))Assign(Name(Store)Dict(ConstantConstantName(Load)Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load)))ClassDef(FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)NotEqName(Load))Raise(Call(Name(Load)Constant)))Assign(Attribute(Name(Load)Store)Constant))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    H = img_h // patch_size[1]\n    W = img_w // patch_size[2]\n    if precomputed_pred_ratio is None:\n        precomputed_pred_ratio = get_pred_ratio(\n            pred_ratio=pred_ratio, pred_ratio_var=pred_ratio_var\n        )\n    # high is the max number of tokens to mask in the input\n    high = precomputed_pred_ratio * T * H * W\n    mask = pred_shape(T, H, W, high)\n\n    if squeeze_dim:\n        # Remove the time dim from the mask since the image doesn't have it\n        mask = np.squeeze(mask, axis=0)\n    ret_dict = {\n        \"data\": image,\n        \"mask\": torch.from_numpy(mask),\n    }\n    return ret_dict\n\n\nclass MaskImageModeling:\n    def __init__(\n        self,\n        pred_ratio: Tuple[float],\n        pred_ratio_var: Tuple[float],\n        patch_size: List[int],\n        pred_shape: Masking = RandMasking,\n        mim_start_epochs: float = 0,\n    ):\n        self.pred_ratio = pred_ratio\n        self.pred_ratio_var = pred_ratio_var\n        self.patch_size = patch_size\n        self.pred_shape = pred_shape\n        self.mim_start_epochs = int(math.floor(mim_start_epochs))\n        if self.mim_start_epochs != mim_start_epochs:\n            raise NotImplementedError(\n                \"The mim_start_epochs must be integer. Fractional not supported yet.\"\n            )\n        self.current_epoch = None\n\n    def set_current_epoch(self, epoch: int):\n        self.current_epoch = epoch\n\n    def __call__(self, image, do_not_mask=False):\n        precomputed_pred_ratio = None\n        if self.mim_start_epochs > 0 and self.mim_start_epochs > self.current_epoch:\n            precomputed_pred_ratio = 0\n        if do_not_mask is True:\n            precomputed_pred_ratio = 0\n        return ibot_style_mask_image(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 222, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    if squeeze_dim:\n        # Remove the time dim from the mask since the image doesn't have it\n        mask = np.squeeze(mask, axis=0)\n    ret_dict = {\n        \"data\": image,\n        \"mask\": torch.from_numpy(mask),\n    }\n    return ret_dict\n\n\nclass MaskImageModeling:\n    def __init__(\n        self,\n        pred_ratio: Tuple[float],\n        pred_ratio_var: Tuple[float],\n        patch_size: List[int],\n        pred_shape: Masking = RandMasking,\n        mim_start_epochs: float = 0,\n    ):\n        self.pred_ratio = pred_ratio\n        self.pred_ratio_var = pred_ratio_var\n        self.patch_size = patch_size\n        self.pred_shape = pred_shape\n        self.mim_start_epochs = int(math.floor(mim_start_epochs))\n        if self.mim_start_epochs != mim_start_epochs:\n            raise NotImplementedError(\n                \"The mim_start_epochs must be integer. Fractional not supported yet.\"\n            )\n        self.current_epoch = None\n\n    def set_current_epoch(self, epoch: int):\n        self.current_epoch = epoch\n\n    def __call__(self, image, do_not_mask=False):\n        precomputed_pred_ratio = None\n        if self.mim_start_epochs > 0 and self.mim_start_epochs > self.current_epoch:\n            precomputed_pred_ratio = 0\n        if do_not_mask is True:\n            precomputed_pred_ratio = 0\n        return ibot_style_mask_image(\n            image,\n            patch_size=self.patch_size,\n            pred_ratio=self.pred_ratio,\n            pred_ratio_var=self.pred_ratio_var,\n            pred_shape=self.pred_shape,\n            precomputed_pred_ratio=precomputed_pred_ratio,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_175-222"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 222, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class MaskImageModeling:\n    def __init__(\n        self,\n        pred_ratio: Tuple[float],\n        pred_ratio_var: Tuple[float],\n        patch_size: List[int],\n        pred_shape: Masking = RandMasking,\n        mim_start_epochs: float = 0,\n    ):\n        self.pred_ratio = pred_ratio\n        self.pred_ratio_var = pred_ratio_var\n        self.patch_size = patch_size\n        self.pred_shape = pred_shape\n        self.mim_start_epochs = int(math.floor(mim_start_epochs))\n        if self.mim_start_epochs != mim_start_epochs:\n            raise NotImplementedError(\n                \"The mim_start_epochs must be integer. Fractional not supported yet.\"\n            )\n        self.current_epoch = None\n\n    def set_current_epoch(self, epoch: int):\n        self.current_epoch = epoch\n\n    def __call__(self, image, do_not_mask=False):\n        precomputed_pred_ratio = None\n        if self.mim_start_epochs > 0 and self.mim_start_epochs > self.current_epoch:\n            precomputed_pred_ratio = 0\n        if do_not_mask is True:\n            precomputed_pred_ratio = 0\n        return ibot_style_mask_image(\n            image,\n            patch_size=self.patch_size,\n            pred_ratio=self.pred_ratio,\n            pred_ratio_var=self.pred_ratio_var,\n            pred_shape=self.pred_shape,\n            precomputed_pred_ratio=precomputed_pred_ratio,\n        )\n\nAST=Module(ClassDef(FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))))If(Compare(Attribute(Name(Load)Load)NotEqName(Load))Raise(Call(Name(Load)Constant)))Assign(Attribute(Name(Load)Store)Constant))FunctionDef(arguments(argarg(Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argargargConstant)Assign(Name(Store)Constant)If(BoolOp(AndCompare(Attribute(Name(Load)Load)GtConstant)Compare(Attribute(Name(Load)Load)GtAttribute(Name(Load)Load)))Assign(Name(Store)Constant))If(Compare(Name(Load)IsConstant)Assign(Name(Store)Constant))Return(Call(Name(Load)Name(Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))keyword(Name(Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_185-222"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "mask_image_modeling.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 222, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.pred_ratio_var = pred_ratio_var\n        self.patch_size = patch_size\n        self.pred_shape = pred_shape\n        self.mim_start_epochs = int(math.floor(mim_start_epochs))\n        if self.mim_start_epochs != mim_start_epochs:\n            raise NotImplementedError(\n                \"The mim_start_epochs must be integer. Fractional not supported yet.\"\n            )\n        self.current_epoch = None\n\n    def set_current_epoch(self, epoch: int):\n        self.current_epoch = epoch\n\n    def __call__(self, image, do_not_mask=False):\n        precomputed_pred_ratio = None\n        if self.mim_start_epochs > 0 and self.mim_start_epochs > self.current_epoch:\n            precomputed_pred_ratio = 0\n        if do_not_mask is True:\n            precomputed_pred_ratio = 0\n        return ibot_style_mask_image(\n            image,\n            patch_size=self.patch_size,\n            pred_ratio=self.pred_ratio,\n            pred_ratio_var=self.pred_ratio_var,\n            pred_shape=self.pred_shape,\n            precomputed_pred_ratio=precomputed_pred_ratio,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-mask_image_modeling.py_195-222"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from,\n# a) https://github.com/facebookresearch/SlowFast\n# b) https://github.com/facebookresearch/vissl\n\nimport math\n\nfrom typing import Sequence, Union\n\nimport numpy as np\nimport pytorchvideo\nimport torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision import transforms\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from,\n# a) https://github.com/facebookresearch/SlowFast\n# b) https://github.com/facebookresearch/vissl\n\nimport math\n\nfrom typing import Sequence, Union\n\nimport numpy as np\nimport pytorchvideo\nimport torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision import transforms\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n\nAST=Module(Import(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(argargarg)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from,\n# a) https://github.com/facebookresearch/SlowFast\n# b) https://github.com/facebookresearch/vissl\n\nimport math\n\nfrom typing import Sequence, Union\n\nimport numpy as np\nimport pytorchvideo\nimport torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision import transforms\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Code modified from,\n# a) https://github.com/facebookresearch/SlowFast\n# b) https://github.com/facebookresearch/vissl\n\nimport math\n\nfrom typing import Sequence, Union\n\nimport numpy as np\nimport pytorchvideo\nimport torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision import transforms\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import pytorchvideo\nimport torch\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision import transforms\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(argargarg)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Subscript(Name(Load)Tuple(SliceList(ConstantConstantLoad)Load)Store)BinOp(Subscript(Name(Load)Tuple(SliceList(ConstantConstantLoad)Load)Load)SubName(Load)))Assign(Subscript(Name(Load)Tuple(SliceList(ConstantConstantLoad)Load)Store)BinOp(Subscript(Name(Load)Tuple(SliceList(ConstantConstantLoad)Load)Load)SubName(Load)))Return(Name(Load)))FunctionDef(arguments(argargargargargConstantConstant)Expr(Constant)Assert(Compare(Name(Load)InList(ConstantConstantConstantLoad)))Assign(Name(Store)Call(Name(Load)Attribute(Name(Load)Load)))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,\n            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,\n            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,\n            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes\n\n\nclass UniformCrop(nn.Module):\n    \"\"\"\n    Wrapper around pytorchvideo.transforms.functional.uniform_crop, without\n        needing the dictionary interface required in\n        pytorchvideo.transforms.UniformCropVideo", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,\n            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes\n\n\nclass UniformCrop(nn.Module):\n    \"\"\"\n    Wrapper around pytorchvideo.transforms.functional.uniform_crop, without\n        needing the dictionary interface required in\n        pytorchvideo.transforms.UniformCropVideo\n    \"\"\"\n\n    def __init__(self, size: int = 256, spatial_idx: Union[int, Sequence[int]] = 1):\n        super().__init__()\n        self.size = size\n        # Convert to list to deal with multiple crops\n        if isinstance(spatial_idx, int):\n            self.spatial_idx = [spatial_idx]\n        else:\n            self.spatial_idx = spatial_idx", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes\n\n\nclass UniformCrop(nn.Module):\n    \"\"\"\n    Wrapper around pytorchvideo.transforms.functional.uniform_crop, without\n        needing the dictionary interface required in\n        pytorchvideo.transforms.UniformCropVideo\n    \"\"\"\n\n    def __init__(self, size: int = 256, spatial_idx: Union[int, Sequence[int]] = 1):\n        super().__init__()\n        self.size = size\n        # Convert to list to deal with multiple crops\n        if isinstance(spatial_idx, int):\n            self.spatial_idx = [spatial_idx]\n        else:\n            self.spatial_idx = spatial_idx\n\n    def forward(self, video):\n        res = []\n        for i in self.spatial_idx:\n            res.append(\n                pytorchvideo.transforms.functional.uniform_crop(video, self.size, i)\n            )\n        if len(res) == 1:  # Don't return as list if only 1 spatial index\n            res = res[0]\n        return res", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes\n\n\nclass UniformCrop(nn.Module):\n    \"\"\"\n    Wrapper around pytorchvideo.transforms.functional.uniform_crop, without\n        needing the dictionary interface required in\n        pytorchvideo.transforms.UniformCropVideo\n    \"\"\"\n\n    def __init__(self, size: int = 256, spatial_idx: Union[int, Sequence[int]] = 1):\n        super().__init__()\n        self.size = size\n        # Convert to list to deal with multiple crops\n        if isinstance(spatial_idx, int):\n            self.spatial_idx = [spatial_idx]\n        else:\n            self.spatial_idx = spatial_idx\n\n    def forward(self, video):\n        res = []\n        for i in self.spatial_idx:\n            res.append(\n                pytorchvideo.transforms.functional.uniform_crop(video, self.size, i)\n            )\n        if len(res) == 1:  # Don't return as list if only 1 spatial index\n            res = res[0]\n        return res\n\n\nclass VideoToListOfFrames(nn.Module):\n    def forward(self, video):\n        \"\"\"\n        Converts a video tensor to image list by moving the time dimension to\n            batch dimension. It will work with the average pooled accuracy\n            meters.\n        Args:\n            video (C, T, H, W)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes\n\n\nclass UniformCrop(nn.Module):\n    \"\"\"\n    Wrapper around pytorchvideo.transforms.functional.uniform_crop, without\n        needing the dictionary interface required in\n        pytorchvideo.transforms.UniformCropVideo\n    \"\"\"\n\n    def __init__(self, size: int = 256, spatial_idx: Union[int, Sequence[int]] = 1):\n        super().__init__()\n        self.size = size\n        # Convert to list to deal with multiple crops\n        if isinstance(spatial_idx, int):\n            self.spatial_idx = [spatial_idx]\n        else:\n            self.spatial_idx = spatial_idx\n\n    def forward(self, video):\n        res = []\n        for i in self.spatial_idx:\n            res.append(\n                pytorchvideo.transforms.functional.uniform_crop(video, self.size, i)\n            )\n        if len(res) == 1:  # Don't return as list if only 1 spatial index\n            res = res[0]\n        return res\n\n\nclass VideoToListOfFrames(nn.Module):\n    def forward(self, video):\n        \"\"\"\n        Converts a video tensor to image list by moving the time dimension to\n            batch dimension. It will work with the average pooled accuracy\n            meters.\n        Args:\n            video (C, T, H, W)\n        Returns:\n            list of [(C, H, W)] * T\n        \"\"\"\n        assert video.ndim == 4\n        return [el.squeeze(1) for el in torch.split(video, 1, dim=1)]\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n\n    def __init__(self, size: int = 256, spatial_idx: Union[int, Sequence[int]] = 1):\n        super().__init__()\n        self.size = size\n        # Convert to list to deal with multiple crops\n        if isinstance(spatial_idx, int):\n            self.spatial_idx = [spatial_idx]\n        else:\n            self.spatial_idx = spatial_idx\n\n    def forward(self, video):\n        res = []\n        for i in self.spatial_idx:\n            res.append(\n                pytorchvideo.transforms.functional.uniform_crop(video, self.size, i)\n            )\n        if len(res) == 1:  # Don't return as list if only 1 spatial index\n            res = res[0]\n        return res\n\n\nclass VideoToListOfFrames(nn.Module):\n    def forward(self, video):\n        \"\"\"\n        Converts a video tensor to image list by moving the time dimension to\n            batch dimension. It will work with the average pooled accuracy\n            meters.\n        Args:\n            video (C, T, H, W)\n        Returns:\n            list of [(C, H, W)] * T\n        \"\"\"\n        assert video.ndim == 4\n        return [el.squeeze(1) for el in torch.split(video, 1, dim=1)]\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(\n        self, frames_per_clip: int = 8, stride: int = 8, frame_stride: int = 1\n    ):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n        self.frame_stride = frame_stride\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def forward(self, video):\n        res = []\n        for i in self.spatial_idx:\n            res.append(\n                pytorchvideo.transforms.functional.uniform_crop(video, self.size, i)\n            )\n        if len(res) == 1:  # Don't return as list if only 1 spatial index\n            res = res[0]\n        return res\n\n\nclass VideoToListOfFrames(nn.Module):\n    def forward(self, video):\n        \"\"\"\n        Converts a video tensor to image list by moving the time dimension to\n            batch dimension. It will work with the average pooled accuracy\n            meters.\n        Args:\n            video (C, T, H, W)\n        Returns:\n            list of [(C, H, W)] * T\n        \"\"\"\n        assert video.ndim == 4\n        return [el.squeeze(1) for el in torch.split(video, 1, dim=1)]\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(\n        self, frames_per_clip: int = 8, stride: int = 8, frame_stride: int = 1\n    ):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n        self.frame_stride = frame_stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(\n            0, video.size(1) - (self.frames * self.frame_stride) + 1, self.stride\n        ):\n            end = start + (self.frames) * self.frame_stride\n            res.append(video[:, start : end : self.frame_stride, ...])\n        return res\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass VideoToListOfFrames(nn.Module):\n    def forward(self, video):\n        \"\"\"\n        Converts a video tensor to image list by moving the time dimension to\n            batch dimension. It will work with the average pooled accuracy\n            meters.\n        Args:\n            video (C, T, H, W)\n        Returns:\n            list of [(C, H, W)] * T\n        \"\"\"\n        assert video.ndim == 4\n        return [el.squeeze(1) for el in torch.split(video, 1, dim=1)]\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(\n        self, frames_per_clip: int = 8, stride: int = 8, frame_stride: int = 1\n    ):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n        self.frame_stride = frame_stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(\n            0, video.size(1) - (self.frames * self.frame_stride) + 1, self.stride\n        ):\n            end = start + (self.frames) * self.frame_stride\n            res.append(video[:, start : end : self.frame_stride, ...])\n        return res\n\n\nclass TemporalCrop2(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally. This version is inspired\n        from Swin Transformer Video. It crops central clips as opposed to\n        starting from the first frame. Also, takes as input the desired\n        number of clips instead of the\n        https://github.com/SwinTransformer/Video-Swin-Transformer/blob/db018fb8896251711791386bbd2127562fd8d6a6/mmaction/datasets/pipelines/loading.py#L164\n    \"\"\"\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Constant)Assert(Compare(Attribute(Name(Load)Load)EqConstant))Return(ListComp(Call(Attribute(Name(Load)Load)Constant)comprehension(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Constantkeyword(Constant)))))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assert(Compare(Attribute(Name(Load)Load)EqConstant)Constant)Assign(Name(Store)List(Load))For(Name(Store)Call(Name(Load)ConstantBinOp(BinOp(Call(Attribute(Name(Load)Load)Constant)SubBinOp(Attribute(Name(Load)Load)MultAttribute(Name(Load)Load)))AddConstant)Attribute(Name(Load)Load))Assign(Name(Store)BinOp(Name(Load)AddBinOp(Attribute(Name(Load)Load)MultAttribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(Name(Load)Name(Load)Attribute(Name(Load)Load))ConstantLoad)Load))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        Returns:\n            list of [(C, H, W)] * T\n        \"\"\"\n        assert video.ndim == 4\n        return [el.squeeze(1) for el in torch.split(video, 1, dim=1)]\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(\n        self, frames_per_clip: int = 8, stride: int = 8, frame_stride: int = 1\n    ):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n        self.frame_stride = frame_stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(\n            0, video.size(1) - (self.frames * self.frame_stride) + 1, self.stride\n        ):\n            end = start + (self.frames) * self.frame_stride\n            res.append(video[:, start : end : self.frame_stride, ...])\n        return res\n\n\nclass TemporalCrop2(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally. This version is inspired\n        from Swin Transformer Video. It crops central clips as opposed to\n        starting from the first frame. Also, takes as input the desired\n        number of clips instead of the\n        https://github.com/SwinTransformer/Video-Swin-Transformer/blob/db018fb8896251711791386bbd2127562fd8d6a6/mmaction/datasets/pipelines/loading.py#L164\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, num_clips: int = 5):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.num_clips = num_clips\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        num_frames = video.size(1)\n        avg_interval = (num_frames - self.frames) / self.num_clips\n        base_offsets = np.arange(self.num_clips) * avg_interval", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n\n    def __init__(\n        self, frames_per_clip: int = 8, stride: int = 8, frame_stride: int = 1\n    ):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n        self.frame_stride = frame_stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(\n            0, video.size(1) - (self.frames * self.frame_stride) + 1, self.stride\n        ):\n            end = start + (self.frames) * self.frame_stride\n            res.append(video[:, start : end : self.frame_stride, ...])\n        return res\n\n\nclass TemporalCrop2(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally. This version is inspired\n        from Swin Transformer Video. It crops central clips as opposed to\n        starting from the first frame. Also, takes as input the desired\n        number of clips instead of the\n        https://github.com/SwinTransformer/Video-Swin-Transformer/blob/db018fb8896251711791386bbd2127562fd8d6a6/mmaction/datasets/pipelines/loading.py#L164\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, num_clips: int = 5):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.num_clips = num_clips\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        num_frames = video.size(1)\n        avg_interval = (num_frames - self.frames) / self.num_clips\n        base_offsets = np.arange(self.num_clips) * avg_interval\n        clip_offsets = (base_offsets + avg_interval / 2.0).astype(np.int)\n        res = []\n        for start in clip_offsets:\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(\n            0, video.size(1) - (self.frames * self.frame_stride) + 1, self.stride\n        ):\n            end = start + (self.frames) * self.frame_stride\n            res.append(video[:, start : end : self.frame_stride, ...])\n        return res\n\n\nclass TemporalCrop2(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally. This version is inspired\n        from Swin Transformer Video. It crops central clips as opposed to\n        starting from the first frame. Also, takes as input the desired\n        number of clips instead of the\n        https://github.com/SwinTransformer/Video-Swin-Transformer/blob/db018fb8896251711791386bbd2127562fd8d6a6/mmaction/datasets/pipelines/loading.py#L164\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, num_clips: int = 5):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.num_clips = num_clips\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        num_frames = video.size(1)\n        avg_interval = (num_frames - self.frames) / self.num_clips\n        base_offsets = np.arange(self.num_clips) * avg_interval\n        clip_offsets = (base_offsets + avg_interval / 2.0).astype(np.int)\n        res = []\n        for start in clip_offsets:\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass TemporalCrop2(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally. This version is inspired\n        from Swin Transformer Video. It crops central clips as opposed to\n        starting from the first frame. Also, takes as input the desired\n        number of clips instead of the\n        https://github.com/SwinTransformer/Video-Swin-Transformer/blob/db018fb8896251711791386bbd2127562fd8d6a6/mmaction/datasets/pipelines/loading.py#L164\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, num_clips: int = 5):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.num_clips = num_clips\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        num_frames = video.size(1)\n        avg_interval = (num_frames - self.frames) / self.num_clips\n        base_offsets = np.arange(self.num_clips) * avg_interval\n        clip_offsets = (base_offsets + avg_interval / 2.0).astype(np.int)\n        res = []\n        for start in clip_offsets:\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, frames_per_clip: int = 8, num_clips: int = 5):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.num_clips = num_clips\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        num_frames = video.size(1)\n        avg_interval = (num_frames - self.frames) / self.num_clips\n        base_offsets = np.arange(self.num_clips) * avg_interval\n        clip_offsets = (base_offsets + avg_interval / 2.0).astype(np.int)\n        res = []\n        for start in clip_offsets:\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        clip_offsets = (base_offsets + avg_interval / 2.0).astype(np.int)\n        res = []\n        for start in clip_offsets:\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n            if not self.flipped_crops_to_ext:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n            if not self.flipped_crops_to_ext:\n                continue\n            flipped_video = transforms.functional.hflip(video)\n            for spatial_idx in self.flipped_crops_to_ext:\n                res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\n\nclass TemporalSegmentSampler(nn.Module):\n    \"\"\"\n    Samples frames from video by dividing into segments and sampling a frame", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if num_crops == 6:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = [0, 1, 2]\n        elif num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n            self.flipped_crops_to_ext = []\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n            self.flipped_crops_to_ext = []\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n            if not self.flipped_crops_to_ext:\n                continue\n            flipped_video = transforms.functional.hflip(video)\n            for spatial_idx in self.flipped_crops_to_ext:\n                res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\n\nclass TemporalSegmentSampler(nn.Module):\n    \"\"\"\n    Samples frames from video by dividing into segments and sampling a frame\n    from each segment similar to TSN (ECCV'16).\n    Also useful for SS-v2 training/testing. Modified from\n    https://github.com/facebookresearch/SlowFast/blob/64abcc90ccfdcbb11cf91d6e525bed60e92a8796/slowfast/datasets/ssv2.py#L159\n    \"\"\"\n\n    def __init__(self, num_samples: int, train_mode: bool = False):\n        super().__init__()\n        self.num_segments = num_samples\n        self.train_mode = train_mode\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n            if not self.flipped_crops_to_ext:\n                continue\n            flipped_video = transforms.functional.hflip(video)\n            for spatial_idx in self.flipped_crops_to_ext:\n                res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\n\nclass TemporalSegmentSampler(nn.Module):\n    \"\"\"\n    Samples frames from video by dividing into segments and sampling a frame\n    from each segment similar to TSN (ECCV'16).\n    Also useful for SS-v2 training/testing. Modified from\n    https://github.com/facebookresearch/SlowFast/blob/64abcc90ccfdcbb11cf91d6e525bed60e92a8796/slowfast/datasets/ssv2.py#L159\n    \"\"\"\n\n    def __init__(self, num_samples: int, train_mode: bool = False):\n        super().__init__()\n        self.num_segments = num_samples\n        self.train_mode = train_mode\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Video tensor with shape (C, T, H, W)\n        \"\"\"\n        num_frames = x.size(1)\n        seg_size = float(num_frames - 1) / self.num_segments\n        seq = []\n        for i in range(self.num_segments):\n            start = int(np.round(seg_size * i))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n            if not self.flipped_crops_to_ext:\n                continue\n            flipped_video = transforms.functional.hflip(video)\n            for spatial_idx in self.flipped_crops_to_ext:\n                res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\n\nclass TemporalSegmentSampler(nn.Module):\n    \"\"\"\n    Samples frames from video by dividing into segments and sampling a frame\n    from each segment similar to TSN (ECCV'16).\n    Also useful for SS-v2 training/testing. Modified from\n    https://github.com/facebookresearch/SlowFast/blob/64abcc90ccfdcbb11cf91d6e525bed60e92a8796/slowfast/datasets/ssv2.py#L159\n    \"\"\"\n\n    def __init__(self, num_samples: int, train_mode: bool = False):\n        super().__init__()\n        self.num_segments = num_samples\n        self.train_mode = train_mode\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Video tensor with shape (C, T, H, W)\n        \"\"\"\n        num_frames = x.size(1)\n        seg_size = float(num_frames - 1) / self.num_segments\n        seq = []\n        for i in range(self.num_segments):\n            start = int(np.round(seg_size * i))\n            end = int(np.round(seg_size * (i + 1)))\n            if self.train_mode:\n                seq.append(np.random.randint(low=start, high=(end + 1)))\n            else:\n                seq.append((start + end) // 2)\n        return x[:, seq, :, :]\n\n\nclass Permute(nn.Module):\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                continue\n            flipped_video = transforms.functional.hflip(video)\n            for spatial_idx in self.flipped_crops_to_ext:\n                res.append(uniform_crop(flipped_video, self.crop_size, spatial_idx)[0])\n        return res\n\n\nclass TemporalSegmentSampler(nn.Module):\n    \"\"\"\n    Samples frames from video by dividing into segments and sampling a frame\n    from each segment similar to TSN (ECCV'16).\n    Also useful for SS-v2 training/testing. Modified from\n    https://github.com/facebookresearch/SlowFast/blob/64abcc90ccfdcbb11cf91d6e525bed60e92a8796/slowfast/datasets/ssv2.py#L159\n    \"\"\"\n\n    def __init__(self, num_samples: int, train_mode: bool = False):\n        super().__init__()\n        self.num_segments = num_samples\n        self.train_mode = train_mode\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Video tensor with shape (C, T, H, W)\n        \"\"\"\n        num_frames = x.size(1)\n        seg_size = float(num_frames - 1) / self.num_segments\n        seq = []\n        for i in range(self.num_segments):\n            start = int(np.round(seg_size * i))\n            end = int(np.round(seg_size * (i + 1)))\n            if self.train_mode:\n                seq.append(np.random.randint(low=start, high=(end + 1)))\n            else:\n                seq.append((start + end) // 2)\n        return x[:, seq, :, :]\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = ordering\n\n    def forward(self, frames):\n        \"\"\"\n        Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    from each segment similar to TSN (ECCV'16).\n    Also useful for SS-v2 training/testing. Modified from\n    https://github.com/facebookresearch/SlowFast/blob/64abcc90ccfdcbb11cf91d6e525bed60e92a8796/slowfast/datasets/ssv2.py#L159\n    \"\"\"\n\n    def __init__(self, num_samples: int, train_mode: bool = False):\n        super().__init__()\n        self.num_segments = num_samples\n        self.train_mode = train_mode\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Video tensor with shape (C, T, H, W)\n        \"\"\"\n        num_frames = x.size(1)\n        seg_size = float(num_frames - 1) / self.num_segments\n        seq = []\n        for i in range(self.num_segments):\n            start = int(np.round(seg_size * i))\n            end = int(np.round(seg_size * (i + 1)))\n            if self.train_mode:\n                seq.append(np.random.randint(low=start, high=(end + 1)))\n            else:\n                seq.append((start + end) // 2)\n        return x[:, seq, :, :]\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = ordering\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass Replicate(nn.Module):\n    \"\"\"\n    Replicate a tensor N times and return a list of N copies", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Video tensor with shape (C, T, H, W)\n        \"\"\"\n        num_frames = x.size(1)\n        seg_size = float(num_frames - 1) / self.num_segments\n        seq = []\n        for i in range(self.num_segments):\n            start = int(np.round(seg_size * i))\n            end = int(np.round(seg_size * (i + 1)))\n            if self.train_mode:\n                seq.append(np.random.randint(low=start, high=(end + 1)))\n            else:\n                seq.append((start + end) // 2)\n        return x[:, seq, :, :]\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = ordering\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass Replicate(nn.Module):\n    \"\"\"\n    Replicate a tensor N times and return a list of N copies\n    N copies do *not* share storage\n    \"\"\"\n\n    def __init__(self, num_times):\n        super().__init__()\n        self.num_times = num_times\n\n    def forward(self, x):\n        assert isinstance(x, torch.Tensor)\n        ret_list = [x.clone() for _ in range(self.num_times)]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            end = int(np.round(seg_size * (i + 1)))\n            if self.train_mode:\n                seq.append(np.random.randint(low=start, high=(end + 1)))\n            else:\n                seq.append((start + end) // 2)\n        return x[:, seq, :, :]\n\n\nclass Permute(nn.Module):\n    \"\"\"\n    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = ordering\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass Replicate(nn.Module):\n    \"\"\"\n    Replicate a tensor N times and return a list of N copies\n    N copies do *not* share storage\n    \"\"\"\n\n    def __init__(self, num_times):\n        super().__init__()\n        self.num_times = num_times\n\n    def forward(self, x):\n        assert isinstance(x, torch.Tensor)\n        ret_list = [x.clone() for _ in range(self.num_times)]\n        return ret_list\n\n    def extra_repr(self):\n        return f\"num_times={self.num_times}\"\n\n\nclass ShuffleList(nn.Module):\n    \"\"\"\n    Shuffle a list of objects randomly.\n    Useful when shuffling temporal crops.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Permutation as an op\n    \"\"\"\n\n    def __init__(self, ordering):\n        super().__init__()\n        self.ordering = ordering\n\n    def forward(self, frames):\n        \"\"\"\n        Args:\n            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass Replicate(nn.Module):\n    \"\"\"\n    Replicate a tensor N times and return a list of N copies\n    N copies do *not* share storage\n    \"\"\"\n\n    def __init__(self, num_times):\n        super().__init__()\n        self.num_times = num_times\n\n    def forward(self, x):\n        assert isinstance(x, torch.Tensor)\n        ret_list = [x.clone() for _ in range(self.num_times)]\n        return ret_list\n\n    def extra_repr(self):\n        return f\"num_times={self.num_times}\"\n\n\nclass ShuffleList(nn.Module):\n    \"\"\"\n    Shuffle a list of objects randomly.\n    Useful when shuffling temporal crops.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        assert isinstance(x, list), f\"Expected list input. Found {type(x)}\"\n        inds = torch.randperm(len(x)).tolist()\n        new_x = [x[ind] for ind in inds]\n        return new_x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            frames in some ordering, by default (C, T, H, W)\n        Returns:\n            frames in the ordering that was specified\n        \"\"\"\n        return frames.permute(self.ordering)\n\n\nclass Replicate(nn.Module):\n    \"\"\"\n    Replicate a tensor N times and return a list of N copies\n    N copies do *not* share storage\n    \"\"\"\n\n    def __init__(self, num_times):\n        super().__init__()\n        self.num_times = num_times\n\n    def forward(self, x):\n        assert isinstance(x, torch.Tensor)\n        ret_list = [x.clone() for _ in range(self.num_times)]\n        return ret_list\n\n    def extra_repr(self):\n        return f\"num_times={self.num_times}\"\n\n\nclass ShuffleList(nn.Module):\n    \"\"\"\n    Shuffle a list of objects randomly.\n    Useful when shuffling temporal crops.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        assert isinstance(x, list), f\"Expected list input. Found {type(x)}\"\n        inds = torch.randperm(len(x)).tolist()\n        new_x = [x[ind] for ind in inds]\n        return new_x\n\n\nclass SingletonVideoToImgPil(nn.Module):\n    \"\"\"\n    Convert a 1 frame video to PIL image for image transforms.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(1) == 1, x.shape\n        x = x[:, 0, ...]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 338, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    N copies do *not* share storage\n    \"\"\"\n\n    def __init__(self, num_times):\n        super().__init__()\n        self.num_times = num_times\n\n    def forward(self, x):\n        assert isinstance(x, torch.Tensor)\n        ret_list = [x.clone() for _ in range(self.num_times)]\n        return ret_list\n\n    def extra_repr(self):\n        return f\"num_times={self.num_times}\"\n\n\nclass ShuffleList(nn.Module):\n    \"\"\"\n    Shuffle a list of objects randomly.\n    Useful when shuffling temporal crops.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        assert isinstance(x, list), f\"Expected list input. Found {type(x)}\"\n        inds = torch.randperm(len(x)).tolist()\n        new_x = [x[ind] for ind in inds]\n        return new_x\n\n\nclass SingletonVideoToImgPil(nn.Module):\n    \"\"\"\n    Convert a 1 frame video to PIL image for image transforms.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(1) == 1, x.shape\n        x = x[:, 0, ...]\n        if x.dtype == torch.float:\n            x = (x * 255.0).to(torch.uint8)\n        return Image.fromarray(x.cpu().permute(1, 2, 0).numpy())", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_295-338"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "pytorchvideo.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 338, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return ret_list\n\n    def extra_repr(self):\n        return f\"num_times={self.num_times}\"\n\n\nclass ShuffleList(nn.Module):\n    \"\"\"\n    Shuffle a list of objects randomly.\n    Useful when shuffling temporal crops.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        assert isinstance(x, list), f\"Expected list input. Found {type(x)}\"\n        inds = torch.randperm(len(x)).tolist()\n        new_x = [x[ind] for ind in inds]\n        return new_x\n\n\nclass SingletonVideoToImgPil(nn.Module):\n    \"\"\"\n    Convert a 1 frame video to PIL image for image transforms.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(1) == 1, x.shape\n        x = x[:, 0, ...]\n        if x.dtype == torch.float:\n            x = (x * 255.0).to(torch.uint8)\n        return Image.fromarray(x.cpu().permute(1, 2, 0).numpy())", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-pytorchvideo.py_305-338"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py,\npulished under an Apache License 2.0, with modifications by Matthew Leavitt (\nito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n-Removed AugMix functionality.\n-Replaced AutoAugment and RandAugment classes, which are no longer passed a\nsingle parameter string that needs to be parsed, but instead individual,\nnamed parameters.\n\nCOMMENT FROM ORIGINAL:\nAutoAugment, RandAugment, and AugMix for PyTorch\nThis code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA\nImplementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py,\npulished under an Apache License 2.0, with modifications by Matthew Leavitt (\nito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n-Removed AugMix functionality.\n-Replaced AutoAugment and RandAugment classes, which are no longer passed a\nsingle parameter string that needs to be parsed, but instead individual,\nnamed parameters.\n\nCOMMENT FROM ORIGINAL:\nAutoAugment, RandAugment, and AugMix for PyTorch\nThis code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA\nImplementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\nAugMix adapted from:\n    https://github.com/google-research/augmix\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data\n    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection\n    https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation...\n    https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py,\npulished under an Apache License 2.0, with modifications by Matthew Leavitt (\nito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n-Removed AugMix functionality.\n-Replaced AutoAugment and RandAugment classes, which are no longer passed a\nsingle parameter string that needs to be parsed, but instead individual,\nnamed parameters.\n\nCOMMENT FROM ORIGINAL:\nAutoAugment, RandAugment, and AugMix for PyTorch\nThis code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA\nImplementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\nAugMix adapted from:\n    https://github.com/google-research/augmix\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data\n    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection\n    https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation...\n    https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and\n    Uncertainty https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\nimport re\n\nimport numpy as np\nimport PIL\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py,\npulished under an Apache License 2.0, with modifications by Matthew Leavitt (\nito@fb.com; matthew.l.leavitt@gmail.com). Modifications are described here and\nnotated where present in the code.\n\nModifications:\n-Removed AugMix functionality.\n-Replaced AutoAugment and RandAugment classes, which are no longer passed a\nsingle parameter string that needs to be parsed, but instead individual,\nnamed parameters.\n\nCOMMENT FROM ORIGINAL:\nAutoAugment, RandAugment, and AugMix for PyTorch\nThis code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA\nImplementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\nAugMix adapted from:\n    https://github.com/google-research/augmix\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data\n    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection\n    https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation...\n    https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and\n    Uncertainty https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\nimport re\n\nimport numpy as np\nimport PIL\nfrom PIL import Image, ImageEnhance, ImageOps\n\n\n_PIL_VER = tuple(int(x) for x in PIL.__version__.split(\".\")[:2])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\"translate_const\": 250, \"img_mean\": _FILL}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\nAST=Module(Expr(Constant)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasalias)Assign(Name(Store)Call(Name(Load)GeneratorExp(Call(Name(Load)Name(Load))comprehension(Name(Store)Subscript(Call(Attribute(Attribute(Name(Load)Load)Load)Constant)Slice(Constant)Load)))))Assign(Name(Store)Tuple(ConstantConstantConstantLoad))Assign(Name(Store)Constant)Assign(Name(Store)Dict(ConstantConstantConstantName(Load)))Assign(Name(Store)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "-Replaced AutoAugment and RandAugment classes, which are no longer passed a\nsingle parameter string that needs to be parsed, but instead individual,\nnamed parameters.\n\nCOMMENT FROM ORIGINAL:\nAutoAugment, RandAugment, and AugMix for PyTorch\nThis code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA\nImplementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\nAugMix adapted from:\n    https://github.com/google-research/augmix\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data\n    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection\n    https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation...\n    https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and\n    Uncertainty https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\nimport re\n\nimport numpy as np\nimport PIL\nfrom PIL import Image, ImageEnhance, ImageOps\n\n\n_PIL_VER = tuple(int(x) for x in PIL.__version__.split(\".\")[:2])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\"translate_const\": 250, \"img_mean\": _FILL}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\nclass RandAugment:\n    \"\"\"\n    Create a RandAugment transform.\n    :param magnitude: integer magnitude of rand augment\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :param num_layers: integer number of transforms\n    :param increasing_severity: boolean that indicates whether to use", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "AugMix adapted from:\n    https://github.com/google-research/augmix\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data\n    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection\n    https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation...\n    https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and\n    Uncertainty https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\nimport re\n\nimport numpy as np\nimport PIL\nfrom PIL import Image, ImageEnhance, ImageOps\n\n\n_PIL_VER = tuple(int(x) for x in PIL.__version__.split(\".\")[:2])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\"translate_const\": 250, \"img_mean\": _FILL}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\nclass RandAugment:\n    \"\"\"\n    Create a RandAugment transform.\n    :param magnitude: integer magnitude of rand augment\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :param num_layers: integer number of transforms\n    :param increasing_severity: boolean that indicates whether to use\n    augmentations that increase severity w/ increasing magnitude. Some\n    augmentations do this by default.\n    :param weight_choice: Index of pre-determined probability distribution\n    over augmentations. Currently only one such distribution available (i.e.\n    no valid values other than 0 or None), unclear if beneficial. Default =\n    None.\n    \"\"\"\n\n    def __init__(\n        self,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Uncertainty https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\nimport re\n\nimport numpy as np\nimport PIL\nfrom PIL import Image, ImageEnhance, ImageOps\n\n\n_PIL_VER = tuple(int(x) for x in PIL.__version__.split(\".\")[:2])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\"translate_const\": 250, \"img_mean\": _FILL}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\nclass RandAugment:\n    \"\"\"\n    Create a RandAugment transform.\n    :param magnitude: integer magnitude of rand augment\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :param num_layers: integer number of transforms\n    :param increasing_severity: boolean that indicates whether to use\n    augmentations that increase severity w/ increasing magnitude. Some\n    augmentations do this by default.\n    :param weight_choice: Index of pre-determined probability distribution\n    over augmentations. Currently only one such distribution available (i.e.\n    no valid values other than 0 or None), unclear if beneficial. Default =\n    None.\n    \"\"\"\n\n    def __init__(\n        self,\n        magnitude=10,\n        magnitude_std=0,\n        num_layers=2,\n        increasing_severity=False,\n        weight_choice=None,\n        **kwargs,\n    ):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from PIL import Image, ImageEnhance, ImageOps\n\n\n_PIL_VER = tuple(int(x) for x in PIL.__version__.split(\".\")[:2])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\"translate_const\": 250, \"img_mean\": _FILL}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\nclass RandAugment:\n    \"\"\"\n    Create a RandAugment transform.\n    :param magnitude: integer magnitude of rand augment\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :param num_layers: integer number of transforms\n    :param increasing_severity: boolean that indicates whether to use\n    augmentations that increase severity w/ increasing magnitude. Some\n    augmentations do this by default.\n    :param weight_choice: Index of pre-determined probability distribution\n    over augmentations. Currently only one such distribution available (i.e.\n    no valid values other than 0 or None), unclear if beneficial. Default =\n    None.\n    \"\"\"\n\n    def __init__(\n        self,\n        magnitude=10,\n        magnitude_std=0,\n        num_layers=2,\n        increasing_severity=False,\n        weight_choice=None,\n        **kwargs,\n    ):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        if increasing_severity:\n            transforms = _RAND_INCREASING_TRANSFORMS\n        else:\n            transforms = _RAND_TRANSFORMS\n        self.num_layers = num_layers\n        self.choice_weights = (\n            None if weight_choice is None else _select_rand_weights(weight_choice)\n        )\n        self.ops = rand_augment_ops(\n            magnitude=magnitude, hparams=hparams, transforms=transforms", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass RandAugment:\n    \"\"\"\n    Create a RandAugment transform.\n    :param magnitude: integer magnitude of rand augment\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :param num_layers: integer number of transforms\n    :param increasing_severity: boolean that indicates whether to use\n    augmentations that increase severity w/ increasing magnitude. Some\n    augmentations do this by default.\n    :param weight_choice: Index of pre-determined probability distribution\n    over augmentations. Currently only one such distribution available (i.e.\n    no valid values other than 0 or None), unclear if beneficial. Default =\n    None.\n    \"\"\"\n\n    def __init__(\n        self,\n        magnitude=10,\n        magnitude_std=0,\n        num_layers=2,\n        increasing_severity=False,\n        weight_choice=None,\n        **kwargs,\n    ):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        if increasing_severity:\n            transforms = _RAND_INCREASING_TRANSFORMS\n        else:\n            transforms = _RAND_TRANSFORMS\n        self.num_layers = num_layers\n        self.choice_weights = (\n            None if weight_choice is None else _select_rand_weights(weight_choice)\n        )\n        self.ops = rand_augment_ops(\n            magnitude=magnitude, hparams=hparams, transforms=transforms\n        )\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n\nAST=Module(ClassDef(Expr(Constant)FunctionDef(arguments(argargargargargargargConstantConstantConstantConstantConstant)Assign(Name(Store)Name(Load))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Assign(Subscript(Name(Load)ConstantStore)Name(Load))If(Name(Load)Assign(Name(Store)Name(Load))Assign(Name(Store)Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)IfExp(Compare(Name(Load)IsConstant)ConstantCall(Name(Load)Name(Load))))Assign(Attribute(Name(Load)Store)Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)keyword(Compare(Attribute(Name(Load)Load)IsConstant))keyword(Attribute(Name(Load)Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    augmentations that increase severity w/ increasing magnitude. Some\n    augmentations do this by default.\n    :param weight_choice: Index of pre-determined probability distribution\n    over augmentations. Currently only one such distribution available (i.e.\n    no valid values other than 0 or None), unclear if beneficial. Default =\n    None.\n    \"\"\"\n\n    def __init__(\n        self,\n        magnitude=10,\n        magnitude_std=0,\n        num_layers=2,\n        increasing_severity=False,\n        weight_choice=None,\n        **kwargs,\n    ):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        if increasing_severity:\n            transforms = _RAND_INCREASING_TRANSFORMS\n        else:\n            transforms = _RAND_TRANSFORMS\n        self.num_layers = num_layers\n        self.choice_weights = (\n            None if weight_choice is None else _select_rand_weights(weight_choice)\n        )\n        self.ops = rand_augment_ops(\n            magnitude=magnitude, hparams=hparams, transforms=transforms\n        )\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n        for op in ops:\n            img = op(img)\n        return img\n\n\nclass AutoAugment:\n    \"\"\"\n    Create a AutoAugment transform. This autoaugment differs from the\n    torchvision implementation by allowing variability in the augmentation\n    intensity.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        magnitude=10,\n        magnitude_std=0,\n        num_layers=2,\n        increasing_severity=False,\n        weight_choice=None,\n        **kwargs,\n    ):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        if increasing_severity:\n            transforms = _RAND_INCREASING_TRANSFORMS\n        else:\n            transforms = _RAND_TRANSFORMS\n        self.num_layers = num_layers\n        self.choice_weights = (\n            None if weight_choice is None else _select_rand_weights(weight_choice)\n        )\n        self.ops = rand_augment_ops(\n            magnitude=magnitude, hparams=hparams, transforms=transforms\n        )\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n        for op in ops:\n            img = op(img)\n        return img\n\n\nclass AutoAugment:\n    \"\"\"\n    Create a AutoAugment transform. This autoaugment differs from the\n    torchvision implementation by allowing variability in the augmentation\n    intensity.\n    \":param policy_name: String. One of 'v0', 'v0r', 'original', 'originalr'.\n    One of a set of learned augmentation sequences.\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :kwargs: Other params for the AutoAugmentation scheme. See RandAugment\n    class above, or AugmentOp class in ClassyVision. Probability and\n    intensity are overwritten because they're determined by the learned\n    AutoAugment policy.\n    \"\"\"\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if increasing_severity:\n            transforms = _RAND_INCREASING_TRANSFORMS\n        else:\n            transforms = _RAND_TRANSFORMS\n        self.num_layers = num_layers\n        self.choice_weights = (\n            None if weight_choice is None else _select_rand_weights(weight_choice)\n        )\n        self.ops = rand_augment_ops(\n            magnitude=magnitude, hparams=hparams, transforms=transforms\n        )\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n        for op in ops:\n            img = op(img)\n        return img\n\n\nclass AutoAugment:\n    \"\"\"\n    Create a AutoAugment transform. This autoaugment differs from the\n    torchvision implementation by allowing variability in the augmentation\n    intensity.\n    \":param policy_name: String. One of 'v0', 'v0r', 'original', 'originalr'.\n    One of a set of learned augmentation sequences.\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :kwargs: Other params for the AutoAugmentation scheme. See RandAugment\n    class above, or AugmentOp class in ClassyVision. Probability and\n    intensity are overwritten because they're determined by the learned\n    AutoAugment policy.\n    \"\"\"\n\n    def __init__(self, policy_name=\"v0\", magnitude_std=0, **kwargs):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        self.policy = auto_augment_policy(policy_name, hparams=hparams)\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n        for op in ops:\n            img = op(img)\n        return img\n\n\nclass AutoAugment:\n    \"\"\"\n    Create a AutoAugment transform. This autoaugment differs from the\n    torchvision implementation by allowing variability in the augmentation\n    intensity.\n    \":param policy_name: String. One of 'v0', 'v0r', 'original', 'originalr'.\n    One of a set of learned augmentation sequences.\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :kwargs: Other params for the AutoAugmentation scheme. See RandAugment\n    class above, or AugmentOp class in ClassyVision. Probability and\n    intensity are overwritten because they're determined by the learned\n    AutoAugment policy.\n    \"\"\"\n\n    def __init__(self, policy_name=\"v0\", magnitude_std=0, **kwargs):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        self.policy = auto_augment_policy(policy_name, hparams=hparams)\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n\n# Everything from here down is copied directly from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for op in ops:\n            img = op(img)\n        return img\n\n\nclass AutoAugment:\n    \"\"\"\n    Create a AutoAugment transform. This autoaugment differs from the\n    torchvision implementation by allowing variability in the augmentation\n    intensity.\n    \":param policy_name: String. One of 'v0', 'v0r', 'original', 'originalr'.\n    One of a set of learned augmentation sequences.\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :kwargs: Other params for the AutoAugmentation scheme. See RandAugment\n    class above, or AugmentOp class in ClassyVision. Probability and\n    intensity are overwritten because they're determined by the learned\n    AutoAugment policy.\n    \"\"\"\n\n    def __init__(self, policy_name=\"v0\", magnitude_std=0, **kwargs):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        self.policy = auto_augment_policy(policy_name, hparams=hparams)\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n\n# Everything from here down is copied directly from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")\n    kwargs[\"resample\"] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \":param policy_name: String. One of 'v0', 'v0r', 'original', 'originalr'.\n    One of a set of learned augmentation sequences.\n    :param magnitude_std: standard deviation of magnitude. If > 0, introduces\n    random variability in the augmentation magnitude.\n    :kwargs: Other params for the AutoAugmentation scheme. See RandAugment\n    class above, or AugmentOp class in ClassyVision. Probability and\n    intensity are overwritten because they're determined by the learned\n    AutoAugment policy.\n    \"\"\"\n\n    def __init__(self, policy_name=\"v0\", magnitude_std=0, **kwargs):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        self.policy = auto_augment_policy(policy_name, hparams=hparams)\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n\n# Everything from here down is copied directly from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")\n    kwargs[\"resample\"] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, policy_name=\"v0\", magnitude_std=0, **kwargs):\n        hparams = kwargs\n        hparams.update(_HPARAMS_DEFAULT)\n        hparams[\"magnitude_std\"] = magnitude_std\n        self.policy = auto_augment_policy(policy_name, hparams=hparams)\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n\n# Everything from here down is copied directly from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")\n    kwargs[\"resample\"] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return img\n\n\n# Everything from here down is copied directly from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")\n    kwargs[\"resample\"] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")\n    kwargs[\"resample\"] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs[\"resample\"])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\nAST=Module(FunctionDef(arguments(argargarg)Expr(Call(Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(ConstantConstantName(Load)ConstantConstantConstantLoad)keyword(Name(Load)))))FunctionDef(arguments(argargarg)Expr(Call(Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)Tuple(ConstantConstantConstantConstantConstantName(Load)Load)keyword(Name(Load)))))FunctionDef(arguments(argargarg)Expr(Call(Name(Load)Name(Load)))If(Compare(Name(Load)GtETuple(ConstantConstantLoad))Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))If(Compare(Name(Load)GtETuple(ConstantConstantLoad))Assign(Tuple(Name(Store)Name(Store)Store)Attribute(Name(Load)Load))Assign(Name(Store)Tuple(ConstantConstantLoad))Assign(Name(Store)Tuple(BinOp(Name(Load)DivConstant)BinOp(Name(Load)DivConstant)Load))Assign(Name(Store)UnaryOp(USubCall(Attribute(Name(Load)Load)Name(Load))))Assign(Name(Store)List(Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))Constant)Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))Constant)ConstantCall(Name(Load)UnaryOp(USubCall(Attribute(Name(Load)Load)Name(Load)))Constant)Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load))Constant)ConstantLoad))FunctionDef(arguments(argargarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Store)Name(Load))Return(Tuple(BinOp(BinOp(BinOp(Name(Load)MultName(Load))AddBinOp(Name(Load)MultName(Load)))AddName(Load))BinOp(BinOp(BinOp(Name(Load)MultName(Load))AddBinOp(Name(Load)MultName(Load)))AddName(Load))Load)))Assign(Tuple(Subscript(Name(Load)ConstantStore)Subscript(Name(Load)ConstantStore)Store)Call(Name(Load)BinOp(UnaryOp(USubSubscript(Name(Load)ConstantLoad))SubSubscript(Name(Load)ConstantLoad))BinOp(UnaryOp(USubSubscript(Name(Load)ConstantLoad))SubSubscript(Name(Load)ConstantLoad))Name(Load)))AugAssign(Subscript(Name(Load)ConstantStore)AddSubscript(Name(Load)ConstantLoad))AugAssign(Subscript(Name(Load)ConstantStore)AddSubscript(Name(Load)ConstantLoad))Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(Subscript(Name(Load)ConstantLoad)))))))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs[\"resample\"])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs[\"resample\"])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs[\"resample\"])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return img.rotate(degrees, resample=kwargs[\"resample\"])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\nAST=Module(FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Name(Load)Load)Name(Load)Name(Load))))FunctionDef(arguments(argargargargConstant)Assign(Name(Store)List(Load))For(Name(Store)Call(Name(Load)Constant)If(Compare(Name(Load)LtName(Load))Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)ConstantBinOp(Name(Load)AddName(Load)))))Expr(Call(Attribute(Name(Load)Load)Name(Load)))))If(Compare(Attribute(Name(Load)Load)InTuple(ConstantConstantLoad))If(BoolOp(AndCompare(Attribute(Name(Load)Load)EqConstant)Compare(Call(Name(Load)Name(Load))EqConstant))Assign(Name(Store)BinOp(BinOp(Name(Load)AddName(Load))AddName(Load))))Return(Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load))))FunctionDef(arguments(argargarg)If(Compare(Name(Load)GtEConstant)Return(Name(Load)))Return(Call(Attribute(Name(Load)Load)Name(Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(arg)Expr(Constant)Return(IfExp(Compare(Call(Attribute(Name(Load)Load))GtConstant)UnaryOp(USubName(Load))Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0\n    # increases the enhancement blend range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * 0.9\n    level = 1.0 + _randomly_negate(level)\n    return (level,)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0\n    # increases the enhancement blend range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * 0.9\n    level = 1.0 + _randomly_negate(level)\n    return (level,)\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n\nAST=Module(FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(arg)Expr(Constant)Return(IfExp(Compare(Call(Attribute(Name(Load)Load))GtConstant)UnaryOp(USubName(Load))Name(Load))))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Return(Tuple(BinOp(BinOp(BinOp(Name(Load)DivName(Load))MultConstant)AddConstant)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)BinOp(ConstantAddCall(Name(Load)Name(Load))))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Subscript(Name(Load)ConstantLoad))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0\n    # increases the enhancement blend range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * 0.9\n    level = 1.0 + _randomly_negate(level)\n    return (level,)\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n\nAST=Module(FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Name(Load))))FunctionDef(arguments(arg)Expr(Constant)Return(IfExp(Compare(Call(Attribute(Name(Load)Load))GtConstant)UnaryOp(USubName(Load))Name(Load))))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Return(Tuple(BinOp(BinOp(BinOp(Name(Load)DivName(Load))MultConstant)AddConstant)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)BinOp(ConstantAddCall(Name(Load)Name(Load))))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultCall(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultName(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0\n    # increases the enhancement blend range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * 0.9\n    level = 1.0 + _randomly_negate(level)\n    return (level,)\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4),)\n\n\n\nAST=Module(FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Return(Tuple(BinOp(BinOp(BinOp(Name(Load)DivName(Load))MultConstant)AddConstant)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)BinOp(ConstantAddCall(Name(Load)Name(Load))))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultCall(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultName(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Return(Tuple(Call(Name(Load)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0\n    # increases the enhancement blend range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * 0.9\n    level = 1.0 + _randomly_negate(level)\n    return (level,)\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4),)\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return (4 - _posterize_level_to_arg(level, hparams)[0],)\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4),)\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return (4 - _posterize_level_to_arg(level, hparams)[0],)\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4) + 4,)\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256),)\n\n\n\nAST=Module(FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultCall(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)BinOp(BinOp(Name(Load)DivName(Load))MultName(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))Return(Tuple(Name(Load)Load)))FunctionDef(arguments(argarg)Return(Tuple(Call(Name(Load)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Load)))FunctionDef(arguments(argarg)Return(Tuple(BinOp(ConstantSubSubscript(Call(Name(Load)Name(Load)Name(Load))ConstantLoad))Load)))FunctionDef(arguments(argarg)Return(Tuple(BinOp(Call(Name(Load)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))AddConstant)Load)))FunctionDef(arguments(argarg)Return(Tuple(Call(Name(Load)BinOp(BinOp(Name(Load)DivName(Load))MultConstant))Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return (level,)\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4),)\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return (4 - _posterize_level_to_arg(level, hparams)[0],)\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4) + 4,)\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256),)\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return (256 - _solarize_level_to_arg(level, _hparams)[0],)\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return (int((level / _MAX_LEVEL) * 110),)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return (level,)\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4),)\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return (4 - _posterize_level_to_arg(level, hparams)[0],)\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4) + 4,)\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256),)\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return (256 - _solarize_level_to_arg(level, _hparams)[0],)\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return (int((level / _MAX_LEVEL) * 110),)\n\n\nLEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various\n    # Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return (4 - _posterize_level_to_arg(level, hparams)[0],)\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4) + 4,)\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256),)\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return (256 - _solarize_level_to_arg(level, _hparams)[0],)\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return (int((level / _MAX_LEVEL) * 110),)\n\n\nLEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various\n    # Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n    \"SolarizeAdd\": _solarize_add_level_to_arg,\n    \"Color\": _enhance_level_to_arg,\n    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n    \"Contrast\": _enhance_level_to_arg,\n    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n    \"Brightness\": _enhance_level_to_arg,\n    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4) + 4,)\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256),)\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return (256 - _solarize_level_to_arg(level, _hparams)[0],)\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return (int((level / _MAX_LEVEL) * 110),)\n\n\nLEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various\n    # Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n    \"SolarizeAdd\": _solarize_add_level_to_arg,\n    \"Color\": _enhance_level_to_arg,\n    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n    \"Contrast\": _enhance_level_to_arg,\n    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n    \"Brightness\": _enhance_level_to_arg,\n    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"Sharpness\": _enhance_level_to_arg,\n    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"ShearX\": _shear_level_to_arg,\n    \"ShearY\": _shear_level_to_arg,\n    \"TranslateX\": _translate_abs_level_to_arg,\n    \"TranslateY\": _translate_abs_level_to_arg,\n    \"TranslateXRel\": _translate_rel_level_to_arg,\n    \"TranslateYRel\": _translate_rel_level_to_arg,\n}\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return (256 - _solarize_level_to_arg(level, _hparams)[0],)\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return (int((level / _MAX_LEVEL) * 110),)\n\n\nLEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various\n    # Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n    \"SolarizeAdd\": _solarize_add_level_to_arg,\n    \"Color\": _enhance_level_to_arg,\n    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n    \"Contrast\": _enhance_level_to_arg,\n    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n    \"Brightness\": _enhance_level_to_arg,\n    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"Sharpness\": _enhance_level_to_arg,\n    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"ShearX\": _shear_level_to_arg,\n    \"ShearY\": _shear_level_to_arg,\n    \"TranslateX\": _translate_abs_level_to_arg,\n    \"TranslateY\": _translate_abs_level_to_arg,\n    \"TranslateXRel\": _translate_rel_level_to_arg,\n    \"TranslateYRel\": _translate_rel_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nLEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various\n    # Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n    \"SolarizeAdd\": _solarize_add_level_to_arg,\n    \"Color\": _enhance_level_to_arg,\n    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n    \"Contrast\": _enhance_level_to_arg,\n    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n    \"Brightness\": _enhance_level_to_arg,\n    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"Sharpness\": _enhance_level_to_arg,\n    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"ShearX\": _shear_level_to_arg,\n    \"ShearY\": _shear_level_to_arg,\n    \"TranslateX\": _translate_abs_level_to_arg,\n    \"TranslateY\": _translate_abs_level_to_arg,\n    \"TranslateXRel\": _translate_rel_level_to_arg,\n    \"TranslateYRel\": _translate_rel_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,\n    \"SolarizeAdd\": solarize_add,\n    \"Color\": color,\n    \"ColorIncreasing\": color,\n    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,\n    \"Brightness\": brightness,\n    \"BrightnessIncreasing\": brightness,\n    \"Sharpness\": sharpness,\n    \"SharpnessIncreasing\": sharpness,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n    \"SolarizeAdd\": _solarize_add_level_to_arg,\n    \"Color\": _enhance_level_to_arg,\n    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n    \"Contrast\": _enhance_level_to_arg,\n    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n    \"Brightness\": _enhance_level_to_arg,\n    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"Sharpness\": _enhance_level_to_arg,\n    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"ShearX\": _shear_level_to_arg,\n    \"ShearY\": _shear_level_to_arg,\n    \"TranslateX\": _translate_abs_level_to_arg,\n    \"TranslateY\": _translate_abs_level_to_arg,\n    \"TranslateXRel\": _translate_rel_level_to_arg,\n    \"TranslateYRel\": _translate_rel_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,\n    \"SolarizeAdd\": solarize_add,\n    \"Color\": color,\n    \"ColorIncreasing\": color,\n    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,\n    \"Brightness\": brightness,\n    \"BrightnessIncreasing\": brightness,\n    \"Sharpness\": sharpness,\n    \"SharpnessIncreasing\": sharpness,\n    \"ShearX\": shear_x,\n    \"ShearY\": shear_y,\n    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,\n    \"TranslateXRel\": translate_x_rel,\n    \"TranslateYRel\": translate_y_rel,\n}\n\n\nclass AugmentOp:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_365-415"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"Sharpness\": _enhance_level_to_arg,\n    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"ShearX\": _shear_level_to_arg,\n    \"ShearY\": _shear_level_to_arg,\n    \"TranslateX\": _translate_abs_level_to_arg,\n    \"TranslateY\": _translate_abs_level_to_arg,\n    \"TranslateXRel\": _translate_rel_level_to_arg,\n    \"TranslateYRel\": _translate_rel_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,\n    \"SolarizeAdd\": solarize_add,\n    \"Color\": color,\n    \"ColorIncreasing\": color,\n    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,\n    \"Brightness\": brightness,\n    \"BrightnessIncreasing\": brightness,\n    \"Sharpness\": sharpness,\n    \"SharpnessIncreasing\": sharpness,\n    \"ShearX\": shear_x,\n    \"ShearY\": shear_y,\n    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,\n    \"TranslateXRel\": translate_x_rel,\n    \"TranslateYRel\": translate_y_rel,\n}\n\n\nclass AugmentOp:\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = {\n            \"fillcolor\": hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n            \"resample\": hparams[\"interpolation\"]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_375-425"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nNAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,\n    \"SolarizeAdd\": solarize_add,\n    \"Color\": color,\n    \"ColorIncreasing\": color,\n    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,\n    \"Brightness\": brightness,\n    \"BrightnessIncreasing\": brightness,\n    \"Sharpness\": sharpness,\n    \"SharpnessIncreasing\": sharpness,\n    \"ShearX\": shear_x,\n    \"ShearY\": shear_y,\n    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,\n    \"TranslateXRel\": translate_x_rel,\n    \"TranslateYRel\": translate_y_rel,\n}\n\n\nclass AugmentOp:\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = {\n            \"fillcolor\": hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n            \"resample\": hparams[\"interpolation\"]\n            if \"interpolation\" in hparams\n            else _RANDOM_INTERPOLATION,\n        }\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\n    def __call__(self, img):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_385-435"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"SolarizeIncreasing\": solarize,\n    \"SolarizeAdd\": solarize_add,\n    \"Color\": color,\n    \"ColorIncreasing\": color,\n    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,\n    \"Brightness\": brightness,\n    \"BrightnessIncreasing\": brightness,\n    \"Sharpness\": sharpness,\n    \"SharpnessIncreasing\": sharpness,\n    \"ShearX\": shear_x,\n    \"ShearY\": shear_y,\n    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,\n    \"TranslateXRel\": translate_x_rel,\n    \"TranslateYRel\": translate_y_rel,\n}\n\n\nclass AugmentOp:\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = {\n            \"fillcolor\": hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n            \"resample\": hparams[\"interpolation\"]\n            if \"interpolation\" in hparams\n            else _RANDOM_INTERPOLATION,\n        }\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = (\n            self.level_fn(magnitude, self.hparams) if self.level_fn is not None else ()\n        )\n        return self.aug_fn(img, *level_args, **self.kwargs)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_395-445"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"ShearX\": shear_x,\n    \"ShearY\": shear_y,\n    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,\n    \"TranslateXRel\": translate_x_rel,\n    \"TranslateYRel\": translate_y_rel,\n}\n\n\nclass AugmentOp:\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = {\n            \"fillcolor\": hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n            \"resample\": hparams[\"interpolation\"]\n            if \"interpolation\" in hparams\n            else _RANDOM_INTERPOLATION,\n        }\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = (\n            self.level_fn(magnitude, self.hparams) if self.level_fn is not None else ()\n        )\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_405-455"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = {\n            \"fillcolor\": hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n            \"resample\": hparams[\"interpolation\"]\n            if \"interpolation\" in hparams\n            else _RANDOM_INTERPOLATION,\n        }\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = (\n            self.level_fn(magnitude, self.hparams) if self.level_fn is not None else ()\n        )\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"Posterize\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_415-465"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if \"interpolation\" in hparams\n            else _RANDOM_INTERPOLATION,\n        }\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = (\n            self.level_fn(magnitude, self.hparams) if self.level_fn is not None else ()\n        )\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"Posterize\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [\n            (\"Posterize\", 0.8, 2),\n            (\"Solarize\", 0.6, 10),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_425-475"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = (\n            self.level_fn(magnitude, self.hparams) if self.level_fn is not None else ()\n        )\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"Posterize\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [\n            (\"Posterize\", 0.8, 2),\n            (\"Solarize\", 0.6, 10),\n        ],  # This results in black image with Tpu posterize\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_435-485"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"Posterize\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [\n            (\"Posterize\", 0.8, 2),\n            (\"Solarize\", 0.6, 10),\n        ],  # This results in black image with Tpu posterize\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_445-495"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"Posterize\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [\n            (\"Posterize\", 0.8, 2),\n            (\"Solarize\", 0.6, 10),\n        ],  # This results in black image with Tpu posterize\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"PosterizeIncreasing\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_455-505"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [\n            (\"Posterize\", 0.8, 2),\n            (\"Solarize\", 0.6, 10),\n        ],  # This results in black image with Tpu posterize\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"PosterizeIncreasing\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 2), (\"Solarize\", 0.6, 10)],\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_465-515"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        ],  # This results in black image with Tpu posterize\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"PosterizeIncreasing\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 2), (\"Solarize\", 0.6, 10)],\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [(\"PosterizeOriginal\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeOriginal\", 0.6, 7), (\"PosterizeOriginal\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_475-525"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [(\"Equalize\", 0.8, 1), (\"ShearY\", 0.8, 4)],\n        [(\"Color\", 0.4, 9), (\"Equalize\", 0.6, 3)],\n        [(\"Color\", 0.4, 1), (\"Rotate\", 0.6, 8)],\n        [(\"Solarize\", 0.8, 3), (\"Equalize\", 0.4, 7)],\n        [(\"Solarize\", 0.4, 2), (\"Solarize\", 0.6, 2)],\n        [(\"Color\", 0.2, 0), (\"Equalize\", 0.8, 8)],\n        [(\"Equalize\", 0.4, 8), (\"SolarizeAdd\", 0.8, 3)],\n        [(\"ShearX\", 0.2, 9), (\"Rotate\", 0.6, 8)],\n        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"PosterizeIncreasing\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 2), (\"Solarize\", 0.6, 10)],\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [(\"PosterizeOriginal\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeOriginal\", 0.6, 7), (\"PosterizeOriginal\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeOriginal\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeOriginal\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_485-535"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Color\", 0.6, 1), (\"Equalize\", 1.0, 2)],\n        [(\"Invert\", 0.4, 9), (\"Rotate\", 0.6, 0)],\n        [(\"Equalize\", 1.0, 9), (\"ShearY\", 0.6, 3)],\n        [(\"Color\", 0.4, 7), (\"Equalize\", 0.6, 0)],\n        [(\"PosterizeIncreasing\", 0.4, 6), (\"AutoContrast\", 0.4, 7)],\n        [(\"Solarize\", 0.6, 8), (\"Color\", 0.6, 9)],\n        [(\"Solarize\", 0.2, 4), (\"Rotate\", 0.8, 9)],\n        [(\"Rotate\", 1.0, 7), (\"TranslateYRel\", 0.8, 9)],\n        [(\"ShearX\", 0.0, 0), (\"Solarize\", 0.8, 4)],\n        [(\"ShearY\", 0.8, 0), (\"Color\", 0.6, 4)],\n        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 2), (\"Solarize\", 0.6, 10)],\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [(\"PosterizeOriginal\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeOriginal\", 0.6, 7), (\"PosterizeOriginal\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeOriginal\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeOriginal\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_495-545"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Color\", 1.0, 0), (\"Rotate\", 0.6, 2)],\n        [(\"Equalize\", 0.8, 4), (\"Equalize\", 0.0, 8)],\n        [(\"Equalize\", 1.0, 4), (\"AutoContrast\", 0.6, 2)],\n        [(\"ShearY\", 0.4, 7), (\"SolarizeAdd\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 2), (\"Solarize\", 0.6, 10)],\n        [(\"Solarize\", 0.6, 8), (\"Equalize\", 0.6, 1)],\n        [(\"Color\", 0.8, 6), (\"Rotate\", 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [(\"PosterizeOriginal\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeOriginal\", 0.6, 7), (\"PosterizeOriginal\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeOriginal\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeOriginal\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [(\"PosterizeIncreasing\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_505-555"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [(\"PosterizeOriginal\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeOriginal\", 0.6, 7), (\"PosterizeOriginal\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeOriginal\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeOriginal\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [(\"PosterizeIncreasing\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeIncreasing\", 0.6, 7), (\"PosterizeIncreasing\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeIncreasing\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_515-565"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeOriginal\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeOriginal\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [(\"PosterizeIncreasing\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeIncreasing\", 0.6, 7), (\"PosterizeIncreasing\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeIncreasing\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_525-575"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [(\"PosterizeIncreasing\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeIncreasing\", 0.6, 7), (\"PosterizeIncreasing\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeIncreasing\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name=\"v0\", hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_535-585"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [(\"PosterizeIncreasing\", 0.4, 8), (\"Rotate\", 0.6, 9)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeIncreasing\", 0.6, 7), (\"PosterizeIncreasing\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeIncreasing\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name=\"v0\", hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == \"original\":\n        return auto_augment_policy_original(hparams)\n    elif name == \"originalr\":\n        return auto_augment_policy_originalr(hparams)\n    elif name == \"v0\":\n        return auto_augment_policy_v0(hparams)\n    elif name == \"v0r\":\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert AssertionError, \"Unknown AA policy (%s)\" % name", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_545-595"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n        [(\"PosterizeIncreasing\", 0.6, 7), (\"PosterizeIncreasing\", 0.6, 6)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Equalize\", 0.4, 4), (\"Rotate\", 0.8, 8)],\n        [(\"Solarize\", 0.6, 3), (\"Equalize\", 0.6, 7)],\n        [(\"PosterizeIncreasing\", 0.8, 5), (\"Equalize\", 1.0, 2)],\n        [(\"Rotate\", 0.2, 3), (\"Solarize\", 0.6, 8)],\n        [(\"Equalize\", 0.6, 8), (\"PosterizeIncreasing\", 0.4, 6)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 0.4, 0)],\n        [(\"Rotate\", 0.4, 9), (\"Equalize\", 0.6, 2)],\n        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name=\"v0\", hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == \"original\":\n        return auto_augment_policy_original(hparams)\n    elif name == \"originalr\":\n        return auto_augment_policy_originalr(hparams)\n    elif name == \"v0\":\n        return auto_augment_policy_v0(hparams)\n    elif name == \"v0r\":\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert AssertionError, \"Unknown AA policy (%s)\" % name\n\n\ndef auto_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    :param config_str: String defining configuration of auto augmentation.\n    Consists of multiple sections separated by dashes ('-'). The first\n    section defines the AutoAugment policy (one of 'v0', 'v0r', 'original',\n    'originalr').\n    The remaining sections, not order sepecific determine", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_555-605"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Equalize\", 0.0, 7), (\"Equalize\", 0.8, 8)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Rotate\", 0.8, 8), (\"Color\", 1.0, 2)],\n        [(\"Color\", 0.8, 8), (\"Solarize\", 0.8, 7)],\n        [(\"Sharpness\", 0.4, 7), (\"Invert\", 0.6, 8)],\n        [(\"ShearX\", 0.6, 5), (\"Equalize\", 1.0, 9)],\n        [(\"Color\", 0.4, 0), (\"Equalize\", 0.6, 3)],\n        [(\"Equalize\", 0.4, 7), (\"Solarize\", 0.2, 4)],\n        [(\"Solarize\", 0.6, 5), (\"AutoContrast\", 0.6, 5)],\n        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name=\"v0\", hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == \"original\":\n        return auto_augment_policy_original(hparams)\n    elif name == \"originalr\":\n        return auto_augment_policy_originalr(hparams)\n    elif name == \"v0\":\n        return auto_augment_policy_v0(hparams)\n    elif name == \"v0r\":\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert AssertionError, \"Unknown AA policy (%s)\" % name\n\n\ndef auto_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    :param config_str: String defining configuration of auto augmentation.\n    Consists of multiple sections separated by dashes ('-'). The first\n    section defines the AutoAugment policy (one of 'v0', 'v0r', 'original',\n    'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_565-615"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        [(\"Invert\", 0.6, 4), (\"Equalize\", 1.0, 8)],\n        [(\"Color\", 0.6, 4), (\"Contrast\", 1.0, 8)],\n        [(\"Equalize\", 0.8, 8), (\"Equalize\", 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name=\"v0\", hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == \"original\":\n        return auto_augment_policy_original(hparams)\n    elif name == \"originalr\":\n        return auto_augment_policy_originalr(hparams)\n    elif name == \"v0\":\n        return auto_augment_policy_v0(hparams)\n    elif name == \"v0r\":\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert AssertionError, \"Unknown AA policy (%s)\" % name\n\n\ndef auto_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    :param config_str: String defining configuration of auto augmentation.\n    Consists of multiple sections separated by dashes ('-'). The first\n    section defines the AutoAugment policy (one of 'v0', 'v0r', 'original',\n    'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert AssertionError, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_575-625"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    if name == \"original\":\n        return auto_augment_policy_original(hparams)\n    elif name == \"originalr\":\n        return auto_augment_policy_originalr(hparams)\n    elif name == \"v0\":\n        return auto_augment_policy_v0(hparams)\n    elif name == \"v0r\":\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert AssertionError, \"Unknown AA policy (%s)\" % name\n\n\ndef auto_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    :param config_str: String defining configuration of auto augmentation.\n    Consists of multiple sections separated by dashes ('-'). The first\n    section defines the AutoAugment policy (one of 'v0', 'v0r', 'original',\n    'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert AssertionError, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_585-635"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef auto_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    :param config_str: String defining configuration of auto augmentation.\n    Consists of multiple sections separated by dashes ('-'). The first\n    section defines the AutoAugment policy (one of 'v0', 'v0r', 'original',\n    'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert AssertionError, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",\n    \"Brightness\",\n    \"Sharpness\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\nAST=Module(FunctionDef(arguments(argarg)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Assign(Name(Store)Subscript(Name(Load)Slice(Constant)Load))For(Name(Store)Name(Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantName(Load)))If(Compare(Call(Name(Load)Name(Load))LtConstant)Continue)Assign(Tuple(Name(Store)Name(Store)Store)Subscript(Name(Load)Slice(Constant)Load))If(Compare(Name(Load)EqConstant)Expr(Call(Attribute(Name(Load)Load)ConstantCall(Name(Load)Name(Load))))Assert(Name(Load)Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Name(Load))))Return(Call(Name(Load)Name(Load))))Assign(Name(Store)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_595-645"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert AssertionError, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",\n    \"Brightness\",\n    \"Sharpness\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_605-655"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert AssertionError, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",\n    \"Brightness\",\n    \"Sharpness\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",\n    \"BrightnessIncreasing\",\n    \"SharpnessIncreasing\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_615-665"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",\n    \"Brightness\",\n    \"Sharpness\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",\n    \"BrightnessIncreasing\",\n    \"SharpnessIncreasing\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_625-675"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"Color\",\n    \"Contrast\",\n    \"Brightness\",\n    \"Sharpness\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",\n    \"BrightnessIncreasing\",\n    \"SharpnessIncreasing\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,\n    \"SolarizeAdd\": 0.005,\n    \"Contrast\": 0.005,\n    \"Brightness\": 0.005,\n    \"Equalize\": 0.005,\n    \"Posterize\": 0,\n    \"Invert\": 0,\n}\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_635-685"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",\n    \"BrightnessIncreasing\",\n    \"SharpnessIncreasing\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,\n    \"SolarizeAdd\": 0.005,\n    \"Contrast\": 0.005,\n    \"Brightness\": 0.005,\n    \"Equalize\": 0.005,\n    \"Posterize\": 0,\n    \"Invert\": 0,\n}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs\n\n\n\nAST=Module(Assign(Name(Store)List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant))FunctionDef(arguments(argargConstantConstant)Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Assert(Compare(Name(Load)EqConstant))Assign(Name(Store)Name(Load))Assign(Name(Store)ListComp(Subscript(Name(Load)Name(Load)Load)comprehension(Name(Store)Name(Load))))AugAssign(Name(Store)DivCall(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_645-695"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 702, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"ContrastIncreasing\",\n    \"BrightnessIncreasing\",\n    \"SharpnessIncreasing\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,\n    \"SolarizeAdd\": 0.005,\n    \"Contrast\": 0.005,\n    \"Brightness\": 0.005,\n    \"Equalize\": 0.005,\n    \"Posterize\": 0,\n    \"Invert\": 0,\n}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs\n\n\ndef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [\n        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n        for name in transforms\n    ]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_655-702"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 702, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,\n    \"SolarizeAdd\": 0.005,\n    \"Contrast\": 0.005,\n    \"Brightness\": 0.005,\n    \"Equalize\": 0.005,\n    \"Posterize\": 0,\n    \"Invert\": 0,\n}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs\n\n\ndef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [\n        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n        for name in transforms\n    ]\n\nAST=Module(Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant))FunctionDef(arguments(argargConstantConstant)Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Assert(Compare(Name(Load)EqConstant))Assign(Name(Store)Name(Load))Assign(Name(Store)ListComp(Subscript(Name(Load)Name(Load)Load)comprehension(Name(Store)Name(Load))))AugAssign(Name(Store)DivCall(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(argargargConstantConstantConstant)Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Return(ListComp(Call(Name(Load)Name(Load)keyword(Constant)keyword(Name(Load))keyword(Name(Load)))comprehension(Name(Store)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_665-702"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "rand_auto_aug.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 702, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,\n    \"SolarizeAdd\": 0.005,\n    \"Contrast\": 0.005,\n    \"Brightness\": 0.005,\n    \"Equalize\": 0.005,\n    \"Posterize\": 0,\n    \"Invert\": 0,\n}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs\n\n\ndef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [\n        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n        for name in transforms\n    ]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-rand_auto_aug.py_675-702"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nAll Transform wrappers take as input a Sample, and return a Sample.\n\"\"\"\nimport copy\nfrom typing import Any, Callable, List\n\nfrom omnivision.data.api import (\n    dataclass_as_dict,\n    Sample,\n    VisionMaskSample,\n    VisionSample,\n)\n\n\nclass SingleFieldTransform(Callable):\n    \"\"\"\n    The most basic transform, where only a single field is transformed. It wraps around\n    any standard torchvision (or other) transformation function\n    \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasalias)ClassDef(Name(Load)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nAll Transform wrappers take as input a Sample, and return a Sample.\n\"\"\"\nimport copy\nfrom typing import Any, Callable, List\n\nfrom omnivision.data.api import (\n    dataclass_as_dict,\n    Sample,\n    VisionMaskSample,\n    VisionSample,\n)\n\n\nclass SingleFieldTransform(Callable):\n    \"\"\"\n    The most basic transform, where only a single field is transformed. It wraps around\n    any standard torchvision (or other) transformation function\n    \"\"\"\n\n    def __init__(self, field: str, base_transform: object) -> None:\n        super().__init__()\n        self.field = field\n        self.base_transform = base_transform\n\n    def __call__(self, sample: Sample) -> Sample:\n        setattr(sample, self.field, self.base_transform(getattr(sample, self.field)))\n        return sample\n\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasalias)ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Constant)FunctionDef(arguments(argarg(Name(Load)))Expr(Call(Name(Load)Name(Load)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))))Return(Name(Load))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nAll Transform wrappers take as input a Sample, and return a Sample.\n\"\"\"\nimport copy\nfrom typing import Any, Callable, List\n\nfrom omnivision.data.api import (\n    dataclass_as_dict,\n    Sample,\n    VisionMaskSample,\n    VisionSample,\n)\n\n\nclass SingleFieldTransform(Callable):\n    \"\"\"\n    The most basic transform, where only a single field is transformed. It wraps around\n    any standard torchvision (or other) transformation function\n    \"\"\"\n\n    def __init__(self, field: str, base_transform: object) -> None:\n        super().__init__()\n        self.field = field\n        self.base_transform = base_transform\n\n    def __call__(self, sample: Sample) -> Sample:\n        setattr(sample, self.field, self.base_transform(getattr(sample, self.field)))\n        return sample\n\n\nclass VisionTransform(SingleFieldTransform):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(\"vision\", *args, **kwargs)\n\n\nclass ListTransform(Callable):\n    \"\"\"\n    Apply transforms to a list of items\n    \"\"\"\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasalias)ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Constant)FunctionDef(arguments(argarg(Name(Load)))Expr(Call(Name(Load)Name(Load)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))))Return(Name(Load))Name(Load)))ClassDef(Name(Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load)ConstantStarred(Name(Load)Load)keyword(Name(Load))))Constant))ClassDef(Name(Load)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\"\"\"\nAll Transform wrappers take as input a Sample, and return a Sample.\n\"\"\"\nimport copy\nfrom typing import Any, Callable, List\n\nfrom omnivision.data.api import (\n    dataclass_as_dict,\n    Sample,\n    VisionMaskSample,\n    VisionSample,\n)\n\n\nclass SingleFieldTransform(Callable):\n    \"\"\"\n    The most basic transform, where only a single field is transformed. It wraps around\n    any standard torchvision (or other) transformation function\n    \"\"\"\n\n    def __init__(self, field: str, base_transform: object) -> None:\n        super().__init__()\n        self.field = field\n        self.base_transform = base_transform\n\n    def __call__(self, sample: Sample) -> Sample:\n        setattr(sample, self.field, self.base_transform(getattr(sample, self.field)))\n        return sample\n\n\nclass VisionTransform(SingleFieldTransform):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(\"vision\", *args, **kwargs)\n\n\nclass ListTransform(Callable):\n    \"\"\"\n    Apply transforms to a list of items\n    \"\"\"\n\n    def __init__(self, base_transform: object, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.base_transform = base_transform\n\n    def __call__(self, items: List[Any]) -> List[Any]:\n        return [self.base_transform(item) for item in items]\n\n\nclass FlattenListOfList(Callable):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    VisionMaskSample,\n    VisionSample,\n)\n\n\nclass SingleFieldTransform(Callable):\n    \"\"\"\n    The most basic transform, where only a single field is transformed. It wraps around\n    any standard torchvision (or other) transformation function\n    \"\"\"\n\n    def __init__(self, field: str, base_transform: object) -> None:\n        super().__init__()\n        self.field = field\n        self.base_transform = base_transform\n\n    def __call__(self, sample: Sample) -> Sample:\n        setattr(sample, self.field, self.base_transform(getattr(sample, self.field)))\n        return sample\n\n\nclass VisionTransform(SingleFieldTransform):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(\"vision\", *args, **kwargs)\n\n\nclass ListTransform(Callable):\n    \"\"\"\n    Apply transforms to a list of items\n    \"\"\"\n\n    def __init__(self, base_transform: object, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.base_transform = base_transform\n\n    def __call__(self, items: List[Any]) -> List[Any]:\n        return [self.base_transform(item) for item in items]\n\n\nclass FlattenListOfList(Callable):\n    \"\"\"\n    Flatten a list of list into a single longer list.\n    \"\"\"\n\n    @staticmethod\n    def __call__(all_samples: List[List[Any]]) -> List[Any]:\n        return [sample for samples in all_samples for sample in samples]\n\n\nclass SingleFieldListToSampleList(Callable):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __init__(self, field: str, base_transform: object) -> None:\n        super().__init__()\n        self.field = field\n        self.base_transform = base_transform\n\n    def __call__(self, sample: Sample) -> Sample:\n        setattr(sample, self.field, self.base_transform(getattr(sample, self.field)))\n        return sample\n\n\nclass VisionTransform(SingleFieldTransform):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(\"vision\", *args, **kwargs)\n\n\nclass ListTransform(Callable):\n    \"\"\"\n    Apply transforms to a list of items\n    \"\"\"\n\n    def __init__(self, base_transform: object, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.base_transform = base_transform\n\n    def __call__(self, items: List[Any]) -> List[Any]:\n        return [self.base_transform(item) for item in items]\n\n\nclass FlattenListOfList(Callable):\n    \"\"\"\n    Flatten a list of list into a single longer list.\n    \"\"\"\n\n    @staticmethod\n    def __call__(all_samples: List[List[Any]]) -> List[Any]:\n        return [sample for samples in all_samples for sample in samples]\n\n\nclass SingleFieldListToSampleList(Callable):\n    \"\"\"\n    Convert a Sample with a list in the data to a list of Samples.\n    \"\"\"\n\n    def __init__(self, field: str):\n        self.field = field\n\n    def __call__(self, sample: Sample) -> List[Sample]:\n        data = getattr(sample, self.field)\n        assert isinstance(data, list)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass VisionTransform(SingleFieldTransform):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(\"vision\", *args, **kwargs)\n\n\nclass ListTransform(Callable):\n    \"\"\"\n    Apply transforms to a list of items\n    \"\"\"\n\n    def __init__(self, base_transform: object, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.base_transform = base_transform\n\n    def __call__(self, items: List[Any]) -> List[Any]:\n        return [self.base_transform(item) for item in items]\n\n\nclass FlattenListOfList(Callable):\n    \"\"\"\n    Flatten a list of list into a single longer list.\n    \"\"\"\n\n    @staticmethod\n    def __call__(all_samples: List[List[Any]]) -> List[Any]:\n        return [sample for samples in all_samples for sample in samples]\n\n\nclass SingleFieldListToSampleList(Callable):\n    \"\"\"\n    Convert a Sample with a list in the data to a list of Samples.\n    \"\"\"\n\n    def __init__(self, field: str):\n        self.field = field\n\n    def __call__(self, sample: Sample) -> List[Sample]:\n        data = getattr(sample, self.field)\n        assert isinstance(data, list)\n        delattr(sample, self.field)\n        ret = []\n        for el in data:\n            new_sample = copy.deepcopy(sample)\n            setattr(new_sample, self.field, el)\n            ret.append(new_sample)\n        return ret\n\n\nclass MaskingTransform(Callable):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __init__(self, base_transform: object, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.base_transform = base_transform\n\n    def __call__(self, items: List[Any]) -> List[Any]:\n        return [self.base_transform(item) for item in items]\n\n\nclass FlattenListOfList(Callable):\n    \"\"\"\n    Flatten a list of list into a single longer list.\n    \"\"\"\n\n    @staticmethod\n    def __call__(all_samples: List[List[Any]]) -> List[Any]:\n        return [sample for samples in all_samples for sample in samples]\n\n\nclass SingleFieldListToSampleList(Callable):\n    \"\"\"\n    Convert a Sample with a list in the data to a list of Samples.\n    \"\"\"\n\n    def __init__(self, field: str):\n        self.field = field\n\n    def __call__(self, sample: Sample) -> List[Sample]:\n        data = getattr(sample, self.field)\n        assert isinstance(data, list)\n        delattr(sample, self.field)\n        ret = []\n        for el in data:\n            new_sample = copy.deepcopy(sample)\n            setattr(new_sample, self.field, el)\n            ret.append(new_sample)\n        return ret\n\n\nclass MaskingTransform(Callable):\n    \"\"\"\n    Creates a mask for the input data. Useful for training MAE for instance.\n    \"\"\"\n\n    def __init__(self, masking_object, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.masking_object = masking_object\n\n    def __call__(self, sample: VisionSample) -> VisionMaskSample:\n        mask = self.masking_object(sample.vision)[\"mask\"]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 96, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    Flatten a list of list into a single longer list.\n    \"\"\"\n\n    @staticmethod\n    def __call__(all_samples: List[List[Any]]) -> List[Any]:\n        return [sample for samples in all_samples for sample in samples]\n\n\nclass SingleFieldListToSampleList(Callable):\n    \"\"\"\n    Convert a Sample with a list in the data to a list of Samples.\n    \"\"\"\n\n    def __init__(self, field: str):\n        self.field = field\n\n    def __call__(self, sample: Sample) -> List[Sample]:\n        data = getattr(sample, self.field)\n        assert isinstance(data, list)\n        delattr(sample, self.field)\n        ret = []\n        for el in data:\n            new_sample = copy.deepcopy(sample)\n            setattr(new_sample, self.field, el)\n            ret.append(new_sample)\n        return ret\n\n\nclass MaskingTransform(Callable):\n    \"\"\"\n    Creates a mask for the input data. Useful for training MAE for instance.\n    \"\"\"\n\n    def __init__(self, masking_object, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.masking_object = masking_object\n\n    def __call__(self, sample: VisionSample) -> VisionMaskSample:\n        mask = self.masking_object(sample.vision)[\"mask\"]\n        return VisionMaskSample(mask=mask, **dataclass_as_dict(sample))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_55-96"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "transform_wrappers.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 96, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    Convert a Sample with a list in the data to a list of Samples.\n    \"\"\"\n\n    def __init__(self, field: str):\n        self.field = field\n\n    def __call__(self, sample: Sample) -> List[Sample]:\n        data = getattr(sample, self.field)\n        assert isinstance(data, list)\n        delattr(sample, self.field)\n        ret = []\n        for el in data:\n            new_sample = copy.deepcopy(sample)\n            setattr(new_sample, self.field, el)\n            ret.append(new_sample)\n        return ret\n\n\nclass MaskingTransform(Callable):\n    \"\"\"\n    Creates a mask for the input data. Useful for training MAE for instance.\n    \"\"\"\n\n    def __init__(self, masking_object, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.masking_object = masking_object\n\n    def __call__(self, sample: VisionSample) -> VisionMaskSample:\n        mask = self.masking_object(sample.vision)[\"mask\"]\n        return VisionMaskSample(mask=mask, **dataclass_as_dict(sample))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-transform_wrappers.py_65-96"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py\npulished under an Apache License 2.0.\n\nCOMMENT FROM ORIGINAL:\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py\npulished under an Apache License 2.0.\n\nCOMMENT FROM ORIGINAL:\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py\npulished under an Apache License 2.0.\n\nCOMMENT FROM ORIGINAL:\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py\npulished under an Apache License 2.0.\n\nCOMMENT FROM ORIGINAL:\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n        self,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\"\"\"\nimport math\nimport random\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n        self,\n        probability=0.5,\n        min_area=0.02,\n        max_area=1 / 3,\n        min_aspect=0.3,\n        max_aspect=None,\n        mode=\"const\",\n        min_count=1,\n        max_count=None,\n        num_splits=0,\n        device=\"cuda\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n        self,\n        probability=0.5,\n        min_area=0.02,\n        max_area=1 / 3,\n        min_aspect=0.3,\n        max_aspect=None,\n        mode=\"const\",\n        min_count=1,\n        max_count=None,\n        num_splits=0,\n        device=\"cuda\",\n        cube=True,\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n        self,\n        probability=0.5,\n        min_area=0.02,\n        max_area=1 / 3,\n        min_aspect=0.3,\n        max_aspect=None,\n        mode=\"const\",\n        min_count=1,\n        max_count=None,\n        num_splits=0,\n        device=\"cuda\",\n        cube=True,\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        self.cube = cube\n        if mode == \"rand\":\n            self.rand_color = True  # per block random normal\n        elif mode == \"pixel\":\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \"const\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n        self,\n        probability=0.5,\n        min_area=0.02,\n        max_area=1 / 3,\n        min_aspect=0.3,\n        max_aspect=None,\n        mode=\"const\",\n        min_count=1,\n        max_count=None,\n        num_splits=0,\n        device=\"cuda\",\n        cube=True,\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        self.cube = cube\n        if mode == \"rand\":\n            self.rand_color = True  # per block random normal\n        elif mode == \"pixel\":\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \"const\"\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        probability=0.5,\n        min_area=0.02,\n        max_area=1 / 3,\n        min_aspect=0.3,\n        max_aspect=None,\n        mode=\"const\",\n        min_count=1,\n        max_count=None,\n        num_splits=0,\n        device=\"cuda\",\n        cube=True,\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        self.cube = cube\n        if mode == \"rand\":\n            self.rand_color = True  # per block random normal\n        elif mode == \"pixel\":\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \"const\"\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(10):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        cube=True,\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        self.cube = cube\n        if mode == \"rand\":\n            self.rand_color = True  # per block random normal\n        elif mode == \"pixel\":\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \"const\"\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(10):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top : top + h, left : left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        self.cube = cube\n        if mode == \"rand\":\n            self.rand_color = True  # per block random normal\n        elif mode == \"pixel\":\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \"const\"\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(10):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top : top + h, left : left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break\n\n    def _erase_cube(\n        self,\n        img,\n        batch_start,\n        batch_size,\n        chan,\n        img_h,\n        img_w,\n        dtype,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(10):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top : top + h, left : left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break\n\n    def _erase_cube(\n        self,\n        img,\n        batch_start,\n        batch_size,\n        chan,\n        img_h,\n        img_w,\n        dtype,\n    ):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        for _ in range(count):\n            for _ in range(10):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top : top + h, left : left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break\n\n    def _erase_cube(\n        self,\n        img,\n        batch_start,\n        batch_size,\n        chan,\n        img_h,\n        img_w,\n        dtype,\n    ):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(100):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top : top + h, left : left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break\n\n    def _erase_cube(\n        self,\n        img,\n        batch_start,\n        batch_size,\n        chan,\n        img_h,\n        img_w,\n        dtype,\n    ):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(100):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    for i in range(batch_start, batch_size):\n                        img_instance = img[i]\n                        img_instance[:, top : top + h, left : left + w] = _get_pixels(\n                            self.per_pixel,\n                            self.rand_color,\n                            (chan, h, w),\n                            dtype=dtype,\n                            device=self.device,\n                        )\n                    break", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _erase_cube(\n        self,\n        img,\n        batch_start,\n        batch_size,\n        chan,\n        img_h,\n        img_w,\n        dtype,\n    ):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(100):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    for i in range(batch_start, batch_size):\n                        img_instance = img[i]\n                        img_instance[:, top : top + h, left : left + w] = _get_pixels(\n                            self.per_pixel,\n                            self.rand_color,\n                            (chan, h, w),\n                            dtype=dtype,\n                            device=self.device,\n                        )\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            if self.cube:\n                self._erase_cube(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count\n            if self.min_count == self.max_count\n            else random.randint(self.min_count, self.max_count)\n        )\n        for _ in range(count):\n            for _ in range(100):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    for i in range(batch_start, batch_size):\n                        img_instance = img[i]\n                        img_instance[:, top : top + h, left : left + w] = _get_pixels(\n                            self.per_pixel,\n                            self.rand_color,\n                            (chan, h, w),\n                            dtype=dtype,\n                            device=self.device,\n                        )\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            if self.cube:\n                self._erase_cube(\n                    input,\n                    batch_start,\n                    batch_size,\n                    chan,\n                    img_h,\n                    img_w,\n                    input.dtype,\n                )\n            else:\n                for i in range(batch_start, batch_size):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 177, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            for _ in range(100):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area / count\n                )\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    for i in range(batch_start, batch_size):\n                        img_instance = img[i]\n                        img_instance[:, top : top + h, left : left + w] = _get_pixels(\n                            self.per_pixel,\n                            self.rand_color,\n                            (chan, h, w),\n                            dtype=dtype,\n                            device=self.device,\n                        )\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            if self.cube:\n                self._erase_cube(\n                    input,\n                    batch_start,\n                    batch_size,\n                    chan,\n                    img_h,\n                    img_w,\n                    input.dtype,\n                )\n            else:\n                for i in range(batch_start, batch_size):\n                    self._erase(input[i], chan, img_h, img_w, input.dtype)\n        return input", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_135-177"}
{"title": "facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "data", "transforms", "video_random_erasing.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 177, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    for i in range(batch_start, batch_size):\n                        img_instance = img[i]\n                        img_instance[:, top : top + h, left : left + w] = _get_pixels(\n                            self.per_pixel,\n                            self.rand_color,\n                            (chan, h, w),\n                            dtype=dtype,\n                            device=self.device,\n                        )\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            if self.cube:\n                self._erase_cube(\n                    input,\n                    batch_start,\n                    batch_size,\n                    chan,\n                    img_h,\n                    img_w,\n                    input.dtype,\n                )\n            else:\n                for i in range(batch_start, batch_size):\n                    self._erase(input[i], chan, img_h, img_w, input.dtype)\n        return input", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-data-transforms-video_random_erasing.py_145-177"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom omnivision.losses import LossWithUpdatedOutput\nfrom omnivision.utils.generic import is_on_gpu\nfrom torch import nn, Tensor\n\n\nclass SmoothCrossEntropy(torch.nn.modules.CrossEntropyLoss):\n    \"\"\"\n    Cross entropy loss that can accommodate smoothed labels\n    \"\"\"\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        if len(target.shape) > 1:\n            log_probs = F.log_softmax(input, 1)\n            # TODO: Implement weight and ignore_index\n            return -torch.mean(torch.sum(log_probs * target, dim=1))\n\nAST=Module(ImportFrom(aliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ClassDef(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))If(Compare(Call(Name(Load)Attribute(Name(Load)Load))GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Constant))Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load))keyword(Constant))))))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom omnivision.losses import LossWithUpdatedOutput\nfrom omnivision.utils.generic import is_on_gpu\nfrom torch import nn, Tensor\n\n\nclass SmoothCrossEntropy(torch.nn.modules.CrossEntropyLoss):\n    \"\"\"\n    Cross entropy loss that can accommodate smoothed labels\n    \"\"\"\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        if len(target.shape) > 1:\n            log_probs = F.log_softmax(input, 1)\n            # TODO: Implement weight and ignore_index\n            return -torch.mean(torch.sum(log_probs * target, dim=1))\n        else:\n            return F.cross_entropy(\n                input, target, weight=self.weight, ignore_index=self.ignore_index\n            )\n\n\nclass CrossEntropyMultipleOutputSingleTargetLoss(nn.Module):\n    \"\"\"\n    Intializer for the sum cross-entropy loss. For a single\n    tensor, this is equivalent to the cross-entropy loss. For a", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom omnivision.losses import LossWithUpdatedOutput\nfrom omnivision.utils.generic import is_on_gpu\nfrom torch import nn, Tensor\n\n\nclass SmoothCrossEntropy(torch.nn.modules.CrossEntropyLoss):\n    \"\"\"\n    Cross entropy loss that can accommodate smoothed labels\n    \"\"\"\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        if len(target.shape) > 1:\n            log_probs = F.log_softmax(input, 1)\n            # TODO: Implement weight and ignore_index\n            return -torch.mean(torch.sum(log_probs * target, dim=1))\n        else:\n            return F.cross_entropy(\n                input, target, weight=self.weight, ignore_index=self.ignore_index\n            )\n\n\nclass CrossEntropyMultipleOutputSingleTargetLoss(nn.Module):\n    \"\"\"\n    Intializer for the sum cross-entropy loss. For a single\n    tensor, this is equivalent to the cross-entropy loss. For a\n    list of tensors, this computes the sum of the cross-entropy\n    losses for each tensor in the list against the target. Can accommodate\n    target vectors, e.g. smoothed labels.\n\n    Config params:\n        weight: weight of sample, optional\n        ignore_index: sample should be ignored for loss, optional\n        reduction: specifies reduction to apply to the output, optional\n        temperature: specify temperature for softmax. Default 1.0\n    \"\"\"\n\nAST=Module(ImportFrom(aliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ClassDef(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))If(Compare(Call(Name(Load)Attribute(Name(Load)Load))GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Constant))Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load))keyword(Constant)))))Return(Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load)))))Name(Load)))ClassDef(Attribute(Name(Load)Load)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom typing import List, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom omnivision.losses import LossWithUpdatedOutput\nfrom omnivision.utils.generic import is_on_gpu\nfrom torch import nn, Tensor\n\n\nclass SmoothCrossEntropy(torch.nn.modules.CrossEntropyLoss):\n    \"\"\"\n    Cross entropy loss that can accommodate smoothed labels\n    \"\"\"\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        if len(target.shape) > 1:\n            log_probs = F.log_softmax(input, 1)\n            # TODO: Implement weight and ignore_index\n            return -torch.mean(torch.sum(log_probs * target, dim=1))\n        else:\n            return F.cross_entropy(\n                input, target, weight=self.weight, ignore_index=self.ignore_index\n            )\n\n\nclass CrossEntropyMultipleOutputSingleTargetLoss(nn.Module):\n    \"\"\"\n    Intializer for the sum cross-entropy loss. For a single\n    tensor, this is equivalent to the cross-entropy loss. For a\n    list of tensors, this computes the sum of the cross-entropy\n    losses for each tensor in the list against the target. Can accommodate\n    target vectors, e.g. smoothed labels.\n\n    Config params:\n        weight: weight of sample, optional\n        ignore_index: sample should be ignored for loss, optional\n        reduction: specifies reduction to apply to the output, optional\n        temperature: specify temperature for softmax. Default 1.0\n    \"\"\"\n\n    def __init__(\n        self,\n        temperature=1.0,\n        check_target_shape=False,\n        update_output_apply_activation=False,\n        weight=None,\n        ignore_index=-1,\n        normalize_output=False,\n    ):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class SmoothCrossEntropy(torch.nn.modules.CrossEntropyLoss):\n    \"\"\"\n    Cross entropy loss that can accommodate smoothed labels\n    \"\"\"\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        if len(target.shape) > 1:\n            log_probs = F.log_softmax(input, 1)\n            # TODO: Implement weight and ignore_index\n            return -torch.mean(torch.sum(log_probs * target, dim=1))\n        else:\n            return F.cross_entropy(\n                input, target, weight=self.weight, ignore_index=self.ignore_index\n            )\n\n\nclass CrossEntropyMultipleOutputSingleTargetLoss(nn.Module):\n    \"\"\"\n    Intializer for the sum cross-entropy loss. For a single\n    tensor, this is equivalent to the cross-entropy loss. For a\n    list of tensors, this computes the sum of the cross-entropy\n    losses for each tensor in the list against the target. Can accommodate\n    target vectors, e.g. smoothed labels.\n\n    Config params:\n        weight: weight of sample, optional\n        ignore_index: sample should be ignored for loss, optional\n        reduction: specifies reduction to apply to the output, optional\n        temperature: specify temperature for softmax. Default 1.0\n    \"\"\"\n\n    def __init__(\n        self,\n        temperature=1.0,\n        check_target_shape=False,\n        update_output_apply_activation=False,\n        weight=None,\n        ignore_index=-1,\n        normalize_output=False,\n    ):\n        super().__init__()\n        self._weight = None\n        self._losses = torch.nn.modules.ModuleList([])\n        self._temperature = temperature\n        self._check_target_shape = check_target_shape\n        self._update_output_apply_activation = update_output_apply_activation\n        self._weight = weight\n        self._ignore_index = ignore_index\n        self._normalize_output = normalize_output\n\n\nAST=Module(ClassDef(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))If(Compare(Call(Name(Load)Attribute(Name(Load)Load))GtConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Constant))Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultName(Load))keyword(Constant)))))Return(Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load)))))Name(Load)))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargargargargargConstantConstantConstantConstantUnaryOp(USubConstant)Constant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)List(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        else:\n            return F.cross_entropy(\n                input, target, weight=self.weight, ignore_index=self.ignore_index\n            )\n\n\nclass CrossEntropyMultipleOutputSingleTargetLoss(nn.Module):\n    \"\"\"\n    Intializer for the sum cross-entropy loss. For a single\n    tensor, this is equivalent to the cross-entropy loss. For a\n    list of tensors, this computes the sum of the cross-entropy\n    losses for each tensor in the list against the target. Can accommodate\n    target vectors, e.g. smoothed labels.\n\n    Config params:\n        weight: weight of sample, optional\n        ignore_index: sample should be ignored for loss, optional\n        reduction: specifies reduction to apply to the output, optional\n        temperature: specify temperature for softmax. Default 1.0\n    \"\"\"\n\n    def __init__(\n        self,\n        temperature=1.0,\n        check_target_shape=False,\n        update_output_apply_activation=False,\n        weight=None,\n        ignore_index=-1,\n        normalize_output=False,\n    ):\n        super().__init__()\n        self._weight = None\n        self._losses = torch.nn.modules.ModuleList([])\n        self._temperature = temperature\n        self._check_target_shape = check_target_shape\n        self._update_output_apply_activation = update_output_apply_activation\n        self._weight = weight\n        self._ignore_index = ignore_index\n        self._normalize_output = normalize_output\n\n    def _create_loss_function(self):\n        copy_to_gpu = is_on_gpu(self._losses)\n        # Instantiating CrossEntropyMultipleOutputSingleTargetLoss, which\n        # internally uses SmoothCrossEntropy loss to accommodate label smoothing,\n        # but defaults to vanilla cross-entropy if provided single-target labels.\n        self._losses.append(\n            SmoothCrossEntropy(weight=self._weight, ignore_index=self._ignore_index)\n        )\n        if copy_to_gpu:\n            self._losses.cuda()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    list of tensors, this computes the sum of the cross-entropy\n    losses for each tensor in the list against the target. Can accommodate\n    target vectors, e.g. smoothed labels.\n\n    Config params:\n        weight: weight of sample, optional\n        ignore_index: sample should be ignored for loss, optional\n        reduction: specifies reduction to apply to the output, optional\n        temperature: specify temperature for softmax. Default 1.0\n    \"\"\"\n\n    def __init__(\n        self,\n        temperature=1.0,\n        check_target_shape=False,\n        update_output_apply_activation=False,\n        weight=None,\n        ignore_index=-1,\n        normalize_output=False,\n    ):\n        super().__init__()\n        self._weight = None\n        self._losses = torch.nn.modules.ModuleList([])\n        self._temperature = temperature\n        self._check_target_shape = check_target_shape\n        self._update_output_apply_activation = update_output_apply_activation\n        self._weight = weight\n        self._ignore_index = ignore_index\n        self._normalize_output = normalize_output\n\n    def _create_loss_function(self):\n        copy_to_gpu = is_on_gpu(self._losses)\n        # Instantiating CrossEntropyMultipleOutputSingleTargetLoss, which\n        # internally uses SmoothCrossEntropy loss to accommodate label smoothing,\n        # but defaults to vanilla cross-entropy if provided single-target labels.\n        self._losses.append(\n            SmoothCrossEntropy(weight=self._weight, ignore_index=self._ignore_index)\n        )\n        if copy_to_gpu:\n            self._losses.cuda()\n        return self\n\n    def forward(\n        self, output: Union[torch.Tensor, List[torch.Tensor]], target: torch.Tensor\n    ) -> Union[torch.Tensor, LossWithUpdatedOutput]:\n        \"\"\"\n        For each output and single target, loss is calculated.\n        The returned loss value is the sum loss across all outputs.\n        \"\"\"\n        if isinstance(output, torch.Tensor):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __init__(\n        self,\n        temperature=1.0,\n        check_target_shape=False,\n        update_output_apply_activation=False,\n        weight=None,\n        ignore_index=-1,\n        normalize_output=False,\n    ):\n        super().__init__()\n        self._weight = None\n        self._losses = torch.nn.modules.ModuleList([])\n        self._temperature = temperature\n        self._check_target_shape = check_target_shape\n        self._update_output_apply_activation = update_output_apply_activation\n        self._weight = weight\n        self._ignore_index = ignore_index\n        self._normalize_output = normalize_output\n\n    def _create_loss_function(self):\n        copy_to_gpu = is_on_gpu(self._losses)\n        # Instantiating CrossEntropyMultipleOutputSingleTargetLoss, which\n        # internally uses SmoothCrossEntropy loss to accommodate label smoothing,\n        # but defaults to vanilla cross-entropy if provided single-target labels.\n        self._losses.append(\n            SmoothCrossEntropy(weight=self._weight, ignore_index=self._ignore_index)\n        )\n        if copy_to_gpu:\n            self._losses.cuda()\n        return self\n\n    def forward(\n        self, output: Union[torch.Tensor, List[torch.Tensor]], target: torch.Tensor\n    ) -> Union[torch.Tensor, LossWithUpdatedOutput]:\n        \"\"\"\n        For each output and single target, loss is calculated.\n        The returned loss value is the sum loss across all outputs.\n        \"\"\"\n        if isinstance(output, torch.Tensor):\n            output = [output]\n        assert isinstance(\n            output, list\n        ), \"Model output should be a list of tensors. Got Type {}\".format(type(output))\n        assert torch.is_tensor(target), \"Target should be a tensor. Got Type {}\".format(\n            type(target)\n        )\n\n        loss = 0\n        for idx, pred in enumerate(output):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        super().__init__()\n        self._weight = None\n        self._losses = torch.nn.modules.ModuleList([])\n        self._temperature = temperature\n        self._check_target_shape = check_target_shape\n        self._update_output_apply_activation = update_output_apply_activation\n        self._weight = weight\n        self._ignore_index = ignore_index\n        self._normalize_output = normalize_output\n\n    def _create_loss_function(self):\n        copy_to_gpu = is_on_gpu(self._losses)\n        # Instantiating CrossEntropyMultipleOutputSingleTargetLoss, which\n        # internally uses SmoothCrossEntropy loss to accommodate label smoothing,\n        # but defaults to vanilla cross-entropy if provided single-target labels.\n        self._losses.append(\n            SmoothCrossEntropy(weight=self._weight, ignore_index=self._ignore_index)\n        )\n        if copy_to_gpu:\n            self._losses.cuda()\n        return self\n\n    def forward(\n        self, output: Union[torch.Tensor, List[torch.Tensor]], target: torch.Tensor\n    ) -> Union[torch.Tensor, LossWithUpdatedOutput]:\n        \"\"\"\n        For each output and single target, loss is calculated.\n        The returned loss value is the sum loss across all outputs.\n        \"\"\"\n        if isinstance(output, torch.Tensor):\n            output = [output]\n        assert isinstance(\n            output, list\n        ), \"Model output should be a list of tensors. Got Type {}\".format(type(output))\n        assert torch.is_tensor(target), \"Target should be a tensor. Got Type {}\".format(\n            type(target)\n        )\n\n        loss = 0\n        for idx, pred in enumerate(output):\n            normalized_pred = pred\n            if self._normalize_output:\n                normalized_pred = nn.functional.normalize(pred, dim=1, p=2)\n\n            if self._check_target_shape:\n                assert (\n                    target.max().item() < pred.shape[1]\n                ), f\"pred.shape[1]={pred.shape[1]} and target.max().item()={target.max().item()}\"\n\n            if idx >= len(self._losses):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def _create_loss_function(self):\n        copy_to_gpu = is_on_gpu(self._losses)\n        # Instantiating CrossEntropyMultipleOutputSingleTargetLoss, which\n        # internally uses SmoothCrossEntropy loss to accommodate label smoothing,\n        # but defaults to vanilla cross-entropy if provided single-target labels.\n        self._losses.append(\n            SmoothCrossEntropy(weight=self._weight, ignore_index=self._ignore_index)\n        )\n        if copy_to_gpu:\n            self._losses.cuda()\n        return self\n\n    def forward(\n        self, output: Union[torch.Tensor, List[torch.Tensor]], target: torch.Tensor\n    ) -> Union[torch.Tensor, LossWithUpdatedOutput]:\n        \"\"\"\n        For each output and single target, loss is calculated.\n        The returned loss value is the sum loss across all outputs.\n        \"\"\"\n        if isinstance(output, torch.Tensor):\n            output = [output]\n        assert isinstance(\n            output, list\n        ), \"Model output should be a list of tensors. Got Type {}\".format(type(output))\n        assert torch.is_tensor(target), \"Target should be a tensor. Got Type {}\".format(\n            type(target)\n        )\n\n        loss = 0\n        for idx, pred in enumerate(output):\n            normalized_pred = pred\n            if self._normalize_output:\n                normalized_pred = nn.functional.normalize(pred, dim=1, p=2)\n\n            if self._check_target_shape:\n                assert (\n                    target.max().item() < pred.shape[1]\n                ), f\"pred.shape[1]={pred.shape[1]} and target.max().item()={target.max().item()}\"\n\n            if idx >= len(self._losses):\n                self._create_loss_function()\n            loss += self._losses[idx](normalized_pred / self._temperature, target)\n\n        if self._update_output_apply_activation:\n            if isinstance(output, torch.Tensor):\n                output = [output]\n\n            assert isinstance(\n                output, list\n            ), \"Model output should be a list of tensors. Got Type {}\".format(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 123, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return self\n\n    def forward(\n        self, output: Union[torch.Tensor, List[torch.Tensor]], target: torch.Tensor\n    ) -> Union[torch.Tensor, LossWithUpdatedOutput]:\n        \"\"\"\n        For each output and single target, loss is calculated.\n        The returned loss value is the sum loss across all outputs.\n        \"\"\"\n        if isinstance(output, torch.Tensor):\n            output = [output]\n        assert isinstance(\n            output, list\n        ), \"Model output should be a list of tensors. Got Type {}\".format(type(output))\n        assert torch.is_tensor(target), \"Target should be a tensor. Got Type {}\".format(\n            type(target)\n        )\n\n        loss = 0\n        for idx, pred in enumerate(output):\n            normalized_pred = pred\n            if self._normalize_output:\n                normalized_pred = nn.functional.normalize(pred, dim=1, p=2)\n\n            if self._check_target_shape:\n                assert (\n                    target.max().item() < pred.shape[1]\n                ), f\"pred.shape[1]={pred.shape[1]} and target.max().item()={target.max().item()}\"\n\n            if idx >= len(self._losses):\n                self._create_loss_function()\n            loss += self._losses[idx](normalized_pred / self._temperature, target)\n\n        if self._update_output_apply_activation:\n            if isinstance(output, torch.Tensor):\n                output = [output]\n\n            assert isinstance(\n                output, list\n            ), \"Model output should be a list of tensors. Got Type {}\".format(\n                type(output)\n            )\n\n            for idx in range(len(output)):\n                output[idx] = torch.nn.functional.softmax(output[idx], dim=-1)\n            return LossWithUpdatedOutput(loss=loss, output=output)\n        else:\n            return loss", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_75-123"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 123, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            output = [output]\n        assert isinstance(\n            output, list\n        ), \"Model output should be a list of tensors. Got Type {}\".format(type(output))\n        assert torch.is_tensor(target), \"Target should be a tensor. Got Type {}\".format(\n            type(target)\n        )\n\n        loss = 0\n        for idx, pred in enumerate(output):\n            normalized_pred = pred\n            if self._normalize_output:\n                normalized_pred = nn.functional.normalize(pred, dim=1, p=2)\n\n            if self._check_target_shape:\n                assert (\n                    target.max().item() < pred.shape[1]\n                ), f\"pred.shape[1]={pred.shape[1]} and target.max().item()={target.max().item()}\"\n\n            if idx >= len(self._losses):\n                self._create_loss_function()\n            loss += self._losses[idx](normalized_pred / self._temperature, target)\n\n        if self._update_output_apply_activation:\n            if isinstance(output, torch.Tensor):\n                output = [output]\n\n            assert isinstance(\n                output, list\n            ), \"Model output should be a list of tensors. Got Type {}\".format(\n                type(output)\n            )\n\n            for idx in range(len(output)):\n                output[idx] = torch.nn.functional.softmax(output[idx], dim=-1)\n            return LossWithUpdatedOutput(loss=loss, output=output)\n        else:\n            return loss", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_85-123"}
{"title": "facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "cross_entropy_multiple_output_single_target.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 123, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            normalized_pred = pred\n            if self._normalize_output:\n                normalized_pred = nn.functional.normalize(pred, dim=1, p=2)\n\n            if self._check_target_shape:\n                assert (\n                    target.max().item() < pred.shape[1]\n                ), f\"pred.shape[1]={pred.shape[1]} and target.max().item()={target.max().item()}\"\n\n            if idx >= len(self._losses):\n                self._create_loss_function()\n            loss += self._losses[idx](normalized_pred / self._temperature, target)\n\n        if self._update_output_apply_activation:\n            if isinstance(output, torch.Tensor):\n                output = [output]\n\n            assert isinstance(\n                output, list\n            ), \"Model output should be a list of tensors. Got Type {}\".format(\n                type(output)\n            )\n\n            for idx in range(len(output)):\n                output[idx] = torch.nn.functional.softmax(output[idx], dim=-1)\n            return LossWithUpdatedOutput(loss=loss, output=output)\n        else:\n            return loss", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-cross_entropy_multiple_output_single_target.py_95-123"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Optional\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionMaskSample\nfrom omnivision.losses import BaseLoss\n\n\nclass MAELoss(BaseLoss):\n    def __init__(\n        self,\n        patch_size: int = 16,\n        norm_pix_loss: bool = True,\n        norm_pix_per_channel: bool = False,\n        unnormalize_img: Any = None,\n        pad_object: Optional[nn.Module] = None,\n    ) -> float:\n        \"\"\"\n        MAE loss implementation that computes a simple MSE loss with optional patch wise", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Optional\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionMaskSample\nfrom omnivision.losses import BaseLoss\n\n\nclass MAELoss(BaseLoss):\n    def __init__(\n        self,\n        patch_size: int = 16,\n        norm_pix_loss: bool = True,\n        norm_pix_per_channel: bool = False,\n        unnormalize_img: Any = None,\n        pad_object: Optional[nn.Module] = None,\n    ) -> float:\n        \"\"\"\n        MAE loss implementation that computes a simple MSE loss with optional patch wise\n        normalization.\n\n        Args:\n            patch_size (int, optional): Defaults to 16.\n            norm_pix_loss (bool, optional): Normalize the pixel values using the mean and std of all\n             the pixel values within a patch. Defaults to True.\n            norm_pix_per_channel (bool, optional): Normalize the pixel values within a patch\n             separately for each of the RGB channels. Defaults to False.\n            unnormalize_img (Any, optional): Defaults to None.\n            pad_object ([type], optional): Defaults to None.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Optional\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionMaskSample\nfrom omnivision.losses import BaseLoss\n\n\nclass MAELoss(BaseLoss):\n    def __init__(\n        self,\n        patch_size: int = 16,\n        norm_pix_loss: bool = True,\n        norm_pix_per_channel: bool = False,\n        unnormalize_img: Any = None,\n        pad_object: Optional[nn.Module] = None,\n    ) -> float:\n        \"\"\"\n        MAE loss implementation that computes a simple MSE loss with optional patch wise\n        normalization.\n\n        Args:\n            patch_size (int, optional): Defaults to 16.\n            norm_pix_loss (bool, optional): Normalize the pixel values using the mean and std of all\n             the pixel values within a patch. Defaults to True.\n            norm_pix_per_channel (bool, optional): Normalize the pixel values within a patch\n             separately for each of the RGB channels. Defaults to False.\n            unnormalize_img (Any, optional): Defaults to None.\n            pad_object ([type], optional): Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.patch_size = patch_size\n        if isinstance(self.patch_size, int):\n            # Must be a tuple specifying the full patch size so it works with videos\n            self.patch_size = [self.patch_size, self.patch_size]\n        self.norm_pix_loss = norm_pix_loss\n        # Computes the pix norm per channel (what VideoMAE does)\n        # instead of altogether for all channels (what ImageMAE does)\n        self.norm_pix_per_channel = norm_pix_per_channel\n\nAST=Module(ImportFrom(aliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))ConstantConstantConstantConstantConstant)Expr(Constant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))If(Call(Name(Load)Attribute(Name(Load)Load)Name(Load))Assign(Attribute(Name(Load)Store)List(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom typing import Any, Optional\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionMaskSample\nfrom omnivision.losses import BaseLoss\n\n\nclass MAELoss(BaseLoss):\n    def __init__(\n        self,\n        patch_size: int = 16,\n        norm_pix_loss: bool = True,\n        norm_pix_per_channel: bool = False,\n        unnormalize_img: Any = None,\n        pad_object: Optional[nn.Module] = None,\n    ) -> float:\n        \"\"\"\n        MAE loss implementation that computes a simple MSE loss with optional patch wise\n        normalization.\n\n        Args:\n            patch_size (int, optional): Defaults to 16.\n            norm_pix_loss (bool, optional): Normalize the pixel values using the mean and std of all\n             the pixel values within a patch. Defaults to True.\n            norm_pix_per_channel (bool, optional): Normalize the pixel values within a patch\n             separately for each of the RGB channels. Defaults to False.\n            unnormalize_img (Any, optional): Defaults to None.\n            pad_object ([type], optional): Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.patch_size = patch_size\n        if isinstance(self.patch_size, int):\n            # Must be a tuple specifying the full patch size so it works with videos\n            self.patch_size = [self.patch_size, self.patch_size]\n        self.norm_pix_loss = norm_pix_loss\n        # Computes the pix norm per channel (what VideoMAE does)\n        # instead of altogether for all channels (what ImageMAE does)\n        self.norm_pix_per_channel = norm_pix_per_channel\n        assert (\n            not self.norm_pix_per_channel or self.norm_pix_loss\n        ), \"Must specify self.norm_pix_loss if using norm_pix_per_channel\"\n        self.unnormalize_img = unnormalize_img\n\n        # Use this to process the image before patchifying it, eg padding the\n        # image by replicate. It can just interpolate\n        # from the MODEL's patchify layer\n        self.pad_object = pad_object\n\n\nAST=Module(ImportFrom(aliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Attribute(Name(Load)Load)Load))ConstantConstantConstantConstantConstant)Expr(Constant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))If(Call(Name(Load)Attribute(Name(Load)Load)Name(Load))Assign(Attribute(Name(Load)Store)List(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assert(BoolOp(OrUnaryOp(NotAttribute(Name(Load)Load))Attribute(Name(Load)Load))Constant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(\n        self,\n        patch_size: int = 16,\n        norm_pix_loss: bool = True,\n        norm_pix_per_channel: bool = False,\n        unnormalize_img: Any = None,\n        pad_object: Optional[nn.Module] = None,\n    ) -> float:\n        \"\"\"\n        MAE loss implementation that computes a simple MSE loss with optional patch wise\n        normalization.\n\n        Args:\n            patch_size (int, optional): Defaults to 16.\n            norm_pix_loss (bool, optional): Normalize the pixel values using the mean and std of all\n             the pixel values within a patch. Defaults to True.\n            norm_pix_per_channel (bool, optional): Normalize the pixel values within a patch\n             separately for each of the RGB channels. Defaults to False.\n            unnormalize_img (Any, optional): Defaults to None.\n            pad_object ([type], optional): Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.patch_size = patch_size\n        if isinstance(self.patch_size, int):\n            # Must be a tuple specifying the full patch size so it works with videos\n            self.patch_size = [self.patch_size, self.patch_size]\n        self.norm_pix_loss = norm_pix_loss\n        # Computes the pix norm per channel (what VideoMAE does)\n        # instead of altogether for all channels (what ImageMAE does)\n        self.norm_pix_per_channel = norm_pix_per_channel\n        assert (\n            not self.norm_pix_per_channel or self.norm_pix_loss\n        ), \"Must specify self.norm_pix_loss if using norm_pix_per_channel\"\n        self.unnormalize_img = unnormalize_img\n\n        # Use this to process the image before patchifying it, eg padding the\n        # image by replicate. It can just interpolate\n        # from the MODEL's patchify layer\n        self.pad_object = pad_object\n\n    def compute_mae_loss(self, pred, mask, img):\n        mask = mask.reshape(mask.shape[0], -1)\n        if self.pad_object is not None:\n            img = self.pad_object(img)\n        # Based on\n        # https://github.com/MCG-NJU/VideoMAE/blob/a8dd8eedf955b3e3cc86c701e19e2553b4665154/engine_for_pretraining.py#L37-L59\n        if self.unnormalize_img is not None:\n            img_mean = (\n                torch.as_tensor(self.unnormalize_img[0])\n                .to(img.device)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        normalization.\n\n        Args:\n            patch_size (int, optional): Defaults to 16.\n            norm_pix_loss (bool, optional): Normalize the pixel values using the mean and std of all\n             the pixel values within a patch. Defaults to True.\n            norm_pix_per_channel (bool, optional): Normalize the pixel values within a patch\n             separately for each of the RGB channels. Defaults to False.\n            unnormalize_img (Any, optional): Defaults to None.\n            pad_object ([type], optional): Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.patch_size = patch_size\n        if isinstance(self.patch_size, int):\n            # Must be a tuple specifying the full patch size so it works with videos\n            self.patch_size = [self.patch_size, self.patch_size]\n        self.norm_pix_loss = norm_pix_loss\n        # Computes the pix norm per channel (what VideoMAE does)\n        # instead of altogether for all channels (what ImageMAE does)\n        self.norm_pix_per_channel = norm_pix_per_channel\n        assert (\n            not self.norm_pix_per_channel or self.norm_pix_loss\n        ), \"Must specify self.norm_pix_loss if using norm_pix_per_channel\"\n        self.unnormalize_img = unnormalize_img\n\n        # Use this to process the image before patchifying it, eg padding the\n        # image by replicate. It can just interpolate\n        # from the MODEL's patchify layer\n        self.pad_object = pad_object\n\n    def compute_mae_loss(self, pred, mask, img):\n        mask = mask.reshape(mask.shape[0], -1)\n        if self.pad_object is not None:\n            img = self.pad_object(img)\n        # Based on\n        # https://github.com/MCG-NJU/VideoMAE/blob/a8dd8eedf955b3e3cc86c701e19e2553b4665154/engine_for_pretraining.py#L37-L59\n        if self.unnormalize_img is not None:\n            img_mean = (\n                torch.as_tensor(self.unnormalize_img[0])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img_std = (\n                torch.as_tensor(self.unnormalize_img[1])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img = img * img_std + img_mean\n        target = self.patchify(img)\n        patches_dim = -2", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        super().__init__()\n        self.patch_size = patch_size\n        if isinstance(self.patch_size, int):\n            # Must be a tuple specifying the full patch size so it works with videos\n            self.patch_size = [self.patch_size, self.patch_size]\n        self.norm_pix_loss = norm_pix_loss\n        # Computes the pix norm per channel (what VideoMAE does)\n        # instead of altogether for all channels (what ImageMAE does)\n        self.norm_pix_per_channel = norm_pix_per_channel\n        assert (\n            not self.norm_pix_per_channel or self.norm_pix_loss\n        ), \"Must specify self.norm_pix_loss if using norm_pix_per_channel\"\n        self.unnormalize_img = unnormalize_img\n\n        # Use this to process the image before patchifying it, eg padding the\n        # image by replicate. It can just interpolate\n        # from the MODEL's patchify layer\n        self.pad_object = pad_object\n\n    def compute_mae_loss(self, pred, mask, img):\n        mask = mask.reshape(mask.shape[0], -1)\n        if self.pad_object is not None:\n            img = self.pad_object(img)\n        # Based on\n        # https://github.com/MCG-NJU/VideoMAE/blob/a8dd8eedf955b3e3cc86c701e19e2553b4665154/engine_for_pretraining.py#L37-L59\n        if self.unnormalize_img is not None:\n            img_mean = (\n                torch.as_tensor(self.unnormalize_img[0])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img_std = (\n                torch.as_tensor(self.unnormalize_img[1])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img = img * img_std + img_mean\n        target = self.patchify(img)\n        patches_dim = -2\n        if self.norm_pix_loss:\n            if not self.norm_pix_per_channel:\n                # Merge the channel with patches and compute mean\n                # over all channels of all patches.\n                # Else, will compute a mean for each channel separately\n                target = torch.flatten(target, patches_dim)\n                patches_dim = -1\n            mean = target.mean(dim=patches_dim, keepdim=True)\n            var = target.var(dim=patches_dim, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert (\n            not self.norm_pix_per_channel or self.norm_pix_loss\n        ), \"Must specify self.norm_pix_loss if using norm_pix_per_channel\"\n        self.unnormalize_img = unnormalize_img\n\n        # Use this to process the image before patchifying it, eg padding the\n        # image by replicate. It can just interpolate\n        # from the MODEL's patchify layer\n        self.pad_object = pad_object\n\n    def compute_mae_loss(self, pred, mask, img):\n        mask = mask.reshape(mask.shape[0], -1)\n        if self.pad_object is not None:\n            img = self.pad_object(img)\n        # Based on\n        # https://github.com/MCG-NJU/VideoMAE/blob/a8dd8eedf955b3e3cc86c701e19e2553b4665154/engine_for_pretraining.py#L37-L59\n        if self.unnormalize_img is not None:\n            img_mean = (\n                torch.as_tensor(self.unnormalize_img[0])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img_std = (\n                torch.as_tensor(self.unnormalize_img[1])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img = img * img_std + img_mean\n        target = self.patchify(img)\n        patches_dim = -2\n        if self.norm_pix_loss:\n            if not self.norm_pix_per_channel:\n                # Merge the channel with patches and compute mean\n                # over all channels of all patches.\n                # Else, will compute a mean for each channel separately\n                target = torch.flatten(target, patches_dim)\n                patches_dim = -1\n            mean = target.mean(dim=patches_dim, keepdim=True)\n            var = target.var(dim=patches_dim, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n            if self.norm_pix_per_channel:\n                # In this case we didn't flatten channel dim earlier, so flatten now\n                target = torch.flatten(target, -2)\n        else:\n            target = torch.flatten(target, patches_dim)\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)\n        mask_sum = mask.sum().clamp(min=1)\n        loss = (loss * mask).sum() / mask_sum", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def compute_mae_loss(self, pred, mask, img):\n        mask = mask.reshape(mask.shape[0], -1)\n        if self.pad_object is not None:\n            img = self.pad_object(img)\n        # Based on\n        # https://github.com/MCG-NJU/VideoMAE/blob/a8dd8eedf955b3e3cc86c701e19e2553b4665154/engine_for_pretraining.py#L37-L59\n        if self.unnormalize_img is not None:\n            img_mean = (\n                torch.as_tensor(self.unnormalize_img[0])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img_std = (\n                torch.as_tensor(self.unnormalize_img[1])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img = img * img_std + img_mean\n        target = self.patchify(img)\n        patches_dim = -2\n        if self.norm_pix_loss:\n            if not self.norm_pix_per_channel:\n                # Merge the channel with patches and compute mean\n                # over all channels of all patches.\n                # Else, will compute a mean for each channel separately\n                target = torch.flatten(target, patches_dim)\n                patches_dim = -1\n            mean = target.mean(dim=patches_dim, keepdim=True)\n            var = target.var(dim=patches_dim, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n            if self.norm_pix_per_channel:\n                # In this case we didn't flatten channel dim earlier, so flatten now\n                target = torch.flatten(target, -2)\n        else:\n            target = torch.flatten(target, patches_dim)\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)\n        mask_sum = mask.sum().clamp(min=1)\n        loss = (loss * mask).sum() / mask_sum\n\n        return loss\n\n    def core_forward(self, output: torch.Tensor, sample: VisionMaskSample):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        img = sample.vision\n        mask = sample.mask\n        pred = output", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img_std = (\n                torch.as_tensor(self.unnormalize_img[1])\n                .to(img.device)\n                .reshape([1, -1] + [1] * (img.ndim - 2))\n            )\n            img = img * img_std + img_mean\n        target = self.patchify(img)\n        patches_dim = -2\n        if self.norm_pix_loss:\n            if not self.norm_pix_per_channel:\n                # Merge the channel with patches and compute mean\n                # over all channels of all patches.\n                # Else, will compute a mean for each channel separately\n                target = torch.flatten(target, patches_dim)\n                patches_dim = -1\n            mean = target.mean(dim=patches_dim, keepdim=True)\n            var = target.var(dim=patches_dim, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n            if self.norm_pix_per_channel:\n                # In this case we didn't flatten channel dim earlier, so flatten now\n                target = torch.flatten(target, -2)\n        else:\n            target = torch.flatten(target, patches_dim)\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)\n        mask_sum = mask.sum().clamp(min=1)\n        loss = (loss * mask).sum() / mask_sum\n\n        return loss\n\n    def core_forward(self, output: torch.Tensor, sample: VisionMaskSample):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        img = sample.vision\n        mask = sample.mask\n        pred = output\n        return self.compute_mae_loss(pred, mask, img)\n\n    def patchify(self, imgs):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up\n        p = self.patch_size\n\n        # Add a dummy time dimension to 2D patches for consistency.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if self.norm_pix_loss:\n            if not self.norm_pix_per_channel:\n                # Merge the channel with patches and compute mean\n                # over all channels of all patches.\n                # Else, will compute a mean for each channel separately\n                target = torch.flatten(target, patches_dim)\n                patches_dim = -1\n            mean = target.mean(dim=patches_dim, keepdim=True)\n            var = target.var(dim=patches_dim, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n            if self.norm_pix_per_channel:\n                # In this case we didn't flatten channel dim earlier, so flatten now\n                target = torch.flatten(target, -2)\n        else:\n            target = torch.flatten(target, patches_dim)\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)\n        mask_sum = mask.sum().clamp(min=1)\n        loss = (loss * mask).sum() / mask_sum\n\n        return loss\n\n    def core_forward(self, output: torch.Tensor, sample: VisionMaskSample):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        img = sample.vision\n        mask = sample.mask\n        pred = output\n        return self.compute_mae_loss(pred, mask, img)\n\n    def patchify(self, imgs):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up\n        p = self.patch_size\n\n        # Add a dummy time dimension to 2D patches for consistency.\n        # Since it is 1, it will not affect the final number of patches\n        if len(p) == 2:\n            p = [\n                1,\n            ] + p\n            imgs = imgs.unsqueeze(-3)\n        assert imgs.ndim - 2 == len(p)  # except batch and channel dims\n        for i in range(1, len(p) + 1):\n            assert (\n                imgs.shape[-i] % p[-i] == 0", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if self.norm_pix_per_channel:\n                # In this case we didn't flatten channel dim earlier, so flatten now\n                target = torch.flatten(target, -2)\n        else:\n            target = torch.flatten(target, patches_dim)\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)\n        mask_sum = mask.sum().clamp(min=1)\n        loss = (loss * mask).sum() / mask_sum\n\n        return loss\n\n    def core_forward(self, output: torch.Tensor, sample: VisionMaskSample):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        img = sample.vision\n        mask = sample.mask\n        pred = output\n        return self.compute_mae_loss(pred, mask, img)\n\n    def patchify(self, imgs):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up\n        p = self.patch_size\n\n        # Add a dummy time dimension to 2D patches for consistency.\n        # Since it is 1, it will not affect the final number of patches\n        if len(p) == 2:\n            p = [\n                1,\n            ] + p\n            imgs = imgs.unsqueeze(-3)\n        assert imgs.ndim - 2 == len(p)  # except batch and channel dims\n        for i in range(1, len(p) + 1):\n            assert (\n                imgs.shape[-i] % p[-i] == 0\n            ), f\"image shape {imgs.shape} & patch shape {p} mismatch at index {i}\"\n\n        h = imgs.shape[-1] // p[-1]\n        w = imgs.shape[-2] // p[-2]\n        t = imgs.shape[-3] // p[-3]\n        x = imgs.reshape(shape=(imgs.shape[0], 3, t, p[-3], h, p[-2], w, p[-1]))\n        x = torch.einsum(\"nctphqwr->nthwpqrc\", x)\n        x = x.reshape(shape=(imgs.shape[0], t * h * w, p[-3] * p[-2] * p[-1], 3))\n\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        return loss\n\n    def core_forward(self, output: torch.Tensor, sample: VisionMaskSample):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        img = sample.vision\n        mask = sample.mask\n        pred = output\n        return self.compute_mae_loss(pred, mask, img)\n\n    def patchify(self, imgs):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up\n        p = self.patch_size\n\n        # Add a dummy time dimension to 2D patches for consistency.\n        # Since it is 1, it will not affect the final number of patches\n        if len(p) == 2:\n            p = [\n                1,\n            ] + p\n            imgs = imgs.unsqueeze(-3)\n        assert imgs.ndim - 2 == len(p)  # except batch and channel dims\n        for i in range(1, len(p) + 1):\n            assert (\n                imgs.shape[-i] % p[-i] == 0\n            ), f\"image shape {imgs.shape} & patch shape {p} mismatch at index {i}\"\n\n        h = imgs.shape[-1] // p[-1]\n        w = imgs.shape[-2] // p[-2]\n        t = imgs.shape[-3] // p[-3]\n        x = imgs.reshape(shape=(imgs.shape[0], 3, t, p[-3], h, p[-2], w, p[-1]))\n        x = torch.einsum(\"nctphqwr->nthwpqrc\", x)\n        x = x.reshape(shape=(imgs.shape[0], t * h * w, p[-3] * p[-2] * p[-1], 3))\n\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_95-135"}
{"title": "facebookresearch_omnivore-omnivision-losses-mae_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "mae_loss.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return self.compute_mae_loss(pred, mask, img)\n\n    def patchify(self, imgs):\n        \"\"\"\n        Modified from: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n        \"\"\"\n        assert imgs.shape[-2] == imgs.shape[-1]  # Spatial dimensions match up\n        p = self.patch_size\n\n        # Add a dummy time dimension to 2D patches for consistency.\n        # Since it is 1, it will not affect the final number of patches\n        if len(p) == 2:\n            p = [\n                1,\n            ] + p\n            imgs = imgs.unsqueeze(-3)\n        assert imgs.ndim - 2 == len(p)  # except batch and channel dims\n        for i in range(1, len(p) + 1):\n            assert (\n                imgs.shape[-i] % p[-i] == 0\n            ), f\"image shape {imgs.shape} & patch shape {p} mismatch at index {i}\"\n\n        h = imgs.shape[-1] // p[-1]\n        w = imgs.shape[-2] // p[-2]\n        t = imgs.shape[-3] // p[-3]\n        x = imgs.reshape(shape=(imgs.shape[0], 3, t, p[-3], h, p[-2], w, p[-1]))\n        x = torch.einsum(\"nctphqwr->nthwpqrc\", x)\n        x = x.reshape(shape=(imgs.shape[0], t * h * w, p[-3] * p[-2] * p[-1], 3))\n\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-mae_loss.py_105-135"}
{"title": "facebookresearch_omnivore-omnivision-losses-scaled_loss.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "scaled_loss.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 17, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "scaled_loss.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 17, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\n\n\nclass ScaledLoss(nn.Module):\n    def __init__(self, loss_fn, scale):\n        super().__init__()\n        self.loss_fn = loss_fn\n        self.scale = scale\n\n    def forward(self, *args, **kwargs):\n        return self.loss_fn(*args, **kwargs) * self.scale\n\nAST=Module(Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argargarg)Return(BinOp(Call(Attribute(Name(Load)Load)Starred(Name(Load)Load)keyword(Name(Load)))MultAttribute(Name(Load)Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-scaled_loss.py_0-17"}
{"title": "facebookresearch_omnivore-omnivision-losses-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import Sample\n\n\n@dataclass\nclass LossWithUpdatedOutput:\n    loss: torch.Tensor\n    output: torch.Tensor\n\n\ndef wrap_base_loss(loss):\n    if isinstance(loss, BaseLoss):\n        return loss\n    return BaseLoss(core_loss=loss)\n\n\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ClassDef(AnnAssign(Name(Store)Attribute(Name(Load)Load))AnnAssign(Name(Store)Attribute(Name(Load)Load))Name(Load))FunctionDef(arguments(arg)If(Call(Name(Load)Name(Load)Name(Load))Return(Name(Load)))Return(Call(Name(Load)keyword(Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-__init__.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-losses-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import Sample\n\n\n@dataclass\nclass LossWithUpdatedOutput:\n    loss: torch.Tensor\n    output: torch.Tensor\n\n\ndef wrap_base_loss(loss):\n    if isinstance(loss, BaseLoss):\n        return loss\n    return BaseLoss(core_loss=loss)\n\n\nclass BaseLoss(nn.Module):\n    \"\"\"\n    The base Omnivore loss API.\n    By default all losses get wrapped into this loss.\n    \"\"\"\n\n    def __init__(self, *args, core_loss=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.core_loss = core_loss\n\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ClassDef(AnnAssign(Name(Store)Attribute(Name(Load)Load))AnnAssign(Name(Store)Attribute(Name(Load)Load))Name(Load))FunctionDef(arguments(arg)If(Call(Name(Load)Name(Load)Name(Load))Return(Name(Load)))Return(Call(Name(Load)keyword(Name(Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargConstantarg)Expr(Call(Attribute(Call(Name(Load))Load)Starred(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-__init__.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-losses-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "__init__.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import Sample\n\n\n@dataclass\nclass LossWithUpdatedOutput:\n    loss: torch.Tensor\n    output: torch.Tensor\n\n\ndef wrap_base_loss(loss):\n    if isinstance(loss, BaseLoss):\n        return loss\n    return BaseLoss(core_loss=loss)\n\n\nclass BaseLoss(nn.Module):\n    \"\"\"\n    The base Omnivore loss API.\n    By default all losses get wrapped into this loss.\n    \"\"\"\n\n    def __init__(self, *args, core_loss=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.core_loss = core_loss\n\n    def core_forward(self, output: torch.Tensor, sample: Sample, *args, **kwargs):\n        return self.core_loss(output, sample.label, *args, **kwargs)\n\n    def forward(self, output: torch.Tensor, sample: Sample, *args, **kwargs):\n        loss_out = self.core_forward(output, sample, *args, **kwargs)\n        if isinstance(loss_out, LossWithUpdatedOutput):\n            loss, output = loss_out.loss, loss_out.output\n        else:\n            loss = loss_out\n        return loss, output\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ClassDef(AnnAssign(Name(Store)Attribute(Name(Load)Load))AnnAssign(Name(Store)Attribute(Name(Load)Load))Name(Load))FunctionDef(arguments(arg)If(Call(Name(Load)Name(Load)Name(Load))Return(Name(Load)))Return(Call(Name(Load)keyword(Name(Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargConstantarg)Expr(Call(Attribute(Call(Name(Load))Load)Starred(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Name(Load))argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)Attribute(Name(Load)Load)Starred(Name(Load)Load)keyword(Name(Load)))))FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Name(Load))argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))If(Call(Name(Load)Name(Load)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load))Assign(Name(Store)Name(Load)))Return(Tuple(Name(Load)Name(Load)Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-__init__.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-losses-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "__init__.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import Sample\n\n\n@dataclass\nclass LossWithUpdatedOutput:\n    loss: torch.Tensor\n    output: torch.Tensor\n\n\ndef wrap_base_loss(loss):\n    if isinstance(loss, BaseLoss):\n        return loss\n    return BaseLoss(core_loss=loss)\n\n\nclass BaseLoss(nn.Module):\n    \"\"\"\n    The base Omnivore loss API.\n    By default all losses get wrapped into this loss.\n    \"\"\"\n\n    def __init__(self, *args, core_loss=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.core_loss = core_loss\n\n    def core_forward(self, output: torch.Tensor, sample: Sample, *args, **kwargs):\n        return self.core_loss(output, sample.label, *args, **kwargs)\n\n    def forward(self, output: torch.Tensor, sample: Sample, *args, **kwargs):\n        loss_out = self.core_forward(output, sample, *args, **kwargs)\n        if isinstance(loss_out, LossWithUpdatedOutput):\n            loss, output = loss_out.loss, loss_out.output\n        else:\n            loss = loss_out\n        return loss, output\n\nAST=Module(ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ClassDef(AnnAssign(Name(Store)Attribute(Name(Load)Load))AnnAssign(Name(Store)Attribute(Name(Load)Load))Name(Load))FunctionDef(arguments(arg)If(Call(Name(Load)Name(Load)Name(Load))Return(Name(Load)))Return(Call(Name(Load)keyword(Name(Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargConstantarg)Expr(Call(Attribute(Call(Name(Load))Load)Starred(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Name(Load))argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)Attribute(Name(Load)Load)Starred(Name(Load)Load)keyword(Name(Load)))))FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Name(Load))argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))If(Call(Name(Load)Name(Load)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load))Assign(Name(Store)Name(Load)))Return(Tuple(Name(Load)Name(Load)Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-__init__.py_5-45"}
{"title": "facebookresearch_omnivore-omnivision-losses-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "losses", "__init__.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    loss: torch.Tensor\n    output: torch.Tensor\n\n\ndef wrap_base_loss(loss):\n    if isinstance(loss, BaseLoss):\n        return loss\n    return BaseLoss(core_loss=loss)\n\n\nclass BaseLoss(nn.Module):\n    \"\"\"\n    The base Omnivore loss API.\n    By default all losses get wrapped into this loss.\n    \"\"\"\n\n    def __init__(self, *args, core_loss=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.core_loss = core_loss\n\n    def core_forward(self, output: torch.Tensor, sample: Sample, *args, **kwargs):\n        return self.core_loss(output, sample.label, *args, **kwargs)\n\n    def forward(self, output: torch.Tensor, sample: Sample, *args, **kwargs):\n        loss_out = self.core_forward(output, sample, *args, **kwargs)\n        if isinstance(loss_out, LossWithUpdatedOutput):\n            loss, output = loss_out.loss, loss_out.output\n        else:\n            loss = loss_out\n        return loss, output", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-losses-__init__.py_15-45"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/recipes/blob/main/torchrecipes/vision/image_classification/metrics/multilabel_accuracy.py\n# but includes a bugfix - torchrecipes cast the target to an int, resulting in all zeros!\n# Based on https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/meters/accuracy_meter.py.\n\n\nimport torch\nfrom omnivision.utils.generic import maybe_convert_to_one_hot\nfrom torchmetrics.metric import Metric\n\n\nclass Accuracy(Metric):\n    \"\"\"Computes top-k accuracy for multilabel targets. A sample is considered\n    correctly classified if the top-k predictions contain any of the labels.\n\n    Args:\n        top_k: Number of highest score predictions considered to find the\n            correct label.\n        dist_sync_on_step: Synchronize metric state across processes at each\n            forward() before returning the value at the step.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/recipes/blob/main/torchrecipes/vision/image_classification/metrics/multilabel_accuracy.py\n# but includes a bugfix - torchrecipes cast the target to an int, resulting in all zeros!\n# Based on https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/meters/accuracy_meter.py.\n\n\nimport torch\nfrom omnivision.utils.generic import maybe_convert_to_one_hot\nfrom torchmetrics.metric import Metric\n\n\nclass Accuracy(Metric):\n    \"\"\"Computes top-k accuracy for multilabel targets. A sample is considered\n    correctly classified if the top-k predictions contain any of the labels.\n\n    Args:\n        top_k: Number of highest score predictions considered to find the\n            correct label.\n        dist_sync_on_step: Synchronize metric state across processes at each\n            forward() before returning the value at the step.\n    \"\"\"\n\n    def __init__(\n        self, top_k: int, compute_on_step: bool = True, dist_sync_on_step: bool = False\n    ) -> None:\n        super().__init__(\n            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n        )\n\n        self._top_k = top_k\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(alias)ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)keyword(Name(Load))keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/recipes/blob/main/torchrecipes/vision/image_classification/metrics/multilabel_accuracy.py\n# but includes a bugfix - torchrecipes cast the target to an int, resulting in all zeros!\n# Based on https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/meters/accuracy_meter.py.\n\n\nimport torch\nfrom omnivision.utils.generic import maybe_convert_to_one_hot\nfrom torchmetrics.metric import Metric\n\n\nclass Accuracy(Metric):\n    \"\"\"Computes top-k accuracy for multilabel targets. A sample is considered\n    correctly classified if the top-k predictions contain any of the labels.\n\n    Args:\n        top_k: Number of highest score predictions considered to find the\n            correct label.\n        dist_sync_on_step: Synchronize metric state across processes at each\n            forward() before returning the value at the step.\n    \"\"\"\n\n    def __init__(\n        self, top_k: int, compute_on_step: bool = True, dist_sync_on_step: bool = False\n    ) -> None:\n        super().__init__(\n            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n        )\n\n        self._top_k = top_k\n        self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n        \"\"\"Updates the state with predictions and target.\n        Args:\n            preds: tensor of shape (B, C) where each value is either logit or\n                class probability.\n            target: tensor of shape (B, C), which is one-hot / multi-label\n                encoded.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#\n# Modified from https://github.com/facebookresearch/recipes/blob/main/torchrecipes/vision/image_classification/metrics/multilabel_accuracy.py\n# but includes a bugfix - torchrecipes cast the target to an int, resulting in all zeros!\n# Based on https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/meters/accuracy_meter.py.\n\n\nimport torch\nfrom omnivision.utils.generic import maybe_convert_to_one_hot\nfrom torchmetrics.metric import Metric\n\n\nclass Accuracy(Metric):\n    \"\"\"Computes top-k accuracy for multilabel targets. A sample is considered\n    correctly classified if the top-k predictions contain any of the labels.\n\n    Args:\n        top_k: Number of highest score predictions considered to find the\n            correct label.\n        dist_sync_on_step: Synchronize metric state across processes at each\n            forward() before returning the value at the step.\n    \"\"\"\n\n    def __init__(\n        self, top_k: int, compute_on_step: bool = True, dist_sync_on_step: bool = False\n    ) -> None:\n        super().__init__(\n            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n        )\n\n        self._top_k = top_k\n        self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n        \"\"\"Updates the state with predictions and target.\n        Args:\n            preds: tensor of shape (B, C) where each value is either logit or\n                class probability.\n            target: tensor of shape (B, C), which is one-hot / multi-label\n                encoded.\n        \"\"\"\n        # Convert target to 0/1 encoding if isn't\n        target = maybe_convert_to_one_hot(target, preds)\n\n        assert preds.shape == target.shape, (\n            \"predictions and target must be of the same shape. \"\n            f\"Got preds({preds.shape}) vs target({target.shape}).\"\n        )\n        num_classes = target.shape[1]\n        assert (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass Accuracy(Metric):\n    \"\"\"Computes top-k accuracy for multilabel targets. A sample is considered\n    correctly classified if the top-k predictions contain any of the labels.\n\n    Args:\n        top_k: Number of highest score predictions considered to find the\n            correct label.\n        dist_sync_on_step: Synchronize metric state across processes at each\n            forward() before returning the value at the step.\n    \"\"\"\n\n    def __init__(\n        self, top_k: int, compute_on_step: bool = True, dist_sync_on_step: bool = False\n    ) -> None:\n        super().__init__(\n            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n        )\n\n        self._top_k = top_k\n        self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n        \"\"\"Updates the state with predictions and target.\n        Args:\n            preds: tensor of shape (B, C) where each value is either logit or\n                class probability.\n            target: tensor of shape (B, C), which is one-hot / multi-label\n                encoded.\n        \"\"\"\n        # Convert target to 0/1 encoding if isn't\n        target = maybe_convert_to_one_hot(target, preds)\n\n        assert preds.shape == target.shape, (\n            \"predictions and target must be of the same shape. \"\n            f\"Got preds({preds.shape}) vs target({target.shape}).\"\n        )\n        num_classes = target.shape[1]\n        assert (\n            num_classes >= self._top_k\n        ), f\"top-k({self._top_k}) is greater than the number of classes({num_classes})\"\n\n        # If Pytorch AMP is being used, model outputs are probably fp16\n        # Since .topk() is not compatible with fp16, we promote the model outputs to\n        # full precision\n        _, top_idx = preds.float().topk(self._top_k, dim=1, largest=True, sorted=True)\n\n        self.correct += (\n            torch.gather(target, dim=1, index=top_idx).max(dim=1).values.sum().item()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 72, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n\n    def __init__(\n        self, top_k: int, compute_on_step: bool = True, dist_sync_on_step: bool = False\n    ) -> None:\n        super().__init__(\n            compute_on_step=compute_on_step, dist_sync_on_step=dist_sync_on_step\n        )\n\n        self._top_k = top_k\n        self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n        \"\"\"Updates the state with predictions and target.\n        Args:\n            preds: tensor of shape (B, C) where each value is either logit or\n                class probability.\n            target: tensor of shape (B, C), which is one-hot / multi-label\n                encoded.\n        \"\"\"\n        # Convert target to 0/1 encoding if isn't\n        target = maybe_convert_to_one_hot(target, preds)\n\n        assert preds.shape == target.shape, (\n            \"predictions and target must be of the same shape. \"\n            f\"Got preds({preds.shape}) vs target({target.shape}).\"\n        )\n        num_classes = target.shape[1]\n        assert (\n            num_classes >= self._top_k\n        ), f\"top-k({self._top_k}) is greater than the number of classes({num_classes})\"\n\n        # If Pytorch AMP is being used, model outputs are probably fp16\n        # Since .topk() is not compatible with fp16, we promote the model outputs to\n        # full precision\n        _, top_idx = preds.float().topk(self._top_k, dim=1, largest=True, sorted=True)\n\n        self.correct += (\n            torch.gather(target, dim=1, index=top_idx).max(dim=1).values.sum().item()\n        )\n        self.total += preds.shape[0]\n\n    def compute(self) -> torch.Tensor:\n        if torch.is_nonzero(self.total):\n            return self.correct / self.total\n        return torch.tensor(0.0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_25-72"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 72, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n        \"\"\"Updates the state with predictions and target.\n        Args:\n            preds: tensor of shape (B, C) where each value is either logit or\n                class probability.\n            target: tensor of shape (B, C), which is one-hot / multi-label\n                encoded.\n        \"\"\"\n        # Convert target to 0/1 encoding if isn't\n        target = maybe_convert_to_one_hot(target, preds)\n\n        assert preds.shape == target.shape, (\n            \"predictions and target must be of the same shape. \"\n            f\"Got preds({preds.shape}) vs target({target.shape}).\"\n        )\n        num_classes = target.shape[1]\n        assert (\n            num_classes >= self._top_k\n        ), f\"top-k({self._top_k}) is greater than the number of classes({num_classes})\"\n\n        # If Pytorch AMP is being used, model outputs are probably fp16\n        # Since .topk() is not compatible with fp16, we promote the model outputs to\n        # full precision\n        _, top_idx = preds.float().topk(self._top_k, dim=1, largest=True, sorted=True)\n\n        self.correct += (\n            torch.gather(target, dim=1, index=top_idx).max(dim=1).values.sum().item()\n        )\n        self.total += preds.shape[0]\n\n    def compute(self) -> torch.Tensor:\n        if torch.is_nonzero(self.total):\n            return self.correct / self.total\n        return torch.tensor(0.0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_35-72"}
{"title": "facebookresearch_omnivore-omnivision-metrics-accuracy.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "accuracy.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 72, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        # Convert target to 0/1 encoding if isn't\n        target = maybe_convert_to_one_hot(target, preds)\n\n        assert preds.shape == target.shape, (\n            \"predictions and target must be of the same shape. \"\n            f\"Got preds({preds.shape}) vs target({target.shape}).\"\n        )\n        num_classes = target.shape[1]\n        assert (\n            num_classes >= self._top_k\n        ), f\"top-k({self._top_k}) is greater than the number of classes({num_classes})\"\n\n        # If Pytorch AMP is being used, model outputs are probably fp16\n        # Since .topk() is not compatible with fp16, we promote the model outputs to\n        # full precision\n        _, top_idx = preds.float().topk(self._top_k, dim=1, largest=True, sorted=True)\n\n        self.correct += (\n            torch.gather(target, dim=1, index=top_idx).max(dim=1).values.sum().item()\n        )\n        self.total += preds.shape[0]\n\n    def compute(self) -> torch.Tensor:\n        if torch.is_nonzero(self.total):\n            return self.correct / self.total\n        return torch.tensor(0.0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-accuracy.py_45-72"}
{"title": "facebookresearch_omnivore-omnivision-metrics-avg_pooled_accuracy_list_meter.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "avg_pooled_accuracy_list_meter.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Union\n\nimport torch\n\nfrom .accuracy import Accuracy\n\n# modified from vissl/meters/accuracy_list_meter_avg_pooled.py\nclass AvgPooledAccuracyListMeter(Accuracy):\n    def update(\n        self,\n        model_output: Union[torch.Tensor, List[torch.Tensor]],\n        target: torch.Tensor,\n    ):\n        if isinstance(model_output, list):\n            assert len(model_output) == 1\n            model_output = model_output[0]\n        assert isinstance(model_output, torch.Tensor)\n        model_output_reshaped = model_output.reshape(\n            (-1, target.size(0)) + model_output.shape[1:]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-avg_pooled_accuracy_list_meter.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-metrics-avg_pooled_accuracy_list_meter.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "avg_pooled_accuracy_list_meter.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 29, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "metrics", "avg_pooled_accuracy_list_meter.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 29, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Union\n\nimport torch\n\nfrom .accuracy import Accuracy\n\n# modified from vissl/meters/accuracy_list_meter_avg_pooled.py\nclass AvgPooledAccuracyListMeter(Accuracy):\n    def update(\n        self,\n        model_output: Union[torch.Tensor, List[torch.Tensor]],\n        target: torch.Tensor,\n    ):\n        if isinstance(model_output, list):\n            assert len(model_output) == 1\n            model_output = model_output[0]\n        assert isinstance(model_output, torch.Tensor)\n        model_output_reshaped = model_output.reshape(\n            (-1, target.size(0)) + model_output.shape[1:]\n        )\n\n        model_output_avg = torch.mean(model_output_reshaped, dim=0)\n        super().update(model_output_avg, target)\n\nAST=Module(ImportFrom(aliasalias)Import(alias)ImportFrom(alias)ClassDef(Name(Load)FunctionDef(arguments(argarg(Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)Load)Load))arg(Attribute(Name(Load)Load)))If(Call(Name(Load)Name(Load)Name(Load))Assert(Compare(Call(Name(Load)Name(Load))EqConstant))Assign(Name(Store)Subscript(Name(Load)ConstantLoad)))Assert(Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Tuple(UnaryOp(USubConstant)Call(Attribute(Name(Load)Load)Constant)Load)AddSubscript(Attribute(Name(Load)Load)Slice(Constant)Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))Expr(Call(Attribute(Call(Name(Load))Load)Name(Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-metrics-avg_pooled_accuracy_list_meter.py_0-29"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\nAST=Module(Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load)Name(Load))))Assert(Compare(Call(Name(Load)Name(Load))GtConstant)JoinedStr(ConstantFormattedValue(Name(Load))Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Call(Attribute(Name(Load)Load)Starred(Name(Load)Load)))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Name(Load)Load)Load)Load))ClassDef(Expr(Constant)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load)))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport fnmatch\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Set, Type, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\nAST=Module(Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load)))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load)Name(Load)Name(Load))))Assert(Compare(Call(Name(Load)Name(Load))GtConstant)JoinedStr(ConstantFormattedValue(Name(Load))Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Call(Attribute(Name(Load)Load)Starred(Name(Load)Load)))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Name(Load)Load)Load)Load))ClassDef(Expr(Constant)FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Name(Load)))Expr(Constant)Assign(Name(Store)Call(Name(Load)Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nfrom .model_wrappers import MIMOHeadWrapper\n\n\ndef _unix_pattern_to_parameter_names(\n    constraints: List[str], all_parameter_names: Set[str]\n) -> Union[None, Set[str]]:\n\n    parameter_names = []\n    for param_name in constraints:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert (\n            len(matching_parameters) > 0\n        ), f\"param_names {param_name} don't match any param in the given names.\"\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\nclass CkptIncludeKernel:\n    \"\"\"\n    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Includes only the keys from the given model state_dict that match the key_pattern.\n    Rest of the keys are removed from the given state_dict.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        include_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptExcludeKernel:\n    \"\"\"\n    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Removes the keys from the given model state_dict that match the key_pattern.\n\n    Args:\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, key_pattern: List[str]):\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        exclude_keys = _unix_pattern_to_parameter_names(\n            self.key_pattern, state_dict.keys()\n        )\n        include_keys = set(state_dict.keys()) - exclude_keys\n\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        new_state_dict = {}\n        for key in include_keys:\n            new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptPrependKernel:\n    \"\"\"\n    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Prepends the given pattern to all the keys in the checkpoint state dict after\n    selecting them with key_pattern.\n\n    For instance, if prepend_pattern  = \"some_prepend.\" and\n    key_pattern = [\"model.head\"], this kernel would prepend \"some_prepend.\" to\n    \"model.key\", thus renaming the key \"model.head\" to \"some_prepend.model.head\".\n\n    Args:\n        prepend_pattern: The pattern to prepend the keys in the state_dict with.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(self, prepend_pattern: str, key_pattern: List[str]):\n        self.prepend_pattern = prepend_pattern\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            new_state_dict[self.prepend_pattern + key] = state_dict[key]\n\n        return new_state_dict\n\n\nclass CkptRenameWithCopyKernel:\n    \"\"\"\n    Renames and also optionally creates copyies of the key-value pairs in the checkpoint\n    state dict. Before doing so, selects the keys to which to apply this kernel by\n    using key_pattern.\n\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    For instance, if source_pattern  = \"model.head\" and\n    target_patterns = [\"model.head_1\", \"model.head_2\"], this kernel would\n    rename the key \"model.head\" to \"model.head_1\" and will also create a copy of the\n    \"model.head\" and assign it a new name \"model.head_2\".\n\n    Args:\n        source_pattern: The pattern that needs to be renamed in the current\n            checkpoint state_dict.\n        target_patterns: A list of patterns to which the source_pattern is to be\n            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            renamed to it. If the list has more than one element, it creates multiple\n            copies of the source_pattern value and assigns then the names given in\n            target_pattern.\n        key_pattern: Patterns used to select the keys in the state_dict\n            that are eligible for this kernel.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_pattern: str,\n        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        target_patterns: List[str],\n        key_pattern: Optional[List[str]] = None,\n    ):\n        self.source_pattern = source_pattern\n        self.target_patterns = target_patterns\n        self.key_pattern = key_pattern\n\n    def __call__(self, state_dict: Dict):\n        \"\"\"\n        Args:\n            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            state_dict: A dictionary representing the given checkpoint's state dict.\n        \"\"\"\n\n        # Replaces only first occurences\n        all_keys = set(state_dict.keys())\n\n        include_keys = set(state_dict.keys())\n        if self.key_pattern is not None:\n            include_keys = _unix_pattern_to_parameter_names(\n                self.key_pattern, state_dict.keys()\n            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:\n        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n\n        excluded_keys = all_keys - include_keys\n\n        # Add excluded keys from re-mapping\n        new_state_dict = {}\n        for k in excluded_keys:\n            new_state_dict[k] = state_dict[k]\n\n        # Add keys from remapping\n        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:\n        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for key in include_keys:\n            if self.source_pattern in key:\n                for target_pattern in self.target_patterns:\n                    new_key = key.replace(self.source_pattern, target_pattern, 1)\n                    new_state_dict[new_key] = state_dict[key]\n            else:\n                new_state_dict[key] = state_dict[key]\n\n        return new_state_dict\n\n\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:\n        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef load_checkpoint(\n    path_list: List[str],\n    pick_recursive_keys: Optional[List[str]] = None,\n    map_location: str = \"cpu\",\n) -> Any:\n    \"\"\"\n    Loads a checkpoint from the specified path.\n\n    Args:\n        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        path_list: A list of paths which contain the checkpoint. Each element\n            is tried (in order) until a file that exists is found. That file is then\n            used to read the checkpoint.\n        pick_recursive_keys: Picks sub dicts from the loaded checkpoint if not None.\n            For pick_recursive_keys = [\"a\", \"b\"], will return checkpoint_dict[\"a\"][\"b\"]\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    path_exists = False\n    for path in path_list:\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n\n    if not path_exists:\n        raise ValueError(f\"No path exists in {path_list}\")\n\n    with g_pathmgr.open(path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        checkpoint = torch.load(f, map_location=map_location)\n\n    logging.info(f\"Loaded checkpoint from {path}\")\n    if pick_recursive_keys is not None:\n        for key in pick_recursive_keys:\n            checkpoint = checkpoint[key]\n    return checkpoint\n\n\ndef load_checkpoint_and_apply_kernels(\n    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    checkpoint_path: str,\n    checkpoint_kernels: List[Callable] = None,\n    ckpt_state_dict_key: str = \"state_dict\",\n    map_location: str = None,\n) -> nn.Module:\n    \"\"\"\n    Performs checkpoint loading with a variety of pre-processing kernel applied in\n    sequence.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        checkpoint_path (str): Path to the checkpoint.\n        checkpoint_kernels List(Callable): A list of checkpoint processing kernels\n            to apply in the specified order. Supported kernels include `CkptIncludeKernel`,\n            `CkptExcludeKernel`, etc. These kernels are applied in the\n            given order.\n        ckpt_state_dict_key (str): Key containing the model state dict.\n        map_location (str): a function, torch.device, string or a dict specifying how to\n            remap storage locations\n\n    Returns: Model with the matchin pre-trained weights loaded.\n    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n\n\ndef load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    assert g_pathmgr.exists(checkpoint_path), \"Checkpoint '{}' not found\".format(\n        checkpoint_path\n    )\n\n    # Load the checkpoint on CPU to avoid GPU mem spike.\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=map_location)\n\n    pre_train_dict = (\n        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n\n\ndef load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        checkpoint[ckpt_state_dict_key] if ckpt_state_dict_key else checkpoint\n    )\n\n    logging.info(\n        \"Loaded Checkpoint State Dict pre-kernel application: %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n    # Apply kernels\n    if checkpoint_kernels is not None:\n        for f in checkpoint_kernels:\n            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n\n\ndef load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:\n            raise KeyError(err)\n    return model\n\n\ndef load_vissl_checkpoint(\n    path_list: List[str],\n    head_id_to_key_mapping: Dict[int, str],\n    map_location: str = \"cpu\",\n    strict_heads: bool = True,\n    use_ema: bool = False,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            pre_train_dict = f(state_dict=pre_train_dict)\n\n    logging.info(\n        \"Loaded Checkpoint State Dict Post-kernel application %s\"\n        % str(\", \".join(list(pre_train_dict.keys())))\n    )\n\n    return pre_train_dict\n\n\ndef load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:\n            raise KeyError(err)\n    return model\n\n\ndef load_vissl_checkpoint(\n    path_list: List[str],\n    head_id_to_key_mapping: Dict[int, str],\n    map_location: str = \"cpu\",\n    strict_heads: bool = True,\n    use_ema: bool = False,\n    target: Type = MIMOHeadWrapper,\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n    assert target == MIMOHeadWrapper, \"Only MIMOHeadWrapper is supported currently\"\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n    model_key = \"ema_model\" if use_ema else \"base_model\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def load_state_dict_into_model(state_dict: Dict, model: nn.Module, strict: bool = True):\n    \"\"\"\n    Loads a state dict into the given model.\n\n    Args:\n        state_dict: A dictionary containing the model's\n            state dict, or a subset if strict is False\n        model: Model to load the checkpoint weights into\n        strict: raise if the state_dict has missing state keys\n    \"\"\"\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:\n            raise KeyError(err)\n    return model\n\n\ndef load_vissl_checkpoint(\n    path_list: List[str],\n    head_id_to_key_mapping: Dict[int, str],\n    map_location: str = \"cpu\",\n    strict_heads: bool = True,\n    use_ema: bool = False,\n    target: Type = MIMOHeadWrapper,\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n    assert target == MIMOHeadWrapper, \"Only MIMOHeadWrapper is supported currently\"\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n    model_key = \"ema_model\" if use_ema else \"base_model\"\n    state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n    trunk_state = state[\"trunk\"]\n    heads_state = state[\"heads\"]\n    out = {}\n    for key, value in trunk_state.items():\n        out[f\"trunk.{key}\"] = value\n    for key, value in heads_state.items():\n        split_idx = key.index(\".\")\n        head_id = int(key[:split_idx])\n        key_remaining = key[split_idx + 1 :]\n\nAST=Module(FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load))arg(Name(Load))Constant)Expr(Constant)Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))Assign(Name(Store)Constant)If(Name(Load)AugAssign(Name(Store)AddJoinedStr(ConstantFormattedValue(Name(Load))Constant)))If(Name(Load)AugAssign(Name(Store)AddJoinedStr(ConstantFormattedValue(Name(Load))Constant)))If(BoolOp(OrName(Load)Name(Load))If(BoolOp(AndUnaryOp(NotName(Load))UnaryOp(NotName(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Raise(Call(Name(Load)Name(Load)))))Return(Name(Load)))FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstantConstantName(Load))Expr(Constant)Assert(Compare(Name(Load)EqName(Load))Constant)Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)IfExp(Name(Load)ConstantConstant))Assign(Name(Store)Subscript(Subscript(Subscript(Name(Load)ConstantLoad)Name(Load)Load)ConstantLoad))Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Assign(Name(Store)Dict)For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assign(Subscript(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))Store)Name(Load)))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))Assign(Name(Store)Call(Name(Load)Subscript(Name(Load)Slice(Name(Load))Load)))Assign(Name(Store)Subscript(Name(Load)Slice(BinOp(Name(Load)AddConstant))Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    err = \"State key mismatch.\"\n    if unexpected_keys:\n        err += f\" Unexpected keys: {unexpected_keys}.\"\n    if missing_keys:\n        err += f\" Missing keys: {missing_keys}.\"\n    if unexpected_keys or missing_keys:\n        if not unexpected_keys and not strict:\n            logging.warning(err)\n        else:\n            raise KeyError(err)\n    return model\n\n\ndef load_vissl_checkpoint(\n    path_list: List[str],\n    head_id_to_key_mapping: Dict[int, str],\n    map_location: str = \"cpu\",\n    strict_heads: bool = True,\n    use_ema: bool = False,\n    target: Type = MIMOHeadWrapper,\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n    assert target == MIMOHeadWrapper, \"Only MIMOHeadWrapper is supported currently\"\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n    model_key = \"ema_model\" if use_ema else \"base_model\"\n    state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n    trunk_state = state[\"trunk\"]\n    heads_state = state[\"heads\"]\n    out = {}\n    for key, value in trunk_state.items():\n        out[f\"trunk.{key}\"] = value\n    for key, value in heads_state.items():\n        split_idx = key.index(\".\")\n        head_id = int(key[:split_idx])\n        key_remaining = key[split_idx + 1 :]\n        head_key = head_id_to_key_mapping.get(head_id)\n        if head_key is None:\n            if strict_heads:\n                raise ValueError(f\"No mapping provided for head id {head_id}\")\n            continue\n        out[f\"heads.{head_key}.{key_remaining}\"] = value\n    return out\n\n\ndef load_vissl_checkpoint_trunk_only(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            raise KeyError(err)\n    return model\n\n\ndef load_vissl_checkpoint(\n    path_list: List[str],\n    head_id_to_key_mapping: Dict[int, str],\n    map_location: str = \"cpu\",\n    strict_heads: bool = True,\n    use_ema: bool = False,\n    target: Type = MIMOHeadWrapper,\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n    assert target == MIMOHeadWrapper, \"Only MIMOHeadWrapper is supported currently\"\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n    model_key = \"ema_model\" if use_ema else \"base_model\"\n    state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n    trunk_state = state[\"trunk\"]\n    heads_state = state[\"heads\"]\n    out = {}\n    for key, value in trunk_state.items():\n        out[f\"trunk.{key}\"] = value\n    for key, value in heads_state.items():\n        split_idx = key.index(\".\")\n        head_id = int(key[:split_idx])\n        key_remaining = key[split_idx + 1 :]\n        head_key = head_id_to_key_mapping.get(head_id)\n        if head_key is None:\n            if strict_heads:\n                raise ValueError(f\"No mapping provided for head id {head_id}\")\n            continue\n        out[f\"heads.{head_key}.{key_remaining}\"] = value\n    return out\n\n\ndef load_vissl_checkpoint_trunk_only(\n    path_list: List[str],\n    map_location: str = \"cpu\",\n    use_ema: bool = False,\n    model_type: str = \"torch\",\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    target: Type = MIMOHeadWrapper,\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n    assert target == MIMOHeadWrapper, \"Only MIMOHeadWrapper is supported currently\"\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n    model_key = \"ema_model\" if use_ema else \"base_model\"\n    state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n    trunk_state = state[\"trunk\"]\n    heads_state = state[\"heads\"]\n    out = {}\n    for key, value in trunk_state.items():\n        out[f\"trunk.{key}\"] = value\n    for key, value in heads_state.items():\n        split_idx = key.index(\".\")\n        head_id = int(key[:split_idx])\n        key_remaining = key[split_idx + 1 :]\n        head_key = head_id_to_key_mapping.get(head_id)\n        if head_key is None:\n            if strict_heads:\n                raise ValueError(f\"No mapping provided for head id {head_id}\")\n            continue\n        out[f\"heads.{head_key}.{key_remaining}\"] = value\n    return out\n\n\ndef load_vissl_checkpoint_trunk_only(\n    path_list: List[str],\n    map_location: str = \"cpu\",\n    use_ema: bool = False,\n    model_type: str = \"torch\",\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n\n    if model_type == \"torch\":\n        model_key = \"ema_model\" if use_ema else \"base_model\"\n        state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n        trunk_state = state[\"trunk\"]\n    else:\n        trunk_state = checkpoint[\"model\"]\n    return trunk_state", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n    trunk_state = state[\"trunk\"]\n    heads_state = state[\"heads\"]\n    out = {}\n    for key, value in trunk_state.items():\n        out[f\"trunk.{key}\"] = value\n    for key, value in heads_state.items():\n        split_idx = key.index(\".\")\n        head_id = int(key[:split_idx])\n        key_remaining = key[split_idx + 1 :]\n        head_key = head_id_to_key_mapping.get(head_id)\n        if head_key is None:\n            if strict_heads:\n                raise ValueError(f\"No mapping provided for head id {head_id}\")\n            continue\n        out[f\"heads.{head_key}.{key_remaining}\"] = value\n    return out\n\n\ndef load_vissl_checkpoint_trunk_only(\n    path_list: List[str],\n    map_location: str = \"cpu\",\n    use_ema: bool = False,\n    model_type: str = \"torch\",\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n\n    if model_type == \"torch\":\n        model_key = \"ema_model\" if use_ema else \"base_model\"\n        state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n        trunk_state = state[\"trunk\"]\n    else:\n        trunk_state = checkpoint[\"model\"]\n    return trunk_state\n\n\ndef init_model_from_consolidated_weights(\n    model, state_dict: Dict[str, Any], inflate: bool = True, ignore: bool = True\n):\n    # load the checkpoint now\n    all_layers = model.state_dict()\n\n    for layername in all_layers.keys():\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        head_key = head_id_to_key_mapping.get(head_id)\n        if head_key is None:\n            if strict_heads:\n                raise ValueError(f\"No mapping provided for head id {head_id}\")\n            continue\n        out[f\"heads.{head_key}.{key_remaining}\"] = value\n    return out\n\n\ndef load_vissl_checkpoint_trunk_only(\n    path_list: List[str],\n    map_location: str = \"cpu\",\n    use_ema: bool = False,\n    model_type: str = \"torch\",\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n\n    if model_type == \"torch\":\n        model_key = \"ema_model\" if use_ema else \"base_model\"\n        state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n        trunk_state = state[\"trunk\"]\n    else:\n        trunk_state = checkpoint[\"model\"]\n    return trunk_state\n\n\ndef init_model_from_consolidated_weights(\n    model, state_dict: Dict[str, Any], inflate: bool = True, ignore: bool = True\n):\n    # load the checkpoint now\n    all_layers = model.state_dict()\n\n    for layername in all_layers.keys():\n\n        if layername in state_dict:\n            param = state_dict[layername]\n            if not isinstance(param, torch.Tensor):\n                param = torch.from_numpy(param)\n\n            # Inflate image models to video\n            if inflate:\n                if (\n                    all_layers[layername].shape != param.shape\n                    and (param.ndim == 4 or param.ndim == 5)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    path_list: List[str],\n    map_location: str = \"cpu\",\n    use_ema: bool = False,\n    model_type: str = \"torch\",\n):\n    \"\"\"\n    Heads are stored in an nn.Sequential in VISSL, but in an nn.ModuleDict in the\n    head attacher. we use the passed mapping to do this conversion.\n    Some (or all) heads can be ignored if strict_heads is False\n    \"\"\"\n\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n\n    if model_type == \"torch\":\n        model_key = \"ema_model\" if use_ema else \"base_model\"\n        state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n        trunk_state = state[\"trunk\"]\n    else:\n        trunk_state = checkpoint[\"model\"]\n    return trunk_state\n\n\ndef init_model_from_consolidated_weights(\n    model, state_dict: Dict[str, Any], inflate: bool = True, ignore: bool = True\n):\n    # load the checkpoint now\n    all_layers = model.state_dict()\n\n    for layername in all_layers.keys():\n\n        if layername in state_dict:\n            param = state_dict[layername]\n            if not isinstance(param, torch.Tensor):\n                param = torch.from_numpy(param)\n\n            # Inflate image models to video\n            if inflate:\n                if (\n                    all_layers[layername].shape != param.shape\n                    and (param.ndim == 4 or param.ndim == 5)\n                    and all_layers[layername].ndim == 5\n                ):\n                    old_shape = param.shape\n                    time_dim = all_layers[layername].size(-3)\n                    if param.ndim == 4:\n                        param = param.unsqueeze(-3)\n                    param = param.repeat(1, 1, time_dim // param.size(2), 1, 1)\n                    param = param / time_dim\n                    logging.warning(\n                        (f\"Inflated {layername} from \" f\"{old_shape} to {param.shape}.\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    checkpoint = load_checkpoint(path_list, map_location=map_location)\n\n    if model_type == \"torch\":\n        model_key = \"ema_model\" if use_ema else \"base_model\"\n        state = checkpoint[\"classy_state_dict\"][model_key][\"model\"]\n        trunk_state = state[\"trunk\"]\n    else:\n        trunk_state = checkpoint[\"model\"]\n    return trunk_state\n\n\ndef init_model_from_consolidated_weights(\n    model, state_dict: Dict[str, Any], inflate: bool = True, ignore: bool = True\n):\n    # load the checkpoint now\n    all_layers = model.state_dict()\n\n    for layername in all_layers.keys():\n\n        if layername in state_dict:\n            param = state_dict[layername]\n            if not isinstance(param, torch.Tensor):\n                param = torch.from_numpy(param)\n\n            # Inflate image models to video\n            if inflate:\n                if (\n                    all_layers[layername].shape != param.shape\n                    and (param.ndim == 4 or param.ndim == 5)\n                    and all_layers[layername].ndim == 5\n                ):\n                    old_shape = param.shape\n                    time_dim = all_layers[layername].size(-3)\n                    if param.ndim == 4:\n                        param = param.unsqueeze(-3)\n                    param = param.repeat(1, 1, time_dim // param.size(2), 1, 1)\n                    param = param / time_dim\n                    logging.warning(\n                        (f\"Inflated {layername} from \" f\"{old_shape} to {param.shape}.\")\n                    )\n\n            if all_layers[layername].shape != param.shape:\n                logging.warning(\n                    f\"{layername} have different shapes: \"\n                    f\"checkpoint: {param.shape}, \"\n                    f\"model: {all_layers[layername].shape}\"\n                )\n                if ignore:\n                    continue", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_365-415"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 420, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef init_model_from_consolidated_weights(\n    model, state_dict: Dict[str, Any], inflate: bool = True, ignore: bool = True\n):\n    # load the checkpoint now\n    all_layers = model.state_dict()\n\n    for layername in all_layers.keys():\n\n        if layername in state_dict:\n            param = state_dict[layername]\n            if not isinstance(param, torch.Tensor):\n                param = torch.from_numpy(param)\n\n            # Inflate image models to video\n            if inflate:\n                if (\n                    all_layers[layername].shape != param.shape\n                    and (param.ndim == 4 or param.ndim == 5)\n                    and all_layers[layername].ndim == 5\n                ):\n                    old_shape = param.shape\n                    time_dim = all_layers[layername].size(-3)\n                    if param.ndim == 4:\n                        param = param.unsqueeze(-3)\n                    param = param.repeat(1, 1, time_dim // param.size(2), 1, 1)\n                    param = param / time_dim\n                    logging.warning(\n                        (f\"Inflated {layername} from \" f\"{old_shape} to {param.shape}.\")\n                    )\n\n            if all_layers[layername].shape != param.shape:\n                logging.warning(\n                    f\"{layername} have different shapes: \"\n                    f\"checkpoint: {param.shape}, \"\n                    f\"model: {all_layers[layername].shape}\"\n                )\n                if ignore:\n                    continue\n                else:\n                    raise ValueError(\"Shape mismatch in checkpoint load\")\n            all_layers[layername].copy_(param)\n\n    return model\n\nAST=Module(FunctionDef(arguments(argarg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)))For(Name(Store)Call(Attribute(Name(Load)Load))If(Compare(Name(Load)InName(Load))Assign(Name(Store)Subscript(Name(Load)Name(Load)Load))If(UnaryOp(NotCall(Name(Load)Name(Load)Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))If(Name(Load)If(BoolOp(AndCompare(Attribute(Subscript(Name(Load)Name(Load)Load)Load)NotEqAttribute(Name(Load)Load))BoolOp(OrCompare(Attribute(Name(Load)Load)EqConstant)Compare(Attribute(Name(Load)Load)EqConstant))Compare(Attribute(Subscript(Name(Load)Name(Load)Load)Load)EqConstant))Assign(Name(Store)Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Subscript(Name(Load)Name(Load)Load)Load)UnaryOp(USubConstant)))If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstantBinOp(Name(Load)FloorDivCall(Attribute(Name(Load)Load)Constant))ConstantConstant))Assign(Name(Store)BinOp(Name(Load)DivName(Load)))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Name(Load))ConstantFormattedValue(Attribute(Name(Load)Load))Constant)))))If(Compare(Attribute(Subscript(Name(Load)Name(Load)Load)Load)NotEqAttribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)JoinedStr(FormattedValue(Name(Load))ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Attribute(Subscript(Name(Load)Name(Load)Load)Load)))))If(Name(Load)ContinueRaise(Call(Name(Load)Constant))))Expr(Call(Attribute(Subscript(Name(Load)Name(Load)Load)Load)Name(Load)))))Return(Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_375-420"}
{"title": "facebookresearch_omnivore-omnivision-model-checkpoint_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "checkpoint_utils.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 420, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if layername in state_dict:\n            param = state_dict[layername]\n            if not isinstance(param, torch.Tensor):\n                param = torch.from_numpy(param)\n\n            # Inflate image models to video\n            if inflate:\n                if (\n                    all_layers[layername].shape != param.shape\n                    and (param.ndim == 4 or param.ndim == 5)\n                    and all_layers[layername].ndim == 5\n                ):\n                    old_shape = param.shape\n                    time_dim = all_layers[layername].size(-3)\n                    if param.ndim == 4:\n                        param = param.unsqueeze(-3)\n                    param = param.repeat(1, 1, time_dim // param.size(2), 1, 1)\n                    param = param / time_dim\n                    logging.warning(\n                        (f\"Inflated {layername} from \" f\"{old_shape} to {param.shape}.\")\n                    )\n\n            if all_layers[layername].shape != param.shape:\n                logging.warning(\n                    f\"{layername} have different shapes: \"\n                    f\"checkpoint: {param.shape}, \"\n                    f\"model: {all_layers[layername].shape}\"\n                )\n                if ignore:\n                    continue\n                else:\n                    raise ValueError(\"Shape mismatch in checkpoint load\")\n            all_layers[layername].copy_(param)\n\n    return model", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-checkpoint_utils.py_385-420"}
{"title": "facebookresearch_omnivore-omnivision-model-model_init_utils.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_init_utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 17, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_init_utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 17, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Callable, Dict\n\nimport torch.nn as nn\n\n\ndef init_parameters(model: nn.Module, init_fns: Dict[str, Callable]) -> nn.Module:\n    for param_name, init_fn in init_fns.items():\n        param = model.get_parameter(param_name)\n        ret = init_fn(param)\n        assert ret is None or ret is param, \"init_fn should update param in place\"\n    return model\n\nAST=Module(ImportFrom(aliasalias)Import(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))Assert(BoolOp(OrCompare(Name(Load)IsConstant)Compare(Name(Load)IsName(Load)))Constant))Return(Name(Load))Attribute(Name(Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_init_utils.py_0-17"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport copy\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Mapping, Optional, Sequence\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\n\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass MIMOHeadWrapper(nn.Module):\n    \"\"\"Attaches multiple input multiple output heads to the trunk using forward hooks.\n\n    Args:\n        trunk: Any model to which you want to attach the heads to.\n        heads: A list of dicts with the following keys:\n            fork_module: The module which the head will be applied to. It can be an\n                empty string, in which case the head is attached to the trunk's output.\n            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            head: The head which is to be attached.\n            input_key: The head will only run on inputs with this key. If set to\n                `None` the head will be applied to all inputs.\n            output_key: The head will produce this output key. If set to `None`, the\n                output key will be the same as the input key.\n\n            An example heads value can look like -\n            ```\n            [\n                {\n                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    \"fork_module\": \"layer_1.layer_a.layer_alpha\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_1\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_1\",\n                    \"output_key\": \"out_2\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Linear(in_feat, out_feat),\n                    \"input_key\": \"dataset_2\",\n                    \"output_key\": \"out_3\",\n                },\n                {\n                    \"fork_module\": \"\",\n                    \"head\": nn.Conv2d(in_feat, out_feat),\n                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    \"input_key\": None,\n                    \"output_key\": None,\n                },\n            ]\n            ```\n        trunk_fields: A list of dicts with the following keys:\n            input_key: The input key this rule applies to. If `None`, applies to all\n                inputs.\n            args: These specific keys will be fetched from the sample and passed as\n                *args to the trunk for the specified `input_key`.\n            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            kwargs: These specific keys will be fetched from the sample and passed as\n                **kwargs to the trunk for the specified `input_key`.\n\n            Example -\n            ```\n            [\n                {\n                    \"input_key\": \"dataset_1\",\n                    \"args\": [\"vision\"]\n                },\n                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                {\n                    \"input_key\": \"dataset_2\",\n                    \"args\": [\"vision\"],\n                    \"kwargs\": {\"mask\": \"mask\"}\n                },\n            ]\n            ```\n\n        Note that two heads cannot produce the same output key in the same forward pass.\n\n    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Returns:\n        A dict with keys corresponding to the output keys which match with the input key.\n    \"\"\"\n\n    @dataclass\n    class HeadArgs:\n        fork_module: str\n        head: nn.Module\n        input_key: Optional[str]\n        output_key: Optional[str]\n\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    @dataclass\n    class TrunkFieldArgs:\n        input_key: Optional[str]\n        args: List[str] = field(default_factory=list)\n        kwargs: Dict[str, str] = field(default_factory=dict)\n\n    def __init__(\n        self,\n        trunk: nn.Module,\n        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        heads: List[Dict],\n        trunk_fields: List[Dict],\n        handle_list_inputs=False,\n    ) -> None:\n        \"\"\"WARNING: handle_list_inputs is a hack which needs to be refactored away.\"\"\"\n        super().__init__()\n\n        self.trunk = trunk\n        self.handle_list_inputs = handle_list_inputs\n\n        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # cast to HeadArgs for input validation\n        heads = [self.HeadArgs(**head_dict) for head_dict in heads]\n        # cast to TrunkFieldArgs for input validation\n        trunk_fields = [\n            self.TrunkFieldArgs(**trunk_fields_dict)\n            for trunk_fields_dict in trunk_fields\n        ]\n\n        self.head_name_to_fork_module = {}\n        self.heads = nn.ModuleList()\n        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.head_input_keys = []\n        self.head_output_keys = []\n        self.head_fork_modules = []\n\n        for head_args in heads:\n            self.heads.append(head_args.head)\n            self.head_input_keys.append(head_args.input_key)\n            self.head_output_keys.append(head_args.output_key)\n            self.head_fork_modules.append(head_args.fork_module)\n\n        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.trunk_field_args = {}\n        self.trunk_field_kwargs = {}\n        for trunk_fields_elem in trunk_fields:\n            input_key = trunk_fields_elem.input_key\n            if input_key in self.trunk_field_args:\n                raise KeyError(\n                    f\"Multiple trunk_fields specified for the same input_key: {input_key}\"\n                )\n            self.trunk_field_args[input_key] = trunk_fields_elem.args\n            self.trunk_field_kwargs[input_key] = trunk_fields_elem.kwargs\n\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # outputs is used as a temporary storage of the head outputs\n        self.outputs = {}\n\n        # input_key is used to specify which key is currently being processed\n        self.input_key = None\n\n        # handles to the hooks which can be used for removing the hooks if needed\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _register_hooks(self):\n        for i, head in enumerate(self.heads):\n            fork_module_name = self.head_fork_modules[i]\n\n            def hook_fn(\n                module,\n                module_in,\n                module_out,\n                # the following variables are passed as kwargs in the closure to avoid\n                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                # late binding in python\n                head_method=head,\n                in_key=self.head_input_keys[i],\n                out_key=self.head_output_keys[i],\n            ):\n                if in_key is not None and self.input_key != in_key:\n                    return\n                if out_key is None:\n                    out_key = self.input_key\n                if out_key in self.outputs:\n                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n\n    def forward(self, batch, *args, **kwargs) -> Dict:\n        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    # reset state before raising\n                    self.outputs = {}\n                    self.input_key = None\n                    raise ValueError(\n                        f\"Two heads produced the same output key `{out_key}` during forward\"\n                    )\n                self.outputs[out_key] = head_method(module_out)\n\n            fork_module = self.trunk.get_submodule(fork_module_name)\n            self.hook_handles.append(fork_module.register_forward_hook(hook_fn))\n\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n\n    def forward(self, batch, *args, **kwargs) -> Dict:\n        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):\n                # FIXME: this only handles list inputs for the field \"vision\"\n                assert len(batch) == 1\n                out_vals = []\n                for e in sub_batch.vision:\n                    e_batch = copy.copy(sub_batch)\n                    e_batch.vision = e\n                    self.forward_sub_batch(e_batch, *args, **kwargs)\n                    assert len(self.outputs) == 1\n                    out_key, out_val = self.outputs.popitem()\n                    out_vals.append(out_val)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 232, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _get_trunk_fields(self):\n        fields_args = self.trunk_field_args.get(self.input_key)\n        fields_kwargs = self.trunk_field_kwargs.get(self.input_key)\n        if fields_args is None:\n            assert fields_kwargs is None\n            fields_args = self.trunk_field_args.get(None)\n            fields_kwargs = self.trunk_field_kwargs.get(None)\n            if fields_args is None:\n                assert fields_kwargs is None\n                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n\n    def forward(self, batch, *args, **kwargs) -> Dict:\n        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):\n                # FIXME: this only handles list inputs for the field \"vision\"\n                assert len(batch) == 1\n                out_vals = []\n                for e in sub_batch.vision:\n                    e_batch = copy.copy(sub_batch)\n                    e_batch.vision = e\n                    self.forward_sub_batch(e_batch, *args, **kwargs)\n                    assert len(self.outputs) == 1\n                    out_key, out_val = self.outputs.popitem()\n                    out_vals.append(out_val)\n                return {out_key: torch.cat(out_vals)}\n            else:\n                self.forward_sub_batch(sub_batch, *args, **kwargs)\n        outputs = self.outputs\n        self.input_key = None\n        self.outputs = {}\n        return outputs", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_185-232"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 232, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                raise ValueError(\n                    f\"No trunk fields specified for input key: {self.input_key}\"\n                )\n        return fields_args, fields_kwargs\n\n    def forward_sub_batch(self, sub_batch, *args, **kwargs):\n        assert isinstance(sub_batch, VisionSample), f\"Received {type(sub_batch)}\"\n        fields_args, fields_kwargs = self._get_trunk_fields()\n        sample_args = [getattr(sub_batch, arg) for arg in fields_args]\n        sample_kwargs = {\n            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n\n    def forward(self, batch, *args, **kwargs) -> Dict:\n        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):\n                # FIXME: this only handles list inputs for the field \"vision\"\n                assert len(batch) == 1\n                out_vals = []\n                for e in sub_batch.vision:\n                    e_batch = copy.copy(sub_batch)\n                    e_batch.vision = e\n                    self.forward_sub_batch(e_batch, *args, **kwargs)\n                    assert len(self.outputs) == 1\n                    out_key, out_val = self.outputs.popitem()\n                    out_vals.append(out_val)\n                return {out_key: torch.cat(out_vals)}\n            else:\n                self.forward_sub_batch(sub_batch, *args, **kwargs)\n        outputs = self.outputs\n        self.input_key = None\n        self.outputs = {}\n        return outputs", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_195-232"}
{"title": "facebookresearch_omnivore-omnivision-model-model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "model", "model_wrappers.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 232, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            key: getattr(sub_batch, field) for key, field in fields_kwargs.items()\n        }\n        self.trunk(*sample_args, *args, **sample_kwargs, **kwargs)\n\n    def forward(self, batch, *args, **kwargs) -> Dict:\n        assert isinstance(batch, Mapping)\n        assert len(self.outputs) == 0\n        for key, sub_batch in batch.items():\n            self.input_key = key\n            if self.handle_list_inputs and isinstance(sub_batch.vision, Sequence):\n                # FIXME: this only handles list inputs for the field \"vision\"\n                assert len(batch) == 1\n                out_vals = []\n                for e in sub_batch.vision:\n                    e_batch = copy.copy(sub_batch)\n                    e_batch.vision = e\n                    self.forward_sub_batch(e_batch, *args, **kwargs)\n                    assert len(self.outputs) == 1\n                    out_key, out_val = self.outputs.popitem()\n                    out_vals.append(out_val)\n                return {out_key: torch.cat(out_vals)}\n            else:\n                self.forward_sub_batch(sub_batch, *args, **kwargs)\n        outputs = self.outputs\n        self.input_key = None\n        self.outputs = {}\n        return outputs", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-model-model_wrappers.py_205-232"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copied and modified from\n# https://raw.githubusercontent.com/SwinTransformer/Video-Swin-Transformer/master/mmaction/models/backbones/swin_transformer.py\n\nimport logging\nfrom functools import lru_cache, reduce\nfrom operator import mul\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom einops import rearrange\nfrom omnivision.utils.checkpoint import load_and_broadcast_checkpoint_list\nfrom timm.models.layers import DropPath, trunc_normal_\n\n\nclass Im2Video(nn.Module):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copied and modified from\n# https://raw.githubusercontent.com/SwinTransformer/Video-Swin-Transformer/master/mmaction/models/backbones/swin_transformer.py\n\nimport logging\nfrom functools import lru_cache, reduce\nfrom operator import mul\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom einops import rearrange\nfrom omnivision.utils.checkpoint import load_and_broadcast_checkpoint_list\nfrom timm.models.layers import DropPath, trunc_normal_\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n\nAST=Module(Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copied and modified from\n# https://raw.githubusercontent.com/SwinTransformer/Video-Swin-Transformer/master/mmaction/models/backbones/swin_transformer.py\n\nimport logging\nfrom functools import lru_cache, reduce\nfrom operator import mul\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom einops import rearrange\nfrom omnivision.utils.checkpoint import load_and_broadcast_checkpoint_list\nfrom timm.models.layers import DropPath, trunc_normal_\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n\n    def __init__(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Copied and modified from\n# https://raw.githubusercontent.com/SwinTransformer/Video-Swin-Transformer/master/mmaction/models/backbones/swin_transformer.py\n\nimport logging\nfrom functools import lru_cache, reduce\nfrom operator import mul\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom einops import rearrange\nfrom omnivision.utils.checkpoint import load_and_broadcast_checkpoint_list\nfrom timm.models.layers import DropPath, trunc_normal_\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\nAST=Module(Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load)))))))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargargargargConstantConstantAttribute(Name(Load)Load)Constant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Assign(Name(Store)BoolOp(OrName(Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom einops import rearrange\nfrom omnivision.utils.checkpoint import load_and_broadcast_checkpoint_list\nfrom timm.models.layers import DropPath, trunc_normal_\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load)))))))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargargargargConstantConstantAttribute(Name(Load)Load)Constant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Assign(Name(Store)BoolOp(OrName(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n\n    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(\n        B,\n        D // window_size[0],\n        window_size[0],\n        H // window_size[1],\n        window_size[1],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n\n    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(\n        B,\n        D // window_size[0],\n        window_size[0],\n        H // window_size[1],\n        window_size[1],\n        W // window_size[2],\n        window_size[2],\n        C,\n    )\n    windows = (\n        x.permute(0, 1, 3, 5, 2, 4, 6, 7)\n        .contiguous()\n        .view(-1, reduce(mul, window_size), C)\n    )\n    return windows", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n\n    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(\n        B,\n        D // window_size[0],\n        window_size[0],\n        H // window_size[1],\n        window_size[1],\n        W // window_size[2],\n        window_size[2],\n        C,\n    )\n    windows = (\n        x.permute(0, 1, 3, 5, 2, 4, 6, 7)\n        .contiguous()\n        .view(-1, reduce(mul, window_size), C)\n    )\n    return windows\n\n\ndef window_partition_image(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n\n    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(\n        B,\n        D // window_size[0],\n        window_size[0],\n        H // window_size[1],\n        window_size[1],\n        W // window_size[2],\n        window_size[2],\n        C,\n    )\n    windows = (\n        x.permute(0, 1, 3, 5, 2, 4, 6, 7)\n        .contiguous()\n        .view(-1, reduce(mul, window_size), C)\n    )\n    return windows\n\n\ndef window_partition_image(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(\n        B, H // window_size[1], window_size[1], W // window_size[2], window_size[2], C\n    )\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5)\n        .contiguous()\n        .view(-1, window_size[1], window_size[2], C)\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(\n        B,\n        D // window_size[0],\n        window_size[0],\n        H // window_size[1],\n        window_size[1],\n        W // window_size[2],\n        window_size[2],\n        C,\n    )\n    windows = (\n        x.permute(0, 1, 3, 5, 2, 4, 6, 7)\n        .contiguous()\n        .view(-1, reduce(mul, window_size), C)\n    )\n    return windows\n\n\ndef window_partition_image(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(\n        B, H // window_size[1], window_size[1], W // window_size[2], window_size[2], C\n    )\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5)\n        .contiguous()\n        .view(-1, window_size[1], window_size[2], C)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        W // window_size[2],\n        window_size[2],\n        C,\n    )\n    windows = (\n        x.permute(0, 1, 3, 5, 2, 4, 6, 7)\n        .contiguous()\n        .view(-1, reduce(mul, window_size), C)\n    )\n    return windows\n\n\ndef window_partition_image(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(\n        B, H // window_size[1], window_size[1], W // window_size[2], window_size[2], C\n    )\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5)\n        .contiguous()\n        .view(-1, window_size[1], window_size[2], C)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.view(\n        B,\n        D // window_size[0],\n        H // window_size[1],\n        W // window_size[2],\n        window_size[0],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef window_partition_image(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(\n        B, H // window_size[1], window_size[1], W // window_size[2], window_size[2], C\n    )\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5)\n        .contiguous()\n        .view(-1, window_size[1], window_size[2], C)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.view(\n        B,\n        D // window_size[0],\n        H // window_size[1],\n        W // window_size[2],\n        window_size[0],\n        window_size[1],\n        window_size[2],\n        -1,\n    )\n    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n    return x\n\n\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n\nAST=Module(FunctionDef(arguments(argarg)Expr(Constant)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))Subscript(Name(Load)ConstantLoad)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))Subscript(Name(Load)ConstantLoad)Name(Load)))Assign(Name(Store)Call(Attribute(Call(Attribute(Call(Attribute(Name(Load)Load)ConstantConstantConstantConstantConstantConstant)Load))Load)UnaryOp(USubConstant)Subscript(Name(Load)ConstantLoad)Subscript(Name(Load)ConstantLoad)Name(Load)))Return(Name(Load)))FunctionDef(arguments(argargargargargarg)Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))BinOp(Name(Load)FloorDivSubscript(Name(Load)ConstantLoad))Subscript(Name(Load)ConstantLoad)Subscript(Name(Load)ConstantLoad)Subscript(Name(Load)ConstantLoad)UnaryOp(USubConstant)))Assign(Name(Store)Call(Attribute(Call(Attribute(Call(Attribute(Name(Load)Load)ConstantConstantConstantConstantConstantConstantConstantConstant)Load))Load)Name(Load)Name(Load)Name(Load)Name(Load)UnaryOp(USubConstant)))Return(Name(Load)))FunctionDef(arguments(argargargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(\n        B, H // window_size[1], window_size[1], W // window_size[2], window_size[2], C\n    )\n    windows = (\n        x.permute(0, 1, 3, 2, 4, 5)\n        .contiguous()\n        .view(-1, window_size[1], window_size[2], C)\n    )\n    return windows\n\n\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.view(\n        B,\n        D // window_size[0],\n        H // window_size[1],\n        W // window_size[2],\n        window_size[0],\n        window_size[1],\n        window_size[2],\n        -1,\n    )\n    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n    return x\n\n\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n\n    if shift_size is None:\n        return tuple(use_window_size)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return windows\n\n\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.view(\n        B,\n        D // window_size[0],\n        H // window_size[1],\n        W // window_size[2],\n        window_size[0],\n        window_size[1],\n        window_size[2],\n        -1,\n    )\n    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n    return x\n\n\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n\n    if shift_size is None:\n        return tuple(use_window_size)\n    else:\n        return tuple(use_window_size), tuple(use_shift_size)\n\n\nclass WindowAttention3D(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.view(\n        B,\n        D // window_size[0],\n        H // window_size[1],\n        W // window_size[2],\n        window_size[0],\n        window_size[1],\n        window_size[2],\n        -1,\n    )\n    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n    return x\n\n\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n\n    if shift_size is None:\n        return tuple(use_window_size)\n    else:\n        return tuple(use_window_size), tuple(use_shift_size)\n\n\nclass WindowAttention3D(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        window_size[1],\n        window_size[2],\n        -1,\n    )\n    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n    return x\n\n\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n\n    if shift_size is None:\n        return tuple(use_window_size)\n    else:\n        return tuple(use_window_size), tuple(use_shift_size)\n\n\nclass WindowAttention3D(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n\n        super().__init__()\n        self.dim = dim", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n\n    if shift_size is None:\n        return tuple(use_window_size)\n    else:\n        return tuple(use_window_size), tuple(use_shift_size)\n\n\nclass WindowAttention3D(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(\n                (2 * window_size[0] - 1)\n                * (2 * window_size[1] - 1)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    else:\n        return tuple(use_window_size), tuple(use_shift_size)\n\n\nclass WindowAttention3D(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(\n                (2 * window_size[0] - 1)\n                * (2 * window_size[1] - 1)\n                * (2 * window_size[2] - 1),\n                num_heads,\n            )\n        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_d = torch.arange(self.window_size[0])\n        coords_h = torch.arange(self.window_size[1])\n        coords_w = torch.arange(self.window_size[2])\n        coords = torch.stack(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        window_size,\n        num_heads,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(\n                (2 * window_size[0] - 1)\n                * (2 * window_size[1] - 1)\n                * (2 * window_size[2] - 1),\n                num_heads,\n            )\n        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_d = torch.arange(self.window_size[0])\n        coords_h = torch.arange(self.window_size[1])\n        coords_w = torch.arange(self.window_size[2])\n        coords = torch.stack(\n            torch.meshgrid(coords_d, coords_h, coords_w)\n        )  # 3, Wd, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        window_size,\n        num_heads,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(\n                (2 * window_size[0] - 1)\n                * (2 * window_size[1] - 1)\n                * (2 * window_size[2] - 1),\n                num_heads,\n            )\n        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_d = torch.arange(self.window_size[0])\n        coords_h = torch.arange(self.window_size[1])\n        coords_w = torch.arange(self.window_size[2])\n        coords = torch.stack(\n            torch.meshgrid(coords_d, coords_h, coords_w)\n        )  # 3, Wd, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (\n            2 * self.window_size[2] - 1\n        )\n        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(\n                (2 * window_size[0] - 1)\n                * (2 * window_size[1] - 1)\n                * (2 * window_size[2] - 1),\n                num_heads,\n            )\n        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_d = torch.arange(self.window_size[0])\n        coords_h = torch.arange(self.window_size[1])\n        coords_w = torch.arange(self.window_size[2])\n        coords = torch.stack(\n            torch.meshgrid(coords_d, coords_h, coords_w)\n        )  # 3, Wd, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (\n            2 * self.window_size[2] - 1\n        )\n        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"Forward function.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                * (2 * window_size[2] - 1),\n                num_heads,\n            )\n        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_d = torch.arange(self.window_size[0])\n        coords_h = torch.arange(self.window_size[1])\n        coords_w = torch.arange(self.window_size[2])\n        coords = torch.stack(\n            torch.meshgrid(coords_d, coords_h, coords_w)\n        )  # 3, Wd, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (\n            2 * self.window_size[2] - 1\n        )\n        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"Forward function.\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            torch.meshgrid(coords_d, coords_h, coords_w)\n        )  # 3, Wd, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = (\n            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        )  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0\n        ).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (\n            2 * self.window_size[2] - 1\n        )\n        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"Forward function.\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index[:N, :N].reshape(-1)\n        ].reshape(\n            N, N, -1\n        )  # Wd*Wh*Ww,Wd*Wh*Ww,nH", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (\n            2 * self.window_size[2] - 1\n        )\n        relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"Forward function.\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index[:N, :N].reshape(-1)\n        ].reshape(\n            N, N, -1\n        )  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"Forward function.\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index[:N, :N].reshape(-1)\n        ].reshape(\n            N, N, -1\n        )  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index[:N, :N].reshape(-1)\n        ].reshape(\n            N, N, -1\n        )  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock3D(nn.Module):\n    \"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index[:N, :N].reshape(-1)\n        ].reshape(\n            N, N, -1\n        )  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock3D(nn.Module):\n    \"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1\n        ).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n                1\n            ).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock3D(nn.Module):\n    \"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=(2, 7, 7),\n        shift_size=(0, 0, 0),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock3D(nn.Module):\n    \"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=(2, 7, 7),\n        shift_size=(0, 0, 0),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return x\n\n\nclass SwinTransformerBlock3D(nn.Module):\n    \"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=(2, 7, 7),\n        shift_size=(0, 0, 0),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n\n        assert (\n            0 <= self.shift_size[0] < self.window_size[0]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[1] < self.window_size[1]\n        ), \"shift_size must in 0-window_size\"\n        assert (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=(2, 7, 7),\n        shift_size=(0, 0, 0),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n\n        assert (\n            0 <= self.shift_size[0] < self.window_size[0]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[1] < self.window_size[1]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[2] < self.window_size[2]\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(\n            dim,\n            window_size=self.window_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=(2, 7, 7),\n        shift_size=(0, 0, 0),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n\n        assert (\n            0 <= self.shift_size[0] < self.window_size[0]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[1] < self.window_size[1]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[2] < self.window_size[2]\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(\n            dim,\n            window_size=self.window_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n\n        assert (\n            0 <= self.shift_size[0] < self.window_size[0]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[1] < self.window_size[1]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[2] < self.window_size[2]\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(\n            dim,\n            window_size=self.window_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward_part1(self, x, mask_matrix):\n        B, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n\n        assert (\n            0 <= self.shift_size[0] < self.window_size[0]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[1] < self.window_size[1]\n        ), \"shift_size must in 0-window_size\"\n        assert (\n            0 <= self.shift_size[2] < self.window_size[2]\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(\n            dim,\n            window_size=self.window_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward_part1(self, x, mask_matrix):\n        B, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n\n        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            0 <= self.shift_size[2] < self.window_size[2]\n        ), \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(\n            dim,\n            window_size=self.window_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward_part1(self, x, mask_matrix):\n        B, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n\n        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):\n            shifted_x = torch.roll(\n                x,\n                shifts=(-shift_size[0], -shift_size[1], -shift_size[2]),\n                dims=(1, 2, 3),\n            )\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward_part1(self, x, mask_matrix):\n        B, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n\n        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):\n            shifted_x = torch.roll(\n                x,\n                shifts=(-shift_size[0], -shift_size[1], -shift_size[2]),\n                dims=(1, 2, 3),\n            )\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows\n        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n        shifted_x = window_reverse(\n            attn_windows, window_size, B, Dp, Hp, Wp\n        )  # B D' H' W' C\n        # reverse cyclic shift\n        if any(i > 0 for i in shift_size):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward_part1(self, x, mask_matrix):\n        B, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n\n        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):\n            shifted_x = torch.roll(\n                x,\n                shifts=(-shift_size[0], -shift_size[1], -shift_size[2]),\n                dims=(1, 2, 3),\n            )\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows\n        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n        shifted_x = window_reverse(\n            attn_windows, window_size, B, Dp, Hp, Wp\n        )  # B D' H' W' C\n        # reverse cyclic shift\n        if any(i > 0 for i in shift_size):\n            x = torch.roll(\n                shifted_x,\n                shifts=(shift_size[0], shift_size[1], shift_size[2]),\n                dims=(1, 2, 3),\n            )\n        else:\n            x = shifted_x\n\n        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :].contiguous()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):\n            shifted_x = torch.roll(\n                x,\n                shifts=(-shift_size[0], -shift_size[1], -shift_size[2]),\n                dims=(1, 2, 3),\n            )\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows\n        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n        shifted_x = window_reverse(\n            attn_windows, window_size, B, Dp, Hp, Wp\n        )  # B D' H' W' C\n        # reverse cyclic shift\n        if any(i > 0 for i in shift_size):\n            x = torch.roll(\n                shifted_x,\n                shifts=(shift_size[0], shift_size[1], shift_size[2]),\n                dims=(1, 2, 3),\n            )\n        else:\n            x = shifted_x\n\n        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :].contiguous()\n        return x\n\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n\n    def forward(self, x, mask_matrix, use_checkpoint=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            shifted_x = torch.roll(\n                x,\n                shifts=(-shift_size[0], -shift_size[1], -shift_size[2]),\n                dims=(1, 2, 3),\n            )\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows\n        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n        shifted_x = window_reverse(\n            attn_windows, window_size, B, Dp, Hp, Wp\n        )  # B D' H' W' C\n        # reverse cyclic shift\n        if any(i > 0 for i in shift_size):\n            x = torch.roll(\n                shifted_x,\n                shifts=(shift_size[0], shift_size[1], shift_size[2]),\n                dims=(1, 2, 3),\n            )\n        else:\n            x = shifted_x\n\n        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :].contiguous()\n        return x\n\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n\n    def forward(self, x, mask_matrix, use_checkpoint=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n\n        shortcut = x\n        if use_checkpoint:\n            x = checkpoint.checkpoint(\n                self.forward_part1, x, mask_matrix, use_reentrant=False\n            )\n        else:\n            x = self.forward_part1(x, mask_matrix)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_365-415"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n        shifted_x = window_reverse(\n            attn_windows, window_size, B, Dp, Hp, Wp\n        )  # B D' H' W' C\n        # reverse cyclic shift\n        if any(i > 0 for i in shift_size):\n            x = torch.roll(\n                shifted_x,\n                shifts=(shift_size[0], shift_size[1], shift_size[2]),\n                dims=(1, 2, 3),\n            )\n        else:\n            x = shifted_x\n\n        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :].contiguous()\n        return x\n\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n\n    def forward(self, x, mask_matrix, use_checkpoint=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n\n        shortcut = x\n        if use_checkpoint:\n            x = checkpoint.checkpoint(\n                self.forward_part1, x, mask_matrix, use_reentrant=False\n            )\n        else:\n            x = self.forward_part1(x, mask_matrix)\n        x = shortcut + self.drop_path(x)\n\n        if use_checkpoint:\n            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n        else:\n            x = x + self.forward_part2(x)\n\n        return x\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_375-425"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            x = torch.roll(\n                shifted_x,\n                shifts=(shift_size[0], shift_size[1], shift_size[2]),\n                dims=(1, 2, 3),\n            )\n        else:\n            x = shifted_x\n\n        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :].contiguous()\n        return x\n\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n\n    def forward(self, x, mask_matrix, use_checkpoint=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n\n        shortcut = x\n        if use_checkpoint:\n            x = checkpoint.checkpoint(\n                self.forward_part1, x, mask_matrix, use_reentrant=False\n            )\n        else:\n            x = self.forward_part1(x, mask_matrix)\n        x = shortcut + self.drop_path(x)\n\n        if use_checkpoint:\n            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n        else:\n            x = x + self.forward_part2(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_385-435"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return x\n\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n\n    def forward(self, x, mask_matrix, use_checkpoint=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n\n        shortcut = x\n        if use_checkpoint:\n            x = checkpoint.checkpoint(\n                self.forward_part1, x, mask_matrix, use_reentrant=False\n            )\n        else:\n            x = self.forward_part1(x, mask_matrix)\n        x = shortcut + self.drop_path(x)\n\n        if use_checkpoint:\n            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n        else:\n            x = x + self.forward_part2(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x, H=None, W=None):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_395-445"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n\n        shortcut = x\n        if use_checkpoint:\n            x = checkpoint.checkpoint(\n                self.forward_part1, x, mask_matrix, use_reentrant=False\n            )\n        else:\n            x = self.forward_part1(x, mask_matrix)\n        x = shortcut + self.drop_path(x)\n\n        if use_checkpoint:\n            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n        else:\n            x = x + self.forward_part2(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x, H=None, W=None):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"\n        if H is None:\n            B, D, H, W, C = x.shape\n\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_405-455"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x = shortcut + self.drop_path(x)\n\n        if use_checkpoint:\n            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n        else:\n            x = x + self.forward_part2(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x, H=None, W=None):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"\n        if H is None:\n            B, D, H, W, C = x.shape\n\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_415-465"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x, H=None, W=None):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"\n        if H is None:\n            B, D, H, W, C = x.shape\n\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\n# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in (\n        slice(-window_size[0]),\n        slice(-window_size[0], -shift_size[0]),\n        slice(-shift_size[0], None),\n    ):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_425-475"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x, H=None, W=None):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"\n        if H is None:\n            B, D, H, W, C = x.shape\n\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\n# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in (\n        slice(-window_size[0]),\n        slice(-window_size[0], -shift_size[0]),\n        slice(-shift_size[0], None),\n    ):\n        for h in (\n            slice(-window_size[1]),\n            slice(-window_size[1], -shift_size[1]),\n            slice(-shift_size[1], None),\n        ):\n            for w in (\n                slice(-window_size[2]),\n                slice(-window_size[2], -shift_size[2]),\n                slice(-shift_size[2], None),\n            ):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_435-485"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if H is None:\n            B, D, H, W, C = x.shape\n\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\n# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in (\n        slice(-window_size[0]),\n        slice(-window_size[0], -shift_size[0]),\n        slice(-shift_size[0], None),\n    ):\n        for h in (\n            slice(-window_size[1]),\n            slice(-window_size[1], -shift_size[1]),\n            slice(-shift_size[1], None),\n        ):\n            for w in (\n                slice(-window_size[2]),\n                slice(-window_size[2], -shift_size[2]),\n                slice(-shift_size[2], None),\n            ):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n        attn_mask == 0, float(0.0)\n    )\n    return attn_mask\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_445-495"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\n# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in (\n        slice(-window_size[0]),\n        slice(-window_size[0], -shift_size[0]),\n        slice(-shift_size[0], None),\n    ):\n        for h in (\n            slice(-window_size[1]),\n            slice(-window_size[1], -shift_size[1]),\n            slice(-shift_size[1], None),\n        ):\n            for w in (\n                slice(-window_size[2]),\n                slice(-window_size[2], -shift_size[2]),\n                slice(-shift_size[2], None),\n            ):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n        attn_mask == 0, float(0.0)\n    )\n    return attn_mask\n\n\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_455-505"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in (\n        slice(-window_size[0]),\n        slice(-window_size[0], -shift_size[0]),\n        slice(-shift_size[0], None),\n    ):\n        for h in (\n            slice(-window_size[1]),\n            slice(-window_size[1], -shift_size[1]),\n            slice(-shift_size[1], None),\n        ):\n            for w in (\n                slice(-window_size[2]),\n                slice(-window_size[2], -shift_size[2]),\n                slice(-shift_size[2], None),\n            ):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n        attn_mask == 0, float(0.0)\n    )\n    return attn_mask\n\n\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_465-515"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for h in (\n            slice(-window_size[1]),\n            slice(-window_size[1], -shift_size[1]),\n            slice(-shift_size[1], None),\n        ):\n            for w in (\n                slice(-window_size[2]),\n                slice(-window_size[2], -shift_size[2]),\n                slice(-shift_size[2], None),\n            ):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n        attn_mask == 0, float(0.0)\n    )\n    return attn_mask\n\n\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        depth,\n        num_heads,\n        window_size=(1, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_475-525"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n        attn_mask == 0, float(0.0)\n    )\n    return attn_mask\n\n\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        depth,\n        num_heads,\n        window_size=(1, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n\n        # build blocks", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_485-535"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        depth,\n        num_heads,\n        window_size=(1, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock3D(\n                    dim=dim,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_495-545"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        depth,\n        num_heads,\n        window_size=(1, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock3D(\n                    dim=dim,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_505-555"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self,\n        dim,\n        depth,\n        num_heads,\n        window_size=(1, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock3D(\n                    dim=dim,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n\n    def forward(self, x, use_checkpoint=False, H=None, W=None, use_seg=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_515-565"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path=0.0,\n        norm_layer=nn.LayerNorm,\n        downsample=None,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n\n        # build blocks\n        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock3D(\n                    dim=dim,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n\n    def forward(self, x, use_checkpoint=False, H=None, W=None, use_seg=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).\n        \"\"\"\n        if use_seg:\n            return self.forward_seg(x, H, W)\n        # calculate attention mask for SW-MSA\n        B, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n        x = rearrange(x, \"b c d h w -> b d h w c\")\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_525-575"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.blocks = nn.ModuleList(\n            [\n                SwinTransformerBlock3D(\n                    dim=dim,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n\n    def forward(self, x, use_checkpoint=False, H=None, W=None, use_seg=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).\n        \"\"\"\n        if use_seg:\n            return self.forward_seg(x, H, W)\n        # calculate attention mask for SW-MSA\n        B, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n        x = rearrange(x, \"b c d h w -> b d h w c\")\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n        for blk in self.blocks:\n            x = blk(x, attn_mask, use_checkpoint=use_checkpoint)\n        x = x.view(B, D, H, W, -1)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = rearrange(x, \"b d h w c -> b c d h w\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_535-585"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    drop=drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i]\n                    if isinstance(drop_path, list)\n                    else drop_path,\n                    norm_layer=norm_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n\n    def forward(self, x, use_checkpoint=False, H=None, W=None, use_seg=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).\n        \"\"\"\n        if use_seg:\n            return self.forward_seg(x, H, W)\n        # calculate attention mask for SW-MSA\n        B, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n        x = rearrange(x, \"b c d h w -> b d h w c\")\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n        for blk in self.blocks:\n            x = blk(x, attn_mask, use_checkpoint=use_checkpoint)\n        x = x.view(B, D, H, W, -1)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = rearrange(x, \"b d h w c -> b c d h w\")\n        return x\n\n    def forward_seg(self, x, H, W):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, H*W, C).\n            H, W: Spatial resolution of the input feature.\n        \"\"\"\n        # calculate attention mask for SW-MSA", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_545-595"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n\n    def forward(self, x, use_checkpoint=False, H=None, W=None, use_seg=False):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).\n        \"\"\"\n        if use_seg:\n            return self.forward_seg(x, H, W)\n        # calculate attention mask for SW-MSA\n        B, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n        x = rearrange(x, \"b c d h w -> b d h w c\")\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n        for blk in self.blocks:\n            x = blk(x, attn_mask, use_checkpoint=use_checkpoint)\n        x = x.view(B, D, H, W, -1)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = rearrange(x, \"b d h w c -> b c d h w\")\n        return x\n\n    def forward_seg(self, x, H, W):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, H*W, C).\n            H, W: Spatial resolution of the input feature.\n        \"\"\"\n        # calculate attention mask for SW-MSA\n        Hp = int(np.ceil(H / self.window_size[1])) * self.window_size[1]\n        Wp = int(np.ceil(W / self.window_size[2])) * self.window_size[2]\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n        h_slices = (\n            slice(0, -self.window_size[1]),\n            slice(-self.window_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n        w_slices = (\n            slice(0, -self.window_size[2]),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_555-605"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        if use_seg:\n            return self.forward_seg(x, H, W)\n        # calculate attention mask for SW-MSA\n        B, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size(\n            (D, H, W), self.window_size, self.shift_size\n        )\n        x = rearrange(x, \"b c d h w -> b d h w c\")\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n        for blk in self.blocks:\n            x = blk(x, attn_mask, use_checkpoint=use_checkpoint)\n        x = x.view(B, D, H, W, -1)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = rearrange(x, \"b d h w c -> b c d h w\")\n        return x\n\n    def forward_seg(self, x, H, W):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, H*W, C).\n            H, W: Spatial resolution of the input feature.\n        \"\"\"\n        # calculate attention mask for SW-MSA\n        Hp = int(np.ceil(H / self.window_size[1])) * self.window_size[1]\n        Wp = int(np.ceil(W / self.window_size[2])) * self.window_size[2]\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n        h_slices = (\n            slice(0, -self.window_size[1]),\n            slice(-self.window_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n        w_slices = (\n            slice(0, -self.window_size[2]),\n            slice(-self.window_size[2], -self.shift_size[2]),\n            slice(-self.shift_size[2], None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition_image(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_565-615"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n        for blk in self.blocks:\n            x = blk(x, attn_mask, use_checkpoint=use_checkpoint)\n        x = x.view(B, D, H, W, -1)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = rearrange(x, \"b d h w c -> b c d h w\")\n        return x\n\n    def forward_seg(self, x, H, W):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, H*W, C).\n            H, W: Spatial resolution of the input feature.\n        \"\"\"\n        # calculate attention mask for SW-MSA\n        Hp = int(np.ceil(H / self.window_size[1])) * self.window_size[1]\n        Wp = int(np.ceil(W / self.window_size[2])) * self.window_size[2]\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n        h_slices = (\n            slice(0, -self.window_size[1]),\n            slice(-self.window_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n        w_slices = (\n            slice(0, -self.window_size[2]),\n            slice(-self.window_size[2], -self.shift_size[2]),\n            slice(-self.shift_size[2], None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition_image(\n            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size[1] * self.window_size[2])\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        for blk in self.blocks:\n            blk.H, blk.W = H, W", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_575-625"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return x\n\n    def forward_seg(self, x, H, W):\n        \"\"\"Forward function.\n\n        Args:\n            x: Input feature, tensor size (B, H*W, C).\n            H, W: Spatial resolution of the input feature.\n        \"\"\"\n        # calculate attention mask for SW-MSA\n        Hp = int(np.ceil(H / self.window_size[1])) * self.window_size[1]\n        Wp = int(np.ceil(W / self.window_size[2])) * self.window_size[2]\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n        h_slices = (\n            slice(0, -self.window_size[1]),\n            slice(-self.window_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n        w_slices = (\n            slice(0, -self.window_size[2]),\n            slice(-self.window_size[2], -self.shift_size[2]),\n            slice(-self.shift_size[2], None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition_image(\n            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size[1] * self.window_size[2])\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        for blk in self.blocks:\n            blk.H, blk.W = H, W\n            if x.ndim == 4:\n                B, D, C, L = x.shape\n                assert L == H * W, \"input feature has wrong size\"\n                x = x.reshape(B, D, C, H, W)\n                x = x.permute(0, 1, 3, 4, 2)\n            assert x.shape[2] == H\n            assert x.shape[3] == W\n            x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x_down = self.downsample(x, H, W)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_585-635"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        Hp = int(np.ceil(H / self.window_size[1])) * self.window_size[1]\n        Wp = int(np.ceil(W / self.window_size[2])) * self.window_size[2]\n        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n        h_slices = (\n            slice(0, -self.window_size[1]),\n            slice(-self.window_size[1], -self.shift_size[1]),\n            slice(-self.shift_size[1], None),\n        )\n        w_slices = (\n            slice(0, -self.window_size[2]),\n            slice(-self.window_size[2], -self.shift_size[2]),\n            slice(-self.shift_size[2], None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition_image(\n            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size[1] * self.window_size[2])\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        for blk in self.blocks:\n            blk.H, blk.W = H, W\n            if x.ndim == 4:\n                B, D, C, L = x.shape\n                assert L == H * W, \"input feature has wrong size\"\n                x = x.reshape(B, D, C, H, W)\n                x = x.permute(0, 1, 3, 4, 2)\n            assert x.shape[2] == H\n            assert x.shape[3] == W\n            x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x_down = self.downsample(x, H, W)\n            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n            return x, H, W, x_down, Wh, Ww\n        else:\n            return x, H, W, x, H, W\n\n\nclass PatchEmbed3D(nn.Module):\n    \"\"\"Video to Patch Embedding.\n\n    Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_595-645"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            slice(-self.window_size[2], -self.shift_size[2]),\n            slice(-self.shift_size[2], None),\n        )\n        cnt = 0\n        for h in h_slices:\n            for w in w_slices:\n                img_mask[:, h, w, :] = cnt\n                cnt += 1\n\n        mask_windows = window_partition_image(\n            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size[1] * self.window_size[2])\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        for blk in self.blocks:\n            blk.H, blk.W = H, W\n            if x.ndim == 4:\n                B, D, C, L = x.shape\n                assert L == H * W, \"input feature has wrong size\"\n                x = x.reshape(B, D, C, H, W)\n                x = x.permute(0, 1, 3, 4, 2)\n            assert x.shape[2] == H\n            assert x.shape[3] == W\n            x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x_down = self.downsample(x, H, W)\n            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n            return x, H, W, x_down, Wh, Ww\n        else:\n            return x, H, W, x, H, W\n\n\nclass PatchEmbed3D(nn.Module):\n    \"\"\"Video to Patch Embedding.\n\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size=(2, 4, 4),\n        in_chans=3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_605-655"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            img_mask, self.window_size\n        )  # nW, window_size, window_size, 1\n        mask_windows = mask_windows.view(-1, self.window_size[1] * self.window_size[2])\n        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n            attn_mask == 0, float(0.0)\n        )\n\n        for blk in self.blocks:\n            blk.H, blk.W = H, W\n            if x.ndim == 4:\n                B, D, C, L = x.shape\n                assert L == H * W, \"input feature has wrong size\"\n                x = x.reshape(B, D, C, H, W)\n                x = x.permute(0, 1, 3, 4, 2)\n            assert x.shape[2] == H\n            assert x.shape[3] == W\n            x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x_down = self.downsample(x, H, W)\n            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n            return x, H, W, x_down, Wh, Ww\n        else:\n            return x, H, W, x, H, W\n\n\nclass PatchEmbed3D(nn.Module):\n    \"\"\"Video to Patch Embedding.\n\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size=(2, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        norm_layer=None,\n        additional_variable_channels=None,\n    ):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.additional_variable_channels = additional_variable_channels", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_615-665"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if x.ndim == 4:\n                B, D, C, L = x.shape\n                assert L == H * W, \"input feature has wrong size\"\n                x = x.reshape(B, D, C, H, W)\n                x = x.permute(0, 1, 3, 4, 2)\n            assert x.shape[2] == H\n            assert x.shape[3] == W\n            x = blk(x, attn_mask)\n        if self.downsample is not None:\n            x_down = self.downsample(x, H, W)\n            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n            return x, H, W, x_down, Wh, Ww\n        else:\n            return x, H, W, x, H, W\n\n\nclass PatchEmbed3D(nn.Module):\n    \"\"\"Video to Patch Embedding.\n\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size=(2, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        norm_layer=None,\n        additional_variable_channels=None,\n    ):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.additional_variable_channels = additional_variable_channels\n\n        self.proj = nn.Conv3d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n        if additional_variable_channels:\n            # we create var_proj separately from proj\n            # this makes it convenient to ignore var_proj on downstream tasks\n            # where we only use RGB\n            self.var_proj = [\n                nn.Conv3d(x, embed_dim, kernel_size=patch_size, stride=patch_size)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_625-675"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n            return x, H, W, x_down, Wh, Ww\n        else:\n            return x, H, W, x, H, W\n\n\nclass PatchEmbed3D(nn.Module):\n    \"\"\"Video to Patch Embedding.\n\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size=(2, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        norm_layer=None,\n        additional_variable_channels=None,\n    ):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.additional_variable_channels = additional_variable_channels\n\n        self.proj = nn.Conv3d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n        if additional_variable_channels:\n            # we create var_proj separately from proj\n            # this makes it convenient to ignore var_proj on downstream tasks\n            # where we only use RGB\n            self.var_proj = [\n                nn.Conv3d(x, embed_dim, kernel_size=patch_size, stride=patch_size)\n                for x in additional_variable_channels\n            ]\n            self.var_proj = nn.ModuleList(self.var_proj)\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def run_variable_channel_forward(self, x):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_635-685"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size=(2, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        norm_layer=None,\n        additional_variable_channels=None,\n    ):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.additional_variable_channels = additional_variable_channels\n\n        self.proj = nn.Conv3d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n        if additional_variable_channels:\n            # we create var_proj separately from proj\n            # this makes it convenient to ignore var_proj on downstream tasks\n            # where we only use RGB\n            self.var_proj = [\n                nn.Conv3d(x, embed_dim, kernel_size=patch_size, stride=patch_size)\n                for x in additional_variable_channels\n            ]\n            self.var_proj = nn.ModuleList(self.var_proj)\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def run_variable_channel_forward(self, x):\n        sidx = 0\n        out = None\n        for idx in range(len(self.additional_variable_channels)):\n            eidx = sidx + self.additional_variable_channels[idx]\n            c_out = self.var_proj[idx](x[:, sidx:eidx, ...])\n            if idx == 0:\n                out = c_out\n            else:\n                out += c_out\n            sidx = eidx", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_645-695"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        embed_dim=96,\n        norm_layer=None,\n        additional_variable_channels=None,\n    ):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.additional_variable_channels = additional_variable_channels\n\n        self.proj = nn.Conv3d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n        if additional_variable_channels:\n            # we create var_proj separately from proj\n            # this makes it convenient to ignore var_proj on downstream tasks\n            # where we only use RGB\n            self.var_proj = [\n                nn.Conv3d(x, embed_dim, kernel_size=patch_size, stride=patch_size)\n                for x in additional_variable_channels\n            ]\n            self.var_proj = nn.ModuleList(self.var_proj)\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def run_variable_channel_forward(self, x):\n        sidx = 0\n        out = None\n        for idx in range(len(self.additional_variable_channels)):\n            eidx = sidx + self.additional_variable_channels[idx]\n            c_out = self.var_proj[idx](x[:, sidx:eidx, ...])\n            if idx == 0:\n                out = c_out\n            else:\n                out += c_out\n            sidx = eidx\n        return out\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        # padding\n        _, _, D, H, W = x.size()\n        if W % self.patch_size[2] != 0:\n            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n        if H % self.patch_size[1] != 0:\n            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_655-705"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        self.proj = nn.Conv3d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n        if additional_variable_channels:\n            # we create var_proj separately from proj\n            # this makes it convenient to ignore var_proj on downstream tasks\n            # where we only use RGB\n            self.var_proj = [\n                nn.Conv3d(x, embed_dim, kernel_size=patch_size, stride=patch_size)\n                for x in additional_variable_channels\n            ]\n            self.var_proj = nn.ModuleList(self.var_proj)\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def run_variable_channel_forward(self, x):\n        sidx = 0\n        out = None\n        for idx in range(len(self.additional_variable_channels)):\n            eidx = sidx + self.additional_variable_channels[idx]\n            c_out = self.var_proj[idx](x[:, sidx:eidx, ...])\n            if idx == 0:\n                out = c_out\n            else:\n                out += c_out\n            sidx = eidx\n        return out\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        # padding\n        _, _, D, H, W = x.size()\n        if W % self.patch_size[2] != 0:\n            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n        if H % self.patch_size[1] != 0:\n            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n        if D % self.patch_size[0] != 0:\n            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n\n        if self.additional_variable_channels:\n            x_rgb = x[:, :3, ...]\n            x_rem = x[:, 3:, ...]\n            x_rgb = self.proj(x_rgb)\n            if x.shape[1] > 3:\n                x_rem = self.run_variable_channel_forward(x_rem)\n                x = x_rgb + x_rem", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_665-715"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                for x in additional_variable_channels\n            ]\n            self.var_proj = nn.ModuleList(self.var_proj)\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def run_variable_channel_forward(self, x):\n        sidx = 0\n        out = None\n        for idx in range(len(self.additional_variable_channels)):\n            eidx = sidx + self.additional_variable_channels[idx]\n            c_out = self.var_proj[idx](x[:, sidx:eidx, ...])\n            if idx == 0:\n                out = c_out\n            else:\n                out += c_out\n            sidx = eidx\n        return out\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        # padding\n        _, _, D, H, W = x.size()\n        if W % self.patch_size[2] != 0:\n            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n        if H % self.patch_size[1] != 0:\n            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n        if D % self.patch_size[0] != 0:\n            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n\n        if self.additional_variable_channels:\n            x_rgb = x[:, :3, ...]\n            x_rem = x[:, 3:, ...]\n            x_rgb = self.proj(x_rgb)\n            if x.shape[1] > 3:\n                x_rem = self.run_variable_channel_forward(x_rem)\n                x = x_rgb + x_rem\n            else:\n                x = x_rgb\n        else:\n            x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:\n            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n            x = x.flatten(2).transpose(1, 2)\n            x = self.norm(x)\n            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_675-725"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        sidx = 0\n        out = None\n        for idx in range(len(self.additional_variable_channels)):\n            eidx = sidx + self.additional_variable_channels[idx]\n            c_out = self.var_proj[idx](x[:, sidx:eidx, ...])\n            if idx == 0:\n                out = c_out\n            else:\n                out += c_out\n            sidx = eidx\n        return out\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        # padding\n        _, _, D, H, W = x.size()\n        if W % self.patch_size[2] != 0:\n            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n        if H % self.patch_size[1] != 0:\n            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n        if D % self.patch_size[0] != 0:\n            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n\n        if self.additional_variable_channels:\n            x_rgb = x[:, :3, ...]\n            x_rem = x[:, 3:, ...]\n            x_rgb = self.proj(x_rgb)\n            if x.shape[1] > 3:\n                x_rem = self.run_variable_channel_forward(x_rem)\n                x = x_rgb + x_rem\n            else:\n                x = x_rgb\n        else:\n            x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:\n            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n            x = x.flatten(2).transpose(1, 2)\n            x = self.norm(x)\n            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n\n        return x\n\n\nclass SwinTransformer3D(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_685-735"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return out\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        # padding\n        _, _, D, H, W = x.size()\n        if W % self.patch_size[2] != 0:\n            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n        if H % self.patch_size[1] != 0:\n            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n        if D % self.patch_size[0] != 0:\n            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n\n        if self.additional_variable_channels:\n            x_rgb = x[:, :3, ...]\n            x_rem = x[:, 3:, ...]\n            x_rgb = self.proj(x_rgb)\n            if x.shape[1] > 3:\n                x_rem = self.run_variable_channel_forward(x_rem)\n                x = x_rgb + x_rem\n            else:\n                x = x_rgb\n        else:\n            x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:\n            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n            x = x.flatten(2).transpose(1, 2)\n            x = self.norm(x)\n            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n\n        return x\n\n\nclass SwinTransformer3D(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_695-745"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if D % self.patch_size[0] != 0:\n            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n\n        if self.additional_variable_channels:\n            x_rgb = x[:, :3, ...]\n            x_rem = x[:, 3:, ...]\n            x_rgb = self.proj(x_rgb)\n            if x.shape[1] > 3:\n                x_rem = self.run_variable_channel_forward(x_rem)\n                x = x_rgb + x_rem\n            else:\n                x = x_rgb\n        else:\n            x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:\n            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n            x = x.flatten(2).transpose(1, 2)\n            x = self.norm(x)\n            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n\n        return x\n\n\nclass SwinTransformer3D(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained=None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_705-755"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            else:\n                x = x_rgb\n        else:\n            x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:\n            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n            x = x.flatten(2).transpose(1, 2)\n            x = self.norm(x)\n            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n\n        return x\n\n\nclass SwinTransformer3D(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained=None,\n        pretrained2d=True,\n        pretrained3d=None,\n        pretrained_model_key=\"base_model\",\n        patch_size=(4, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(2, 7, 7),\n        mlp_ratio=4.0,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_715-765"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return x\n\n\nclass SwinTransformer3D(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained=None,\n        pretrained2d=True,\n        pretrained3d=None,\n        pretrained_model_key=\"base_model\",\n        patch_size=(4, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(2, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n        norm_layer=nn.LayerNorm,\n        patch_norm=False,\n        frozen_stages=-1,\n        depth_mode=None,\n        depth_patch_embed_separate_params=True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_725-775"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained=None,\n        pretrained2d=True,\n        pretrained3d=None,\n        pretrained_model_key=\"base_model\",\n        patch_size=(4, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(2, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n        norm_layer=nn.LayerNorm,\n        patch_norm=False,\n        frozen_stages=-1,\n        depth_mode=None,\n        depth_patch_embed_separate_params=True,\n    ):\n        super().__init__()\n\n        self.im2vid = Im2Video()\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.pretrained3d = pretrained3d\n        self.pretrained_model_key = pretrained_model_key\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_735-785"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained=None,\n        pretrained2d=True,\n        pretrained3d=None,\n        pretrained_model_key=\"base_model\",\n        patch_size=(4, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(2, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n        norm_layer=nn.LayerNorm,\n        patch_norm=False,\n        frozen_stages=-1,\n        depth_mode=None,\n        depth_patch_embed_separate_params=True,\n    ):\n        super().__init__()\n\n        self.im2vid = Im2Video()\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.pretrained3d = pretrained3d\n        self.pretrained_model_key = pretrained_model_key\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n\n        self.depth_mode = depth_mode\n        depth_chans = None\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        # split image into non-overlapping patches", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_745-795"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        pretrained2d=True,\n        pretrained3d=None,\n        pretrained_model_key=\"base_model\",\n        patch_size=(4, 4, 4),\n        in_chans=3,\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(2, 7, 7),\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n        norm_layer=nn.LayerNorm,\n        patch_norm=False,\n        frozen_stages=-1,\n        depth_mode=None,\n        depth_patch_embed_separate_params=True,\n    ):\n        super().__init__()\n\n        self.im2vid = Im2Video()\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.pretrained3d = pretrained3d\n        self.pretrained_model_key = pretrained_model_key\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n\n        self.depth_mode = depth_mode\n        depth_chans = None\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        if depth_mode is not None:\n            msg = f\"Using depth mode {depth_mode}\"\n            logging.info(msg)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_755-805"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.2,\n        norm_layer=nn.LayerNorm,\n        patch_norm=False,\n        frozen_stages=-1,\n        depth_mode=None,\n        depth_patch_embed_separate_params=True,\n    ):\n        super().__init__()\n\n        self.im2vid = Im2Video()\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.pretrained3d = pretrained3d\n        self.pretrained_model_key = pretrained_model_key\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n\n        self.depth_mode = depth_mode\n        depth_chans = None\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        if depth_mode is not None:\n            msg = f\"Using depth mode {depth_mode}\"\n            logging.info(msg)\n            assert depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\", \"rgbd\"]\n            if depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\"]:\n                depth_chans = 1\n                assert (\n                    depth_patch_embed_separate_params\n                ), \"separate tokenization needs separate parameters\"\n                if depth_mode == \"separate_d_tokens\":\n                    raise NotImplementedError()\n            else:\n                assert depth_mode == \"rgbd\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_765-815"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ):\n        super().__init__()\n\n        self.im2vid = Im2Video()\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.pretrained3d = pretrained3d\n        self.pretrained_model_key = pretrained_model_key\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n\n        self.depth_mode = depth_mode\n        depth_chans = None\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        if depth_mode is not None:\n            msg = f\"Using depth mode {depth_mode}\"\n            logging.info(msg)\n            assert depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\", \"rgbd\"]\n            if depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\"]:\n                depth_chans = 1\n                assert (\n                    depth_patch_embed_separate_params\n                ), \"separate tokenization needs separate parameters\"\n                if depth_mode == \"separate_d_tokens\":\n                    raise NotImplementedError()\n            else:\n                assert depth_mode == \"rgbd\"\n                depth_chans = 4\n\n            self.depth_patch_embed_separate_params = depth_patch_embed_separate_params\n\n            if depth_patch_embed_separate_params:\n                self.depth_patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=depth_chans,\n                    embed_dim=embed_dim,\n                    norm_layer=norm_layer if self.patch_norm else None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_775-825"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n\n        self.depth_mode = depth_mode\n        depth_chans = None\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        if depth_mode is not None:\n            msg = f\"Using depth mode {depth_mode}\"\n            logging.info(msg)\n            assert depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\", \"rgbd\"]\n            if depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\"]:\n                depth_chans = 1\n                assert (\n                    depth_patch_embed_separate_params\n                ), \"separate tokenization needs separate parameters\"\n                if depth_mode == \"separate_d_tokens\":\n                    raise NotImplementedError()\n            else:\n                assert depth_mode == \"rgbd\"\n                depth_chans = 4\n\n            self.depth_patch_embed_separate_params = depth_patch_embed_separate_params\n\n            if depth_patch_embed_separate_params:\n                self.depth_patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=depth_chans,\n                    embed_dim=embed_dim,\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n            else:\n                # share parameters with patch_embed\n                # delete the layer we built above\n                del self.patch_embed\n                assert depth_chans == 4\n                logging.info(\n                    \"Certain channels of patch projection may not be used in forward pass\"\n                )\n                logging.info(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_785-835"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None,\n        )\n\n        if depth_mode is not None:\n            msg = f\"Using depth mode {depth_mode}\"\n            logging.info(msg)\n            assert depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\", \"rgbd\"]\n            if depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\"]:\n                depth_chans = 1\n                assert (\n                    depth_patch_embed_separate_params\n                ), \"separate tokenization needs separate parameters\"\n                if depth_mode == \"separate_d_tokens\":\n                    raise NotImplementedError()\n            else:\n                assert depth_mode == \"rgbd\"\n                depth_chans = 4\n\n            self.depth_patch_embed_separate_params = depth_patch_embed_separate_params\n\n            if depth_patch_embed_separate_params:\n                self.depth_patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=depth_chans,\n                    embed_dim=embed_dim,\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n            else:\n                # share parameters with patch_embed\n                # delete the layer we built above\n                del self.patch_embed\n                assert depth_chans == 4\n                logging.info(\n                    \"Certain channels of patch projection may not be used in forward pass\"\n                )\n                logging.info(\n                    \"Make sure config.DISTRIBUTED.FIND_UNUSED_PARAMETERS is set to True\"\n                )\n                self.patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=3,\n                    embed_dim=embed_dim,\n                    additional_variable_channels=[1],\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_795-845"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            assert depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\", \"rgbd\"]\n            if depth_mode in [\"separate_d_tokens\", \"summed_rgb_d_tokens\"]:\n                depth_chans = 1\n                assert (\n                    depth_patch_embed_separate_params\n                ), \"separate tokenization needs separate parameters\"\n                if depth_mode == \"separate_d_tokens\":\n                    raise NotImplementedError()\n            else:\n                assert depth_mode == \"rgbd\"\n                depth_chans = 4\n\n            self.depth_patch_embed_separate_params = depth_patch_embed_separate_params\n\n            if depth_patch_embed_separate_params:\n                self.depth_patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=depth_chans,\n                    embed_dim=embed_dim,\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n            else:\n                # share parameters with patch_embed\n                # delete the layer we built above\n                del self.patch_embed\n                assert depth_chans == 4\n                logging.info(\n                    \"Certain channels of patch projection may not be used in forward pass\"\n                )\n                logging.info(\n                    \"Make sure config.DISTRIBUTED.FIND_UNUSED_PARAMETERS is set to True\"\n                )\n                self.patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=3,\n                    embed_dim=embed_dim,\n                    additional_variable_channels=[1],\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_805-855"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                depth_chans = 4\n\n            self.depth_patch_embed_separate_params = depth_patch_embed_separate_params\n\n            if depth_patch_embed_separate_params:\n                self.depth_patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=depth_chans,\n                    embed_dim=embed_dim,\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n            else:\n                # share parameters with patch_embed\n                # delete the layer we built above\n                del self.patch_embed\n                assert depth_chans == 4\n                logging.info(\n                    \"Certain channels of patch projection may not be used in forward pass\"\n                )\n                logging.info(\n                    \"Make sure config.DISTRIBUTED.FIND_UNUSED_PARAMETERS is set to True\"\n                )\n                self.patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=3,\n                    embed_dim=embed_dim,\n                    additional_variable_channels=[1],\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_815-865"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                )\n            else:\n                # share parameters with patch_embed\n                # delete the layer we built above\n                del self.patch_embed\n                assert depth_chans == 4\n                logging.info(\n                    \"Certain channels of patch projection may not be used in forward pass\"\n                )\n                logging.info(\n                    \"Make sure config.DISTRIBUTED.FIND_UNUSED_PARAMETERS is set to True\"\n                )\n                self.patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=3,\n                    embed_dim=embed_dim,\n                    additional_variable_channels=[1],\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if i_layer < self.num_layers - 1 else None,\n            )\n            self.layers.append(layer)\n\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_825-875"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    \"Make sure config.DISTRIBUTED.FIND_UNUSED_PARAMETERS is set to True\"\n                )\n                self.patch_embed = PatchEmbed3D(\n                    patch_size=patch_size,\n                    in_chans=3,\n                    embed_dim=embed_dim,\n                    additional_variable_channels=[1],\n                    norm_layer=norm_layer if self.patch_norm else None,\n                )\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if i_layer < self.num_layers - 1 else None,\n            )\n            self.layers.append(layer)\n\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)\n\n        # init the weights\n        self.init_weights()\n\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_835-885"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if i_layer < self.num_layers - 1 else None,\n            )\n            self.layers.append(layer)\n\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)\n\n        # init the weights\n        self.init_weights()\n\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n\n        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.requires_grad = False\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_845-895"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if i_layer < self.num_layers - 1 else None,\n            )\n            self.layers.append(layer)\n\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)\n\n        # init the weights\n        self.init_weights()\n\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n\n        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.requires_grad = False\n\n    def inflate_weights(self, logger):\n        \"\"\"Inflate the swin2d parameters to swin3d.\n        The differences between swin3d and swin2d mainly lie in an extra\n        axis. To utilize the pretrained parameters in 2d model,\n        the weight of swin2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging infomation.\n        \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_855-905"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if i_layer < self.num_layers - 1 else None,\n            )\n            self.layers.append(layer)\n\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)\n\n        # init the weights\n        self.init_weights()\n\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n\n        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.requires_grad = False\n\n    def inflate_weights(self, logger):\n        \"\"\"Inflate the swin2d parameters to swin3d.\n        The differences between swin3d and swin2d mainly lie in an extra\n        axis. To utilize the pretrained parameters in 2d model,\n        the weight of swin2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging infomation.\n        \"\"\"\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_865-915"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 925, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # init the weights\n        self.init_weights()\n\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n\n        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.requires_grad = False\n\n    def inflate_weights(self, logger):\n        \"\"\"Inflate the swin2d parameters to swin3d.\n        The differences between swin3d and swin2d mainly lie in an extra\n        axis. To utilize the pretrained parameters in 2d model,\n        the weight of swin2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging infomation.\n        \"\"\"\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_875-925"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 935, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                param.requires_grad = False\n\n        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.requires_grad = False\n\n    def inflate_weights(self, logger):\n        \"\"\"Inflate the swin2d parameters to swin3d.\n        The differences between swin3d and swin2d mainly lie in an extra\n        axis. To utilize the pretrained parameters in 2d model,\n        the weight of swin2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging infomation.\n        \"\"\"\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        if state_dict[\"patch_embed.proj.weight\"].ndim == 4:\n            state_dict[\"patch_embed.proj.weight\"] = state_dict[\n                \"patch_embed.proj.weight\"\n            ].unsqueeze(2)\n        state_dict[\"patch_embed.proj.weight\"] = (\n            state_dict[\"patch_embed.proj.weight\"].repeat(1, 1, self.patch_size[0], 1, 1)\n            / self.patch_size[0]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_885-935"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 945, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def inflate_weights(self, logger):\n        \"\"\"Inflate the swin2d parameters to swin3d.\n        The differences between swin3d and swin2d mainly lie in an extra\n        axis. To utilize the pretrained parameters in 2d model,\n        the weight of swin2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging infomation.\n        \"\"\"\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        if state_dict[\"patch_embed.proj.weight\"].ndim == 4:\n            state_dict[\"patch_embed.proj.weight\"] = state_dict[\n                \"patch_embed.proj.weight\"\n            ].unsqueeze(2)\n        state_dict[\"patch_embed.proj.weight\"] = (\n            state_dict[\"patch_embed.proj.weight\"].repeat(1, 1, self.patch_size[0], 1, 1)\n            / self.patch_size[0]\n        )\n        if (\n            \"depth_patch_embed.proj.weight\" in state_dict\n            and state_dict[\"depth_patch_embed.proj.weight\"].ndim == 4\n        ):\n            state_dict[\"depth_patch_embed.proj.weight\"] = state_dict[\n                \"depth_patch_embed.proj.weight\"\n            ].unsqueeze(2)\n\n        # bicubic interpolate relative_position_bias_table if not match", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_895-945"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 930, "start_line_no": 905, "end_line_no": 955, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        if state_dict[\"patch_embed.proj.weight\"].ndim == 4:\n            state_dict[\"patch_embed.proj.weight\"] = state_dict[\n                \"patch_embed.proj.weight\"\n            ].unsqueeze(2)\n        state_dict[\"patch_embed.proj.weight\"] = (\n            state_dict[\"patch_embed.proj.weight\"].repeat(1, 1, self.patch_size[0], 1, 1)\n            / self.patch_size[0]\n        )\n        if (\n            \"depth_patch_embed.proj.weight\" in state_dict\n            and state_dict[\"depth_patch_embed.proj.weight\"].ndim == 4\n        ):\n            state_dict[\"depth_patch_embed.proj.weight\"] = state_dict[\n                \"depth_patch_embed.proj.weight\"\n            ].unsqueeze(2)\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n            wd = self.window_size[0]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_905-955"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 940, "start_line_no": 915, "end_line_no": 965, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        if state_dict[\"patch_embed.proj.weight\"].ndim == 4:\n            state_dict[\"patch_embed.proj.weight\"] = state_dict[\n                \"patch_embed.proj.weight\"\n            ].unsqueeze(2)\n        state_dict[\"patch_embed.proj.weight\"] = (\n            state_dict[\"patch_embed.proj.weight\"].repeat(1, 1, self.patch_size[0], 1, 1)\n            / self.patch_size[0]\n        )\n        if (\n            \"depth_patch_embed.proj.weight\" in state_dict\n            and state_dict[\"depth_patch_embed.proj.weight\"].ndim == 4\n        ):\n            state_dict[\"depth_patch_embed.proj.weight\"] = state_dict[\n                \"depth_patch_embed.proj.weight\"\n            ].unsqueeze(2)\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n            wd = self.window_size[0]\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1**0.5)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            relative_position_bias_table_pretrained.permute(1, 0).view(\n                                1, nH1, S1, S1\n                            ),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_915-965"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 950, "start_line_no": 925, "end_line_no": 975, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for k in attn_mask_keys:\n            del state_dict[k]\n\n        if state_dict[\"patch_embed.proj.weight\"].ndim == 4:\n            state_dict[\"patch_embed.proj.weight\"] = state_dict[\n                \"patch_embed.proj.weight\"\n            ].unsqueeze(2)\n        state_dict[\"patch_embed.proj.weight\"] = (\n            state_dict[\"patch_embed.proj.weight\"].repeat(1, 1, self.patch_size[0], 1, 1)\n            / self.patch_size[0]\n        )\n        if (\n            \"depth_patch_embed.proj.weight\" in state_dict\n            and state_dict[\"depth_patch_embed.proj.weight\"].ndim == 4\n        ):\n            state_dict[\"depth_patch_embed.proj.weight\"] = state_dict[\n                \"depth_patch_embed.proj.weight\"\n            ].unsqueeze(2)\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n            wd = self.window_size[0]\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1**0.5)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            relative_position_bias_table_pretrained.permute(1, 0).view(\n                                1, nH1, S1, S1\n                            ),\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.view(\n                            nH2, L2", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_925-975"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 960, "start_line_no": 935, "end_line_no": 985, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        if (\n            \"depth_patch_embed.proj.weight\" in state_dict\n            and state_dict[\"depth_patch_embed.proj.weight\"].ndim == 4\n        ):\n            state_dict[\"depth_patch_embed.proj.weight\"] = state_dict[\n                \"depth_patch_embed.proj.weight\"\n            ].unsqueeze(2)\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n            wd = self.window_size[0]\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1**0.5)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            relative_position_bias_table_pretrained.permute(1, 0).view(\n                                1, nH1, S1, S1\n                            ),\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.view(\n                            nH2, L2\n                        ).permute(1, 0)\n                    )\n            state_dict[k] = relative_position_bias_table_pretrained.repeat(\n                2 * wd - 1, 1\n            )\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_935-985"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 970, "start_line_no": 945, "end_line_no": 995, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n            wd = self.window_size[0]\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1**0.5)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            relative_position_bias_table_pretrained.permute(1, 0).view(\n                                1, nH1, S1, S1\n                            ),\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.view(\n                            nH2, L2\n                        ).permute(1, 0)\n                    )\n            state_dict[k] = relative_position_bias_table_pretrained.repeat(\n                2 * wd - 1, 1\n            )\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def load_and_interpolate_3d_weights(self, logger):\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n        assert self.pretrained3d is not None and self.pretrained2d is False\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_945-995"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 980, "start_line_no": 955, "end_line_no": 1005, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1**0.5)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            relative_position_bias_table_pretrained.permute(1, 0).view(\n                                1, nH1, S1, S1\n                            ),\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.view(\n                            nH2, L2\n                        ).permute(1, 0)\n                    )\n            state_dict[k] = relative_position_bias_table_pretrained.repeat(\n                2 * wd - 1, 1\n            )\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def load_and_interpolate_3d_weights(self, logger):\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n        assert self.pretrained3d is not None and self.pretrained2d is False\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_955-1005"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 990, "start_line_no": 965, "end_line_no": 1015, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.view(\n                            nH2, L2\n                        ).permute(1, 0)\n                    )\n            state_dict[k] = relative_position_bias_table_pretrained.repeat(\n                2 * wd - 1, 1\n            )\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def load_and_interpolate_3d_weights(self, logger):\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n        assert self.pretrained3d is not None and self.pretrained2d is False\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_965-1015"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1000, "start_line_no": 975, "end_line_no": 1025, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                        ).permute(1, 0)\n                    )\n            state_dict[k] = relative_position_bias_table_pretrained.repeat(\n                2 * wd - 1, 1\n            )\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def load_and_interpolate_3d_weights(self, logger):\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n        assert self.pretrained3d is not None and self.pretrained2d is False\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        pretrained_window_size = self.pretrained3d\n        T1 = 2 * pretrained_window_size[0] - 1\n        S11 = 2 * pretrained_window_size[1] - 1\n        S12 = 2 * pretrained_window_size[2] - 1\n        assert (\n            pretrained_window_size[0] == self.window_size[0]\n        ), \"Interpolating along time not supported\"\n\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_975-1025"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1010, "start_line_no": 985, "end_line_no": 1035, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def load_and_interpolate_3d_weights(self, logger):\n        checkpoint = load_and_broadcast_checkpoint_list(self.pretrained)\n        assert self.pretrained3d is not None and self.pretrained2d is False\n\n        if \"classy_state_dict\" in checkpoint:\n            # checkpoints trained in omnivore\n            state_dict = checkpoint[\"classy_state_dict\"][self.pretrained_model_key][\n                \"model\"\n            ][\"trunk\"]\n        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        pretrained_window_size = self.pretrained3d\n        T1 = 2 * pretrained_window_size[0] - 1\n        S11 = 2 * pretrained_window_size[1] - 1\n        S12 = 2 * pretrained_window_size[2] - 1\n        assert (\n            pretrained_window_size[0] == self.window_size[0]\n        ), \"Interpolating along time not supported\"\n\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (\n                (2 * self.window_size[0] - 1)\n                * (2 * self.window_size[1] - 1)\n                * (2 * self.window_size[2] - 1)\n            )\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_985-1035"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1020, "start_line_no": 995, "end_line_no": 1045, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        else:\n            # checkpoints trained outside omnivore\n            state_dict = checkpoint[\"model\"]\n\n        # delete relative_position_index since we always re-init it\n        relative_position_index_keys = [\n            k for k in state_dict.keys() if \"relative_position_index\" in k\n        ]\n        for k in relative_position_index_keys:\n            del state_dict[k]\n\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        pretrained_window_size = self.pretrained3d\n        T1 = 2 * pretrained_window_size[0] - 1\n        S11 = 2 * pretrained_window_size[1] - 1\n        S12 = 2 * pretrained_window_size[2] - 1\n        assert (\n            pretrained_window_size[0] == self.window_size[0]\n        ), \"Interpolating along time not supported\"\n\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (\n                (2 * self.window_size[0] - 1)\n                * (2 * self.window_size[1] - 1)\n                * (2 * self.window_size[2] - 1)\n            )\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    pretrained_bias = relative_position_bias_table_pretrained.view(\n                        T1, S11, S12, nH1\n                    )\n                    pretrained_bias = pretrained_bias.permute(0, 3, 1, 2)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            pretrained_bias,\n                            size=(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_995-1045"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1030, "start_line_no": 1005, "end_line_no": 1055, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # delete attn_mask since we always re-init it\n        attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n        for k in attn_mask_keys:\n            del state_dict[k]\n\n        # bicubic interpolate relative_position_bias_table if not match\n        relative_position_bias_table_keys = [\n            k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n        ]\n        pretrained_window_size = self.pretrained3d\n        T1 = 2 * pretrained_window_size[0] - 1\n        S11 = 2 * pretrained_window_size[1] - 1\n        S12 = 2 * pretrained_window_size[2] - 1\n        assert (\n            pretrained_window_size[0] == self.window_size[0]\n        ), \"Interpolating along time not supported\"\n\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (\n                (2 * self.window_size[0] - 1)\n                * (2 * self.window_size[1] - 1)\n                * (2 * self.window_size[2] - 1)\n            )\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    pretrained_bias = relative_position_bias_table_pretrained.view(\n                        T1, S11, S12, nH1\n                    )\n                    pretrained_bias = pretrained_bias.permute(0, 3, 1, 2)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            pretrained_bias,\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained_resized = (\n                        relative_position_bias_table_pretrained_resized.permute(\n                            0, 2, 3, 1\n                        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1005-1055"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1040, "start_line_no": 1015, "end_line_no": 1065, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        pretrained_window_size = self.pretrained3d\n        T1 = 2 * pretrained_window_size[0] - 1\n        S11 = 2 * pretrained_window_size[1] - 1\n        S12 = 2 * pretrained_window_size[2] - 1\n        assert (\n            pretrained_window_size[0] == self.window_size[0]\n        ), \"Interpolating along time not supported\"\n\n        for k in relative_position_bias_table_keys:\n            relative_position_bias_table_pretrained = state_dict[k]\n            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (\n                (2 * self.window_size[0] - 1)\n                * (2 * self.window_size[1] - 1)\n                * (2 * self.window_size[2] - 1)\n            )\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    pretrained_bias = relative_position_bias_table_pretrained.view(\n                        T1, S11, S12, nH1\n                    )\n                    pretrained_bias = pretrained_bias.permute(0, 3, 1, 2)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            pretrained_bias,\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained_resized = (\n                        relative_position_bias_table_pretrained_resized.permute(\n                            0, 2, 3, 1\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.reshape(L2, nH2)\n                    )\n\n            state_dict[k] = relative_position_bias_table_pretrained\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1015-1065"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1050, "start_line_no": 1025, "end_line_no": 1075, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            relative_position_bias_table_current = self.state_dict()[k]\n            L1, nH1 = relative_position_bias_table_pretrained.size()\n            L2, nH2 = relative_position_bias_table_current.size()\n            L2 = (\n                (2 * self.window_size[0] - 1)\n                * (2 * self.window_size[1] - 1)\n                * (2 * self.window_size[2] - 1)\n            )\n            if nH1 != nH2:\n                logger.warning(f\"Error in loading {k}, passing\")\n            else:\n                if L1 != L2:\n                    pretrained_bias = relative_position_bias_table_pretrained.view(\n                        T1, S11, S12, nH1\n                    )\n                    pretrained_bias = pretrained_bias.permute(0, 3, 1, 2)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            pretrained_bias,\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained_resized = (\n                        relative_position_bias_table_pretrained_resized.permute(\n                            0, 2, 3, 1\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.reshape(L2, nH2)\n                    )\n\n            state_dict[k] = relative_position_bias_table_pretrained\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Initialize the weights in backbone.\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n\n        def _init_weights(m):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1025-1075"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1060, "start_line_no": 1035, "end_line_no": 1085, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            else:\n                if L1 != L2:\n                    pretrained_bias = relative_position_bias_table_pretrained.view(\n                        T1, S11, S12, nH1\n                    )\n                    pretrained_bias = pretrained_bias.permute(0, 3, 1, 2)\n                    relative_position_bias_table_pretrained_resized = (\n                        torch.nn.functional.interpolate(\n                            pretrained_bias,\n                            size=(\n                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained_resized = (\n                        relative_position_bias_table_pretrained_resized.permute(\n                            0, 2, 3, 1\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.reshape(L2, nH2)\n                    )\n\n            state_dict[k] = relative_position_bias_table_pretrained\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Initialize the weights in backbone.\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n\n        def _init_weights(m):\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=0.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n\n        if pretrained:\n            self.pretrained = pretrained", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1035-1085"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1070, "start_line_no": 1045, "end_line_no": 1095, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                                2 * self.window_size[1] - 1,\n                                2 * self.window_size[2] - 1,\n                            ),\n                            mode=\"bicubic\",\n                        )\n                    )\n                    relative_position_bias_table_pretrained_resized = (\n                        relative_position_bias_table_pretrained_resized.permute(\n                            0, 2, 3, 1\n                        )\n                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.reshape(L2, nH2)\n                    )\n\n            state_dict[k] = relative_position_bias_table_pretrained\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Initialize the weights in backbone.\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n\n        def _init_weights(m):\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=0.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n\n        if pretrained:\n            self.pretrained = pretrained\n        if isinstance(self.pretrained, str) or isinstance(self.pretrained, list):\n            self.apply(_init_weights)\n            logging.info(f\"load model from: {self.pretrained}\")\n\n            if self.pretrained2d:\n                # Inflate 2D model into 3D model.\n                logging.info(f\"Inflating with {self.pretrained_model_key}\")\n                self.inflate_weights(logging)\n            elif self.pretrained3d:\n                logging.info(f\"Loading 3D model with {self.pretrained_model_key}\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1045-1095"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1080, "start_line_no": 1055, "end_line_no": 1105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    )\n                    relative_position_bias_table_pretrained = (\n                        relative_position_bias_table_pretrained_resized.reshape(L2, nH2)\n                    )\n\n            state_dict[k] = relative_position_bias_table_pretrained\n        msg = self.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"=> loaded successfully '{self.pretrained}'\")\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Initialize the weights in backbone.\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n\n        def _init_weights(m):\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=0.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n\n        if pretrained:\n            self.pretrained = pretrained\n        if isinstance(self.pretrained, str) or isinstance(self.pretrained, list):\n            self.apply(_init_weights)\n            logging.info(f\"load model from: {self.pretrained}\")\n\n            if self.pretrained2d:\n                # Inflate 2D model into 3D model.\n                logging.info(f\"Inflating with {self.pretrained_model_key}\")\n                self.inflate_weights(logging)\n            elif self.pretrained3d:\n                logging.info(f\"Loading 3D model with {self.pretrained_model_key}\")\n                self.load_and_interpolate_3d_weights(logging)\n            else:\n                raise ValueError(\n                    \"Use VISSL loading for this. This code \"\n                    \"is only for Swin inflation.\"\n                )\n                # # Directly load 3D model.\n                # load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            self.apply(_init_weights)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1055-1105"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1090, "start_line_no": 1065, "end_line_no": 1115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        torch.cuda.empty_cache()\n\n    def init_weights(self, pretrained=None):\n        \"\"\"Initialize the weights in backbone.\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n\n        def _init_weights(m):\n            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=0.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n\n        if pretrained:\n            self.pretrained = pretrained\n        if isinstance(self.pretrained, str) or isinstance(self.pretrained, list):\n            self.apply(_init_weights)\n            logging.info(f\"load model from: {self.pretrained}\")\n\n            if self.pretrained2d:\n                # Inflate 2D model into 3D model.\n                logging.info(f\"Inflating with {self.pretrained_model_key}\")\n                self.inflate_weights(logging)\n            elif self.pretrained3d:\n                logging.info(f\"Loading 3D model with {self.pretrained_model_key}\")\n                self.load_and_interpolate_3d_weights(logging)\n            else:\n                raise ValueError(\n                    \"Use VISSL loading for this. This code \"\n                    \"is only for Swin inflation.\"\n                )\n                # # Directly load 3D model.\n                # load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            self.apply(_init_weights)\n        else:\n            raise TypeError(\n                f\"pretrained must be a str or None but found: {type(self.pretrained)}\"\n            )\n\n    def _apply_norm(self, x):\n        x = rearrange(x, \"n c d h w -> n d h w c\")\n        x = self.norm(x)\n        x = rearrange(x, \"n d h w c -> n c d h w\")\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1065-1115"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1100, "start_line_no": 1075, "end_line_no": 1125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if isinstance(m, nn.Linear):\n                trunc_normal_(m.weight, std=0.02)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.constant_(m.bias, 0)\n                nn.init.constant_(m.weight, 1.0)\n\n        if pretrained:\n            self.pretrained = pretrained\n        if isinstance(self.pretrained, str) or isinstance(self.pretrained, list):\n            self.apply(_init_weights)\n            logging.info(f\"load model from: {self.pretrained}\")\n\n            if self.pretrained2d:\n                # Inflate 2D model into 3D model.\n                logging.info(f\"Inflating with {self.pretrained_model_key}\")\n                self.inflate_weights(logging)\n            elif self.pretrained3d:\n                logging.info(f\"Loading 3D model with {self.pretrained_model_key}\")\n                self.load_and_interpolate_3d_weights(logging)\n            else:\n                raise ValueError(\n                    \"Use VISSL loading for this. This code \"\n                    \"is only for Swin inflation.\"\n                )\n                # # Directly load 3D model.\n                # load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            self.apply(_init_weights)\n        else:\n            raise TypeError(\n                f\"pretrained must be a str or None but found: {type(self.pretrained)}\"\n            )\n\n    def _apply_norm(self, x):\n        x = rearrange(x, \"n c d h w -> n d h w c\")\n        x = self.norm(x)\n        x = rearrange(x, \"n d h w c -> n c d h w\")\n        return x\n\n    def forward_intermediate_features(self, stage_outputs, out_feat_keys):\n        \"\"\"\n        Inputs\n        - stage_outputs: list of features without self.norm() applied to them\n        - out_feat_keys: list of feature names (str)\n                         specified as \"stage<int>\" for feature with norm\n                         or \"interim<int>\" for feature without norm\n        \"\"\"\n        out_features = []", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1075-1125"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1110, "start_line_no": 1085, "end_line_no": 1135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if isinstance(self.pretrained, str) or isinstance(self.pretrained, list):\n            self.apply(_init_weights)\n            logging.info(f\"load model from: {self.pretrained}\")\n\n            if self.pretrained2d:\n                # Inflate 2D model into 3D model.\n                logging.info(f\"Inflating with {self.pretrained_model_key}\")\n                self.inflate_weights(logging)\n            elif self.pretrained3d:\n                logging.info(f\"Loading 3D model with {self.pretrained_model_key}\")\n                self.load_and_interpolate_3d_weights(logging)\n            else:\n                raise ValueError(\n                    \"Use VISSL loading for this. This code \"\n                    \"is only for Swin inflation.\"\n                )\n                # # Directly load 3D model.\n                # load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            self.apply(_init_weights)\n        else:\n            raise TypeError(\n                f\"pretrained must be a str or None but found: {type(self.pretrained)}\"\n            )\n\n    def _apply_norm(self, x):\n        x = rearrange(x, \"n c d h w -> n d h w c\")\n        x = self.norm(x)\n        x = rearrange(x, \"n d h w c -> n c d h w\")\n        return x\n\n    def forward_intermediate_features(self, stage_outputs, out_feat_keys):\n        \"\"\"\n        Inputs\n        - stage_outputs: list of features without self.norm() applied to them\n        - out_feat_keys: list of feature names (str)\n                         specified as \"stage<int>\" for feature with norm\n                         or \"interim<int>\" for feature without norm\n        \"\"\"\n        out_features = []\n        for key in out_feat_keys:\n            if key.startswith(\"stage\"):\n                rep = \"stage\"\n            elif key.startswith(\"interim\"):\n                rep = \"interim\"\n            else:\n                raise ValueError(f\"Invalid key {key}\")\n            idx = int(key.replace(rep, \"\"))\n            feat = stage_outputs[idx]\n            if rep == \"stage\":", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1085-1135"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1120, "start_line_no": 1095, "end_line_no": 1145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                self.load_and_interpolate_3d_weights(logging)\n            else:\n                raise ValueError(\n                    \"Use VISSL loading for this. This code \"\n                    \"is only for Swin inflation.\"\n                )\n                # # Directly load 3D model.\n                # load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            self.apply(_init_weights)\n        else:\n            raise TypeError(\n                f\"pretrained must be a str or None but found: {type(self.pretrained)}\"\n            )\n\n    def _apply_norm(self, x):\n        x = rearrange(x, \"n c d h w -> n d h w c\")\n        x = self.norm(x)\n        x = rearrange(x, \"n d h w c -> n c d h w\")\n        return x\n\n    def forward_intermediate_features(self, stage_outputs, out_feat_keys):\n        \"\"\"\n        Inputs\n        - stage_outputs: list of features without self.norm() applied to them\n        - out_feat_keys: list of feature names (str)\n                         specified as \"stage<int>\" for feature with norm\n                         or \"interim<int>\" for feature without norm\n        \"\"\"\n        out_features = []\n        for key in out_feat_keys:\n            if key.startswith(\"stage\"):\n                rep = \"stage\"\n            elif key.startswith(\"interim\"):\n                rep = \"interim\"\n            else:\n                raise ValueError(f\"Invalid key {key}\")\n            idx = int(key.replace(rep, \"\"))\n            feat = stage_outputs[idx]\n            if rep == \"stage\":\n                feat = self._apply_norm(feat)\n            out_features.append(feat)\n        return out_features\n\n    def get_patch_embedding(self, x):\n        # x: B x C x T x H x W\n        assert x.ndim == 5\n        has_depth = x.shape[1] == 4\n\n        if has_depth:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1095-1145"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1130, "start_line_no": 1105, "end_line_no": 1155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        else:\n            raise TypeError(\n                f\"pretrained must be a str or None but found: {type(self.pretrained)}\"\n            )\n\n    def _apply_norm(self, x):\n        x = rearrange(x, \"n c d h w -> n d h w c\")\n        x = self.norm(x)\n        x = rearrange(x, \"n d h w c -> n c d h w\")\n        return x\n\n    def forward_intermediate_features(self, stage_outputs, out_feat_keys):\n        \"\"\"\n        Inputs\n        - stage_outputs: list of features without self.norm() applied to them\n        - out_feat_keys: list of feature names (str)\n                         specified as \"stage<int>\" for feature with norm\n                         or \"interim<int>\" for feature without norm\n        \"\"\"\n        out_features = []\n        for key in out_feat_keys:\n            if key.startswith(\"stage\"):\n                rep = \"stage\"\n            elif key.startswith(\"interim\"):\n                rep = \"interim\"\n            else:\n                raise ValueError(f\"Invalid key {key}\")\n            idx = int(key.replace(rep, \"\"))\n            feat = stage_outputs[idx]\n            if rep == \"stage\":\n                feat = self._apply_norm(feat)\n            out_features.append(feat)\n        return out_features\n\n    def get_patch_embedding(self, x):\n        # x: B x C x T x H x W\n        assert x.ndim == 5\n        has_depth = x.shape[1] == 4\n\n        if has_depth:\n            if self.depth_mode in [\"summed_rgb_d_tokens\"]:\n                x_rgb = x[:, :3, ...]\n                x_d = x[:, 3:, ...]\n                x_d = self.depth_patch_embed(x_d)\n                x_rgb = self.patch_embed(x_rgb)\n                # sum the two sets of tokens\n                x = x_rgb + x_d\n            elif self.depth_mode == \"rgbd\":\n                if self.depth_patch_embed_separate_params:\n                    x = self.depth_patch_embed(x)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1105-1155"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1140, "start_line_no": 1115, "end_line_no": 1165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def forward_intermediate_features(self, stage_outputs, out_feat_keys):\n        \"\"\"\n        Inputs\n        - stage_outputs: list of features without self.norm() applied to them\n        - out_feat_keys: list of feature names (str)\n                         specified as \"stage<int>\" for feature with norm\n                         or \"interim<int>\" for feature without norm\n        \"\"\"\n        out_features = []\n        for key in out_feat_keys:\n            if key.startswith(\"stage\"):\n                rep = \"stage\"\n            elif key.startswith(\"interim\"):\n                rep = \"interim\"\n            else:\n                raise ValueError(f\"Invalid key {key}\")\n            idx = int(key.replace(rep, \"\"))\n            feat = stage_outputs[idx]\n            if rep == \"stage\":\n                feat = self._apply_norm(feat)\n            out_features.append(feat)\n        return out_features\n\n    def get_patch_embedding(self, x):\n        # x: B x C x T x H x W\n        assert x.ndim == 5\n        has_depth = x.shape[1] == 4\n\n        if has_depth:\n            if self.depth_mode in [\"summed_rgb_d_tokens\"]:\n                x_rgb = x[:, :3, ...]\n                x_d = x[:, 3:, ...]\n                x_d = self.depth_patch_embed(x_d)\n                x_rgb = self.patch_embed(x_rgb)\n                # sum the two sets of tokens\n                x = x_rgb + x_d\n            elif self.depth_mode == \"rgbd\":\n                if self.depth_patch_embed_separate_params:\n                    x = self.depth_patch_embed(x)\n                else:\n                    x = self.patch_embed(x)\n            else:\n                logging.info(\"Depth mode %s not supported\" % self.depth_mode)\n                raise NotImplementedError()\n        else:\n            x = self.patch_embed(x)\n        return x\n\n    def forward(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1115-1165"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1150, "start_line_no": 1125, "end_line_no": 1175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for key in out_feat_keys:\n            if key.startswith(\"stage\"):\n                rep = \"stage\"\n            elif key.startswith(\"interim\"):\n                rep = \"interim\"\n            else:\n                raise ValueError(f\"Invalid key {key}\")\n            idx = int(key.replace(rep, \"\"))\n            feat = stage_outputs[idx]\n            if rep == \"stage\":\n                feat = self._apply_norm(feat)\n            out_features.append(feat)\n        return out_features\n\n    def get_patch_embedding(self, x):\n        # x: B x C x T x H x W\n        assert x.ndim == 5\n        has_depth = x.shape[1] == 4\n\n        if has_depth:\n            if self.depth_mode in [\"summed_rgb_d_tokens\"]:\n                x_rgb = x[:, :3, ...]\n                x_d = x[:, 3:, ...]\n                x_d = self.depth_patch_embed(x_d)\n                x_rgb = self.patch_embed(x_rgb)\n                # sum the two sets of tokens\n                x = x_rgb + x_d\n            elif self.depth_mode == \"rgbd\":\n                if self.depth_patch_embed_separate_params:\n                    x = self.depth_patch_embed(x)\n                else:\n                    x = self.patch_embed(x)\n            else:\n                logging.info(\"Depth mode %s not supported\" % self.depth_mode)\n                raise NotImplementedError()\n        else:\n            x = self.patch_embed(x)\n        return x\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        use_checkpoint: bool = False,\n    ) -> List[torch.Tensor]:\n        \"\"\"Forward function.\"\"\"\n        # Convert to singleton video if not already\n        x = self.im2vid(x)\n\n        x = self.get_patch_embedding(x)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1125-1175"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1160, "start_line_no": 1135, "end_line_no": 1185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                feat = self._apply_norm(feat)\n            out_features.append(feat)\n        return out_features\n\n    def get_patch_embedding(self, x):\n        # x: B x C x T x H x W\n        assert x.ndim == 5\n        has_depth = x.shape[1] == 4\n\n        if has_depth:\n            if self.depth_mode in [\"summed_rgb_d_tokens\"]:\n                x_rgb = x[:, :3, ...]\n                x_d = x[:, 3:, ...]\n                x_d = self.depth_patch_embed(x_d)\n                x_rgb = self.patch_embed(x_rgb)\n                # sum the two sets of tokens\n                x = x_rgb + x_d\n            elif self.depth_mode == \"rgbd\":\n                if self.depth_patch_embed_separate_params:\n                    x = self.depth_patch_embed(x)\n                else:\n                    x = self.patch_embed(x)\n            else:\n                logging.info(\"Depth mode %s not supported\" % self.depth_mode)\n                raise NotImplementedError()\n        else:\n            x = self.patch_embed(x)\n        return x\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        use_checkpoint: bool = False,\n    ) -> List[torch.Tensor]:\n        \"\"\"Forward function.\"\"\"\n        # Convert to singleton video if not already\n        x = self.im2vid(x)\n\n        x = self.get_patch_embedding(x)\n\n        x = self.pos_drop(x)\n\n        stage_outputs = []\n\n        for layer in self.layers:\n            x = layer(x.contiguous(), use_checkpoint=use_checkpoint)\n            stage_outputs.append(x)\n\n        if out_feat_keys is not None and len(out_feat_keys) > 0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1135-1185"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1170, "start_line_no": 1145, "end_line_no": 1195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if self.depth_mode in [\"summed_rgb_d_tokens\"]:\n                x_rgb = x[:, :3, ...]\n                x_d = x[:, 3:, ...]\n                x_d = self.depth_patch_embed(x_d)\n                x_rgb = self.patch_embed(x_rgb)\n                # sum the two sets of tokens\n                x = x_rgb + x_d\n            elif self.depth_mode == \"rgbd\":\n                if self.depth_patch_embed_separate_params:\n                    x = self.depth_patch_embed(x)\n                else:\n                    x = self.patch_embed(x)\n            else:\n                logging.info(\"Depth mode %s not supported\" % self.depth_mode)\n                raise NotImplementedError()\n        else:\n            x = self.patch_embed(x)\n        return x\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        use_checkpoint: bool = False,\n    ) -> List[torch.Tensor]:\n        \"\"\"Forward function.\"\"\"\n        # Convert to singleton video if not already\n        x = self.im2vid(x)\n\n        x = self.get_patch_embedding(x)\n\n        x = self.pos_drop(x)\n\n        stage_outputs = []\n\n        for layer in self.layers:\n            x = layer(x.contiguous(), use_checkpoint=use_checkpoint)\n            stage_outputs.append(x)\n\n        if out_feat_keys is not None and len(out_feat_keys) > 0:\n            return self.forward_intermediate_features(stage_outputs, out_feat_keys)\n        else:\n            x = self._apply_norm(x)\n            # Mean over the spatiotemporal dimensions\n            x = torch.mean(x, [-3, -2, -1])\n\n            return x\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1145-1195"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1180, "start_line_no": 1155, "end_line_no": 1197, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                else:\n                    x = self.patch_embed(x)\n            else:\n                logging.info(\"Depth mode %s not supported\" % self.depth_mode)\n                raise NotImplementedError()\n        else:\n            x = self.patch_embed(x)\n        return x\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        use_checkpoint: bool = False,\n    ) -> List[torch.Tensor]:\n        \"\"\"Forward function.\"\"\"\n        # Convert to singleton video if not already\n        x = self.im2vid(x)\n\n        x = self.get_patch_embedding(x)\n\n        x = self.pos_drop(x)\n\n        stage_outputs = []\n\n        for layer in self.layers:\n            x = layer(x.contiguous(), use_checkpoint=use_checkpoint)\n            stage_outputs.append(x)\n\n        if out_feat_keys is not None and len(out_feat_keys) > 0:\n            return self.forward_intermediate_features(stage_outputs, out_feat_keys)\n        else:\n            x = self._apply_norm(x)\n            # Mean over the spatiotemporal dimensions\n            x = torch.mean(x, [-3, -2, -1])\n\n            return x\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super(SwinTransformer3D, self).train(mode)\n        self._freeze_stages()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1155-1197"}
{"title": "facebookresearch_omnivore-omnivision-models-swin_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "swin_transformer.py"], "line_no": 1190, "start_line_no": 1165, "end_line_no": 1197, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        use_checkpoint: bool = False,\n    ) -> List[torch.Tensor]:\n        \"\"\"Forward function.\"\"\"\n        # Convert to singleton video if not already\n        x = self.im2vid(x)\n\n        x = self.get_patch_embedding(x)\n\n        x = self.pos_drop(x)\n\n        stage_outputs = []\n\n        for layer in self.layers:\n            x = layer(x.contiguous(), use_checkpoint=use_checkpoint)\n            stage_outputs.append(x)\n\n        if out_feat_keys is not None and len(out_feat_keys) > 0:\n            return self.forward_intermediate_features(stage_outputs, out_feat_keys)\n        else:\n            x = self._apply_norm(x)\n            # Mean over the spatiotemporal dimensions\n            x = torch.mean(x, [-3, -2, -1])\n\n            return x\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super(SwinTransformer3D, self).train(mode)\n        self._freeze_stages()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-swin_transformer.py_1165-1197"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py ;\n# https://github.com/facebookresearch/deit/blob/main/models.py\n# and https://github.com/facebookresearch/vissl/blob/main/vissl/models/trunks/vision_transformer.py\n# and is licensed under the license found in the\n# NOTICE file in the root directory of this source tree.\n\n\nimport math\nfrom functools import partial\nfrom typing import List, Optional\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom torch.nn.modules.utils import _ntuple\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py ;\n# https://github.com/facebookresearch/deit/blob/main/models.py\n# and https://github.com/facebookresearch/vissl/blob/main/vissl/models/trunks/vision_transformer.py\n# and is licensed under the license found in the\n# NOTICE file in the root directory of this source tree.\n\n\nimport math\nfrom functools import partial\nfrom typing import List, Optional\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom torch.nn.modules.utils import _ntuple\n\n\nto_2tuple = _ntuple(2)\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    \"\"\"Sinusoid position encoding table\"\"\"\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py ;\n# https://github.com/facebookresearch/deit/blob/main/models.py\n# and https://github.com/facebookresearch/vissl/blob/main/vissl/models/trunks/vision_transformer.py\n# and is licensed under the license found in the\n# NOTICE file in the root directory of this source tree.\n\n\nimport math\nfrom functools import partial\nfrom typing import List, Optional\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom torch.nn.modules.utils import _ntuple\n\n\nto_2tuple = _ntuple(2)\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    \"\"\"Sinusoid position encoding table\"\"\"\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)Assign(Name(Store)Call(Name(Load)Constant))FunctionDef(arguments(argarg)Expr(Constant)FunctionDef(arguments(arg)Return(ListComp(BinOp(Name(Load)DivCall(Attribute(Name(Load)Load)ConstantBinOp(BinOp(ConstantMultBinOp(Name(Load)FloorDivConstant))DivName(Load))))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ListComp(Call(Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Assign(Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Load)))Assign(Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Code modified from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py ;\n# https://github.com/facebookresearch/deit/blob/main/models.py\n# and https://github.com/facebookresearch/vissl/blob/main/vissl/models/trunks/vision_transformer.py\n# and is licensed under the license found in the\n# NOTICE file in the root directory of this source tree.\n\n\nimport math\nfrom functools import partial\nfrom typing import List, Optional\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom torch.nn.modules.utils import _ntuple\n\n\nto_2tuple = _ntuple(2)\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    \"\"\"Sinusoid position encoding table\"\"\"\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\nclass PadIm2Video(torch.nn.Module):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)Assign(Name(Store)Call(Name(Load)Constant))FunctionDef(arguments(argarg)Expr(Constant)FunctionDef(arguments(arg)Return(ListComp(BinOp(Name(Load)DivCall(Attribute(Name(Load)Load)ConstantBinOp(BinOp(ConstantMultBinOp(Name(Load)FloorDivConstant))DivName(Load))))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ListComp(Call(Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Assign(Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Load)))Assign(Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Load)))Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Constant)))ClassDef(Attribute(Attribute(Name(Load)Load)Load)FunctionDef(arguments(argargargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assert(Compare(Name(Load)GtConstant))Assert(Compare(Name(Load)InList(ConstantConstantLoad)))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from functools import partial\nfrom typing import List, Optional\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom torch.nn.modules.utils import _ntuple\n\n\nto_2tuple = _ntuple(2)\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    \"\"\"Sinusoid position encoding table\"\"\"\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\nclass PadIm2Video(torch.nn.Module):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            x = x.unsqueeze(self.time_dim)\n\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n\nAST=Module(ImportFrom(alias)ImportFrom(aliasalias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)Assign(Name(Store)Call(Name(Load)Constant))FunctionDef(arguments(argarg)Expr(Constant)FunctionDef(arguments(arg)Return(ListComp(BinOp(Name(Load)DivCall(Attribute(Name(Load)Load)ConstantBinOp(BinOp(ConstantMultBinOp(Name(Load)FloorDivConstant))DivName(Load))))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Assign(Name(Store)Call(Attribute(Name(Load)Load)ListComp(Call(Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))Assign(Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Load)))Assign(Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(ConstantConstant)Load)Load)))Return(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load)Constant)))ClassDef(Attribute(Attribute(Name(Load)Load)Load)FunctionDef(arguments(argargargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assert(Compare(Name(Load)GtConstant))Assert(Compare(Name(Load)InList(ConstantConstantLoad)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load))))If(Compare(Subscript(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)EqConstant)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)BinOp(List(ConstantLoad)MultCall(Name(Load)Attribute(Name(Load)Load)))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nto_2tuple = _ntuple(2)\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    \"\"\"Sinusoid position encoding table\"\"\"\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\nclass PadIm2Video(torch.nn.Module):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            x = x.unsqueeze(self.time_dim)\n\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = torch.nn.functional.pad(x, padarg)\n        return x\n\n\nclass Mlp(nn.Module):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n    )\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\nclass PadIm2Video(torch.nn.Module):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            x = x.unsqueeze(self.time_dim)\n\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = torch.nn.functional.pad(x, padarg)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n\nclass PadIm2Video(torch.nn.Module):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            x = x.unsqueeze(self.time_dim)\n\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = torch.nn.functional.pad(x, padarg)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.pad_type = pad_type\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            x = x.unsqueeze(self.time_dim)\n\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = torch.nn.functional.pad(x, padarg)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = torch.nn.functional.pad(x, padarg)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version,\n        # can set manually to be compat with prev weights", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version,\n        # can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version,\n        # can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version,\n        # can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version,\n        # can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        attn_target,\n        mlp_ratio=4.0,\n        drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        attn_target,\n        mlp_ratio=4.0,\n        drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if isinstance(attn_target, nn.Module):\n            self.attn = attn_target\n        else:\n            self.attn = attn_target(dim=dim)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        attn_target,\n        mlp_ratio=4.0,\n        drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if isinstance(attn_target, nn.Module):\n            self.attn = attn_target\n        else:\n            self.attn = attn_target(dim=dim)\n\n        if drop_path > 0.0:\n            self.drop_path = DropPath(drop_path)\n        else:\n            self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        attn_target,\n        mlp_ratio=4.0,\n        drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if isinstance(attn_target, nn.Module):\n            self.attn = attn_target\n        else:\n            self.attn = attn_target(dim=dim)\n\n        if drop_path > 0.0:\n            self.drop_path = DropPath(drop_path)\n        else:\n            self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n        self.layer_scale_type = layer_scale_type\n\n        # Layerscale\n        if self.layer_scale_type is not None:\n            assert self.layer_scale_type in [\n                \"per_channel\",\n                \"scalar\",\n            ], f\"Found Layer scale type {self.layer_scale_type}\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        attn_target,\n        mlp_ratio=4.0,\n        drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if isinstance(attn_target, nn.Module):\n            self.attn = attn_target\n        else:\n            self.attn = attn_target(dim=dim)\n\n        if drop_path > 0.0:\n            self.drop_path = DropPath(drop_path)\n        else:\n            self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n        self.layer_scale_type = layer_scale_type\n\n        # Layerscale\n        if self.layer_scale_type is not None:\n            assert self.layer_scale_type in [\n                \"per_channel\",\n                \"scalar\",\n            ], f\"Found Layer scale type {self.layer_scale_type}\"\n            if self.layer_scale_type == \"per_channel\":\n                # one gamma value per channel\n                gamma_shape = [1, 1, dim]\n            elif self.layer_scale_type == \"scalar\":\n                # single gamma value for all channels\n                gamma_shape = [1, 1, 1]\n            # two gammas: for each part of the fwd in the encoder\n            self.layer_scale_gamma1 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if isinstance(attn_target, nn.Module):\n            self.attn = attn_target\n        else:\n            self.attn = attn_target(dim=dim)\n\n        if drop_path > 0.0:\n            self.drop_path = DropPath(drop_path)\n        else:\n            self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n        self.layer_scale_type = layer_scale_type\n\n        # Layerscale\n        if self.layer_scale_type is not None:\n            assert self.layer_scale_type in [\n                \"per_channel\",\n                \"scalar\",\n            ], f\"Found Layer scale type {self.layer_scale_type}\"\n            if self.layer_scale_type == \"per_channel\":\n                # one gamma value per channel\n                gamma_shape = [1, 1, dim]\n            elif self.layer_scale_type == \"scalar\":\n                # single gamma value for all channels\n                gamma_shape = [1, 1, 1]\n            # two gammas: for each part of the fwd in the encoder\n            self.layer_scale_gamma1 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n            self.layer_scale_gamma2 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n\n    def forward(self, x):\n        if self.layer_scale_type is None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if drop_path > 0.0:\n            self.drop_path = DropPath(drop_path)\n        else:\n            self.drop_path = nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n        self.layer_scale_type = layer_scale_type\n\n        # Layerscale\n        if self.layer_scale_type is not None:\n            assert self.layer_scale_type in [\n                \"per_channel\",\n                \"scalar\",\n            ], f\"Found Layer scale type {self.layer_scale_type}\"\n            if self.layer_scale_type == \"per_channel\":\n                # one gamma value per channel\n                gamma_shape = [1, 1, dim]\n            elif self.layer_scale_type == \"scalar\":\n                # single gamma value for all channels\n                gamma_shape = [1, 1, 1]\n            # two gammas: for each part of the fwd in the encoder\n            self.layer_scale_gamma1 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n            self.layer_scale_gamma2 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n\n    def forward(self, x):\n        if self.layer_scale_type is None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.attn(self.norm1(x)) * self.layer_scale_gamma1)\n            x = x + self.drop_path(self.mlp(self.norm2(x)) * self.layer_scale_gamma2)\n        return x\n\n    def extra_repr(self) -> str:\n        named_modules = set()\n        for p in self.named_modules():\n            named_modules.update([p[0]])\n        named_modules = list(named_modules)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            drop=drop,\n        )\n        self.layer_scale_type = layer_scale_type\n\n        # Layerscale\n        if self.layer_scale_type is not None:\n            assert self.layer_scale_type in [\n                \"per_channel\",\n                \"scalar\",\n            ], f\"Found Layer scale type {self.layer_scale_type}\"\n            if self.layer_scale_type == \"per_channel\":\n                # one gamma value per channel\n                gamma_shape = [1, 1, dim]\n            elif self.layer_scale_type == \"scalar\":\n                # single gamma value for all channels\n                gamma_shape = [1, 1, 1]\n            # two gammas: for each part of the fwd in the encoder\n            self.layer_scale_gamma1 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n            self.layer_scale_gamma2 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n\n    def forward(self, x):\n        if self.layer_scale_type is None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.attn(self.norm1(x)) * self.layer_scale_gamma1)\n            x = x + self.drop_path(self.mlp(self.norm2(x)) * self.layer_scale_gamma2)\n        return x\n\n    def extra_repr(self) -> str:\n        named_modules = set()\n        for p in self.named_modules():\n            named_modules.update([p[0]])\n        named_modules = list(named_modules)\n\n        string_repr = \"\"\n        for p in self.named_parameters():\n            name = p[0].split(\".\")[0]\n            if name not in named_modules:\n                string_repr = (\n                    string_repr\n                    + \"(\"\n                    + name\n                    + \"): \"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if self.layer_scale_type == \"per_channel\":\n                # one gamma value per channel\n                gamma_shape = [1, 1, dim]\n            elif self.layer_scale_type == \"scalar\":\n                # single gamma value for all channels\n                gamma_shape = [1, 1, 1]\n            # two gammas: for each part of the fwd in the encoder\n            self.layer_scale_gamma1 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n            self.layer_scale_gamma2 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n\n    def forward(self, x):\n        if self.layer_scale_type is None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.attn(self.norm1(x)) * self.layer_scale_gamma1)\n            x = x + self.drop_path(self.mlp(self.norm2(x)) * self.layer_scale_gamma2)\n        return x\n\n    def extra_repr(self) -> str:\n        named_modules = set()\n        for p in self.named_modules():\n            named_modules.update([p[0]])\n        named_modules = list(named_modules)\n\n        string_repr = \"\"\n        for p in self.named_parameters():\n            name = p[0].split(\".\")[0]\n            if name not in named_modules:\n                string_repr = (\n                    string_repr\n                    + \"(\"\n                    + name\n                    + \"): \"\n                    + \"tensor(\"\n                    + str(tuple(p[1].shape))\n                    + \", requires_grad=\"\n                    + str(p[1].requires_grad)\n                    + \")\\n\"\n                )\n\n        return string_repr\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n            self.layer_scale_gamma2 = nn.Parameter(\n                torch.ones(size=gamma_shape) * layer_scale_init_value,\n                requires_grad=True,\n            )\n\n    def forward(self, x):\n        if self.layer_scale_type is None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.attn(self.norm1(x)) * self.layer_scale_gamma1)\n            x = x + self.drop_path(self.mlp(self.norm2(x)) * self.layer_scale_gamma2)\n        return x\n\n    def extra_repr(self) -> str:\n        named_modules = set()\n        for p in self.named_modules():\n            named_modules.update([p[0]])\n        named_modules = list(named_modules)\n\n        string_repr = \"\"\n        for p in self.named_parameters():\n            name = p[0].split(\".\")[0]\n            if name not in named_modules:\n                string_repr = (\n                    string_repr\n                    + \"(\"\n                    + name\n                    + \"): \"\n                    + \"tensor(\"\n                    + str(tuple(p[1].shape))\n                    + \", requires_grad=\"\n                    + str(p[1].requires_grad)\n                    + \")\\n\"\n                )\n\n        return string_repr\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_layout = (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        else:\n            x = x + self.drop_path(self.attn(self.norm1(x)) * self.layer_scale_gamma1)\n            x = x + self.drop_path(self.mlp(self.norm2(x)) * self.layer_scale_gamma2)\n        return x\n\n    def extra_repr(self) -> str:\n        named_modules = set()\n        for p in self.named_modules():\n            named_modules.update([p[0]])\n        named_modules = list(named_modules)\n\n        string_repr = \"\"\n        for p in self.named_parameters():\n            name = p[0].split(\".\")[0]\n            if name not in named_modules:\n                string_repr = (\n                    string_repr\n                    + \"(\"\n                    + name\n                    + \"): \"\n                    + \"tensor(\"\n                    + str(tuple(p[1].shape))\n                    + \", requires_grad=\"\n                    + str(p[1].requires_grad)\n                    + \")\\n\"\n                )\n\n        return string_repr\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_layout = (\n            1,\n            img_size[0] // patch_size[0],\n            img_size[1] // patch_size[1],\n        )\n        self.num_patches = np.prod(self.patches_layout)\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        string_repr = \"\"\n        for p in self.named_parameters():\n            name = p[0].split(\".\")[0]\n            if name not in named_modules:\n                string_repr = (\n                    string_repr\n                    + \"(\"\n                    + name\n                    + \"): \"\n                    + \"tensor(\"\n                    + str(tuple(p[1].shape))\n                    + \", requires_grad=\"\n                    + str(p[1].requires_grad)\n                    + \")\\n\"\n                )\n\n        return string_repr\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_layout = (\n            1,\n            img_size[0] // patch_size[0],\n            img_size[1] // patch_size[1],\n        )\n        self.num_patches = np.prod(self.patches_layout)\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass PatchEmbedGeneric(nn.Module):\n    \"\"\"\n    PatchEmbed from Hydra\n    \"\"\"\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    + \"tensor(\"\n                    + str(tuple(p[1].shape))\n                    + \", requires_grad=\"\n                    + str(p[1].requires_grad)\n                    + \")\\n\"\n                )\n\n        return string_repr\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_layout = (\n            1,\n            img_size[0] // patch_size[0],\n            img_size[1] // patch_size[1],\n        )\n        self.num_patches = np.prod(self.patches_layout)\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass PatchEmbedGeneric(nn.Module):\n    \"\"\"\n    PatchEmbed from Hydra\n    \"\"\"\n\n    def __init__(self, proj_stem, img_size):\n        super().__init__()\n\n        if len(proj_stem) > 1:\n            self.proj = nn.Sequential(*proj_stem)\n        else:\n            # Special case to be able to load pre-trained models that were\n            # trained with a standard stem\n            self.proj = proj_stem[0]\n        # get the num_patches", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_layout = (\n            1,\n            img_size[0] // patch_size[0],\n            img_size[1] // patch_size[1],\n        )\n        self.num_patches = np.prod(self.patches_layout)\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass PatchEmbedGeneric(nn.Module):\n    \"\"\"\n    PatchEmbed from Hydra\n    \"\"\"\n\n    def __init__(self, proj_stem, img_size):\n        super().__init__()\n\n        if len(proj_stem) > 1:\n            self.proj = nn.Sequential(*proj_stem)\n        else:\n            # Special case to be able to load pre-trained models that were\n            # trained with a standard stem\n            self.proj = proj_stem[0]\n        # get the num_patches\n        assert (\n            isinstance(img_size, list) and len(img_size) >= 3\n        ), \"Need the full C[xT]xHxW in generic\"\n        # compute num_tokens with a forward\n        with torch.no_grad():\n            dummy_img = torch.zeros(\n                [\n                    1,\n                ]\n                + img_size", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            1,\n            img_size[0] // patch_size[0],\n            img_size[1] // patch_size[1],\n        )\n        self.num_patches = np.prod(self.patches_layout)\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass PatchEmbedGeneric(nn.Module):\n    \"\"\"\n    PatchEmbed from Hydra\n    \"\"\"\n\n    def __init__(self, proj_stem, img_size):\n        super().__init__()\n\n        if len(proj_stem) > 1:\n            self.proj = nn.Sequential(*proj_stem)\n        else:\n            # Special case to be able to load pre-trained models that were\n            # trained with a standard stem\n            self.proj = proj_stem[0]\n        # get the num_patches\n        assert (\n            isinstance(img_size, list) and len(img_size) >= 3\n        ), \"Need the full C[xT]xHxW in generic\"\n        # compute num_tokens with a forward\n        with torch.no_grad():\n            dummy_img = torch.zeros(\n                [\n                    1,\n                ]\n                + img_size\n            )\n            self.patches_layout = tuple(self.proj(dummy_img).shape[2:])\n            self.num_patches = np.prod(self.patches_layout)\n\n    def forward(self, x):\n        # rgirdhar: no flatten here since the projection can handle it in the list of ops\n        x = self.proj(x)\n        # B C (T) H W -> B (T)HW C\n        return x.flatten(2).transpose(1, 2)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass PatchEmbedGeneric(nn.Module):\n    \"\"\"\n    PatchEmbed from Hydra\n    \"\"\"\n\n    def __init__(self, proj_stem, img_size):\n        super().__init__()\n\n        if len(proj_stem) > 1:\n            self.proj = nn.Sequential(*proj_stem)\n        else:\n            # Special case to be able to load pre-trained models that were\n            # trained with a standard stem\n            self.proj = proj_stem[0]\n        # get the num_patches\n        assert (\n            isinstance(img_size, list) and len(img_size) >= 3\n        ), \"Need the full C[xT]xHxW in generic\"\n        # compute num_tokens with a forward\n        with torch.no_grad():\n            dummy_img = torch.zeros(\n                [\n                    1,\n                ]\n                + img_size\n            )\n            self.patches_layout = tuple(self.proj(dummy_img).shape[2:])\n            self.num_patches = np.prod(self.patches_layout)\n\n    def forward(self, x):\n        # rgirdhar: no flatten here since the projection can handle it in the list of ops\n        x = self.proj(x)\n        # B C (T) H W -> B (T)HW C\n        return x.flatten(2).transpose(1, 2)\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        first_patch_idx,\n        patches_layout,\n        attn_target,\n        embed_dim,\n        decoder_embed_dim=512,\n        decoder_depth=8,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, proj_stem, img_size):\n        super().__init__()\n\n        if len(proj_stem) > 1:\n            self.proj = nn.Sequential(*proj_stem)\n        else:\n            # Special case to be able to load pre-trained models that were\n            # trained with a standard stem\n            self.proj = proj_stem[0]\n        # get the num_patches\n        assert (\n            isinstance(img_size, list) and len(img_size) >= 3\n        ), \"Need the full C[xT]xHxW in generic\"\n        # compute num_tokens with a forward\n        with torch.no_grad():\n            dummy_img = torch.zeros(\n                [\n                    1,\n                ]\n                + img_size\n            )\n            self.patches_layout = tuple(self.proj(dummy_img).shape[2:])\n            self.num_patches = np.prod(self.patches_layout)\n\n    def forward(self, x):\n        # rgirdhar: no flatten here since the projection can handle it in the list of ops\n        x = self.proj(x)\n        # B C (T) H W -> B (T)HW C\n        return x.flatten(2).transpose(1, 2)\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        first_patch_idx,\n        patches_layout,\n        attn_target,\n        embed_dim,\n        decoder_embed_dim=512,\n        decoder_depth=8,\n        drop_path_rate=0.0,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        layer_norm_eps=1e-6,\n        return_interim_layers=False,\n        share_pos_embed=False,\n        learnable_pos_embed=True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert (\n            isinstance(img_size, list) and len(img_size) >= 3\n        ), \"Need the full C[xT]xHxW in generic\"\n        # compute num_tokens with a forward\n        with torch.no_grad():\n            dummy_img = torch.zeros(\n                [\n                    1,\n                ]\n                + img_size\n            )\n            self.patches_layout = tuple(self.proj(dummy_img).shape[2:])\n            self.num_patches = np.prod(self.patches_layout)\n\n    def forward(self, x):\n        # rgirdhar: no flatten here since the projection can handle it in the list of ops\n        x = self.proj(x)\n        # B C (T) H W -> B (T)HW C\n        return x.flatten(2).transpose(1, 2)\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        first_patch_idx,\n        patches_layout,\n        attn_target,\n        embed_dim,\n        decoder_embed_dim=512,\n        decoder_depth=8,\n        drop_path_rate=0.0,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        layer_norm_eps=1e-6,\n        return_interim_layers=False,\n        share_pos_embed=False,\n        learnable_pos_embed=True,\n        init_pos_embed_random=False,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n        final_projection=None,\n        pos_sum_embed_only=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.patches_layout = patches_layout\n        self.first_patch_idx = first_patch_idx", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n            self.patches_layout = tuple(self.proj(dummy_img).shape[2:])\n            self.num_patches = np.prod(self.patches_layout)\n\n    def forward(self, x):\n        # rgirdhar: no flatten here since the projection can handle it in the list of ops\n        x = self.proj(x)\n        # B C (T) H W -> B (T)HW C\n        return x.flatten(2).transpose(1, 2)\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        first_patch_idx,\n        patches_layout,\n        attn_target,\n        embed_dim,\n        decoder_embed_dim=512,\n        decoder_depth=8,\n        drop_path_rate=0.0,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        layer_norm_eps=1e-6,\n        return_interim_layers=False,\n        share_pos_embed=False,\n        learnable_pos_embed=True,\n        init_pos_embed_random=False,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n        final_projection=None,\n        pos_sum_embed_only=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.patches_layout = patches_layout\n        self.first_patch_idx = first_patch_idx\n        assert first_patch_idx == 0 or first_patch_idx == 1\n        self.share_pos_embed = share_pos_embed\n        self.build_pos_embedding(\n            share_pos_embed=share_pos_embed,\n            learnable_pos_embed=learnable_pos_embed,\n            patches_layout=patches_layout,\n            first_patch_idx=first_patch_idx,\n            embed_dim=embed_dim,\n            init_pos_embed_random=init_pos_embed_random,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        first_patch_idx,\n        patches_layout,\n        attn_target,\n        embed_dim,\n        decoder_embed_dim=512,\n        decoder_depth=8,\n        drop_path_rate=0.0,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        layer_norm_eps=1e-6,\n        return_interim_layers=False,\n        share_pos_embed=False,\n        learnable_pos_embed=True,\n        init_pos_embed_random=False,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n        final_projection=None,\n        pos_sum_embed_only=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.patches_layout = patches_layout\n        self.first_patch_idx = first_patch_idx\n        assert first_patch_idx == 0 or first_patch_idx == 1\n        self.share_pos_embed = share_pos_embed\n        self.build_pos_embedding(\n            share_pos_embed=share_pos_embed,\n            learnable_pos_embed=learnable_pos_embed,\n            patches_layout=patches_layout,\n            first_patch_idx=first_patch_idx,\n            embed_dim=embed_dim,\n            init_pos_embed_random=init_pos_embed_random,\n        )\n        self.pos_sum_embed_only = pos_sum_embed_only\n        if pos_sum_embed_only:\n            # another flag to catch if someone set this accidentally\n            # recommended to use the `PosEmbedSumDecoder` class\n            assert decoder_depth == -1, \"Do not specify decoder_depth\"\n            return\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n        self.norm = norm_layer(decoder_embed_dim)\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargargargargargargargargargargargargargargargargargargargConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assert(BoolOp(OrCompare(Name(Load)EqConstant)Compare(Name(Load)EqConstant)))Assign(Attribute(Name(Load)Store)Name(Load))Expr(Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))If(Name(Load)Assert(Compare(Name(Load)EqUnaryOp(USubConstant))Constant)Return)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Constant)))Assign(Name(Store)Call(Name(Load)Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Call(Name(Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path_rate=0.0,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        layer_norm_eps=1e-6,\n        return_interim_layers=False,\n        share_pos_embed=False,\n        learnable_pos_embed=True,\n        init_pos_embed_random=False,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n        final_projection=None,\n        pos_sum_embed_only=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.patches_layout = patches_layout\n        self.first_patch_idx = first_patch_idx\n        assert first_patch_idx == 0 or first_patch_idx == 1\n        self.share_pos_embed = share_pos_embed\n        self.build_pos_embedding(\n            share_pos_embed=share_pos_embed,\n            learnable_pos_embed=learnable_pos_embed,\n            patches_layout=patches_layout,\n            first_patch_idx=first_patch_idx,\n            embed_dim=embed_dim,\n            init_pos_embed_random=init_pos_embed_random,\n        )\n        self.pos_sum_embed_only = pos_sum_embed_only\n        if pos_sum_embed_only:\n            # another flag to catch if someone set this accidentally\n            # recommended to use the `PosEmbedSumDecoder` class\n            assert decoder_depth == -1, \"Do not specify decoder_depth\"\n            return\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n        self.norm = norm_layer(decoder_embed_dim)\n\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, decoder_depth)\n        ]  # stochastic depth decay rule\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=decoder_embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        init_pos_embed_random=False,\n        layer_scale_type=None,  # from cait; possible values are None, \"per_channel\", \"scalar\"\n        layer_scale_init_value=1e-4,  # from cait; float\n        final_projection=None,\n        pos_sum_embed_only=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.patches_layout = patches_layout\n        self.first_patch_idx = first_patch_idx\n        assert first_patch_idx == 0 or first_patch_idx == 1\n        self.share_pos_embed = share_pos_embed\n        self.build_pos_embedding(\n            share_pos_embed=share_pos_embed,\n            learnable_pos_embed=learnable_pos_embed,\n            patches_layout=patches_layout,\n            first_patch_idx=first_patch_idx,\n            embed_dim=embed_dim,\n            init_pos_embed_random=init_pos_embed_random,\n        )\n        self.pos_sum_embed_only = pos_sum_embed_only\n        if pos_sum_embed_only:\n            # another flag to catch if someone set this accidentally\n            # recommended to use the `PosEmbedSumDecoder` class\n            assert decoder_depth == -1, \"Do not specify decoder_depth\"\n            return\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n        self.norm = norm_layer(decoder_embed_dim)\n\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, decoder_depth)\n        ]  # stochastic depth decay rule\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=decoder_embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n        self.return_interim_layers = return_interim_layers", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert first_patch_idx == 0 or first_patch_idx == 1\n        self.share_pos_embed = share_pos_embed\n        self.build_pos_embedding(\n            share_pos_embed=share_pos_embed,\n            learnable_pos_embed=learnable_pos_embed,\n            patches_layout=patches_layout,\n            first_patch_idx=first_patch_idx,\n            embed_dim=embed_dim,\n            init_pos_embed_random=init_pos_embed_random,\n        )\n        self.pos_sum_embed_only = pos_sum_embed_only\n        if pos_sum_embed_only:\n            # another flag to catch if someone set this accidentally\n            # recommended to use the `PosEmbedSumDecoder` class\n            assert decoder_depth == -1, \"Do not specify decoder_depth\"\n            return\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n        self.norm = norm_layer(decoder_embed_dim)\n\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, decoder_depth)\n        ]  # stochastic depth decay rule\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=decoder_embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n        self.return_interim_layers = return_interim_layers\n        self.final_projection = None\n        if final_projection is not None:\n            self.final_projection = hydra.utils.instantiate(\n                final_projection, _convert_=\"all\", _recursive_=False\n            )\n\n    def build_pos_embedding(\n        self,\n        share_pos_embed,\n        learnable_pos_embed,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.pos_sum_embed_only = pos_sum_embed_only\n        if pos_sum_embed_only:\n            # another flag to catch if someone set this accidentally\n            # recommended to use the `PosEmbedSumDecoder` class\n            assert decoder_depth == -1, \"Do not specify decoder_depth\"\n            return\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n        self.norm = norm_layer(decoder_embed_dim)\n\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, decoder_depth)\n        ]  # stochastic depth decay rule\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=decoder_embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n        self.return_interim_layers = return_interim_layers\n        self.final_projection = None\n        if final_projection is not None:\n            self.final_projection = hydra.utils.instantiate(\n                final_projection, _convert_=\"all\", _recursive_=False\n            )\n\n    def build_pos_embedding(\n        self,\n        share_pos_embed,\n        learnable_pos_embed,\n        patches_layout,\n        first_patch_idx,\n        embed_dim,\n        init_pos_embed_random,\n    ):\n        if share_pos_embed is True:\n            # we expect pos_embed to be passed during `forward`\n            # sharing nn.Parameter objects across modules is not recommended practice in PyTorch\n            self.pos_embed = None\n        elif learnable_pos_embed is True:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, decoder_depth)\n        ]  # stochastic depth decay rule\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=decoder_embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n        self.return_interim_layers = return_interim_layers\n        self.final_projection = None\n        if final_projection is not None:\n            self.final_projection = hydra.utils.instantiate(\n                final_projection, _convert_=\"all\", _recursive_=False\n            )\n\n    def build_pos_embedding(\n        self,\n        share_pos_embed,\n        learnable_pos_embed,\n        patches_layout,\n        first_patch_idx,\n        embed_dim,\n        init_pos_embed_random,\n    ):\n        if share_pos_embed is True:\n            # we expect pos_embed to be passed during `forward`\n            # sharing nn.Parameter objects across modules is not recommended practice in PyTorch\n            self.pos_embed = None\n        elif learnable_pos_embed is True:\n            self.pos_embed = nn.Parameter(\n                # adding first_patch_idx since it is 1 if there is cls token, else 0\n                torch.zeros(1, np.prod(patches_layout) + first_patch_idx, embed_dim)\n            )\n            if init_pos_embed_random:\n                trunc_normal_(self.pos_embed, std=0.02)\n        else:\n            self.register_buffer(\n                \"pos_embed\",\n                get_sinusoid_encoding_table(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n        self.return_interim_layers = return_interim_layers\n        self.final_projection = None\n        if final_projection is not None:\n            self.final_projection = hydra.utils.instantiate(\n                final_projection, _convert_=\"all\", _recursive_=False\n            )\n\n    def build_pos_embedding(\n        self,\n        share_pos_embed,\n        learnable_pos_embed,\n        patches_layout,\n        first_patch_idx,\n        embed_dim,\n        init_pos_embed_random,\n    ):\n        if share_pos_embed is True:\n            # we expect pos_embed to be passed during `forward`\n            # sharing nn.Parameter objects across modules is not recommended practice in PyTorch\n            self.pos_embed = None\n        elif learnable_pos_embed is True:\n            self.pos_embed = nn.Parameter(\n                # adding first_patch_idx since it is 1 if there is cls token, else 0\n                torch.zeros(1, np.prod(patches_layout) + first_patch_idx, embed_dim)\n            )\n            if init_pos_embed_random:\n                trunc_normal_(self.pos_embed, std=0.02)\n        else:\n            self.register_buffer(\n                \"pos_embed\",\n                get_sinusoid_encoding_table(\n                    np.prod(patches_layout) + first_patch_idx, embed_dim\n                ),\n            )\n\n    def forward(\n        self,\n        x,\n        orig_input_shape,\n        input_pos_embed=None,\n        use_checkpoint=False,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.final_projection = None\n        if final_projection is not None:\n            self.final_projection = hydra.utils.instantiate(\n                final_projection, _convert_=\"all\", _recursive_=False\n            )\n\n    def build_pos_embedding(\n        self,\n        share_pos_embed,\n        learnable_pos_embed,\n        patches_layout,\n        first_patch_idx,\n        embed_dim,\n        init_pos_embed_random,\n    ):\n        if share_pos_embed is True:\n            # we expect pos_embed to be passed during `forward`\n            # sharing nn.Parameter objects across modules is not recommended practice in PyTorch\n            self.pos_embed = None\n        elif learnable_pos_embed is True:\n            self.pos_embed = nn.Parameter(\n                # adding first_patch_idx since it is 1 if there is cls token, else 0\n                torch.zeros(1, np.prod(patches_layout) + first_patch_idx, embed_dim)\n            )\n            if init_pos_embed_random:\n                trunc_normal_(self.pos_embed, std=0.02)\n        else:\n            self.register_buffer(\n                \"pos_embed\",\n                get_sinusoid_encoding_table(\n                    np.prod(patches_layout) + first_patch_idx, embed_dim\n                ),\n            )\n\n    def forward(\n        self,\n        x,\n        orig_input_shape,\n        input_pos_embed=None,\n        use_checkpoint=False,\n    ):\n        curr_pos_embed = input_pos_embed if self.share_pos_embed else self.pos_embed\n        pos_embed = VisionTransformer.get_pos_embedding(\n            x.size(1) - self.first_patch_idx,\n            curr_pos_embed,\n            self.patches_layout,\n            input_shape=orig_input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        x = x + pos_embed", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_365-415"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patches_layout,\n        first_patch_idx,\n        embed_dim,\n        init_pos_embed_random,\n    ):\n        if share_pos_embed is True:\n            # we expect pos_embed to be passed during `forward`\n            # sharing nn.Parameter objects across modules is not recommended practice in PyTorch\n            self.pos_embed = None\n        elif learnable_pos_embed is True:\n            self.pos_embed = nn.Parameter(\n                # adding first_patch_idx since it is 1 if there is cls token, else 0\n                torch.zeros(1, np.prod(patches_layout) + first_patch_idx, embed_dim)\n            )\n            if init_pos_embed_random:\n                trunc_normal_(self.pos_embed, std=0.02)\n        else:\n            self.register_buffer(\n                \"pos_embed\",\n                get_sinusoid_encoding_table(\n                    np.prod(patches_layout) + first_patch_idx, embed_dim\n                ),\n            )\n\n    def forward(\n        self,\n        x,\n        orig_input_shape,\n        input_pos_embed=None,\n        use_checkpoint=False,\n    ):\n        curr_pos_embed = input_pos_embed if self.share_pos_embed else self.pos_embed\n        pos_embed = VisionTransformer.get_pos_embedding(\n            x.size(1) - self.first_patch_idx,\n            curr_pos_embed,\n            self.patches_layout,\n            input_shape=orig_input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        x = x + pos_embed\n        if self.pos_sum_embed_only:\n            return x\n        x = self.decoder_embed(x)\n        interim = []\n        for i, blk in enumerate(self.decoder_blocks):\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            if self.return_interim_layers or i == (len(self.decoder_blocks) - 1):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_375-425"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.pos_embed = nn.Parameter(\n                # adding first_patch_idx since it is 1 if there is cls token, else 0\n                torch.zeros(1, np.prod(patches_layout) + first_patch_idx, embed_dim)\n            )\n            if init_pos_embed_random:\n                trunc_normal_(self.pos_embed, std=0.02)\n        else:\n            self.register_buffer(\n                \"pos_embed\",\n                get_sinusoid_encoding_table(\n                    np.prod(patches_layout) + first_patch_idx, embed_dim\n                ),\n            )\n\n    def forward(\n        self,\n        x,\n        orig_input_shape,\n        input_pos_embed=None,\n        use_checkpoint=False,\n    ):\n        curr_pos_embed = input_pos_embed if self.share_pos_embed else self.pos_embed\n        pos_embed = VisionTransformer.get_pos_embedding(\n            x.size(1) - self.first_patch_idx,\n            curr_pos_embed,\n            self.patches_layout,\n            input_shape=orig_input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        x = x + pos_embed\n        if self.pos_sum_embed_only:\n            return x\n        x = self.decoder_embed(x)\n        interim = []\n        for i, blk in enumerate(self.decoder_blocks):\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            if self.return_interim_layers or i == (len(self.decoder_blocks) - 1):\n                interim.append(x)\n\n        interim = [self.norm(el) for el in interim]\n        if self.final_projection is not None:\n            interim = [self.final_projection(el) for el in interim]\n        if self.return_interim_layers:\n            return interim\n        return interim[-1]\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_385-435"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    np.prod(patches_layout) + first_patch_idx, embed_dim\n                ),\n            )\n\n    def forward(\n        self,\n        x,\n        orig_input_shape,\n        input_pos_embed=None,\n        use_checkpoint=False,\n    ):\n        curr_pos_embed = input_pos_embed if self.share_pos_embed else self.pos_embed\n        pos_embed = VisionTransformer.get_pos_embedding(\n            x.size(1) - self.first_patch_idx,\n            curr_pos_embed,\n            self.patches_layout,\n            input_shape=orig_input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        x = x + pos_embed\n        if self.pos_sum_embed_only:\n            return x\n        x = self.decoder_embed(x)\n        interim = []\n        for i, blk in enumerate(self.decoder_blocks):\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            if self.return_interim_layers or i == (len(self.decoder_blocks) - 1):\n                interim.append(x)\n\n        interim = [self.norm(el) for el in interim]\n        if self.final_projection is not None:\n            interim = [self.final_projection(el) for el in interim]\n        if self.return_interim_layers:\n            return interim\n        return interim[-1]\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"\n    Vision transformer. Adding stochastic depth makes it a DeiT.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_395-445"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ):\n        curr_pos_embed = input_pos_embed if self.share_pos_embed else self.pos_embed\n        pos_embed = VisionTransformer.get_pos_embedding(\n            x.size(1) - self.first_patch_idx,\n            curr_pos_embed,\n            self.patches_layout,\n            input_shape=orig_input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        x = x + pos_embed\n        if self.pos_sum_embed_only:\n            return x\n        x = self.decoder_embed(x)\n        interim = []\n        for i, blk in enumerate(self.decoder_blocks):\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            if self.return_interim_layers or i == (len(self.decoder_blocks) - 1):\n                interim.append(x)\n\n        interim = [self.norm(el) for el in interim]\n        if self.final_projection is not None:\n            interim = [self.final_projection(el) for el in interim]\n        if self.return_interim_layers:\n            return interim\n        return interim[-1]\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"\n    Vision transformer. Adding stochastic depth makes it a DeiT.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"cls_token\",\n        use_cls_token=True,\n        learnable_pos_embed=True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_405-455"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if self.pos_sum_embed_only:\n            return x\n        x = self.decoder_embed(x)\n        interim = []\n        for i, blk in enumerate(self.decoder_blocks):\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            if self.return_interim_layers or i == (len(self.decoder_blocks) - 1):\n                interim.append(x)\n\n        interim = [self.norm(el) for el in interim]\n        if self.final_projection is not None:\n            interim = [self.final_projection(el) for el in interim]\n        if self.return_interim_layers:\n            return interim\n        return interim[-1]\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"\n    Vision transformer. Adding stochastic depth makes it a DeiT.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"cls_token\",\n        use_cls_token=True,\n        learnable_pos_embed=True,\n        layer_scale_type=None,\n        layer_scale_init_value=1e-4,\n        patch_embed_type=\"linear\",\n        patch_embed_params_list=None,\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_415-465"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                interim.append(x)\n\n        interim = [self.norm(el) for el in interim]\n        if self.final_projection is not None:\n            interim = [self.final_projection(el) for el in interim]\n        if self.return_interim_layers:\n            return interim\n        return interim[-1]\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"\n    Vision transformer. Adding stochastic depth makes it a DeiT.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"cls_token\",\n        use_cls_token=True,\n        learnable_pos_embed=True,\n        layer_scale_type=None,\n        layer_scale_init_value=1e-4,\n        patch_embed_type=\"linear\",\n        patch_embed_params_list=None,\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n        patch_drop_max_patches=-1,\n    ):\n        super().__init__()\n\n        assert use_cls_token or classifier_feature == \"global_pool\"\n        self.patch_drop_max_patches = patch_drop_max_patches\n        self.masked_image_modeling = masked_image_modeling\n\n        self.add_pos_same_dtype = add_pos_same_dtype", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_425-475"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class VisionTransformer(nn.Module):\n    \"\"\"\n    Vision transformer. Adding stochastic depth makes it a DeiT.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"cls_token\",\n        use_cls_token=True,\n        learnable_pos_embed=True,\n        layer_scale_type=None,\n        layer_scale_init_value=1e-4,\n        patch_embed_type=\"linear\",\n        patch_embed_params_list=None,\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n        patch_drop_max_patches=-1,\n    ):\n        super().__init__()\n\n        assert use_cls_token or classifier_feature == \"global_pool\"\n        self.patch_drop_max_patches = patch_drop_max_patches\n        self.masked_image_modeling = masked_image_modeling\n\n        self.add_pos_same_dtype = add_pos_same_dtype\n\n        # turn off mae masking for eval\n        self.patch_dropping = patch_dropping\n\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n\n        self.num_features = (\n            self.embed_dim\n        ) = embed_dim  # num_features for consistency with other models\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargargargargargargargargargargargargargargargargargargargargargargargargConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantUnaryOp(USubConstant))Expr(Call(Attribute(Call(Name(Load))Load)))Assert(BoolOp(OrName(Load)Compare(Name(Load)EqConstant)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Name(Store)Call(Name(Load)Attribute(Name(Load)Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_435-485"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        embed_dim=768,\n        depth=12,\n        mlp_ratio=4,\n        attn_target=None,\n        drop_rate=0.0,\n        drop_path_rate=0.1,\n        drop_path_type=\"progressive\",\n        classifier_feature=\"cls_token\",\n        use_cls_token=True,\n        learnable_pos_embed=True,\n        layer_scale_type=None,\n        layer_scale_init_value=1e-4,\n        patch_embed_type=\"linear\",\n        patch_embed_params_list=None,\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n        patch_drop_max_patches=-1,\n    ):\n        super().__init__()\n\n        assert use_cls_token or classifier_feature == \"global_pool\"\n        self.patch_drop_max_patches = patch_drop_max_patches\n        self.masked_image_modeling = masked_image_modeling\n\n        self.add_pos_same_dtype = add_pos_same_dtype\n\n        # turn off mae masking for eval\n        self.patch_dropping = patch_dropping\n\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n\n        self.num_features = (\n            self.embed_dim\n        ) = embed_dim  # num_features for consistency with other models\n\n        assert classifier_feature in [\"cls_token\", \"global_pool\"]\n        self.classifier_feature = classifier_feature\n\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        if patch_embed_type == \"linear\":\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_445-495"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        layer_scale_type=None,\n        layer_scale_init_value=1e-4,\n        patch_embed_type=\"linear\",\n        patch_embed_params_list=None,\n        layer_norm_eps=1e-6,\n        masked_image_modeling=False,\n        add_pos_same_dtype=False,\n        patch_dropping=False,\n        post_encoder_params=None,\n        decoder=None,\n        mask_token_embed_dim=None,\n        patch_drop_max_patches=-1,\n    ):\n        super().__init__()\n\n        assert use_cls_token or classifier_feature == \"global_pool\"\n        self.patch_drop_max_patches = patch_drop_max_patches\n        self.masked_image_modeling = masked_image_modeling\n\n        self.add_pos_same_dtype = add_pos_same_dtype\n\n        # turn off mae masking for eval\n        self.patch_dropping = patch_dropping\n\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n\n        self.num_features = (\n            self.embed_dim\n        ) = embed_dim  # num_features for consistency with other models\n\n        assert classifier_feature in [\"cls_token\", \"global_pool\"]\n        self.classifier_feature = classifier_feature\n\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        if patch_embed_type == \"linear\":\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n            )\n\n        elif patch_embed_type == \"generic\":\n\n            self.patch_embed = PatchEmbedGeneric(\n                patch_embed_params_list, img_size=img_size\n            )\n\n        num_patches = self.patch_embed.num_patches", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_455-505"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mask_token_embed_dim=None,\n        patch_drop_max_patches=-1,\n    ):\n        super().__init__()\n\n        assert use_cls_token or classifier_feature == \"global_pool\"\n        self.patch_drop_max_patches = patch_drop_max_patches\n        self.masked_image_modeling = masked_image_modeling\n\n        self.add_pos_same_dtype = add_pos_same_dtype\n\n        # turn off mae masking for eval\n        self.patch_dropping = patch_dropping\n\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n\n        self.num_features = (\n            self.embed_dim\n        ) = embed_dim  # num_features for consistency with other models\n\n        assert classifier_feature in [\"cls_token\", \"global_pool\"]\n        self.classifier_feature = classifier_feature\n\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        if patch_embed_type == \"linear\":\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n            )\n\n        elif patch_embed_type == \"generic\":\n\n            self.patch_embed = PatchEmbedGeneric(\n                patch_embed_params_list, img_size=img_size\n            )\n\n        num_patches = self.patch_embed.num_patches\n        assert (\n            self.patch_embed.patches_layout[-1] == self.patch_embed.patches_layout[-2]\n        ), \"Interpolation of pos embed not supported for non-square layouts\"\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            self.first_patch_idx = 1\n            total_num_patches = num_patches + 1\n        else:\n            self.cls_token = None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_465-515"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # turn off mae masking for eval\n        self.patch_dropping = patch_dropping\n\n        norm_layer = partial(nn.LayerNorm, eps=layer_norm_eps)\n\n        self.num_features = (\n            self.embed_dim\n        ) = embed_dim  # num_features for consistency with other models\n\n        assert classifier_feature in [\"cls_token\", \"global_pool\"]\n        self.classifier_feature = classifier_feature\n\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        if patch_embed_type == \"linear\":\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n            )\n\n        elif patch_embed_type == \"generic\":\n\n            self.patch_embed = PatchEmbedGeneric(\n                patch_embed_params_list, img_size=img_size\n            )\n\n        num_patches = self.patch_embed.num_patches\n        assert (\n            self.patch_embed.patches_layout[-1] == self.patch_embed.patches_layout[-2]\n        ), \"Interpolation of pos embed not supported for non-square layouts\"\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            self.first_patch_idx = 1\n            total_num_patches = num_patches + 1\n        else:\n            self.cls_token = None\n            self.first_patch_idx = 0\n            total_num_patches = num_patches\n\n        if learnable_pos_embed:\n            self.pos_embed = nn.Parameter(torch.zeros(1, total_num_patches, embed_dim))\n        else:\n            self.register_buffer(\n                \"pos_embed\", get_sinusoid_encoding_table(total_num_patches, embed_dim)\n            )\n        self.pos_drop = nn.Dropout(p=drop_rate)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_475-525"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert classifier_feature in [\"cls_token\", \"global_pool\"]\n        self.classifier_feature = classifier_feature\n\n        assert in_chans == 3, \"Only 3 channels supported\"\n\n        if patch_embed_type == \"linear\":\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n            )\n\n        elif patch_embed_type == \"generic\":\n\n            self.patch_embed = PatchEmbedGeneric(\n                patch_embed_params_list, img_size=img_size\n            )\n\n        num_patches = self.patch_embed.num_patches\n        assert (\n            self.patch_embed.patches_layout[-1] == self.patch_embed.patches_layout[-2]\n        ), \"Interpolation of pos embed not supported for non-square layouts\"\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            self.first_patch_idx = 1\n            total_num_patches = num_patches + 1\n        else:\n            self.cls_token = None\n            self.first_patch_idx = 0\n            total_num_patches = num_patches\n\n        if learnable_pos_embed:\n            self.pos_embed = nn.Parameter(torch.zeros(1, total_num_patches, embed_dim))\n        else:\n            self.register_buffer(\n                \"pos_embed\", get_sinusoid_encoding_table(total_num_patches, embed_dim)\n            )\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth decay rule\n        assert drop_path_type in [\n            \"progressive\",\n            \"uniform\",\n        ], f\"Drop path types are: [progressive, uniform]. Got {drop_path_type}.\"\n        if drop_path_type == \"progressive\":\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        elif drop_path_type == \"uniform\":\n            dpr = [drop_path_rate for i in range(depth)]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_485-535"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                embed_dim=embed_dim,\n            )\n\n        elif patch_embed_type == \"generic\":\n\n            self.patch_embed = PatchEmbedGeneric(\n                patch_embed_params_list, img_size=img_size\n            )\n\n        num_patches = self.patch_embed.num_patches\n        assert (\n            self.patch_embed.patches_layout[-1] == self.patch_embed.patches_layout[-2]\n        ), \"Interpolation of pos embed not supported for non-square layouts\"\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            self.first_patch_idx = 1\n            total_num_patches = num_patches + 1\n        else:\n            self.cls_token = None\n            self.first_patch_idx = 0\n            total_num_patches = num_patches\n\n        if learnable_pos_embed:\n            self.pos_embed = nn.Parameter(torch.zeros(1, total_num_patches, embed_dim))\n        else:\n            self.register_buffer(\n                \"pos_embed\", get_sinusoid_encoding_table(total_num_patches, embed_dim)\n            )\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth decay rule\n        assert drop_path_type in [\n            \"progressive\",\n            \"uniform\",\n        ], f\"Drop path types are: [progressive, uniform]. Got {drop_path_type}.\"\n        if drop_path_type == \"progressive\":\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        elif drop_path_type == \"uniform\":\n            dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_495-545"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert (\n            self.patch_embed.patches_layout[-1] == self.patch_embed.patches_layout[-2]\n        ), \"Interpolation of pos embed not supported for non-square layouts\"\n\n        if use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            self.first_patch_idx = 1\n            total_num_patches = num_patches + 1\n        else:\n            self.cls_token = None\n            self.first_patch_idx = 0\n            total_num_patches = num_patches\n\n        if learnable_pos_embed:\n            self.pos_embed = nn.Parameter(torch.zeros(1, total_num_patches, embed_dim))\n        else:\n            self.register_buffer(\n                \"pos_embed\", get_sinusoid_encoding_table(total_num_patches, embed_dim)\n            )\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth decay rule\n        assert drop_path_type in [\n            \"progressive\",\n            \"uniform\",\n        ], f\"Drop path types are: [progressive, uniform]. Got {drop_path_type}.\"\n        if drop_path_type == \"progressive\":\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        elif drop_path_type == \"uniform\":\n            dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # FIXME: Verify if we use Post encoder, if not, remove it.\n        self.post_encoder = None\n\n        if post_encoder_params is not None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_505-555"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.first_patch_idx = 0\n            total_num_patches = num_patches\n\n        if learnable_pos_embed:\n            self.pos_embed = nn.Parameter(torch.zeros(1, total_num_patches, embed_dim))\n        else:\n            self.register_buffer(\n                \"pos_embed\", get_sinusoid_encoding_table(total_num_patches, embed_dim)\n            )\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth decay rule\n        assert drop_path_type in [\n            \"progressive\",\n            \"uniform\",\n        ], f\"Drop path types are: [progressive, uniform]. Got {drop_path_type}.\"\n        if drop_path_type == \"progressive\":\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        elif drop_path_type == \"uniform\":\n            dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # FIXME: Verify if we use Post encoder, if not, remove it.\n        self.post_encoder = None\n\n        if post_encoder_params is not None:\n            self.post_encoder = hydra.utils.instantiate(\n                post_encoder_params,\n                _convert_=\"all\",\n            )\n\n        if self.patch_dropping and decoder is None:\n            self.decoder = None\n\n        if mask_token_embed_dim is None:\n            mask_token_embed_dim = embed_dim", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_515-565"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # stochastic depth decay rule\n        assert drop_path_type in [\n            \"progressive\",\n            \"uniform\",\n        ], f\"Drop path types are: [progressive, uniform]. Got {drop_path_type}.\"\n        if drop_path_type == \"progressive\":\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        elif drop_path_type == \"uniform\":\n            dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # FIXME: Verify if we use Post encoder, if not, remove it.\n        self.post_encoder = None\n\n        if post_encoder_params is not None:\n            self.post_encoder = hydra.utils.instantiate(\n                post_encoder_params,\n                _convert_=\"all\",\n            )\n\n        if self.patch_dropping and decoder is None:\n            self.decoder = None\n\n        if mask_token_embed_dim is None:\n            mask_token_embed_dim = embed_dim\n\n        if decoder is not None:\n            self.decoder = decoder(\n                first_patch_idx=self.first_patch_idx,\n                patches_layout=self.patch_embed.patches_layout,\n                embed_dim=mask_token_embed_dim,\n            )\n\n        self.norm = norm_layer(embed_dim)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_525-575"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    attn_target=attn_target,\n                    mlp_ratio=mlp_ratio,\n                    drop=drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    layer_scale_type=layer_scale_type,\n                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # FIXME: Verify if we use Post encoder, if not, remove it.\n        self.post_encoder = None\n\n        if post_encoder_params is not None:\n            self.post_encoder = hydra.utils.instantiate(\n                post_encoder_params,\n                _convert_=\"all\",\n            )\n\n        if self.patch_dropping and decoder is None:\n            self.decoder = None\n\n        if mask_token_embed_dim is None:\n            mask_token_embed_dim = embed_dim\n\n        if decoder is not None:\n            self.decoder = decoder(\n                first_patch_idx=self.first_patch_idx,\n                patches_layout=self.patch_embed.patches_layout,\n                embed_dim=mask_token_embed_dim,\n            )\n\n        self.norm = norm_layer(embed_dim)\n\n        self.pre_logits = nn.Identity()\n\n        if learnable_pos_embed:\n            trunc_normal_(self.pos_embed, std=0.02)\n        if use_cls_token:\n            trunc_normal_(self.cls_token, std=0.02)\n        if self.patch_dropping and patch_embed_type == \"linear\":\n            # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n            w = self.patch_embed.proj.weight.data\n            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_535-585"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    layer_scale_init_value=layer_scale_init_value,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        # FIXME: Verify if we use Post encoder, if not, remove it.\n        self.post_encoder = None\n\n        if post_encoder_params is not None:\n            self.post_encoder = hydra.utils.instantiate(\n                post_encoder_params,\n                _convert_=\"all\",\n            )\n\n        if self.patch_dropping and decoder is None:\n            self.decoder = None\n\n        if mask_token_embed_dim is None:\n            mask_token_embed_dim = embed_dim\n\n        if decoder is not None:\n            self.decoder = decoder(\n                first_patch_idx=self.first_patch_idx,\n                patches_layout=self.patch_embed.patches_layout,\n                embed_dim=mask_token_embed_dim,\n            )\n\n        self.norm = norm_layer(embed_dim)\n\n        self.pre_logits = nn.Identity()\n\n        if learnable_pos_embed:\n            trunc_normal_(self.pos_embed, std=0.02)\n        if use_cls_token:\n            trunc_normal_(self.cls_token, std=0.02)\n        if self.patch_dropping and patch_embed_type == \"linear\":\n            # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n            w = self.patch_embed.proj.weight.data\n            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        self.apply(self._init_weights)\n\n        if self.masked_image_modeling:\n            assert self.patch_drop_max_patches == -1\n            # initialized to zeros following iBOT\n            self.mask_token = nn.Parameter(torch.zeros(1, mask_token_embed_dim))\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_545-595"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.post_encoder = hydra.utils.instantiate(\n                post_encoder_params,\n                _convert_=\"all\",\n            )\n\n        if self.patch_dropping and decoder is None:\n            self.decoder = None\n\n        if mask_token_embed_dim is None:\n            mask_token_embed_dim = embed_dim\n\n        if decoder is not None:\n            self.decoder = decoder(\n                first_patch_idx=self.first_patch_idx,\n                patches_layout=self.patch_embed.patches_layout,\n                embed_dim=mask_token_embed_dim,\n            )\n\n        self.norm = norm_layer(embed_dim)\n\n        self.pre_logits = nn.Identity()\n\n        if learnable_pos_embed:\n            trunc_normal_(self.pos_embed, std=0.02)\n        if use_cls_token:\n            trunc_normal_(self.cls_token, std=0.02)\n        if self.patch_dropping and patch_embed_type == \"linear\":\n            # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n            w = self.patch_embed.proj.weight.data\n            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        self.apply(self._init_weights)\n\n        if self.masked_image_modeling:\n            assert self.patch_drop_max_patches == -1\n            # initialized to zeros following iBOT\n            self.mask_token = nn.Parameter(torch.zeros(1, mask_token_embed_dim))\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            if self.patch_dropping:  # Based on MAE and official Jax ViT implementation\n                torch.nn.init.xavier_uniform_(m.weight)\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_555-605"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        if decoder is not None:\n            self.decoder = decoder(\n                first_patch_idx=self.first_patch_idx,\n                patches_layout=self.patch_embed.patches_layout,\n                embed_dim=mask_token_embed_dim,\n            )\n\n        self.norm = norm_layer(embed_dim)\n\n        self.pre_logits = nn.Identity()\n\n        if learnable_pos_embed:\n            trunc_normal_(self.pos_embed, std=0.02)\n        if use_cls_token:\n            trunc_normal_(self.cls_token, std=0.02)\n        if self.patch_dropping and patch_embed_type == \"linear\":\n            # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n            w = self.patch_embed.proj.weight.data\n            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        self.apply(self._init_weights)\n\n        if self.masked_image_modeling:\n            assert self.patch_drop_max_patches == -1\n            # initialized to zeros following iBOT\n            self.mask_token = nn.Parameter(torch.zeros(1, mask_token_embed_dim))\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            if self.patch_dropping:  # Based on MAE and official Jax ViT implementation\n                torch.nn.init.xavier_uniform_(m.weight)\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def masked_patch_drop(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        cls_token, patches = (\n            x[:, : self.first_patch_idx],\n            x[:, self.first_patch_idx :],\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_565-615"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.pre_logits = nn.Identity()\n\n        if learnable_pos_embed:\n            trunc_normal_(self.pos_embed, std=0.02)\n        if use_cls_token:\n            trunc_normal_(self.cls_token, std=0.02)\n        if self.patch_dropping and patch_embed_type == \"linear\":\n            # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n            w = self.patch_embed.proj.weight.data\n            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        self.apply(self._init_weights)\n\n        if self.masked_image_modeling:\n            assert self.patch_drop_max_patches == -1\n            # initialized to zeros following iBOT\n            self.mask_token = nn.Parameter(torch.zeros(1, mask_token_embed_dim))\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            if self.patch_dropping:  # Based on MAE and official Jax ViT implementation\n                torch.nn.init.xavier_uniform_(m.weight)\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def masked_patch_drop(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        cls_token, patches = (\n            x[:, : self.first_patch_idx],\n            x[:, self.first_patch_idx :],\n        )\n        # If the following fails, means different batch elements have\n        # different amounts of masking. To fix ensure the masking amount\n        # is fixed in the config for all datasets being trained on.\n        patches = patches[~mask].reshape(x.shape[0], -1, patches.shape[-1])\n        x = torch.cat([cls_token, patches], dim=1)\n        return x\n\n    def apply_mask(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        x[mask, :] = self.mask_token.to(x.dtype)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_575-625"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        self.apply(self._init_weights)\n\n        if self.masked_image_modeling:\n            assert self.patch_drop_max_patches == -1\n            # initialized to zeros following iBOT\n            self.mask_token = nn.Parameter(torch.zeros(1, mask_token_embed_dim))\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            if self.patch_dropping:  # Based on MAE and official Jax ViT implementation\n                torch.nn.init.xavier_uniform_(m.weight)\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def masked_patch_drop(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        cls_token, patches = (\n            x[:, : self.first_patch_idx],\n            x[:, self.first_patch_idx :],\n        )\n        # If the following fails, means different batch elements have\n        # different amounts of masking. To fix ensure the masking amount\n        # is fixed in the config for all datasets being trained on.\n        patches = patches[~mask].reshape(x.shape[0], -1, patches.shape[-1])\n        x = torch.cat([cls_token, patches], dim=1)\n        return x\n\n    def apply_mask(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        x[mask, :] = self.mask_token.to(x.dtype)\n        return x\n\n    def insert_masks(self, x, mask):\n        embed_dim = x.shape[-1]\n        mask = mask.view(x.shape[0], -1)\n        B, N = mask.shape\n        tmp = torch.empty(B, N, embed_dim).to(x.device)\n        tmp[mask] = self.mask_token.to(x.dtype)\n        tmp[~mask] = x[:, self.first_patch_idx :].reshape(-1, x.shape[-1])\n        x = torch.cat([x[:, : self.first_patch_idx], tmp], dim=1)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_585-635"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if self.patch_dropping:  # Based on MAE and official Jax ViT implementation\n                torch.nn.init.xavier_uniform_(m.weight)\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def masked_patch_drop(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        cls_token, patches = (\n            x[:, : self.first_patch_idx],\n            x[:, self.first_patch_idx :],\n        )\n        # If the following fails, means different batch elements have\n        # different amounts of masking. To fix ensure the masking amount\n        # is fixed in the config for all datasets being trained on.\n        patches = patches[~mask].reshape(x.shape[0], -1, patches.shape[-1])\n        x = torch.cat([cls_token, patches], dim=1)\n        return x\n\n    def apply_mask(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        x[mask, :] = self.mask_token.to(x.dtype)\n        return x\n\n    def insert_masks(self, x, mask):\n        embed_dim = x.shape[-1]\n        mask = mask.view(x.shape[0], -1)\n        B, N = mask.shape\n        tmp = torch.empty(B, N, embed_dim).to(x.device)\n        tmp[mask] = self.mask_token.to(x.dtype)\n        tmp[~mask] = x[:, self.first_patch_idx :].reshape(-1, x.shape[-1])\n        x = torch.cat([x[:, : self.first_patch_idx], tmp], dim=1)\n        return x\n\n    def prepare_tokens(self, x, npatch_to_keep, mask):\n        B = x.shape[0]\n        input_shape = x.shape\n\n        x = self.patch_embed(x)\n        npatch_per_img = x.shape[1]\n\n        if self.patch_dropping is False and mask is not None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_595-645"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def masked_patch_drop(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        cls_token, patches = (\n            x[:, : self.first_patch_idx],\n            x[:, self.first_patch_idx :],\n        )\n        # If the following fails, means different batch elements have\n        # different amounts of masking. To fix ensure the masking amount\n        # is fixed in the config for all datasets being trained on.\n        patches = patches[~mask].reshape(x.shape[0], -1, patches.shape[-1])\n        x = torch.cat([cls_token, patches], dim=1)\n        return x\n\n    def apply_mask(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        x[mask, :] = self.mask_token.to(x.dtype)\n        return x\n\n    def insert_masks(self, x, mask):\n        embed_dim = x.shape[-1]\n        mask = mask.view(x.shape[0], -1)\n        B, N = mask.shape\n        tmp = torch.empty(B, N, embed_dim).to(x.device)\n        tmp[mask] = self.mask_token.to(x.dtype)\n        tmp[~mask] = x[:, self.first_patch_idx :].reshape(-1, x.shape[-1])\n        x = torch.cat([x[:, : self.first_patch_idx], tmp], dim=1)\n        return x\n\n    def prepare_tokens(self, x, npatch_to_keep, mask):\n        B = x.shape[0]\n        input_shape = x.shape\n\n        x = self.patch_embed(x)\n        npatch_per_img = x.shape[1]\n\n        if self.patch_dropping is False and mask is not None:\n            x = self.apply_mask(x, mask)\n\n        if self.cls_token is not None:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            x = torch.cat((class_tokens, x), dim=1)\n\n        pos_embed = self.get_pos_embedding(\n            npatch_per_img,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_605-655"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # If the following fails, means different batch elements have\n        # different amounts of masking. To fix ensure the masking amount\n        # is fixed in the config for all datasets being trained on.\n        patches = patches[~mask].reshape(x.shape[0], -1, patches.shape[-1])\n        x = torch.cat([cls_token, patches], dim=1)\n        return x\n\n    def apply_mask(self, x, mask):\n        mask = mask.view(x.shape[0], -1)\n        x[mask, :] = self.mask_token.to(x.dtype)\n        return x\n\n    def insert_masks(self, x, mask):\n        embed_dim = x.shape[-1]\n        mask = mask.view(x.shape[0], -1)\n        B, N = mask.shape\n        tmp = torch.empty(B, N, embed_dim).to(x.device)\n        tmp[mask] = self.mask_token.to(x.dtype)\n        tmp[~mask] = x[:, self.first_patch_idx :].reshape(-1, x.shape[-1])\n        x = torch.cat([x[:, : self.first_patch_idx], tmp], dim=1)\n        return x\n\n    def prepare_tokens(self, x, npatch_to_keep, mask):\n        B = x.shape[0]\n        input_shape = x.shape\n\n        x = self.patch_embed(x)\n        npatch_per_img = x.shape[1]\n\n        if self.patch_dropping is False and mask is not None:\n            x = self.apply_mask(x, mask)\n\n        if self.cls_token is not None:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            x = torch.cat((class_tokens, x), dim=1)\n\n        pos_embed = self.get_pos_embedding(\n            npatch_per_img,\n            self.pos_embed,\n            self.patch_embed.patches_layout,\n            input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        if self.add_pos_same_dtype:\n            pos_embed = pos_embed.type_as(x)\n        x = x + pos_embed\n\n        if self.patch_dropping and mask is not None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_615-665"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return x\n\n    def insert_masks(self, x, mask):\n        embed_dim = x.shape[-1]\n        mask = mask.view(x.shape[0], -1)\n        B, N = mask.shape\n        tmp = torch.empty(B, N, embed_dim).to(x.device)\n        tmp[mask] = self.mask_token.to(x.dtype)\n        tmp[~mask] = x[:, self.first_patch_idx :].reshape(-1, x.shape[-1])\n        x = torch.cat([x[:, : self.first_patch_idx], tmp], dim=1)\n        return x\n\n    def prepare_tokens(self, x, npatch_to_keep, mask):\n        B = x.shape[0]\n        input_shape = x.shape\n\n        x = self.patch_embed(x)\n        npatch_per_img = x.shape[1]\n\n        if self.patch_dropping is False and mask is not None:\n            x = self.apply_mask(x, mask)\n\n        if self.cls_token is not None:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            x = torch.cat((class_tokens, x), dim=1)\n\n        pos_embed = self.get_pos_embedding(\n            npatch_per_img,\n            self.pos_embed,\n            self.patch_embed.patches_layout,\n            input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        if self.add_pos_same_dtype:\n            pos_embed = pos_embed.type_as(x)\n        x = x + pos_embed\n\n        if self.patch_dropping and mask is not None:\n            x = self.masked_patch_drop(x, mask)\n        x = self.pos_drop(x)\n        return x\n\n    @classmethod\n    def get_pos_embedding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_625-675"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return x\n\n    def prepare_tokens(self, x, npatch_to_keep, mask):\n        B = x.shape[0]\n        input_shape = x.shape\n\n        x = self.patch_embed(x)\n        npatch_per_img = x.shape[1]\n\n        if self.patch_dropping is False and mask is not None:\n            x = self.apply_mask(x, mask)\n\n        if self.cls_token is not None:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            x = torch.cat((class_tokens, x), dim=1)\n\n        pos_embed = self.get_pos_embedding(\n            npatch_per_img,\n            self.pos_embed,\n            self.patch_embed.patches_layout,\n            input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        if self.add_pos_same_dtype:\n            pos_embed = pos_embed.type_as(x)\n        x = x + pos_embed\n\n        if self.patch_dropping and mask is not None:\n            x = self.masked_patch_drop(x, mask)\n        x = self.pos_drop(x)\n        return x\n\n    @classmethod\n    def get_pos_embedding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape,\n        first_patch_idx=1,\n    ):\n        pos_embed = cls.interpolate_pos_encoding(\n            npatch_per_img,\n            pos_embed,\n            patches_layout,\n            input_shape=input_shape,\n            first_patch_idx=first_patch_idx,\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_635-685"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            x = self.apply_mask(x, mask)\n\n        if self.cls_token is not None:\n            class_tokens = self.cls_token.expand(\n                B, -1, -1\n            )  # stole class_tokens impl from Phil Wang, thanks\n            x = torch.cat((class_tokens, x), dim=1)\n\n        pos_embed = self.get_pos_embedding(\n            npatch_per_img,\n            self.pos_embed,\n            self.patch_embed.patches_layout,\n            input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        if self.add_pos_same_dtype:\n            pos_embed = pos_embed.type_as(x)\n        x = x + pos_embed\n\n        if self.patch_dropping and mask is not None:\n            x = self.masked_patch_drop(x, mask)\n        x = self.pos_drop(x)\n        return x\n\n    @classmethod\n    def get_pos_embedding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape,\n        first_patch_idx=1,\n    ):\n        pos_embed = cls.interpolate_pos_encoding(\n            npatch_per_img,\n            pos_embed,\n            patches_layout,\n            input_shape=input_shape,\n            first_patch_idx=first_patch_idx,\n        )\n        return pos_embed\n\n    def forward_features(self, x, npatch_to_keep, mask=None, use_checkpoint=False):\n        assert npatch_to_keep is None\n        if mask is not None and isinstance(mask, list) and not all(mask):\n            mask = None\n\n        orig_input_shape = x.shape\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_645-695"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.pos_embed,\n            self.patch_embed.patches_layout,\n            input_shape,\n            first_patch_idx=self.first_patch_idx,\n        )\n        if self.add_pos_same_dtype:\n            pos_embed = pos_embed.type_as(x)\n        x = x + pos_embed\n\n        if self.patch_dropping and mask is not None:\n            x = self.masked_patch_drop(x, mask)\n        x = self.pos_drop(x)\n        return x\n\n    @classmethod\n    def get_pos_embedding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape,\n        first_patch_idx=1,\n    ):\n        pos_embed = cls.interpolate_pos_encoding(\n            npatch_per_img,\n            pos_embed,\n            patches_layout,\n            input_shape=input_shape,\n            first_patch_idx=first_patch_idx,\n        )\n        return pos_embed\n\n    def forward_features(self, x, npatch_to_keep, mask=None, use_checkpoint=False):\n        assert npatch_to_keep is None\n        if mask is not None and isinstance(mask, list) and not all(mask):\n            mask = None\n\n        orig_input_shape = x.shape\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n\n        if self.classifier_feature == \"cls_token\" and (\n            mask is None or self.decoder is None\n        ):\n            assert self.first_patch_idx == 1, \"Must have a CLS token at 0\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_655-705"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            x = self.masked_patch_drop(x, mask)\n        x = self.pos_drop(x)\n        return x\n\n    @classmethod\n    def get_pos_embedding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape,\n        first_patch_idx=1,\n    ):\n        pos_embed = cls.interpolate_pos_encoding(\n            npatch_per_img,\n            pos_embed,\n            patches_layout,\n            input_shape=input_shape,\n            first_patch_idx=first_patch_idx,\n        )\n        return pos_embed\n\n    def forward_features(self, x, npatch_to_keep, mask=None, use_checkpoint=False):\n        assert npatch_to_keep is None\n        if mask is not None and isinstance(mask, list) and not all(mask):\n            mask = None\n\n        orig_input_shape = x.shape\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n\n        if self.classifier_feature == \"cls_token\" and (\n            mask is None or self.decoder is None\n        ):\n            assert self.first_patch_idx == 1, \"Must have a CLS token at 0\"\n            x = x[:, 0]\n        elif self.classifier_feature == \"global_pool\" and (\n            mask is None or self.decoder is None\n        ):\n            x = x[:, self.first_patch_idx :, ...].mean(dim=1)\n        elif self.patch_dropping and mask is not None and self.decoder is not None:\n            x = self.norm(x)\n            if self.post_encoder:\n                x_dtype = x.dtype\n                x = self.post_encoder(x).to(x_dtype)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_665-715"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        input_shape,\n        first_patch_idx=1,\n    ):\n        pos_embed = cls.interpolate_pos_encoding(\n            npatch_per_img,\n            pos_embed,\n            patches_layout,\n            input_shape=input_shape,\n            first_patch_idx=first_patch_idx,\n        )\n        return pos_embed\n\n    def forward_features(self, x, npatch_to_keep, mask=None, use_checkpoint=False):\n        assert npatch_to_keep is None\n        if mask is not None and isinstance(mask, list) and not all(mask):\n            mask = None\n\n        orig_input_shape = x.shape\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n\n        if self.classifier_feature == \"cls_token\" and (\n            mask is None or self.decoder is None\n        ):\n            assert self.first_patch_idx == 1, \"Must have a CLS token at 0\"\n            x = x[:, 0]\n        elif self.classifier_feature == \"global_pool\" and (\n            mask is None or self.decoder is None\n        ):\n            x = x[:, self.first_patch_idx :, ...].mean(dim=1)\n        elif self.patch_dropping and mask is not None and self.decoder is not None:\n            x = self.norm(x)\n            if self.post_encoder:\n                x_dtype = x.dtype\n                x = self.post_encoder(x).to(x_dtype)\n            x = self.insert_masks(x, mask)\n            if self.first_patch_idx == 0:\n                cls_token = None\n            else:\n                cls_token = x[:, self.first_patch_idx]\n            x = self.decoder(\n                x, orig_input_shape, self.pos_embed, use_checkpoint=use_checkpoint\n            )\n            # cls_token comes from the encoder and the x comes from\n            # the decoder. Since they can have different dimensionality, they", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_675-725"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return pos_embed\n\n    def forward_features(self, x, npatch_to_keep, mask=None, use_checkpoint=False):\n        assert npatch_to_keep is None\n        if mask is not None and isinstance(mask, list) and not all(mask):\n            mask = None\n\n        orig_input_shape = x.shape\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n\n        if self.classifier_feature == \"cls_token\" and (\n            mask is None or self.decoder is None\n        ):\n            assert self.first_patch_idx == 1, \"Must have a CLS token at 0\"\n            x = x[:, 0]\n        elif self.classifier_feature == \"global_pool\" and (\n            mask is None or self.decoder is None\n        ):\n            x = x[:, self.first_patch_idx :, ...].mean(dim=1)\n        elif self.patch_dropping and mask is not None and self.decoder is not None:\n            x = self.norm(x)\n            if self.post_encoder:\n                x_dtype = x.dtype\n                x = self.post_encoder(x).to(x_dtype)\n            x = self.insert_masks(x, mask)\n            if self.first_patch_idx == 0:\n                cls_token = None\n            else:\n                cls_token = x[:, self.first_patch_idx]\n            x = self.decoder(\n                x, orig_input_shape, self.pos_embed, use_checkpoint=use_checkpoint\n            )\n            # cls_token comes from the encoder and the x comes from\n            # the decoder. Since they can have different dimensionality, they\n            # can't be concatenated together and have to be returned as a tuple\n\n            if isinstance(x, list):\n                decoder_patch_features = [el[:, self.first_patch_idx :] for el in x]\n            else:\n                decoder_patch_features = x[:, self.first_patch_idx :]\n            return cls_token, decoder_patch_features\n        elif mask is not None:\n            pass\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_685-735"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n\n        if self.classifier_feature == \"cls_token\" and (\n            mask is None or self.decoder is None\n        ):\n            assert self.first_patch_idx == 1, \"Must have a CLS token at 0\"\n            x = x[:, 0]\n        elif self.classifier_feature == \"global_pool\" and (\n            mask is None or self.decoder is None\n        ):\n            x = x[:, self.first_patch_idx :, ...].mean(dim=1)\n        elif self.patch_dropping and mask is not None and self.decoder is not None:\n            x = self.norm(x)\n            if self.post_encoder:\n                x_dtype = x.dtype\n                x = self.post_encoder(x).to(x_dtype)\n            x = self.insert_masks(x, mask)\n            if self.first_patch_idx == 0:\n                cls_token = None\n            else:\n                cls_token = x[:, self.first_patch_idx]\n            x = self.decoder(\n                x, orig_input_shape, self.pos_embed, use_checkpoint=use_checkpoint\n            )\n            # cls_token comes from the encoder and the x comes from\n            # the decoder. Since they can have different dimensionality, they\n            # can't be concatenated together and have to be returned as a tuple\n\n            if isinstance(x, list):\n                decoder_patch_features = [el[:, self.first_patch_idx :] for el in x]\n            else:\n                decoder_patch_features = x[:, self.first_patch_idx :]\n            return cls_token, decoder_patch_features\n        elif mask is not None:\n            pass\n\n        x = self.norm(x)\n        return self.pre_logits(x)\n\n    def get_intermediate_features(\n        self, x, names, npatch_to_keep, mask, use_checkpoint=False\n    ):\n        interms = []\n\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_695-745"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            x = x[:, 0]\n        elif self.classifier_feature == \"global_pool\" and (\n            mask is None or self.decoder is None\n        ):\n            x = x[:, self.first_patch_idx :, ...].mean(dim=1)\n        elif self.patch_dropping and mask is not None and self.decoder is not None:\n            x = self.norm(x)\n            if self.post_encoder:\n                x_dtype = x.dtype\n                x = self.post_encoder(x).to(x_dtype)\n            x = self.insert_masks(x, mask)\n            if self.first_patch_idx == 0:\n                cls_token = None\n            else:\n                cls_token = x[:, self.first_patch_idx]\n            x = self.decoder(\n                x, orig_input_shape, self.pos_embed, use_checkpoint=use_checkpoint\n            )\n            # cls_token comes from the encoder and the x comes from\n            # the decoder. Since they can have different dimensionality, they\n            # can't be concatenated together and have to be returned as a tuple\n\n            if isinstance(x, list):\n                decoder_patch_features = [el[:, self.first_patch_idx :] for el in x]\n            else:\n                decoder_patch_features = x[:, self.first_patch_idx :]\n            return cls_token, decoder_patch_features\n        elif mask is not None:\n            pass\n\n        x = self.norm(x)\n        return self.pre_logits(x)\n\n    def get_intermediate_features(\n        self, x, names, npatch_to_keep, mask, use_checkpoint=False\n    ):\n        interms = []\n\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        # get feature from every intermediate block and apply norm\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            interms.append(self.norm(x))\n\n        if self.post_encoder:\n            assert len(names) == 1 and names[0] in [\"last_all\"]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_705-755"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            x = self.insert_masks(x, mask)\n            if self.first_patch_idx == 0:\n                cls_token = None\n            else:\n                cls_token = x[:, self.first_patch_idx]\n            x = self.decoder(\n                x, orig_input_shape, self.pos_embed, use_checkpoint=use_checkpoint\n            )\n            # cls_token comes from the encoder and the x comes from\n            # the decoder. Since they can have different dimensionality, they\n            # can't be concatenated together and have to be returned as a tuple\n\n            if isinstance(x, list):\n                decoder_patch_features = [el[:, self.first_patch_idx :] for el in x]\n            else:\n                decoder_patch_features = x[:, self.first_patch_idx :]\n            return cls_token, decoder_patch_features\n        elif mask is not None:\n            pass\n\n        x = self.norm(x)\n        return self.pre_logits(x)\n\n    def get_intermediate_features(\n        self, x, names, npatch_to_keep, mask, use_checkpoint=False\n    ):\n        interms = []\n\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        # get feature from every intermediate block and apply norm\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            interms.append(self.norm(x))\n\n        if self.post_encoder:\n            assert len(names) == 1 and names[0] in [\"last_all\"]\n            interms.append(self.post_encoder(interms[-1]))\n\n        # feature names are as follows\n        # blkCLS[integer] => CLS token of blk[integer]\n        # concatCLS[integer] => concat of CLS token from last \"integer\" blocks\n        # lastCLS => CLS token of last block\n\n        output = []\n\n        for name in names:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_715-765"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            # can't be concatenated together and have to be returned as a tuple\n\n            if isinstance(x, list):\n                decoder_patch_features = [el[:, self.first_patch_idx :] for el in x]\n            else:\n                decoder_patch_features = x[:, self.first_patch_idx :]\n            return cls_token, decoder_patch_features\n        elif mask is not None:\n            pass\n\n        x = self.norm(x)\n        return self.pre_logits(x)\n\n    def get_intermediate_features(\n        self, x, names, npatch_to_keep, mask, use_checkpoint=False\n    ):\n        interms = []\n\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        # get feature from every intermediate block and apply norm\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            interms.append(self.norm(x))\n\n        if self.post_encoder:\n            assert len(names) == 1 and names[0] in [\"last_all\"]\n            interms.append(self.post_encoder(interms[-1]))\n\n        # feature names are as follows\n        # blkCLS[integer] => CLS token of blk[integer]\n        # concatCLS[integer] => concat of CLS token from last \"integer\" blocks\n        # lastCLS => CLS token of last block\n\n        output = []\n\n        for name in names:\n            if name.startswith(\"blkCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"blkCLS\", \"\"))\n                output.append(interms[v][:, 0])\n            elif name.startswith(\"concatCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"concatCLS\", \"\"))\n                feat = torch.cat([x[:, 0] for x in interms[-v:]], dim=-1)\n                output.append(feat)\n            elif name == \"lastCLS\":", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_725-775"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x = self.norm(x)\n        return self.pre_logits(x)\n\n    def get_intermediate_features(\n        self, x, names, npatch_to_keep, mask, use_checkpoint=False\n    ):\n        interms = []\n\n        x = self.prepare_tokens(x, npatch_to_keep, mask=mask)\n\n        # get feature from every intermediate block and apply norm\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            interms.append(self.norm(x))\n\n        if self.post_encoder:\n            assert len(names) == 1 and names[0] in [\"last_all\"]\n            interms.append(self.post_encoder(interms[-1]))\n\n        # feature names are as follows\n        # blkCLS[integer] => CLS token of blk[integer]\n        # concatCLS[integer] => concat of CLS token from last \"integer\" blocks\n        # lastCLS => CLS token of last block\n\n        output = []\n\n        for name in names:\n            if name.startswith(\"blkCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"blkCLS\", \"\"))\n                output.append(interms[v][:, 0])\n            elif name.startswith(\"concatCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"concatCLS\", \"\"))\n                feat = torch.cat([x[:, 0] for x in interms[-v:]], dim=-1)\n                output.append(feat)\n            elif name == \"lastCLS\":\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                output.append(interms[-1][:, 0])\n            elif name == \"last_all\":\n                output.append(interms[-1])\n            elif name == \"last_patch_avg\":\n                output.append(interms[-1][:, self.first_patch_idx :, ...].mean(dim=1))\n        return output\n\n    def forward(\n        self,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_735-785"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # get feature from every intermediate block and apply norm\n        for blk in self.blocks:\n            if use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, use_reentrant=False)\n            else:\n                x = blk(x)\n            interms.append(self.norm(x))\n\n        if self.post_encoder:\n            assert len(names) == 1 and names[0] in [\"last_all\"]\n            interms.append(self.post_encoder(interms[-1]))\n\n        # feature names are as follows\n        # blkCLS[integer] => CLS token of blk[integer]\n        # concatCLS[integer] => concat of CLS token from last \"integer\" blocks\n        # lastCLS => CLS token of last block\n\n        output = []\n\n        for name in names:\n            if name.startswith(\"blkCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"blkCLS\", \"\"))\n                output.append(interms[v][:, 0])\n            elif name.startswith(\"concatCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"concatCLS\", \"\"))\n                feat = torch.cat([x[:, 0] for x in interms[-v:]], dim=-1)\n                output.append(feat)\n            elif name == \"lastCLS\":\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                output.append(interms[-1][:, 0])\n            elif name == \"last_all\":\n                output.append(interms[-1])\n            elif name == \"last_patch_avg\":\n                output.append(interms[-1][:, self.first_patch_idx :, ...].mean(dim=1))\n        return output\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        npatch_to_keep: int = None,\n        use_checkpoint: bool = False,\n        mask: Optional[torch.Tensor] = None,\n    ) -> List[torch.Tensor]:\n        assert (not self.masked_image_modeling) or (mask is not None)\n        if out_feat_keys is None or len(out_feat_keys) == 0:\n            x = self.forward_features(\n                x, npatch_to_keep, mask=mask, use_checkpoint=use_checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_745-795"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            interms.append(self.post_encoder(interms[-1]))\n\n        # feature names are as follows\n        # blkCLS[integer] => CLS token of blk[integer]\n        # concatCLS[integer] => concat of CLS token from last \"integer\" blocks\n        # lastCLS => CLS token of last block\n\n        output = []\n\n        for name in names:\n            if name.startswith(\"blkCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"blkCLS\", \"\"))\n                output.append(interms[v][:, 0])\n            elif name.startswith(\"concatCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"concatCLS\", \"\"))\n                feat = torch.cat([x[:, 0] for x in interms[-v:]], dim=-1)\n                output.append(feat)\n            elif name == \"lastCLS\":\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                output.append(interms[-1][:, 0])\n            elif name == \"last_all\":\n                output.append(interms[-1])\n            elif name == \"last_patch_avg\":\n                output.append(interms[-1][:, self.first_patch_idx :, ...].mean(dim=1))\n        return output\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        npatch_to_keep: int = None,\n        use_checkpoint: bool = False,\n        mask: Optional[torch.Tensor] = None,\n    ) -> List[torch.Tensor]:\n        assert (not self.masked_image_modeling) or (mask is not None)\n        if out_feat_keys is None or len(out_feat_keys) == 0:\n            x = self.forward_features(\n                x, npatch_to_keep, mask=mask, use_checkpoint=use_checkpoint\n            )\n        else:\n            # we specified a feature layer name\n            # Follow DINO (https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L159)\n            x = self.get_intermediate_features(\n                x,\n                out_feat_keys,\n                npatch_to_keep,\n                mask=mask,\n                use_checkpoint=use_checkpoint,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_755-805"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if name.startswith(\"blkCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"blkCLS\", \"\"))\n                output.append(interms[v][:, 0])\n            elif name.startswith(\"concatCLS\"):\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                v = int(name.replace(\"concatCLS\", \"\"))\n                feat = torch.cat([x[:, 0] for x in interms[-v:]], dim=-1)\n                output.append(feat)\n            elif name == \"lastCLS\":\n                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                output.append(interms[-1][:, 0])\n            elif name == \"last_all\":\n                output.append(interms[-1])\n            elif name == \"last_patch_avg\":\n                output.append(interms[-1][:, self.first_patch_idx :, ...].mean(dim=1))\n        return output\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        npatch_to_keep: int = None,\n        use_checkpoint: bool = False,\n        mask: Optional[torch.Tensor] = None,\n    ) -> List[torch.Tensor]:\n        assert (not self.masked_image_modeling) or (mask is not None)\n        if out_feat_keys is None or len(out_feat_keys) == 0:\n            x = self.forward_features(\n                x, npatch_to_keep, mask=mask, use_checkpoint=use_checkpoint\n            )\n        else:\n            # we specified a feature layer name\n            # Follow DINO (https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L159)\n            x = self.get_intermediate_features(\n                x,\n                out_feat_keys,\n                npatch_to_keep,\n                mask=mask,\n                use_checkpoint=use_checkpoint,\n            )\n        return x\n\n    @staticmethod\n    def interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n        N = pos_embed.shape[1]\n        if N == target_spatial_size:\n            return pos_embed\n        dim = pos_embed.shape[-1]\n        pos_embed = nn.functional.interpolate(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_765-815"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                assert self.first_patch_idx == 1, \"Must have CLS token at 0\"\n                output.append(interms[-1][:, 0])\n            elif name == \"last_all\":\n                output.append(interms[-1])\n            elif name == \"last_patch_avg\":\n                output.append(interms[-1][:, self.first_patch_idx :, ...].mean(dim=1))\n        return output\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        npatch_to_keep: int = None,\n        use_checkpoint: bool = False,\n        mask: Optional[torch.Tensor] = None,\n    ) -> List[torch.Tensor]:\n        assert (not self.masked_image_modeling) or (mask is not None)\n        if out_feat_keys is None or len(out_feat_keys) == 0:\n            x = self.forward_features(\n                x, npatch_to_keep, mask=mask, use_checkpoint=use_checkpoint\n            )\n        else:\n            # we specified a feature layer name\n            # Follow DINO (https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L159)\n            x = self.get_intermediate_features(\n                x,\n                out_feat_keys,\n                npatch_to_keep,\n                mask=mask,\n                use_checkpoint=use_checkpoint,\n            )\n        return x\n\n    @staticmethod\n    def interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n        N = pos_embed.shape[1]\n        if N == target_spatial_size:\n            return pos_embed\n        dim = pos_embed.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(target_spatial_size / N),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return pos_embed\n\n    @classmethod", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_775-825"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x: torch.Tensor,\n        out_feat_keys: List[str] = None,\n        npatch_to_keep: int = None,\n        use_checkpoint: bool = False,\n        mask: Optional[torch.Tensor] = None,\n    ) -> List[torch.Tensor]:\n        assert (not self.masked_image_modeling) or (mask is not None)\n        if out_feat_keys is None or len(out_feat_keys) == 0:\n            x = self.forward_features(\n                x, npatch_to_keep, mask=mask, use_checkpoint=use_checkpoint\n            )\n        else:\n            # we specified a feature layer name\n            # Follow DINO (https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L159)\n            x = self.get_intermediate_features(\n                x,\n                out_feat_keys,\n                npatch_to_keep,\n                mask=mask,\n                use_checkpoint=use_checkpoint,\n            )\n        return x\n\n    @staticmethod\n    def interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n        N = pos_embed.shape[1]\n        if N == target_spatial_size:\n            return pos_embed\n        dim = pos_embed.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(target_spatial_size / N),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return pos_embed\n\n    @classmethod\n    def interpolate_pos_encoding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape=None,\n        first_patch_idx=1,\n    ):\n        assert (\n            first_patch_idx == 0 or first_patch_idx == 1", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_785-835"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n        else:\n            # we specified a feature layer name\n            # Follow DINO (https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L159)\n            x = self.get_intermediate_features(\n                x,\n                out_feat_keys,\n                npatch_to_keep,\n                mask=mask,\n                use_checkpoint=use_checkpoint,\n            )\n        return x\n\n    @staticmethod\n    def interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n        N = pos_embed.shape[1]\n        if N == target_spatial_size:\n            return pos_embed\n        dim = pos_embed.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(target_spatial_size / N),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return pos_embed\n\n    @classmethod\n    def interpolate_pos_encoding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape=None,\n        first_patch_idx=1,\n    ):\n        assert (\n            first_patch_idx == 0 or first_patch_idx == 1\n        ), \"there is 1 CLS token or none\"\n        N = pos_embed.shape[1] - first_patch_idx  # since it's 1 if cls_token exists\n        if npatch_per_img == N:\n            return pos_embed\n        class_emb = pos_embed[:, :first_patch_idx]\n        pos_embed = pos_embed[:, first_patch_idx:]\n\n        if input_shape is None or patches_layout[0] == 1:\n            # simple 2D pos embedding, no temporal component\n            pos_embed = cls.interpolate_pos_encoding_2d(npatch_per_img, pos_embed)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_795-845"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n        return x\n\n    @staticmethod\n    def interpolate_pos_encoding_2d(target_spatial_size, pos_embed):\n        N = pos_embed.shape[1]\n        if N == target_spatial_size:\n            return pos_embed\n        dim = pos_embed.shape[-1]\n        pos_embed = nn.functional.interpolate(\n            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(target_spatial_size / N),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return pos_embed\n\n    @classmethod\n    def interpolate_pos_encoding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape=None,\n        first_patch_idx=1,\n    ):\n        assert (\n            first_patch_idx == 0 or first_patch_idx == 1\n        ), \"there is 1 CLS token or none\"\n        N = pos_embed.shape[1] - first_patch_idx  # since it's 1 if cls_token exists\n        if npatch_per_img == N:\n            return pos_embed\n        class_emb = pos_embed[:, :first_patch_idx]\n        pos_embed = pos_embed[:, first_patch_idx:]\n\n        if input_shape is None or patches_layout[0] == 1:\n            # simple 2D pos embedding, no temporal component\n            pos_embed = cls.interpolate_pos_encoding_2d(npatch_per_img, pos_embed)\n        elif patches_layout[0] > 1:\n            # pos embed has a temporal component\n            assert len(input_shape) == 4, \"temporal interpolation not supported\"\n            # we only support 2D interpolation in this case\n            num_frames = patches_layout[0]\n            num_spatial_tokens = patches_layout[1] * patches_layout[2]\n            pos_embed = pos_embed.view(1, num_frames, num_spatial_tokens, -1)\n            # interpolate embedding for zeroth frame\n            pos_embed = cls.interpolate_pos_encoding_2d(\n                npatch_per_img, pos_embed[0, 0, ...].unsqueeze(0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_805-855"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(\n                0, 3, 1, 2\n            ),\n            scale_factor=math.sqrt(target_spatial_size / N),\n            mode=\"bicubic\",\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return pos_embed\n\n    @classmethod\n    def interpolate_pos_encoding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape=None,\n        first_patch_idx=1,\n    ):\n        assert (\n            first_patch_idx == 0 or first_patch_idx == 1\n        ), \"there is 1 CLS token or none\"\n        N = pos_embed.shape[1] - first_patch_idx  # since it's 1 if cls_token exists\n        if npatch_per_img == N:\n            return pos_embed\n        class_emb = pos_embed[:, :first_patch_idx]\n        pos_embed = pos_embed[:, first_patch_idx:]\n\n        if input_shape is None or patches_layout[0] == 1:\n            # simple 2D pos embedding, no temporal component\n            pos_embed = cls.interpolate_pos_encoding_2d(npatch_per_img, pos_embed)\n        elif patches_layout[0] > 1:\n            # pos embed has a temporal component\n            assert len(input_shape) == 4, \"temporal interpolation not supported\"\n            # we only support 2D interpolation in this case\n            num_frames = patches_layout[0]\n            num_spatial_tokens = patches_layout[1] * patches_layout[2]\n            pos_embed = pos_embed.view(1, num_frames, num_spatial_tokens, -1)\n            # interpolate embedding for zeroth frame\n            pos_embed = cls.interpolate_pos_encoding_2d(\n                npatch_per_img, pos_embed[0, 0, ...].unsqueeze(0)\n            )\n        else:\n            raise ValueError(\"This type of interpolation isn't implemented\")\n\n        return torch.cat((class_emb, pos_embed), dim=1)\n\n    def get_layer_id(self, layer_name):\n        # https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L33\n        num_layers = self.get_num_layers()\n        if layer_name in [\"cls_token\", \"pos_embed\"]:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_815-865"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def interpolate_pos_encoding(\n        cls,\n        npatch_per_img,\n        pos_embed,\n        patches_layout,\n        input_shape=None,\n        first_patch_idx=1,\n    ):\n        assert (\n            first_patch_idx == 0 or first_patch_idx == 1\n        ), \"there is 1 CLS token or none\"\n        N = pos_embed.shape[1] - first_patch_idx  # since it's 1 if cls_token exists\n        if npatch_per_img == N:\n            return pos_embed\n        class_emb = pos_embed[:, :first_patch_idx]\n        pos_embed = pos_embed[:, first_patch_idx:]\n\n        if input_shape is None or patches_layout[0] == 1:\n            # simple 2D pos embedding, no temporal component\n            pos_embed = cls.interpolate_pos_encoding_2d(npatch_per_img, pos_embed)\n        elif patches_layout[0] > 1:\n            # pos embed has a temporal component\n            assert len(input_shape) == 4, \"temporal interpolation not supported\"\n            # we only support 2D interpolation in this case\n            num_frames = patches_layout[0]\n            num_spatial_tokens = patches_layout[1] * patches_layout[2]\n            pos_embed = pos_embed.view(1, num_frames, num_spatial_tokens, -1)\n            # interpolate embedding for zeroth frame\n            pos_embed = cls.interpolate_pos_encoding_2d(\n                npatch_per_img, pos_embed[0, 0, ...].unsqueeze(0)\n            )\n        else:\n            raise ValueError(\"This type of interpolation isn't implemented\")\n\n        return torch.cat((class_emb, pos_embed), dim=1)\n\n    def get_layer_id(self, layer_name):\n        # https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L33\n        num_layers = self.get_num_layers()\n        if layer_name in [\"cls_token\", \"pos_embed\"]:\n            return 0\n        elif layer_name.find(\"patch_embed\") != -1:\n            return 0\n        elif layer_name.find(\"blocks\") != -1:\n            return int(layer_name.split(\"blocks\")[1].split(\".\")[1]) + 1\n        else:\n            return num_layers\n\n    def get_num_layers(self):\n        return len(self.blocks) + 1", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_825-875"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 875, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        ), \"there is 1 CLS token or none\"\n        N = pos_embed.shape[1] - first_patch_idx  # since it's 1 if cls_token exists\n        if npatch_per_img == N:\n            return pos_embed\n        class_emb = pos_embed[:, :first_patch_idx]\n        pos_embed = pos_embed[:, first_patch_idx:]\n\n        if input_shape is None or patches_layout[0] == 1:\n            # simple 2D pos embedding, no temporal component\n            pos_embed = cls.interpolate_pos_encoding_2d(npatch_per_img, pos_embed)\n        elif patches_layout[0] > 1:\n            # pos embed has a temporal component\n            assert len(input_shape) == 4, \"temporal interpolation not supported\"\n            # we only support 2D interpolation in this case\n            num_frames = patches_layout[0]\n            num_spatial_tokens = patches_layout[1] * patches_layout[2]\n            pos_embed = pos_embed.view(1, num_frames, num_spatial_tokens, -1)\n            # interpolate embedding for zeroth frame\n            pos_embed = cls.interpolate_pos_encoding_2d(\n                npatch_per_img, pos_embed[0, 0, ...].unsqueeze(0)\n            )\n        else:\n            raise ValueError(\"This type of interpolation isn't implemented\")\n\n        return torch.cat((class_emb, pos_embed), dim=1)\n\n    def get_layer_id(self, layer_name):\n        # https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L33\n        num_layers = self.get_num_layers()\n        if layer_name in [\"cls_token\", \"pos_embed\"]:\n            return 0\n        elif layer_name.find(\"patch_embed\") != -1:\n            return 0\n        elif layer_name.find(\"blocks\") != -1:\n            return int(layer_name.split(\"blocks\")[1].split(\".\")[1]) + 1\n        else:\n            return num_layers\n\n    def get_num_layers(self):\n        return len(self.blocks) + 1", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_835-875"}
{"title": "facebookresearch_omnivore-omnivision-models-vision_transformer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "vision_transformer.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 875, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        elif patches_layout[0] > 1:\n            # pos embed has a temporal component\n            assert len(input_shape) == 4, \"temporal interpolation not supported\"\n            # we only support 2D interpolation in this case\n            num_frames = patches_layout[0]\n            num_spatial_tokens = patches_layout[1] * patches_layout[2]\n            pos_embed = pos_embed.view(1, num_frames, num_spatial_tokens, -1)\n            # interpolate embedding for zeroth frame\n            pos_embed = cls.interpolate_pos_encoding_2d(\n                npatch_per_img, pos_embed[0, 0, ...].unsqueeze(0)\n            )\n        else:\n            raise ValueError(\"This type of interpolation isn't implemented\")\n\n        return torch.cat((class_emb, pos_embed), dim=1)\n\n    def get_layer_id(self, layer_name):\n        # https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L33\n        num_layers = self.get_num_layers()\n        if layer_name in [\"cls_token\", \"pos_embed\"]:\n            return 0\n        elif layer_name.find(\"patch_embed\") != -1:\n            return 0\n        elif layer_name.find(\"blocks\") != -1:\n            return int(layer_name.split(\"blocks\")[1].split(\".\")[1]) + 1\n        else:\n            return num_layers\n\n    def get_num_layers(self):\n        return len(self.blocks) + 1", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-vision_transformer.py_845-875"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport hydra\nimport torch.nn as nn\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    layer_params = hydra.utils.instantiate(layer, _convert_=\"all\")\n    if init_weight is not None:\n        hydra.utils.instantiate(\n            init_weight, _convert_=\"all\", tensor=layer_params.weight.data\n        )\n    if init_bias is not None:\n        hydra.utils.instantiate(\n            init_bias, _convert_=\"all\", tensor=layer_params.bias.data\n        )\n    return layer_params\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport hydra\nimport torch.nn as nn\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    layer_params = hydra.utils.instantiate(layer, _convert_=\"all\")\n    if init_weight is not None:\n        hydra.utils.instantiate(\n            init_weight, _convert_=\"all\", tensor=layer_params.weight.data\n        )\n    if init_bias is not None:\n        hydra.utils.instantiate(\n            init_bias, _convert_=\"all\", tensor=layer_params.bias.data\n        )\n    return layer_params\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n\nAST=Module(Import(alias)Import(alias)FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)))If(Compare(Name(Load)IsNotConstant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)keyword(Attribute(Attribute(Name(Load)Load)Load)))))If(Compare(Name(Load)IsNotConstant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)keyword(Attribute(Attribute(Name(Load)Load)Load)))))Return(Name(Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)List(Subscript(Attribute(Name(Load)Load)ConstantLoad)UnaryOp(USubConstant)Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport hydra\nimport torch.nn as nn\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    layer_params = hydra.utils.instantiate(layer, _convert_=\"all\")\n    if init_weight is not None:\n        hydra.utils.instantiate(\n            init_weight, _convert_=\"all\", tensor=layer_params.weight.data\n        )\n    if init_bias is not None:\n        hydra.utils.instantiate(\n            init_bias, _convert_=\"all\", tensor=layer_params.bias.data\n        )\n    return layer_params\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\n\nAST=Module(Import(alias)Import(alias)FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)))If(Compare(Name(Load)IsNotConstant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)keyword(Attribute(Attribute(Name(Load)Load)Load)))))If(Compare(Name(Load)IsNotConstant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)keyword(Attribute(Attribute(Name(Load)Load)Load)))))Return(Name(Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)List(Subscript(Attribute(Name(Load)Load)ConstantLoad)UnaryOp(USubConstant)Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport hydra\nimport torch.nn as nn\n\n\ndef make_conv_or_linear(layer, init_weight=None, init_bias=None):\n    layer_params = hydra.utils.instantiate(layer, _convert_=\"all\")\n    if init_weight is not None:\n        hydra.utils.instantiate(\n            init_weight, _convert_=\"all\", tensor=layer_params.weight.data\n        )\n    if init_bias is not None:\n        hydra.utils.instantiate(\n            init_bias, _convert_=\"all\", tensor=layer_params.bias.data\n        )\n    return layer_params\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass PadIm2Video(Im2Video):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__(time_dim=time_dim)\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        x = super().forward(x)\n\nAST=Module(Import(alias)Import(alias)FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)))If(Compare(Name(Load)IsNotConstant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)keyword(Attribute(Attribute(Name(Load)Load)Load)))))If(Compare(Name(Load)IsNotConstant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Constant)keyword(Attribute(Attribute(Name(Load)Load)Load)))))Return(Name(Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)List(Subscript(Attribute(Name(Load)Load)ConstantLoad)UnaryOp(USubConstant)Load)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))If(Compare(Attribute(Name(Load)Load)EqConstant)Return(Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load)))))))))ClassDef(Name(Load)FunctionDef(arguments(argargargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)keyword(Name(Load))))Assert(Compare(Name(Load)GtConstant))Assert(Compare(Name(Load)InList(ConstantConstantLoad)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Call(Name(Load))Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n    if init_bias is not None:\n        hydra.utils.instantiate(\n            init_bias, _convert_=\"all\", tensor=layer_params.bias.data\n        )\n    return layer_params\n\n\ndef reshape_and_init_as_mlp(tensor):\n    # Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n    nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass PadIm2Video(Im2Video):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__(time_dim=time_dim)\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        x = super().forward(x)\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = nn.functional.pad(x, padarg)\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    nn.init.xavier_uniform_(tensor.view([tensor.shape[0], -1]))\n\n\nclass Im2Video(nn.Module):\n    \"\"\"Convert an image into a trivial video.\"\"\"\n\n    def __init__(self, time_dim=2):\n        super().__init__()\n        self.time_dim = time_dim\n\n    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass PadIm2Video(Im2Video):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__(time_dim=time_dim)\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        x = super().forward(x)\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = nn.functional.pad(x, padarg)\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_25-65"}
{"title": "facebookresearch_omnivore-omnivision-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "__init__.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def forward(self, x):\n        if x.ndim == 4:\n            # B, C, H, W -> B, C, T, H, W\n            return x.unsqueeze(self.time_dim)\n        elif x.ndim == 5:\n            return x\n        else:\n            raise ValueError(f\"Dimension incorrect {x.shape}\")\n\n\nclass PadIm2Video(Im2Video):\n    def __init__(self, ntimes, pad_type, time_dim=2):\n        super().__init__(time_dim=time_dim)\n        assert ntimes > 0\n        assert pad_type in [\"zero\", \"repeat\"]\n        self.ntimes = ntimes\n        self.pad_type = pad_type\n\n    def forward(self, x):\n        x = super().forward(x)\n        if x.shape[self.time_dim] == 1:\n            if self.pad_type == \"repeat\":\n                new_shape = [1] * len(x.shape)\n                new_shape[self.time_dim] = self.ntimes\n                x = x.repeat(new_shape)\n            elif self.pad_type == \"zero\":\n                padarg = [0, 0] * len(x.shape)\n                padarg[2 * self.time_dim + 1] = self.ntimes - x.shape[self.time_dim]\n                x = nn.functional.pad(x, padarg)\n        return x", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-__init__.py_35-65"}
{"title": "facebookresearch_omnivore-omnivision-models-heads-mae_head.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "heads", "mae_head.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\n\nfrom timm.models.layers import trunc_normal_\n\n\nclass MAEHead(nn.Module):\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.projector = nn.Linear(in_features, out_features, bias=True)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, batch):\n        if isinstance(batch, tuple):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-heads-mae_head.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-models-heads-mae_head.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "heads", "mae_head.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 27, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "models", "heads", "mae_head.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 27, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\n\nfrom timm.models.layers import trunc_normal_\n\n\nclass MAEHead(nn.Module):\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.projector = nn.Linear(in_features, out_features, bias=True)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, batch):\n        if isinstance(batch, tuple):\n            batch = batch[1]\n            return self.projector(batch)\n\nAST=Module(Import(alias)ImportFrom(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load)))Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load))))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Expr(Call(Name(Load)Attribute(Name(Load)Load)keyword(Constant)))If(BoolOp(AndCall(Name(Load)Name(Load)Attribute(Name(Load)Load))Compare(Attribute(Name(Load)Load)IsNotConstant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Constant)))))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Subscript(Name(Load)ConstantLoad))Return(Call(Attribute(Name(Load)Load)Name(Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-models-heads-mae_head.py_0-27"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport torch\n\n\nclass LARS(torch.optim.Optimizer):\n    \"\"\"\n    This class is adapted from\n    https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py to\n    include ignoring LARS application specific parameters (e.g. 1D params)\n\n    Args:\n        optimizer (torch.optim): Pytorch optimizer to wrap and modify learning rate for.\n        trust_coefficient: Trust coefficient for calculating the lr.\n            See https://arxiv.org/abs/1708.03888\n        clip (bool): Decides between clipping or scaling mode of LARS. If `clip=True` the\n            learning rate is set to `min(optimizer_lr, local_lr)` for each parameter.\n            If `clip=False` the learning rate is set to `local_lr*optimizer_lr`.\n        eps (float): epsilon kludge to help with numerical stability while calculating", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport torch\n\n\nclass LARS(torch.optim.Optimizer):\n    \"\"\"\n    This class is adapted from\n    https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py to\n    include ignoring LARS application specific parameters (e.g. 1D params)\n\n    Args:\n        optimizer (torch.optim): Pytorch optimizer to wrap and modify learning rate for.\n        trust_coefficient: Trust coefficient for calculating the lr.\n            See https://arxiv.org/abs/1708.03888\n        clip (bool): Decides between clipping or scaling mode of LARS. If `clip=True` the\n            learning rate is set to `min(optimizer_lr, local_lr)` for each parameter.\n            If `clip=False` the learning rate is set to `local_lr*optimizer_lr`.\n        eps (float): epsilon kludge to help with numerical stability while calculating\n        adaptive_lr.\n        ignore_1d_param (float): If true, does not update 1 dimentional parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        trust_coefficient=0.02,\n        clip=True,\n        eps=1e-8,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Portions Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport torch\n\n\nclass LARS(torch.optim.Optimizer):\n    \"\"\"\n    This class is adapted from\n    https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py to\n    include ignoring LARS application specific parameters (e.g. 1D params)\n\n    Args:\n        optimizer (torch.optim): Pytorch optimizer to wrap and modify learning rate for.\n        trust_coefficient: Trust coefficient for calculating the lr.\n            See https://arxiv.org/abs/1708.03888\n        clip (bool): Decides between clipping or scaling mode of LARS. If `clip=True` the\n            learning rate is set to `min(optimizer_lr, local_lr)` for each parameter.\n            If `clip=False` the learning rate is set to `local_lr*optimizer_lr`.\n        eps (float): epsilon kludge to help with numerical stability while calculating\n        adaptive_lr.\n        ignore_1d_param (float): If true, does not update 1 dimentional parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        trust_coefficient=0.02,\n        clip=True,\n        eps=1e-8,\n        ignore_1d_param=True,\n    ) -> None:\n        self.optim = optimizer\n        self.trust_coefficient = trust_coefficient\n        self.eps = eps\n        self.clip = clip\n        self.ignore_1d_param = ignore_1d_param\n\n        self.defaults = self.optim.defaults\n\n\nAST=Module(Import(alias)ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)FunctionDef(arguments(argargargargargargConstantConstantConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Attribute(Attribute(Name(Load)Load)Load))Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# pyre-ignore-all-errors\n\nimport torch\n\n\nclass LARS(torch.optim.Optimizer):\n    \"\"\"\n    This class is adapted from\n    https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py to\n    include ignoring LARS application specific parameters (e.g. 1D params)\n\n    Args:\n        optimizer (torch.optim): Pytorch optimizer to wrap and modify learning rate for.\n        trust_coefficient: Trust coefficient for calculating the lr.\n            See https://arxiv.org/abs/1708.03888\n        clip (bool): Decides between clipping or scaling mode of LARS. If `clip=True` the\n            learning rate is set to `min(optimizer_lr, local_lr)` for each parameter.\n            If `clip=False` the learning rate is set to `local_lr*optimizer_lr`.\n        eps (float): epsilon kludge to help with numerical stability while calculating\n        adaptive_lr.\n        ignore_1d_param (float): If true, does not update 1 dimentional parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        trust_coefficient=0.02,\n        clip=True,\n        eps=1e-8,\n        ignore_1d_param=True,\n    ) -> None:\n        self.optim = optimizer\n        self.trust_coefficient = trust_coefficient\n        self.eps = eps\n        self.clip = clip\n        self.ignore_1d_param = ignore_1d_param\n\n        self.defaults = self.optim.defaults\n\n    def __getstate__(self):\n        return self.optim.__getstate__()\n\n    def __setstate__(self, state):\n        self.optim.__setstate__(state)\n\n    @property\n    def state(self):\n        return self.optim.state\n\n\nAST=Module(Import(alias)ClassDef(Attribute(Attribute(Name(Load)Load)Load)Expr(Constant)FunctionDef(arguments(argargargargargargConstantConstantConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Attribute(Attribute(Name(Load)Load)Load))Constant)FunctionDef(arguments(arg)Return(Call(Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load))))FunctionDef(arguments(arg)Return(Attribute(Attribute(Name(Load)Load)Load))Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    include ignoring LARS application specific parameters (e.g. 1D params)\n\n    Args:\n        optimizer (torch.optim): Pytorch optimizer to wrap and modify learning rate for.\n        trust_coefficient: Trust coefficient for calculating the lr.\n            See https://arxiv.org/abs/1708.03888\n        clip (bool): Decides between clipping or scaling mode of LARS. If `clip=True` the\n            learning rate is set to `min(optimizer_lr, local_lr)` for each parameter.\n            If `clip=False` the learning rate is set to `local_lr*optimizer_lr`.\n        eps (float): epsilon kludge to help with numerical stability while calculating\n        adaptive_lr.\n        ignore_1d_param (float): If true, does not update 1 dimentional parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        trust_coefficient=0.02,\n        clip=True,\n        eps=1e-8,\n        ignore_1d_param=True,\n    ) -> None:\n        self.optim = optimizer\n        self.trust_coefficient = trust_coefficient\n        self.eps = eps\n        self.clip = clip\n        self.ignore_1d_param = ignore_1d_param\n\n        self.defaults = self.optim.defaults\n\n    def __getstate__(self):\n        return self.optim.__getstate__()\n\n    def __setstate__(self, state):\n        self.optim.__setstate__(state)\n\n    @property\n    def state(self):\n        return self.optim.state\n\n    def __repr__(self):\n        return self.optim.__repr__()\n\n    @property\n    def param_groups(self):\n        return self.optim.param_groups\n\n    @param_groups.setter\n    def param_groups(self, value):\n        self.optim.param_groups = value", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        adaptive_lr.\n        ignore_1d_param (float): If true, does not update 1 dimentional parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        trust_coefficient=0.02,\n        clip=True,\n        eps=1e-8,\n        ignore_1d_param=True,\n    ) -> None:\n        self.optim = optimizer\n        self.trust_coefficient = trust_coefficient\n        self.eps = eps\n        self.clip = clip\n        self.ignore_1d_param = ignore_1d_param\n\n        self.defaults = self.optim.defaults\n\n    def __getstate__(self):\n        return self.optim.__getstate__()\n\n    def __setstate__(self, state):\n        self.optim.__setstate__(state)\n\n    @property\n    def state(self):\n        return self.optim.state\n\n    def __repr__(self):\n        return self.optim.__repr__()\n\n    @property\n    def param_groups(self):\n        return self.optim.param_groups\n\n    @param_groups.setter\n    def param_groups(self, value):\n        self.optim.param_groups = value\n\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        ignore_1d_param=True,\n    ) -> None:\n        self.optim = optimizer\n        self.trust_coefficient = trust_coefficient\n        self.eps = eps\n        self.clip = clip\n        self.ignore_1d_param = ignore_1d_param\n\n        self.defaults = self.optim.defaults\n\n    def __getstate__(self):\n        return self.optim.__getstate__()\n\n    def __setstate__(self, state):\n        self.optim.__setstate__(state)\n\n    @property\n    def state(self):\n        return self.optim.state\n\n    def __repr__(self):\n        return self.optim.__repr__()\n\n    @property\n    def param_groups(self):\n        return self.optim.param_groups\n\n    @param_groups.setter\n    def param_groups(self, value):\n        self.optim.param_groups = value\n\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n    def add_param_group(self, param_group):\n        self.optim.add_param_group(param_group)\n\n    def step(self, closure=None):\n        with torch.no_grad():\n            weight_decays = []\n            for group in self.optim.param_groups:\n                # absorb weight decay control from optimizer\n                weight_decay = group[\"weight_decay\"] if \"weight_decay\" in group else 0\n                weight_decays.append(weight_decay)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __getstate__(self):\n        return self.optim.__getstate__()\n\n    def __setstate__(self, state):\n        self.optim.__setstate__(state)\n\n    @property\n    def state(self):\n        return self.optim.state\n\n    def __repr__(self):\n        return self.optim.__repr__()\n\n    @property\n    def param_groups(self):\n        return self.optim.param_groups\n\n    @param_groups.setter\n    def param_groups(self, value):\n        self.optim.param_groups = value\n\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n    def add_param_group(self, param_group):\n        self.optim.add_param_group(param_group)\n\n    def step(self, closure=None):\n        with torch.no_grad():\n            weight_decays = []\n            for group in self.optim.param_groups:\n                # absorb weight decay control from optimizer\n                weight_decay = group[\"weight_decay\"] if \"weight_decay\" in group else 0\n                weight_decays.append(weight_decay)\n                apply_LARS = group[\"apply_LARS\"] if \"apply_LARS\" in group else True\n                if not apply_LARS:\n                    continue\n                group[\"weight_decay\"] = 0\n                for p in group[\"params\"]:\n                    if p.grad is None:\n                        continue\n                    if self.ignore_1d_param and p.ndim == 1:  # ignore bias\n                        continue\n                    param_norm = torch.norm(p.data)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __repr__(self):\n        return self.optim.__repr__()\n\n    @property\n    def param_groups(self):\n        return self.optim.param_groups\n\n    @param_groups.setter\n    def param_groups(self, value):\n        self.optim.param_groups = value\n\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n    def add_param_group(self, param_group):\n        self.optim.add_param_group(param_group)\n\n    def step(self, closure=None):\n        with torch.no_grad():\n            weight_decays = []\n            for group in self.optim.param_groups:\n                # absorb weight decay control from optimizer\n                weight_decay = group[\"weight_decay\"] if \"weight_decay\" in group else 0\n                weight_decays.append(weight_decay)\n                apply_LARS = group[\"apply_LARS\"] if \"apply_LARS\" in group else True\n                if not apply_LARS:\n                    continue\n                group[\"weight_decay\"] = 0\n                for p in group[\"params\"]:\n                    if p.grad is None:\n                        continue\n                    if self.ignore_1d_param and p.ndim == 1:  # ignore bias\n                        continue\n                    param_norm = torch.norm(p.data)\n                    grad_norm = torch.norm(p.grad.data)\n\n                    if param_norm != 0 and grad_norm != 0:\n                        # calculate adaptive lr + weight decay\n                        adaptive_lr = (\n                            self.trust_coefficient\n                            * (param_norm)\n                            / (grad_norm + param_norm * weight_decay + self.eps)\n                        )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n    def add_param_group(self, param_group):\n        self.optim.add_param_group(param_group)\n\n    def step(self, closure=None):\n        with torch.no_grad():\n            weight_decays = []\n            for group in self.optim.param_groups:\n                # absorb weight decay control from optimizer\n                weight_decay = group[\"weight_decay\"] if \"weight_decay\" in group else 0\n                weight_decays.append(weight_decay)\n                apply_LARS = group[\"apply_LARS\"] if \"apply_LARS\" in group else True\n                if not apply_LARS:\n                    continue\n                group[\"weight_decay\"] = 0\n                for p in group[\"params\"]:\n                    if p.grad is None:\n                        continue\n                    if self.ignore_1d_param and p.ndim == 1:  # ignore bias\n                        continue\n                    param_norm = torch.norm(p.data)\n                    grad_norm = torch.norm(p.grad.data)\n\n                    if param_norm != 0 and grad_norm != 0:\n                        # calculate adaptive lr + weight decay\n                        adaptive_lr = (\n                            self.trust_coefficient\n                            * (param_norm)\n                            / (grad_norm + param_norm * weight_decay + self.eps)\n                        )\n\n                        # clip learning rate for LARS\n                        if self.clip:\n                            # calculation of adaptive_lr so that when multiplied\n                            # by lr it equals `min(adaptive_lr, lr)`\n                            adaptive_lr = min(adaptive_lr / group[\"lr\"], 1)\n\n                        p.grad.data += weight_decay * p.data\n                        p.grad.data *= adaptive_lr\n\n        self.optim.step()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 118, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def add_param_group(self, param_group):\n        self.optim.add_param_group(param_group)\n\n    def step(self, closure=None):\n        with torch.no_grad():\n            weight_decays = []\n            for group in self.optim.param_groups:\n                # absorb weight decay control from optimizer\n                weight_decay = group[\"weight_decay\"] if \"weight_decay\" in group else 0\n                weight_decays.append(weight_decay)\n                apply_LARS = group[\"apply_LARS\"] if \"apply_LARS\" in group else True\n                if not apply_LARS:\n                    continue\n                group[\"weight_decay\"] = 0\n                for p in group[\"params\"]:\n                    if p.grad is None:\n                        continue\n                    if self.ignore_1d_param and p.ndim == 1:  # ignore bias\n                        continue\n                    param_norm = torch.norm(p.data)\n                    grad_norm = torch.norm(p.grad.data)\n\n                    if param_norm != 0 and grad_norm != 0:\n                        # calculate adaptive lr + weight decay\n                        adaptive_lr = (\n                            self.trust_coefficient\n                            * (param_norm)\n                            / (grad_norm + param_norm * weight_decay + self.eps)\n                        )\n\n                        # clip learning rate for LARS\n                        if self.clip:\n                            # calculation of adaptive_lr so that when multiplied\n                            # by lr it equals `min(adaptive_lr, lr)`\n                            adaptive_lr = min(adaptive_lr / group[\"lr\"], 1)\n\n                        p.grad.data += weight_decay * p.data\n                        p.grad.data *= adaptive_lr\n\n        self.optim.step()\n        # return weight decay control to optimizer\n        for i, group in enumerate(self.optim.param_groups):\n            group[\"weight_decay\"] = weight_decays[i]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_75-118"}
{"title": "facebookresearch_omnivore-omnivision-optim-lars.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "lars.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 118, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                apply_LARS = group[\"apply_LARS\"] if \"apply_LARS\" in group else True\n                if not apply_LARS:\n                    continue\n                group[\"weight_decay\"] = 0\n                for p in group[\"params\"]:\n                    if p.grad is None:\n                        continue\n                    if self.ignore_1d_param and p.ndim == 1:  # ignore bias\n                        continue\n                    param_norm = torch.norm(p.data)\n                    grad_norm = torch.norm(p.grad.data)\n\n                    if param_norm != 0 and grad_norm != 0:\n                        # calculate adaptive lr + weight decay\n                        adaptive_lr = (\n                            self.trust_coefficient\n                            * (param_norm)\n                            / (grad_norm + param_norm * weight_decay + self.eps)\n                        )\n\n                        # clip learning rate for LARS\n                        if self.clip:\n                            # calculation of adaptive_lr so that when multiplied\n                            # by lr it equals `min(adaptive_lr, lr)`\n                            adaptive_lr = min(adaptive_lr / group[\"lr\"], 1)\n\n                        p.grad.data += weight_decay * p.data\n                        p.grad.data *= adaptive_lr\n\n        self.optim.step()\n        # return weight decay control to optimizer\n        for i, group in enumerate(self.optim.param_groups):\n            group[\"weight_decay\"] = weight_decays[i]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-lars.py_85-118"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Dict, List\n\n\nclass ValueScaler(object):\n    def __init__(self, scheduler, mult_val: float):\n        self.scheduler = scheduler\n        self.mult_val = mult_val\n\n    def __call__(self, *args, **kwargs):\n        val = self.scheduler(*args, **kwargs)\n        return val * self.mult_val\n\n\ndef layer_decay_param_modifier(\n    scheduler_cfgs: List[List[Dict]], model, layer_decay_value: float\n) -> List[List[Dict]]:\n    \"\"\"\n    Args", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Dict, List\n\n\nclass ValueScaler(object):\n    def __init__(self, scheduler, mult_val: float):\n        self.scheduler = scheduler\n        self.mult_val = mult_val\n\n    def __call__(self, *args, **kwargs):\n        val = self.scheduler(*args, **kwargs)\n        return val * self.mult_val\n\n\ndef layer_decay_param_modifier(\n    scheduler_cfgs: List[List[Dict]], model, layer_decay_value: float\n) -> List[List[Dict]]:\n    \"\"\"\n    Args\n    - scheduler_cfgs: a list of omegaconf.ListConfigs.\n        Each element in the list is a omegaconfg.DictConfig with the following structure\n        {\n            \"scheduler\": <some fvcore scheduler>\n            \"option\": <value> possible options are \"lr\", \"weight_decay\" etc.\n            \"parameter_names\": Set of str indicating param names that this scheduler applies to\n        }\n    - model: a model that implements a method `get_layer_id` that maps layer_name to an integer\n    - layer_decay_value: float\n    Returns", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Dict, List\n\n\nclass ValueScaler(object):\n    def __init__(self, scheduler, mult_val: float):\n        self.scheduler = scheduler\n        self.mult_val = mult_val\n\n    def __call__(self, *args, **kwargs):\n        val = self.scheduler(*args, **kwargs)\n        return val * self.mult_val\n\n\ndef layer_decay_param_modifier(\n    scheduler_cfgs: List[List[Dict]], model, layer_decay_value: float\n) -> List[List[Dict]]:\n    \"\"\"\n    Args\n    - scheduler_cfgs: a list of omegaconf.ListConfigs.\n        Each element in the list is a omegaconfg.DictConfig with the following structure\n        {\n            \"scheduler\": <some fvcore scheduler>\n            \"option\": <value> possible options are \"lr\", \"weight_decay\" etc.\n            \"parameter_names\": Set of str indicating param names that this scheduler applies to\n        }\n    - model: a model that implements a method `get_layer_id` that maps layer_name to an integer\n    - layer_decay_value: float\n    Returns\n    - scheduler_configs: same structure as the input, elements can be modified\n    \"\"\"\n    # FIXME: make sure the model API supports this\n    num_layers = model.trunk.get_num_layers() + 1\n    layer_decays = [\n        layer_decay_value ** (num_layers - i) for i in range(num_layers + 1)\n    ]\n    final_scheduler_cfgs = []\n    # scheduler_cfgs is a list of lists\n    for scheduler_cfg_group in scheduler_cfgs:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nfrom typing import Dict, List\n\n\nclass ValueScaler(object):\n    def __init__(self, scheduler, mult_val: float):\n        self.scheduler = scheduler\n        self.mult_val = mult_val\n\n    def __call__(self, *args, **kwargs):\n        val = self.scheduler(*args, **kwargs)\n        return val * self.mult_val\n\n\ndef layer_decay_param_modifier(\n    scheduler_cfgs: List[List[Dict]], model, layer_decay_value: float\n) -> List[List[Dict]]:\n    \"\"\"\n    Args\n    - scheduler_cfgs: a list of omegaconf.ListConfigs.\n        Each element in the list is a omegaconfg.DictConfig with the following structure\n        {\n            \"scheduler\": <some fvcore scheduler>\n            \"option\": <value> possible options are \"lr\", \"weight_decay\" etc.\n            \"parameter_names\": Set of str indicating param names that this scheduler applies to\n        }\n    - model: a model that implements a method `get_layer_id` that maps layer_name to an integer\n    - layer_decay_value: float\n    Returns\n    - scheduler_configs: same structure as the input, elements can be modified\n    \"\"\"\n    # FIXME: make sure the model API supports this\n    num_layers = model.trunk.get_num_layers() + 1\n    layer_decays = [\n        layer_decay_value ** (num_layers - i) for i in range(num_layers + 1)\n    ]\n    final_scheduler_cfgs = []\n    # scheduler_cfgs is a list of lists\n    for scheduler_cfg_group in scheduler_cfgs:\n        curr_cfg_group = []\n        # scheduler_cfg_group is a list of dictionaries\n        for scheduler_cfg in scheduler_cfg_group:\n            if scheduler_cfg[\"option\"] != \"lr\":\n                curr_cfg_group.append(scheduler_cfg)\n                continue\n            # Need sorted so that the list of parameter names is deterministic and consistent\n            # across re-runs of this job. Else it was causing issues with loading the optimizer\n            # state during a job restart (D38591759)\n            parameter_names = sorted(scheduler_cfg[\"parameter_names\"])\n\nAST=Module(ImportFrom(aliasalias)ClassDef(Name(Load)FunctionDef(arguments(argargarg(Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Starred(Name(Load)Load)keyword(Name(Load))))Return(BinOp(Name(Load)MultAttribute(Name(Load)Load)))))FunctionDef(arguments(arg(Subscript(Name(Load)Subscript(Name(Load)Name(Load)Load)Load))argarg(Name(Load)))Expr(Constant)Assign(Name(Store)BinOp(Call(Attribute(Attribute(Name(Load)Load)Load))AddConstant))Assign(Name(Store)ListComp(BinOp(Name(Load)PowBinOp(Name(Load)SubName(Load)))comprehension(Name(Store)Call(Name(Load)BinOp(Name(Load)AddConstant)))))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Assign(Name(Store)List(Load))For(Name(Store)Name(Load)If(Compare(Subscript(Name(Load)ConstantLoad)NotEqConstant)Expr(Call(Attribute(Name(Load)Load)Name(Load)))Continue)Assign(Name(Store)Call(Name(Load)Subscript(Name(Load)ConstantLoad)))))Subscript(Name(Load)Subscript(Name(Load)Name(Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __call__(self, *args, **kwargs):\n        val = self.scheduler(*args, **kwargs)\n        return val * self.mult_val\n\n\ndef layer_decay_param_modifier(\n    scheduler_cfgs: List[List[Dict]], model, layer_decay_value: float\n) -> List[List[Dict]]:\n    \"\"\"\n    Args\n    - scheduler_cfgs: a list of omegaconf.ListConfigs.\n        Each element in the list is a omegaconfg.DictConfig with the following structure\n        {\n            \"scheduler\": <some fvcore scheduler>\n            \"option\": <value> possible options are \"lr\", \"weight_decay\" etc.\n            \"parameter_names\": Set of str indicating param names that this scheduler applies to\n        }\n    - model: a model that implements a method `get_layer_id` that maps layer_name to an integer\n    - layer_decay_value: float\n    Returns\n    - scheduler_configs: same structure as the input, elements can be modified\n    \"\"\"\n    # FIXME: make sure the model API supports this\n    num_layers = model.trunk.get_num_layers() + 1\n    layer_decays = [\n        layer_decay_value ** (num_layers - i) for i in range(num_layers + 1)\n    ]\n    final_scheduler_cfgs = []\n    # scheduler_cfgs is a list of lists\n    for scheduler_cfg_group in scheduler_cfgs:\n        curr_cfg_group = []\n        # scheduler_cfg_group is a list of dictionaries\n        for scheduler_cfg in scheduler_cfg_group:\n            if scheduler_cfg[\"option\"] != \"lr\":\n                curr_cfg_group.append(scheduler_cfg)\n                continue\n            # Need sorted so that the list of parameter names is deterministic and consistent\n            # across re-runs of this job. Else it was causing issues with loading the optimizer\n            # state during a job restart (D38591759)\n            parameter_names = sorted(scheduler_cfg[\"parameter_names\"])\n            for param_name in parameter_names:\n                layer_id = model.trunk.get_layer_id(param_name)\n                this_scale = layer_decays[layer_id]\n                curr_param = {\n                    \"option\": scheduler_cfg[\"option\"],\n                    \"scheduler\": ValueScaler(scheduler_cfg[\"scheduler\"], this_scale),\n                    \"parameter_names\": {param_name},\n                }\n                curr_cfg_group.append(curr_param)\n        final_scheduler_cfgs.append(curr_cfg_group)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 66, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    - scheduler_cfgs: a list of omegaconf.ListConfigs.\n        Each element in the list is a omegaconfg.DictConfig with the following structure\n        {\n            \"scheduler\": <some fvcore scheduler>\n            \"option\": <value> possible options are \"lr\", \"weight_decay\" etc.\n            \"parameter_names\": Set of str indicating param names that this scheduler applies to\n        }\n    - model: a model that implements a method `get_layer_id` that maps layer_name to an integer\n    - layer_decay_value: float\n    Returns\n    - scheduler_configs: same structure as the input, elements can be modified\n    \"\"\"\n    # FIXME: make sure the model API supports this\n    num_layers = model.trunk.get_num_layers() + 1\n    layer_decays = [\n        layer_decay_value ** (num_layers - i) for i in range(num_layers + 1)\n    ]\n    final_scheduler_cfgs = []\n    # scheduler_cfgs is a list of lists\n    for scheduler_cfg_group in scheduler_cfgs:\n        curr_cfg_group = []\n        # scheduler_cfg_group is a list of dictionaries\n        for scheduler_cfg in scheduler_cfg_group:\n            if scheduler_cfg[\"option\"] != \"lr\":\n                curr_cfg_group.append(scheduler_cfg)\n                continue\n            # Need sorted so that the list of parameter names is deterministic and consistent\n            # across re-runs of this job. Else it was causing issues with loading the optimizer\n            # state during a job restart (D38591759)\n            parameter_names = sorted(scheduler_cfg[\"parameter_names\"])\n            for param_name in parameter_names:\n                layer_id = model.trunk.get_layer_id(param_name)\n                this_scale = layer_decays[layer_id]\n                curr_param = {\n                    \"option\": scheduler_cfg[\"option\"],\n                    \"scheduler\": ValueScaler(scheduler_cfg[\"scheduler\"], this_scale),\n                    \"parameter_names\": {param_name},\n                }\n                curr_cfg_group.append(curr_param)\n        final_scheduler_cfgs.append(curr_cfg_group)\n    return final_scheduler_cfgs", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_25-66"}
{"title": "facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "layer_decay_param_modifier.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 66, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    - scheduler_configs: same structure as the input, elements can be modified\n    \"\"\"\n    # FIXME: make sure the model API supports this\n    num_layers = model.trunk.get_num_layers() + 1\n    layer_decays = [\n        layer_decay_value ** (num_layers - i) for i in range(num_layers + 1)\n    ]\n    final_scheduler_cfgs = []\n    # scheduler_cfgs is a list of lists\n    for scheduler_cfg_group in scheduler_cfgs:\n        curr_cfg_group = []\n        # scheduler_cfg_group is a list of dictionaries\n        for scheduler_cfg in scheduler_cfg_group:\n            if scheduler_cfg[\"option\"] != \"lr\":\n                curr_cfg_group.append(scheduler_cfg)\n                continue\n            # Need sorted so that the list of parameter names is deterministic and consistent\n            # across re-runs of this job. Else it was causing issues with loading the optimizer\n            # state during a job restart (D38591759)\n            parameter_names = sorted(scheduler_cfg[\"parameter_names\"])\n            for param_name in parameter_names:\n                layer_id = model.trunk.get_layer_id(param_name)\n                this_scale = layer_decays[layer_id]\n                curr_param = {\n                    \"option\": scheduler_cfg[\"option\"],\n                    \"scheduler\": ValueScaler(scheduler_cfg[\"scheduler\"], this_scale),\n                    \"parameter_names\": {param_name},\n                }\n                curr_cfg_group.append(curr_param)\n        final_scheduler_cfgs.append(curr_cfg_group)\n    return final_scheduler_cfgs", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-layer_decay_param_modifier.py_35-66"}
{"title": "facebookresearch_omnivore-omnivision-optim-omni_optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n\n    def _validate_optimizer_schedulers(self):\n        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argargargConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Expr(Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Constant))Constant)FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Name(Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assert(Compare(Name(Load)InAttribute(Attribute(Name(Load)Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load))))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-omni_optimizer.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-optim-omni_optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n\n    def _validate_optimizer_schedulers(self):\n        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n\n    def step_schedulers(self, where: float) -> None:\n        if self.schedulers is None:\n            return\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            for option, scheduler in self.schedulers[i].items():\n                new_value = scheduler(where)\n                param_group[option] = new_value\n\n    def step(self, where, closure=None):\n        self.step_schedulers(where)\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argargargConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Expr(Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Constant))Constant)FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Name(Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assert(Compare(Name(Load)InAttribute(Attribute(Name(Load)Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load))))))))FunctionDef(arguments(argarg(Name(Load)))If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Subscript(Attribute(Name(Load)Load)Name(Load)Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Subscript(Name(Load)Name(Load)Store)Name(Load))))Constant)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Name(Load)Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-omni_optimizer.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-optim-omni_optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 39, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n\n    def _validate_optimizer_schedulers(self):\n        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n\n    def step_schedulers(self, where: float) -> None:\n        if self.schedulers is None:\n            return\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            for option, scheduler in self.schedulers[i].items():\n                new_value = scheduler(where)\n                param_group[option] = new_value\n\n    def step(self, where, closure=None):\n        self.step_schedulers(where)\n        return self.optimizer.step(closure)\n\n    def zero_grad(self):\n        return self.optimizer.zero_grad()\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argargargConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Expr(Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Constant))Constant)FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Name(Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assert(Compare(Name(Load)InAttribute(Attribute(Name(Load)Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load))))))))FunctionDef(arguments(argarg(Name(Load)))If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Subscript(Attribute(Name(Load)Load)Name(Load)Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Subscript(Name(Load)Name(Load)Store)Name(Load))))Constant)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load))))FunctionDef(arguments(arg)Return(Call(Attribute(Attribute(Name(Load)Load)Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-omni_optimizer.py_0-39"}
{"title": "facebookresearch_omnivore-omnivision-optim-omni_optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "omni_optimizer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 39, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass OmniOptimizer(object):\n    def __init__(self, optimizer, schedulers=None) -> None:\n        self.optimizer = optimizer\n        self.schedulers = schedulers\n        self._validate_optimizer_schedulers()\n        self.step_schedulers(0.0)\n\n    def _validate_optimizer_schedulers(self):\n        if self.schedulers is None:\n            return\n        for _, set_of_schedulers in enumerate(self.schedulers):\n            for option, _ in set_of_schedulers.items():\n                assert option in self.optimizer.defaults, (\n                    \"Optimizer option \"\n                    f\"{option} not found in {self.optimizer}. Valid options are \"\n                    f\"{self.optimizer.defaults.keys()}\"\n                )\n\n    def step_schedulers(self, where: float) -> None:\n        if self.schedulers is None:\n            return\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            for option, scheduler in self.schedulers[i].items():\n                new_value = scheduler(where)\n                param_group[option] = new_value\n\n    def step(self, where, closure=None):\n        self.step_schedulers(where)\n        return self.optimizer.step(closure)\n\n    def zero_grad(self):\n        return self.optimizer.zero_grad()\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argargargConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Expr(Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Constant))Constant)FunctionDef(arguments(arg)If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Name(Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assert(Compare(Name(Load)InAttribute(Attribute(Name(Load)Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load))))))))FunctionDef(arguments(argarg(Name(Load)))If(Compare(Attribute(Name(Load)Load)IsConstant)Return)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Attribute(Name(Load)Load)Load))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Subscript(Attribute(Name(Load)Load)Name(Load)Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Subscript(Name(Load)Name(Load)Store)Name(Load))))Constant)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load))))FunctionDef(arguments(arg)Return(Call(Attribute(Attribute(Name(Load)Load)Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-omni_optimizer.py_5-39"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Name(Load))))Return(Call(Name(Load)Name(Load)keyword(Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Name(Load))))Return(Call(Name(Load)Name(Load)keyword(Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)ListComp(Call(Name(Load)Subscript(Name(Load)ConstantLoad))comprehension(Name(Store)Name(Load))))Assign(Name(Store)SetComp(Name(Load)comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)Constant)Assert(Call(Attribute(Name(Load)Load)Name(Load))Constant))Assert(Compare(Call(Attribute(Name(Load)Load)Starred(Name(Load)Load))EqName(Load))Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)keyword(Name(Load))))Return(Call(Name(Load)Name(Load)keyword(Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)ListComp(Call(Name(Load)Subscript(Name(Load)ConstantLoad))comprehension(Name(Store)Name(Load))))Assign(Name(Store)SetComp(Name(Load)comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)Constant)Assert(Call(Attribute(Name(Load)Load)Name(Load))Constant))Assert(Compare(Call(Attribute(Name(Load)Load)Starred(Name(Load)Load))EqName(Load))Constant))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load)))If(BoolOp(AndCompare(ConstantNotInName(Load))Compare(ConstantNotInName(Load)))Return(Constant))Return(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)Call(Name(Load)Name(Load)Name(Load))))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Name(Load)Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# pyre-ignore-all-errors\n\nimport fnmatch\nimport itertools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig, MISSING\n\nfrom . import LARS, OmniOptimizer\n\n\ndef create_lars_optimizer(params, opt, **lars_params):\n    optim = hydra.utils.instantiate(opt, params=params)\n    return LARS(optim, **lars_params)\n\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef validate_param_group_params(param_groups, model):\n    parameters = [set(param_group[\"params\"]) for param_group in param_groups]\n    model_parameters = {parameter for _, parameter in model.named_parameters()}\n    for p1, p2 in itertools.permutations(parameters, 2):\n        assert p1.isdisjoint(p2), \"Scheduler generated param_groups should be disjoint\"\n    assert (\n        set.union(*parameters) == model_parameters\n    ), \"Scheduler generated param_groups include all parameters of the model\"\n\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n\nAST=Module(FunctionDef(arguments(argarg)Assign(Name(Store)ListComp(Call(Name(Load)Subscript(Name(Load)ConstantLoad))comprehension(Name(Store)Name(Load))))Assign(Name(Store)SetComp(Name(Load)comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Name(Load)Constant)Assert(Call(Attribute(Name(Load)Load)Name(Load))Constant))Assert(Compare(Call(Attribute(Name(Load)Load)Starred(Name(Load)Load))EqName(Load))Constant))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load)))If(BoolOp(AndCompare(ConstantNotInName(Load))Compare(ConstantNotInName(Load)))Return(Constant))Return(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)Call(Name(Load)Name(Load)Name(Load))))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Name(Load)Load)Load)Load))FunctionDef(arguments(argarg)If(Compare(Name(Load)EqConstant)Return(Name(Load)))Return(JoinedStr(FormattedValue(Name(Load))ConstantFormattedValue(Name(Load)))))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load)))If(Compare(ConstantNotInName(Load))Return(Call(Name(Load))))Assign(Name(Store)Dict)For(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)Call(Name(Load))))AugAssign(Subscript(Name(Load)Name(Load)Store)BitOrCall(Name(Load)GeneratorExp(Call(Name(Load)Name(Load)Name(Load))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))))Assign(Name(Store)List(Load))For(Name(Store)Attribute(Name(Load)Load)Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Name(Load))))Assert(Compare(Call(Name(Load)Name(Load))GtConstant)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))ConstantFormattedValue(Name(Load))Constant)))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Name(Load)Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef unix_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig, model: nn.Module\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg and \"module_cls_names\" not in scheduler_cfg:\n        return None\n    return unix_param_pattern_to_parameter_names(scheduler_cfg, model).union(\n        unix_module_cls_pattern_to_parameter_names(scheduler_cfg, model)\n    )\n\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_full_parameter_name(module_name, param_name):\n    if module_name == \"\":\n        return param_name\n    return f\"{module_name}.{param_name}\"\n\n\ndef unix_module_cls_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"module_cls_names\" not in scheduler_cfg:\n        return set()\n    module_cls_to_params = {}\n    for module_name, module in model.named_modules():\n        module_cls = type(module)\n        module_cls_to_params.setdefault(module_cls, set())\n        module_cls_to_params[module_cls] |= set(\n            get_full_parameter_name(module_name, param_name)\n            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            for param_name, _ in module.named_parameters()\n        )\n    parameter_names = []\n    for module_cls_name in scheduler_cfg.module_cls_names:\n        module_cls = hydra.utils.get_class(module_cls_name)\n        matching_parameters = module_cls_to_params.get(module_cls, set())\n        assert len(matching_parameters) > 0, (\n            f\"Optimizer option for {scheduler_cfg.option} module_cls_name\"\n            f\" {module_cls_name} does not match any classes in the model\"\n        )\n        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        logging.info(\n            f\"Matches for module_cls_name [{module_cls_name}]: {matching_parameters} \"\n        )\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef unix_param_pattern_to_parameter_names(\n    scheduler_cfg: DictConfig,\n    model: nn.Module,\n) -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n\n\ndef name_constraints_to_parameters(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": ") -> Union[None, Set[str]]:\n    if \"param_names\" not in scheduler_cfg:\n        return set()\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    parameter_names = []\n    for param_name in scheduler_cfg.param_names:\n        matching_parameters = set(fnmatch.filter(all_parameter_names, param_name))\n        assert len(matching_parameters) >= 1, (\n            f\"Optimizer option for {scheduler_cfg.option} param_names {param_name} \"\n            \"does not match any parameters in the model\"\n        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n\n\ndef name_constraints_to_parameters(\n    param_constraints: List[Set[str]], model: torch.nn.Module\n) -> List[torch.nn.Parameter]:\n    matching_names = set.intersection(*param_constraints)\n    return [value for name, value in model.named_parameters() if name in matching_names]\n\n\ndef map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:\n    schedulers = []", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        logging.info(f\"Matches for param_name [{param_name}]: {matching_parameters}\")\n        parameter_names.append(matching_parameters)\n    return set.union(*parameter_names)\n\n\ndef set_default_parameters(\n    scheduler_cfgs: List[DictConfig], all_parameter_names: Set[str]\n) -> None:\n    constraints = [\n        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n\n\ndef name_constraints_to_parameters(\n    param_constraints: List[Set[str]], model: torch.nn.Module\n) -> List[torch.nn.Parameter]:\n    matching_names = set.intersection(*param_constraints)\n    return [value for name, value in model.named_parameters() if name in matching_names]\n\n\ndef map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:\n    schedulers = []\n    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        scheduler_cfg.parameter_names\n        for scheduler_cfg in scheduler_cfgs\n        if scheduler_cfg.parameter_names is not None\n    ]\n    if len(constraints) == 0:\n        default_params = set(all_parameter_names)\n    else:\n\n        default_params = all_parameter_names - set.union(*constraints)\n    default_count = 0\n    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n\n\ndef name_constraints_to_parameters(\n    param_constraints: List[Set[str]], model: torch.nn.Module\n) -> List[torch.nn.Parameter]:\n    matching_names = set.intersection(*param_constraints)\n    return [value for name, value in model.named_parameters() if name in matching_names]\n\n\ndef map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:\n    schedulers = []\n    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]\n            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n\n\ndef construct_optimizer(\n    model: torch.nn.Module,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    for scheduler_cfg in scheduler_cfgs:\n        if scheduler_cfg.parameter_names is None:\n            scheduler_cfg.parameter_names = default_params\n            default_count += 1\n    assert default_count <= 1, \"Only one scheduler per option can be default\"\n    if default_count == 0:  # Add defaults without options\n        scheduler_cfgs.append({\"parameter_names\": default_params})\n\n\ndef name_constraints_to_parameters(\n    param_constraints: List[Set[str]], model: torch.nn.Module\n) -> List[torch.nn.Parameter]:\n    matching_names = set.intersection(*param_constraints)\n    return [value for name, value in model.named_parameters() if name in matching_names]\n\n\ndef map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:\n    schedulers = []\n    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]\n            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n\n\ndef construct_optimizer(\n    model: torch.nn.Module,\n    optimizer_conf,\n    options_conf=None,\n    param_group_modifiers_conf=None,\n) -> OmniOptimizer:  # noqa\n    \"\"\"\n    Constructs a stochastic gradient descent or ADAM (or ADAMw) optimizer\n    with momentum. i.e, constructs a torch.optim.Optimizer with zero-weight decay\n    Batchnorm and/or no-update 1-D parameters support, based on the config.\n\n    Supports wrapping the optimizer with Layer-wise Adaptive Rate Scaling", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    param_constraints: List[Set[str]], model: torch.nn.Module\n) -> List[torch.nn.Parameter]:\n    matching_names = set.intersection(*param_constraints)\n    return [value for name, value in model.named_parameters() if name in matching_names]\n\n\ndef map_scheduler_cfgs_to_param_groups(\n    scheduler_cfgs_per_param_group: Iterable[List[Dict]], model: torch.nn.Module\n) -> Tuple[List[Dict[Any, Any]], List[Dict[str, List[torch.nn.Parameter]]]]:\n    schedulers = []\n    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]\n            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n\n\ndef construct_optimizer(\n    model: torch.nn.Module,\n    optimizer_conf,\n    options_conf=None,\n    param_group_modifiers_conf=None,\n) -> OmniOptimizer:  # noqa\n    \"\"\"\n    Constructs a stochastic gradient descent or ADAM (or ADAMw) optimizer\n    with momentum. i.e, constructs a torch.optim.Optimizer with zero-weight decay\n    Batchnorm and/or no-update 1-D parameters support, based on the config.\n\n    Supports wrapping the optimizer with Layer-wise Adaptive Rate Scaling\n    (LARS): https://arxiv.org/abs/1708.03888\n\n    Args:\n        model (nn.Module): model to perform stochastic gradient descent\n            optimization or ADAM optimization.\n        cfg (OptimizerConf): Hydra/Omega conf object consisting hyper-parameters\n            of SGD or ADAM, includes base learning rate,  momentum, weight_decay,\n            dampening and etc. The supported config schema is `OptimizerConf`.\n    \"\"\"\n    if not options_conf:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    param_groups = []\n    for scheduler_cfgs in scheduler_cfgs_per_param_group:\n        param_constraints = [\n            scheduler_cfg[\"parameter_names\"] for scheduler_cfg in scheduler_cfgs\n        ]\n        matching_parameters = name_constraints_to_parameters(param_constraints, model)\n        if len(matching_parameters) == 0:  # If no overlap of parameters, skip\n            continue\n        schedulers_for_group = {\n            scheduler_cfg[\"option\"]: scheduler_cfg[\"scheduler\"]\n            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n\n\ndef construct_optimizer(\n    model: torch.nn.Module,\n    optimizer_conf,\n    options_conf=None,\n    param_group_modifiers_conf=None,\n) -> OmniOptimizer:  # noqa\n    \"\"\"\n    Constructs a stochastic gradient descent or ADAM (or ADAMw) optimizer\n    with momentum. i.e, constructs a torch.optim.Optimizer with zero-weight decay\n    Batchnorm and/or no-update 1-D parameters support, based on the config.\n\n    Supports wrapping the optimizer with Layer-wise Adaptive Rate Scaling\n    (LARS): https://arxiv.org/abs/1708.03888\n\n    Args:\n        model (nn.Module): model to perform stochastic gradient descent\n            optimization or ADAM optimization.\n        cfg (OptimizerConf): Hydra/Omega conf object consisting hyper-parameters\n            of SGD or ADAM, includes base learning rate,  momentum, weight_decay,\n            dampening and etc. The supported config schema is `OptimizerConf`.\n    \"\"\"\n    if not options_conf:\n        optimizer = hydra.utils.instantiate(optimizer_conf, params=model.parameters())\n        return OmniOptimizer(optimizer)\n\n    scheduler_cfgs_per_option = hydra.utils.instantiate(options_conf)\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    flattened_scheduler_cfgs = []\n    for option, scheduler_cfgs in scheduler_cfgs_per_option.items():\n        for config in scheduler_cfgs:\n            config.option = option\n            config.parameter_names = unix_pattern_to_parameter_names(config, model)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            for scheduler_cfg in scheduler_cfgs\n            if \"option\" in scheduler_cfg\n        }\n        schedulers.append(schedulers_for_group)\n        param_groups.append({\"params\": matching_parameters})\n    return schedulers, param_groups\n\n\ndef construct_optimizer(\n    model: torch.nn.Module,\n    optimizer_conf,\n    options_conf=None,\n    param_group_modifiers_conf=None,\n) -> OmniOptimizer:  # noqa\n    \"\"\"\n    Constructs a stochastic gradient descent or ADAM (or ADAMw) optimizer\n    with momentum. i.e, constructs a torch.optim.Optimizer with zero-weight decay\n    Batchnorm and/or no-update 1-D parameters support, based on the config.\n\n    Supports wrapping the optimizer with Layer-wise Adaptive Rate Scaling\n    (LARS): https://arxiv.org/abs/1708.03888\n\n    Args:\n        model (nn.Module): model to perform stochastic gradient descent\n            optimization or ADAM optimization.\n        cfg (OptimizerConf): Hydra/Omega conf object consisting hyper-parameters\n            of SGD or ADAM, includes base learning rate,  momentum, weight_decay,\n            dampening and etc. The supported config schema is `OptimizerConf`.\n    \"\"\"\n    if not options_conf:\n        optimizer = hydra.utils.instantiate(optimizer_conf, params=model.parameters())\n        return OmniOptimizer(optimizer)\n\n    scheduler_cfgs_per_option = hydra.utils.instantiate(options_conf)\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    flattened_scheduler_cfgs = []\n    for option, scheduler_cfgs in scheduler_cfgs_per_option.items():\n        for config in scheduler_cfgs:\n            config.option = option\n            config.parameter_names = unix_pattern_to_parameter_names(config, model)\n        set_default_parameters(scheduler_cfgs, all_parameter_names)\n        flattened_scheduler_cfgs.append(scheduler_cfgs)\n\n    if param_group_modifiers_conf:\n        for custom_param_modifier in param_group_modifiers_conf:\n            custom_param_modifier = hydra.utils.instantiate(custom_param_modifier)\n            flattened_scheduler_cfgs = custom_param_modifier(\n                scheduler_cfgs=flattened_scheduler_cfgs, model=model\n            )\n    schedulers, param_groups = map_scheduler_cfgs_to_param_groups(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 200, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    optimizer_conf,\n    options_conf=None,\n    param_group_modifiers_conf=None,\n) -> OmniOptimizer:  # noqa\n    \"\"\"\n    Constructs a stochastic gradient descent or ADAM (or ADAMw) optimizer\n    with momentum. i.e, constructs a torch.optim.Optimizer with zero-weight decay\n    Batchnorm and/or no-update 1-D parameters support, based on the config.\n\n    Supports wrapping the optimizer with Layer-wise Adaptive Rate Scaling\n    (LARS): https://arxiv.org/abs/1708.03888\n\n    Args:\n        model (nn.Module): model to perform stochastic gradient descent\n            optimization or ADAM optimization.\n        cfg (OptimizerConf): Hydra/Omega conf object consisting hyper-parameters\n            of SGD or ADAM, includes base learning rate,  momentum, weight_decay,\n            dampening and etc. The supported config schema is `OptimizerConf`.\n    \"\"\"\n    if not options_conf:\n        optimizer = hydra.utils.instantiate(optimizer_conf, params=model.parameters())\n        return OmniOptimizer(optimizer)\n\n    scheduler_cfgs_per_option = hydra.utils.instantiate(options_conf)\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    flattened_scheduler_cfgs = []\n    for option, scheduler_cfgs in scheduler_cfgs_per_option.items():\n        for config in scheduler_cfgs:\n            config.option = option\n            config.parameter_names = unix_pattern_to_parameter_names(config, model)\n        set_default_parameters(scheduler_cfgs, all_parameter_names)\n        flattened_scheduler_cfgs.append(scheduler_cfgs)\n\n    if param_group_modifiers_conf:\n        for custom_param_modifier in param_group_modifiers_conf:\n            custom_param_modifier = hydra.utils.instantiate(custom_param_modifier)\n            flattened_scheduler_cfgs = custom_param_modifier(\n                scheduler_cfgs=flattened_scheduler_cfgs, model=model\n            )\n    schedulers, param_groups = map_scheduler_cfgs_to_param_groups(\n        itertools.product(*flattened_scheduler_cfgs), model\n    )\n    validate_param_group_params(param_groups, model)\n    optimizer = hydra.utils.instantiate(optimizer_conf, param_groups)\n    return OmniOptimizer(optimizer, schedulers)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_155-200"}
{"title": "facebookresearch_omnivore-omnivision-optim-optimizer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "optimizer.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 200, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    (LARS): https://arxiv.org/abs/1708.03888\n\n    Args:\n        model (nn.Module): model to perform stochastic gradient descent\n            optimization or ADAM optimization.\n        cfg (OptimizerConf): Hydra/Omega conf object consisting hyper-parameters\n            of SGD or ADAM, includes base learning rate,  momentum, weight_decay,\n            dampening and etc. The supported config schema is `OptimizerConf`.\n    \"\"\"\n    if not options_conf:\n        optimizer = hydra.utils.instantiate(optimizer_conf, params=model.parameters())\n        return OmniOptimizer(optimizer)\n\n    scheduler_cfgs_per_option = hydra.utils.instantiate(options_conf)\n    all_parameter_names = {name for name, _ in model.named_parameters()}\n    flattened_scheduler_cfgs = []\n    for option, scheduler_cfgs in scheduler_cfgs_per_option.items():\n        for config in scheduler_cfgs:\n            config.option = option\n            config.parameter_names = unix_pattern_to_parameter_names(config, model)\n        set_default_parameters(scheduler_cfgs, all_parameter_names)\n        flattened_scheduler_cfgs.append(scheduler_cfgs)\n\n    if param_group_modifiers_conf:\n        for custom_param_modifier in param_group_modifiers_conf:\n            custom_param_modifier = hydra.utils.instantiate(custom_param_modifier)\n            flattened_scheduler_cfgs = custom_param_modifier(\n                scheduler_cfgs=flattened_scheduler_cfgs, model=model\n            )\n    schedulers, param_groups = map_scheduler_cfgs_to_param_groups(\n        itertools.product(*flattened_scheduler_cfgs), model\n    )\n    validate_param_group_params(param_groups, model)\n    optimizer = hydra.utils.instantiate(optimizer_conf, param_groups)\n    return OmniOptimizer(optimizer, schedulers)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-optimizer.py_165-200"}
{"title": "facebookresearch_omnivore-omnivision-optim-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 11, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "optim", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 11, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .lars import LARS\nfrom .omni_optimizer import OmniOptimizer  # usort:skip\nfrom .optimizer import construct_optimizer, create_lars_optimizer  # usort:skip\n\n__all__ = [\"construct_optimizer\", \"OmniOptimizer\", \"create_lars_optimizer\", \"LARS\"]\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)Assign(Name(Store)List(ConstantConstantConstantConstantLoad)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-optim-__init__.py_0-11"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom hydra.utils import instantiate\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import Sample\nfrom omnivision.data.concat_dataset import ConcatDataset\nfrom omnivision.data.torch_dataset import TorchDataset\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom hydra.utils import instantiate\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import Sample\nfrom omnivision.data.concat_dataset import ConcatDataset\nfrom omnivision.data.torch_dataset import TorchDataset\nfrom omnivision.losses import wrap_base_loss\nfrom omnivision.optim import construct_optimizer\nfrom omnivision.utils.train import (\n    AverageMeter,\n    copy_data_to_device,\n    get_amp_type,\n    get_machine_local_and_dist_rank,\n    get_resume_checkpoint,\n    is_dist_avail_and_initialized,\n    makedir,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom hydra.utils import instantiate\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import Sample\nfrom omnivision.data.concat_dataset import ConcatDataset\nfrom omnivision.data.torch_dataset import TorchDataset\nfrom omnivision.losses import wrap_base_loss\nfrom omnivision.optim import construct_optimizer\nfrom omnivision.utils.train import (\n    AverageMeter,\n    copy_data_to_device,\n    get_amp_type,\n    get_machine_local_and_dist_rank,\n    get_resume_checkpoint,\n    is_dist_avail_and_initialized,\n    makedir,\n    ProgressMeter,\n    set_seeds,\n    setup_distributed_backend,\n    setup_logging,\n)\n\n\ndef chunk_batch_for_accum_steps(batch, accum_steps):\n    return [get_chunk_from_data(batch, i, accum_steps) for i in range(accum_steps)]\n\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasaliasaliasalias)FunctionDef(arguments(argarg)Return(ListComp(Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport contextlib\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom hydra.utils import instantiate\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import Sample\nfrom omnivision.data.concat_dataset import ConcatDataset\nfrom omnivision.data.torch_dataset import TorchDataset\nfrom omnivision.losses import wrap_base_loss\nfrom omnivision.optim import construct_optimizer\nfrom omnivision.utils.train import (\n    AverageMeter,\n    copy_data_to_device,\n    get_amp_type,\n    get_machine_local_and_dist_rank,\n    get_resume_checkpoint,\n    is_dist_avail_and_initialized,\n    makedir,\n    ProgressMeter,\n    set_seeds,\n    setup_distributed_backend,\n    setup_logging,\n)\n\n\ndef chunk_batch_for_accum_steps(batch, accum_steps):\n    return [get_chunk_from_data(batch, i, accum_steps) for i in range(accum_steps)]\n\n\ndef get_chunk_from_data(data, chunk_id, num_chunks):\n    \"\"\"\n    Recursively splits all the tensors inside the passed data object into num_chunks.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        assert len(data) % num_chunks == 0\n        start = (len(data) // num_chunks) * chunk_id\n        end = (len(data) // num_chunks) * (chunk_id + 1)\n        return data[start:end]\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasaliasaliasalias)FunctionDef(arguments(argarg)Return(ListComp(Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))FunctionDef(arguments(argargarg)Expr(Constant)If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Assert(Compare(BinOp(Call(Name(Load)Name(Load))ModName(Load))EqConstant))Assign(Name(Store)BinOp(BinOp(Call(Name(Load)Name(Load))FloorDivName(Load))MultName(Load)))Assign(Name(Store)BinOp(BinOp(Call(Name(Load)Name(Load))FloorDivName(Load))MultBinOp(Name(Load)AddConstant)))Return(Subscript(Name(Load)Slice(Name(Load)Name(Load))Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from typing import Any, Dict, List, Mapping, Optional, Sequence\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom hydra.utils import instantiate\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.data.api import Sample\nfrom omnivision.data.concat_dataset import ConcatDataset\nfrom omnivision.data.torch_dataset import TorchDataset\nfrom omnivision.losses import wrap_base_loss\nfrom omnivision.optim import construct_optimizer\nfrom omnivision.utils.train import (\n    AverageMeter,\n    copy_data_to_device,\n    get_amp_type,\n    get_machine_local_and_dist_rank,\n    get_resume_checkpoint,\n    is_dist_avail_and_initialized,\n    makedir,\n    ProgressMeter,\n    set_seeds,\n    setup_distributed_backend,\n    setup_logging,\n)\n\n\ndef chunk_batch_for_accum_steps(batch, accum_steps):\n    return [get_chunk_from_data(batch, i, accum_steps) for i in range(accum_steps)]\n\n\ndef get_chunk_from_data(data, chunk_id, num_chunks):\n    \"\"\"\n    Recursively splits all the tensors inside the passed data object into num_chunks.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        assert len(data) % num_chunks == 0\n        start = (len(data) // num_chunks) * chunk_id\n        end = (len(data) // num_chunks) * (chunk_id + 1)\n        return data[start:end]\n    elif isinstance(data, Mapping):\n        return {\n            key: get_chunk_from_data(value, chunk_id, num_chunks)\n            for key, value in data.items()\n        }\n    elif isinstance(data, Sequence):\n        return [get_chunk_from_data(value, chunk_id, num_chunks) for value in data]\n    elif isinstance(data, Sample):\n        data_cls = type(data)\n        data = data.__dict__\n\nAST=Module(ImportFrom(aliasaliasaliasaliasaliasalias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasaliasaliasalias)FunctionDef(arguments(argarg)Return(ListComp(Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))FunctionDef(arguments(argargarg)Expr(Constant)If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Assert(Compare(BinOp(Call(Name(Load)Name(Load))ModName(Load))EqConstant))Assign(Name(Store)BinOp(BinOp(Call(Name(Load)Name(Load))FloorDivName(Load))MultName(Load)))Assign(Name(Store)BinOp(BinOp(Call(Name(Load)Name(Load))FloorDivName(Load))MultBinOp(Name(Load)AddConstant)))Return(Subscript(Name(Load)Slice(Name(Load)Name(Load))Load))If(Call(Name(Load)Name(Load)Name(Load))Return(DictComp(Name(Load)Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))If(Call(Name(Load)Name(Load)Name(Load))Return(ListComp(Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load))))If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Attribute(Name(Load)Load))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from omnivision.losses import wrap_base_loss\nfrom omnivision.optim import construct_optimizer\nfrom omnivision.utils.train import (\n    AverageMeter,\n    copy_data_to_device,\n    get_amp_type,\n    get_machine_local_and_dist_rank,\n    get_resume_checkpoint,\n    is_dist_avail_and_initialized,\n    makedir,\n    ProgressMeter,\n    set_seeds,\n    setup_distributed_backend,\n    setup_logging,\n)\n\n\ndef chunk_batch_for_accum_steps(batch, accum_steps):\n    return [get_chunk_from_data(batch, i, accum_steps) for i in range(accum_steps)]\n\n\ndef get_chunk_from_data(data, chunk_id, num_chunks):\n    \"\"\"\n    Recursively splits all the tensors inside the passed data object into num_chunks.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        assert len(data) % num_chunks == 0\n        start = (len(data) // num_chunks) * chunk_id\n        end = (len(data) // num_chunks) * (chunk_id + 1)\n        return data[start:end]\n    elif isinstance(data, Mapping):\n        return {\n            key: get_chunk_from_data(value, chunk_id, num_chunks)\n            for key, value in data.items()\n        }\n    elif isinstance(data, Sequence):\n        return [get_chunk_from_data(value, chunk_id, num_chunks) for value in data]\n    elif isinstance(data, Sample):\n        data_cls = type(data)\n        data = data.__dict__\n        return data_cls(**get_chunk_from_data(data, chunk_id, num_chunks))\n    else:\n        return data\n\n\n@dataclass\nclass OmnivisionOptimAMPConf:\n    enabled: bool = False\n    amp_dtype: str = \"float16\"\n\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasaliasaliasalias)FunctionDef(arguments(argarg)Return(ListComp(Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Name(Load))))))FunctionDef(arguments(argargarg)Expr(Constant)If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Assert(Compare(BinOp(Call(Name(Load)Name(Load))ModName(Load))EqConstant))Assign(Name(Store)BinOp(BinOp(Call(Name(Load)Name(Load))FloorDivName(Load))MultName(Load)))Assign(Name(Store)BinOp(BinOp(Call(Name(Load)Name(Load))FloorDivName(Load))MultBinOp(Name(Load)AddConstant)))Return(Subscript(Name(Load)Slice(Name(Load)Name(Load))Load))If(Call(Name(Load)Name(Load)Name(Load))Return(DictComp(Name(Load)Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))If(Call(Name(Load)Name(Load)Name(Load))Return(ListComp(Call(Name(Load)Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load))))If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Attribute(Name(Load)Load))Return(Call(Name(Load)keyword(Call(Name(Load)Name(Load)Name(Load)Name(Load)))))Return(Name(Load)))))))ClassDef(AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ProgressMeter,\n    set_seeds,\n    setup_distributed_backend,\n    setup_logging,\n)\n\n\ndef chunk_batch_for_accum_steps(batch, accum_steps):\n    return [get_chunk_from_data(batch, i, accum_steps) for i in range(accum_steps)]\n\n\ndef get_chunk_from_data(data, chunk_id, num_chunks):\n    \"\"\"\n    Recursively splits all the tensors inside the passed data object into num_chunks.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        assert len(data) % num_chunks == 0\n        start = (len(data) // num_chunks) * chunk_id\n        end = (len(data) // num_chunks) * (chunk_id + 1)\n        return data[start:end]\n    elif isinstance(data, Mapping):\n        return {\n            key: get_chunk_from_data(value, chunk_id, num_chunks)\n            for key, value in data.items()\n        }\n    elif isinstance(data, Sequence):\n        return [get_chunk_from_data(value, chunk_id, num_chunks) for value in data]\n    elif isinstance(data, Sample):\n        data_cls = type(data)\n        data = data.__dict__\n        return data_cls(**get_chunk_from_data(data, chunk_id, num_chunks))\n    else:\n        return data\n\n\n@dataclass\nclass OmnivisionOptimAMPConf:\n    enabled: bool = False\n    amp_dtype: str = \"float16\"\n\n\n@dataclass\nclass OmnivisionOptimConf:\n    optimizer: torch.optim.Optimizer = None\n    options: Optional[Dict[str, Any]] = None\n    param_group_modifiers: Optional[List] = None\n    amp: Optional[Dict[str, Any]] = None\n    gradient_clip: Any = None\n\n    def __post_init__(self):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef get_chunk_from_data(data, chunk_id, num_chunks):\n    \"\"\"\n    Recursively splits all the tensors inside the passed data object into num_chunks.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        assert len(data) % num_chunks == 0\n        start = (len(data) // num_chunks) * chunk_id\n        end = (len(data) // num_chunks) * (chunk_id + 1)\n        return data[start:end]\n    elif isinstance(data, Mapping):\n        return {\n            key: get_chunk_from_data(value, chunk_id, num_chunks)\n            for key, value in data.items()\n        }\n    elif isinstance(data, Sequence):\n        return [get_chunk_from_data(value, chunk_id, num_chunks) for value in data]\n    elif isinstance(data, Sample):\n        data_cls = type(data)\n        data = data.__dict__\n        return data_cls(**get_chunk_from_data(data, chunk_id, num_chunks))\n    else:\n        return data\n\n\n@dataclass\nclass OmnivisionOptimAMPConf:\n    enabled: bool = False\n    amp_dtype: str = \"float16\"\n\n\n@dataclass\nclass OmnivisionOptimConf:\n    optimizer: torch.optim.Optimizer = None\n    options: Optional[Dict[str, Any]] = None\n    param_group_modifiers: Optional[List] = None\n    amp: Optional[Dict[str, Any]] = None\n    gradient_clip: Any = None\n\n    def __post_init__(self):\n        # amp\n        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n\n\n@dataclass\nclass OmnivisionDistributedConf:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif isinstance(data, Mapping):\n        return {\n            key: get_chunk_from_data(value, chunk_id, num_chunks)\n            for key, value in data.items()\n        }\n    elif isinstance(data, Sequence):\n        return [get_chunk_from_data(value, chunk_id, num_chunks) for value in data]\n    elif isinstance(data, Sample):\n        data_cls = type(data)\n        data = data.__dict__\n        return data_cls(**get_chunk_from_data(data, chunk_id, num_chunks))\n    else:\n        return data\n\n\n@dataclass\nclass OmnivisionOptimAMPConf:\n    enabled: bool = False\n    amp_dtype: str = \"float16\"\n\n\n@dataclass\nclass OmnivisionOptimConf:\n    optimizer: torch.optim.Optimizer = None\n    options: Optional[Dict[str, Any]] = None\n    param_group_modifiers: Optional[List] = None\n    amp: Optional[Dict[str, Any]] = None\n    gradient_clip: Any = None\n\n    def __post_init__(self):\n        # amp\n        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n\n\n@dataclass\nclass OmnivisionDistributedConf:\n    backend: Optional[str] = None  # inferred from accelerator type\n    comms_dtype: Optional[str] = None\n    find_unused_parameters: bool = False\n\n\n@dataclass\nclass OmnivisionCudaConf:\n    cudnn_deterministic: bool = False\n    cudnn_benchmark: bool = True\n    allow_tf32: bool = False", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return data_cls(**get_chunk_from_data(data, chunk_id, num_chunks))\n    else:\n        return data\n\n\n@dataclass\nclass OmnivisionOptimAMPConf:\n    enabled: bool = False\n    amp_dtype: str = \"float16\"\n\n\n@dataclass\nclass OmnivisionOptimConf:\n    optimizer: torch.optim.Optimizer = None\n    options: Optional[Dict[str, Any]] = None\n    param_group_modifiers: Optional[List] = None\n    amp: Optional[Dict[str, Any]] = None\n    gradient_clip: Any = None\n\n    def __post_init__(self):\n        # amp\n        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n\n\n@dataclass\nclass OmnivisionDistributedConf:\n    backend: Optional[str] = None  # inferred from accelerator type\n    comms_dtype: Optional[str] = None\n    find_unused_parameters: bool = False\n\n\n@dataclass\nclass OmnivisionCudaConf:\n    cudnn_deterministic: bool = False\n    cudnn_benchmark: bool = True\n    allow_tf32: bool = False\n\n\n@dataclass\nclass OmnivisionCheckpointConf:\n    save_dir: str\n    save_freq: int\n    model_weight_initializer: Any = None\n\n\nclass OmnivisionTrainer(object):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n@dataclass\nclass OmnivisionOptimConf:\n    optimizer: torch.optim.Optimizer = None\n    options: Optional[Dict[str, Any]] = None\n    param_group_modifiers: Optional[List] = None\n    amp: Optional[Dict[str, Any]] = None\n    gradient_clip: Any = None\n\n    def __post_init__(self):\n        # amp\n        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n\n\n@dataclass\nclass OmnivisionDistributedConf:\n    backend: Optional[str] = None  # inferred from accelerator type\n    comms_dtype: Optional[str] = None\n    find_unused_parameters: bool = False\n\n\n@dataclass\nclass OmnivisionCudaConf:\n    cudnn_deterministic: bool = False\n    cudnn_benchmark: bool = True\n    allow_tf32: bool = False\n\n\n@dataclass\nclass OmnivisionCheckpointConf:\n    save_dir: str\n    save_freq: int\n    model_weight_initializer: Any = None\n\n\nclass OmnivisionTrainer(object):\n    \"\"\"\n    Omnivision Trainer supporting the DDP training strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,  # the order of these args can change at any time, so they are keyword-only\n        data: Dict[str, Any],\n        model: Dict[str, Any],\n        logging: Dict[str, Any],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # amp\n        if not isinstance(self.amp, OmnivisionOptimAMPConf):\n            if self.amp is None:\n                self.amp = {}\n            assert isinstance(self.amp, Mapping)\n            self.amp = OmnivisionOptimAMPConf(**self.amp)\n\n\n@dataclass\nclass OmnivisionDistributedConf:\n    backend: Optional[str] = None  # inferred from accelerator type\n    comms_dtype: Optional[str] = None\n    find_unused_parameters: bool = False\n\n\n@dataclass\nclass OmnivisionCudaConf:\n    cudnn_deterministic: bool = False\n    cudnn_benchmark: bool = True\n    allow_tf32: bool = False\n\n\n@dataclass\nclass OmnivisionCheckpointConf:\n    save_dir: str\n    save_freq: int\n    model_weight_initializer: Any = None\n\n\nclass OmnivisionTrainer(object):\n    \"\"\"\n    Omnivision Trainer supporting the DDP training strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,  # the order of these args can change at any time, so they are keyword-only\n        data: Dict[str, Any],\n        model: Dict[str, Any],\n        logging: Dict[str, Any],\n        checkpoint: Dict[str, Any],\n        max_epochs: int,\n        mode: str = \"train\",\n        accelerator: str = \"cuda\",\n        seed_value: int = 123,\n        val_epoch_freq: int = 1,\n        distributed: Dict[str, bool] = None,\n        cuda: Dict[str, bool] = None,\n        limit_train_batches: Optional[int] = None,\n        limit_val_batches: Optional[int] = None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    backend: Optional[str] = None  # inferred from accelerator type\n    comms_dtype: Optional[str] = None\n    find_unused_parameters: bool = False\n\n\n@dataclass\nclass OmnivisionCudaConf:\n    cudnn_deterministic: bool = False\n    cudnn_benchmark: bool = True\n    allow_tf32: bool = False\n\n\n@dataclass\nclass OmnivisionCheckpointConf:\n    save_dir: str\n    save_freq: int\n    model_weight_initializer: Any = None\n\n\nclass OmnivisionTrainer(object):\n    \"\"\"\n    Omnivision Trainer supporting the DDP training strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,  # the order of these args can change at any time, so they are keyword-only\n        data: Dict[str, Any],\n        model: Dict[str, Any],\n        logging: Dict[str, Any],\n        checkpoint: Dict[str, Any],\n        max_epochs: int,\n        mode: str = \"train\",\n        accelerator: str = \"cuda\",\n        seed_value: int = 123,\n        val_epoch_freq: int = 1,\n        distributed: Dict[str, bool] = None,\n        cuda: Dict[str, bool] = None,\n        limit_train_batches: Optional[int] = None,\n        limit_val_batches: Optional[int] = None,\n        env_variables: Optional[Dict[str, Any]] = None,\n        optim: Optional[Dict[str, Any]] = None,\n        metrics: Optional[Dict[str, Any]] = None,\n        loss: Optional[Dict[str, Any]] = None,\n    ):\n        ## TODO: Re-factor to expose train_step as target.\n        ## TODO: Support for Sync batchnorm.\n\n        self.data_conf = data\n        self.model_conf = model", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n@dataclass\nclass OmnivisionCheckpointConf:\n    save_dir: str\n    save_freq: int\n    model_weight_initializer: Any = None\n\n\nclass OmnivisionTrainer(object):\n    \"\"\"\n    Omnivision Trainer supporting the DDP training strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,  # the order of these args can change at any time, so they are keyword-only\n        data: Dict[str, Any],\n        model: Dict[str, Any],\n        logging: Dict[str, Any],\n        checkpoint: Dict[str, Any],\n        max_epochs: int,\n        mode: str = \"train\",\n        accelerator: str = \"cuda\",\n        seed_value: int = 123,\n        val_epoch_freq: int = 1,\n        distributed: Dict[str, bool] = None,\n        cuda: Dict[str, bool] = None,\n        limit_train_batches: Optional[int] = None,\n        limit_val_batches: Optional[int] = None,\n        env_variables: Optional[Dict[str, Any]] = None,\n        optim: Optional[Dict[str, Any]] = None,\n        metrics: Optional[Dict[str, Any]] = None,\n        loss: Optional[Dict[str, Any]] = None,\n    ):\n        ## TODO: Re-factor to expose train_step as target.\n        ## TODO: Support for Sync batchnorm.\n\n        self.data_conf = data\n        self.model_conf = model\n        self.logging_conf = logging\n        self.checkpoint_conf = OmnivisionCheckpointConf(**checkpoint)\n        self.max_epochs = max_epochs\n        self.mode = mode\n        self.val_epoch_freq = val_epoch_freq\n        self.limit_train_batches = limit_train_batches\n        self.limit_val_batches = limit_val_batches\n        self.optim_conf = OmnivisionOptimConf(**optim or {})\n        self.metrics_conf = metrics\n        self.loss_conf = loss\n\nAST=Module(ClassDef(AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Name(Load)Constant)Name(Load))ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argarg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Name(Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Name(Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Name(Load)keyword(BoolOp(OrName(Load)Dict))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    Omnivision Trainer supporting the DDP training strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,  # the order of these args can change at any time, so they are keyword-only\n        data: Dict[str, Any],\n        model: Dict[str, Any],\n        logging: Dict[str, Any],\n        checkpoint: Dict[str, Any],\n        max_epochs: int,\n        mode: str = \"train\",\n        accelerator: str = \"cuda\",\n        seed_value: int = 123,\n        val_epoch_freq: int = 1,\n        distributed: Dict[str, bool] = None,\n        cuda: Dict[str, bool] = None,\n        limit_train_batches: Optional[int] = None,\n        limit_val_batches: Optional[int] = None,\n        env_variables: Optional[Dict[str, Any]] = None,\n        optim: Optional[Dict[str, Any]] = None,\n        metrics: Optional[Dict[str, Any]] = None,\n        loss: Optional[Dict[str, Any]] = None,\n    ):\n        ## TODO: Re-factor to expose train_step as target.\n        ## TODO: Support for Sync batchnorm.\n\n        self.data_conf = data\n        self.model_conf = model\n        self.logging_conf = logging\n        self.checkpoint_conf = OmnivisionCheckpointConf(**checkpoint)\n        self.max_epochs = max_epochs\n        self.mode = mode\n        self.val_epoch_freq = val_epoch_freq\n        self.limit_train_batches = limit_train_batches\n        self.limit_val_batches = limit_val_batches\n        self.optim_conf = OmnivisionOptimConf(**optim or {})\n        self.metrics_conf = metrics\n        self.loss_conf = loss\n        distributed = OmnivisionDistributedConf(**distributed or {})\n        cuda = OmnivisionCudaConf(**cuda or {})\n\n        self._maybe_infer_distributed_backend(distributed, accelerator)\n\n        self._setup_env_variables(env_variables)\n        self._setup_device(accelerator)\n\n        makedir(self.logging_conf.log_dir)\n        setup_logging(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        checkpoint: Dict[str, Any],\n        max_epochs: int,\n        mode: str = \"train\",\n        accelerator: str = \"cuda\",\n        seed_value: int = 123,\n        val_epoch_freq: int = 1,\n        distributed: Dict[str, bool] = None,\n        cuda: Dict[str, bool] = None,\n        limit_train_batches: Optional[int] = None,\n        limit_val_batches: Optional[int] = None,\n        env_variables: Optional[Dict[str, Any]] = None,\n        optim: Optional[Dict[str, Any]] = None,\n        metrics: Optional[Dict[str, Any]] = None,\n        loss: Optional[Dict[str, Any]] = None,\n    ):\n        ## TODO: Re-factor to expose train_step as target.\n        ## TODO: Support for Sync batchnorm.\n\n        self.data_conf = data\n        self.model_conf = model\n        self.logging_conf = logging\n        self.checkpoint_conf = OmnivisionCheckpointConf(**checkpoint)\n        self.max_epochs = max_epochs\n        self.mode = mode\n        self.val_epoch_freq = val_epoch_freq\n        self.limit_train_batches = limit_train_batches\n        self.limit_val_batches = limit_val_batches\n        self.optim_conf = OmnivisionOptimConf(**optim or {})\n        self.metrics_conf = metrics\n        self.loss_conf = loss\n        distributed = OmnivisionDistributedConf(**distributed or {})\n        cuda = OmnivisionCudaConf(**cuda or {})\n\n        self._maybe_infer_distributed_backend(distributed, accelerator)\n\n        self._setup_env_variables(env_variables)\n        self._setup_device(accelerator)\n\n        makedir(self.logging_conf.log_dir)\n        setup_logging(\n            __name__,\n            output_dir=self.logging_conf.log_dir,\n            rank=self.local_rank,\n        )\n        # TODO: Enable seperate seed setting for each data worker.\n        set_seeds(seed_value, self.max_epochs, self.distributed_rank)\n\n        self._setup_torch_dist_and_backend(cuda, distributed)\n\n        assert (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        env_variables: Optional[Dict[str, Any]] = None,\n        optim: Optional[Dict[str, Any]] = None,\n        metrics: Optional[Dict[str, Any]] = None,\n        loss: Optional[Dict[str, Any]] = None,\n    ):\n        ## TODO: Re-factor to expose train_step as target.\n        ## TODO: Support for Sync batchnorm.\n\n        self.data_conf = data\n        self.model_conf = model\n        self.logging_conf = logging\n        self.checkpoint_conf = OmnivisionCheckpointConf(**checkpoint)\n        self.max_epochs = max_epochs\n        self.mode = mode\n        self.val_epoch_freq = val_epoch_freq\n        self.limit_train_batches = limit_train_batches\n        self.limit_val_batches = limit_val_batches\n        self.optim_conf = OmnivisionOptimConf(**optim or {})\n        self.metrics_conf = metrics\n        self.loss_conf = loss\n        distributed = OmnivisionDistributedConf(**distributed or {})\n        cuda = OmnivisionCudaConf(**cuda or {})\n\n        self._maybe_infer_distributed_backend(distributed, accelerator)\n\n        self._setup_env_variables(env_variables)\n        self._setup_device(accelerator)\n\n        makedir(self.logging_conf.log_dir)\n        setup_logging(\n            __name__,\n            output_dir=self.logging_conf.log_dir,\n            rank=self.local_rank,\n        )\n        # TODO: Enable seperate seed setting for each data worker.\n        set_seeds(seed_value, self.max_epochs, self.distributed_rank)\n\n        self._setup_torch_dist_and_backend(cuda, distributed)\n\n        assert (\n            is_dist_avail_and_initialized()\n        ), \"Torch distributed needs to be initialized before calling the trainer.\"\n\n        self._setup_components()  # Except Optimizer everything is setup here.\n        self._move_to_device()\n        self._construct_optimizer()\n        self.load_checkpoint()\n        self._setup_ddp_components(distributed, accelerator)\n        self._setup_dataloaders()\n        dist.barrier()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.logging_conf = logging\n        self.checkpoint_conf = OmnivisionCheckpointConf(**checkpoint)\n        self.max_epochs = max_epochs\n        self.mode = mode\n        self.val_epoch_freq = val_epoch_freq\n        self.limit_train_batches = limit_train_batches\n        self.limit_val_batches = limit_val_batches\n        self.optim_conf = OmnivisionOptimConf(**optim or {})\n        self.metrics_conf = metrics\n        self.loss_conf = loss\n        distributed = OmnivisionDistributedConf(**distributed or {})\n        cuda = OmnivisionCudaConf(**cuda or {})\n\n        self._maybe_infer_distributed_backend(distributed, accelerator)\n\n        self._setup_env_variables(env_variables)\n        self._setup_device(accelerator)\n\n        makedir(self.logging_conf.log_dir)\n        setup_logging(\n            __name__,\n            output_dir=self.logging_conf.log_dir,\n            rank=self.local_rank,\n        )\n        # TODO: Enable seperate seed setting for each data worker.\n        set_seeds(seed_value, self.max_epochs, self.distributed_rank)\n\n        self._setup_torch_dist_and_backend(cuda, distributed)\n\n        assert (\n            is_dist_avail_and_initialized()\n        ), \"Torch distributed needs to be initialized before calling the trainer.\"\n\n        self._setup_components()  # Except Optimizer everything is setup here.\n        self._move_to_device()\n        self._construct_optimizer()\n        self.load_checkpoint()\n        self._setup_ddp_components(distributed, accelerator)\n        self._setup_dataloaders()\n        dist.barrier()\n\n    def _maybe_infer_distributed_backend(self, distributed_conf, accelerator):\n        if distributed_conf.backend is None:\n            distributed_conf.backend = \"nccl\" if accelerator == \"cuda\" else \"gloo\"\n\n    def _setup_env_variables(self, env_variables_conf) -> None:\n        if env_variables_conf is not None:\n            for variable_name, value in env_variables_conf.items():\n                os.environ[variable_name] = value\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        distributed = OmnivisionDistributedConf(**distributed or {})\n        cuda = OmnivisionCudaConf(**cuda or {})\n\n        self._maybe_infer_distributed_backend(distributed, accelerator)\n\n        self._setup_env_variables(env_variables)\n        self._setup_device(accelerator)\n\n        makedir(self.logging_conf.log_dir)\n        setup_logging(\n            __name__,\n            output_dir=self.logging_conf.log_dir,\n            rank=self.local_rank,\n        )\n        # TODO: Enable seperate seed setting for each data worker.\n        set_seeds(seed_value, self.max_epochs, self.distributed_rank)\n\n        self._setup_torch_dist_and_backend(cuda, distributed)\n\n        assert (\n            is_dist_avail_and_initialized()\n        ), \"Torch distributed needs to be initialized before calling the trainer.\"\n\n        self._setup_components()  # Except Optimizer everything is setup here.\n        self._move_to_device()\n        self._construct_optimizer()\n        self.load_checkpoint()\n        self._setup_ddp_components(distributed, accelerator)\n        self._setup_dataloaders()\n        dist.barrier()\n\n    def _maybe_infer_distributed_backend(self, distributed_conf, accelerator):\n        if distributed_conf.backend is None:\n            distributed_conf.backend = \"nccl\" if accelerator == \"cuda\" else \"gloo\"\n\n    def _setup_env_variables(self, env_variables_conf) -> None:\n        if env_variables_conf is not None:\n            for variable_name, value in env_variables_conf.items():\n                os.environ[variable_name] = value\n\n    def _setup_torch_dist_and_backend(self, cuda_conf, distributed_conf) -> None:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = cuda_conf.cudnn_deterministic\n            torch.backends.cudnn.benchmark = cuda_conf.cudnn_benchmark\n            torch.backends.cuda.matmul.allow_tf32 = cuda_conf.allow_tf32\n            torch.backends.cudnn.allow_tf32 = cuda_conf.allow_tf32\n\n        setup_distributed_backend(distributed_conf.backend)\n\n    def _setup_device(self, accelerator):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            __name__,\n            output_dir=self.logging_conf.log_dir,\n            rank=self.local_rank,\n        )\n        # TODO: Enable seperate seed setting for each data worker.\n        set_seeds(seed_value, self.max_epochs, self.distributed_rank)\n\n        self._setup_torch_dist_and_backend(cuda, distributed)\n\n        assert (\n            is_dist_avail_and_initialized()\n        ), \"Torch distributed needs to be initialized before calling the trainer.\"\n\n        self._setup_components()  # Except Optimizer everything is setup here.\n        self._move_to_device()\n        self._construct_optimizer()\n        self.load_checkpoint()\n        self._setup_ddp_components(distributed, accelerator)\n        self._setup_dataloaders()\n        dist.barrier()\n\n    def _maybe_infer_distributed_backend(self, distributed_conf, accelerator):\n        if distributed_conf.backend is None:\n            distributed_conf.backend = \"nccl\" if accelerator == \"cuda\" else \"gloo\"\n\n    def _setup_env_variables(self, env_variables_conf) -> None:\n        if env_variables_conf is not None:\n            for variable_name, value in env_variables_conf.items():\n                os.environ[variable_name] = value\n\n    def _setup_torch_dist_and_backend(self, cuda_conf, distributed_conf) -> None:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = cuda_conf.cudnn_deterministic\n            torch.backends.cudnn.benchmark = cuda_conf.cudnn_benchmark\n            torch.backends.cuda.matmul.allow_tf32 = cuda_conf.allow_tf32\n            torch.backends.cudnn.allow_tf32 = cuda_conf.allow_tf32\n\n        setup_distributed_backend(distributed_conf.backend)\n\n    def _setup_device(self, accelerator):\n        self.local_rank, self.distributed_rank = get_machine_local_and_dist_rank()\n        if accelerator == \"cuda\":\n            self.device = torch.device(\"cuda\", self.local_rank)\n            torch.cuda.set_device(self.local_rank)\n        elif accelerator == \"cpu\":\n            self.device = torch.device(\"cpu\")\n        else:\n            raise ValueError(f\"Unsupported accelerator: {accelerator}\")\n\n    def _setup_ddp_components(self, distributed_conf, accelerator):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            is_dist_avail_and_initialized()\n        ), \"Torch distributed needs to be initialized before calling the trainer.\"\n\n        self._setup_components()  # Except Optimizer everything is setup here.\n        self._move_to_device()\n        self._construct_optimizer()\n        self.load_checkpoint()\n        self._setup_ddp_components(distributed, accelerator)\n        self._setup_dataloaders()\n        dist.barrier()\n\n    def _maybe_infer_distributed_backend(self, distributed_conf, accelerator):\n        if distributed_conf.backend is None:\n            distributed_conf.backend = \"nccl\" if accelerator == \"cuda\" else \"gloo\"\n\n    def _setup_env_variables(self, env_variables_conf) -> None:\n        if env_variables_conf is not None:\n            for variable_name, value in env_variables_conf.items():\n                os.environ[variable_name] = value\n\n    def _setup_torch_dist_and_backend(self, cuda_conf, distributed_conf) -> None:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = cuda_conf.cudnn_deterministic\n            torch.backends.cudnn.benchmark = cuda_conf.cudnn_benchmark\n            torch.backends.cuda.matmul.allow_tf32 = cuda_conf.allow_tf32\n            torch.backends.cudnn.allow_tf32 = cuda_conf.allow_tf32\n\n        setup_distributed_backend(distributed_conf.backend)\n\n    def _setup_device(self, accelerator):\n        self.local_rank, self.distributed_rank = get_machine_local_and_dist_rank()\n        if accelerator == \"cuda\":\n            self.device = torch.device(\"cuda\", self.local_rank)\n            torch.cuda.set_device(self.local_rank)\n        elif accelerator == \"cpu\":\n            self.device = torch.device(\"cpu\")\n        else:\n            raise ValueError(f\"Unsupported accelerator: {accelerator}\")\n\n    def _setup_ddp_components(self, distributed_conf, accelerator):\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank] if accelerator == \"cuda\" else [],\n            find_unused_parameters=distributed_conf.find_unused_parameters,\n        )\n\n        if distributed_conf.comms_dtype is not None:  # noqa\n\n            from torch.distributed.algorithms import ddp_comm_hooks\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _maybe_infer_distributed_backend(self, distributed_conf, accelerator):\n        if distributed_conf.backend is None:\n            distributed_conf.backend = \"nccl\" if accelerator == \"cuda\" else \"gloo\"\n\n    def _setup_env_variables(self, env_variables_conf) -> None:\n        if env_variables_conf is not None:\n            for variable_name, value in env_variables_conf.items():\n                os.environ[variable_name] = value\n\n    def _setup_torch_dist_and_backend(self, cuda_conf, distributed_conf) -> None:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = cuda_conf.cudnn_deterministic\n            torch.backends.cudnn.benchmark = cuda_conf.cudnn_benchmark\n            torch.backends.cuda.matmul.allow_tf32 = cuda_conf.allow_tf32\n            torch.backends.cudnn.allow_tf32 = cuda_conf.allow_tf32\n\n        setup_distributed_backend(distributed_conf.backend)\n\n    def _setup_device(self, accelerator):\n        self.local_rank, self.distributed_rank = get_machine_local_and_dist_rank()\n        if accelerator == \"cuda\":\n            self.device = torch.device(\"cuda\", self.local_rank)\n            torch.cuda.set_device(self.local_rank)\n        elif accelerator == \"cpu\":\n            self.device = torch.device(\"cpu\")\n        else:\n            raise ValueError(f\"Unsupported accelerator: {accelerator}\")\n\n    def _setup_ddp_components(self, distributed_conf, accelerator):\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank] if accelerator == \"cuda\" else [],\n            find_unused_parameters=distributed_conf.find_unused_parameters,\n        )\n\n        if distributed_conf.comms_dtype is not None:  # noqa\n\n            from torch.distributed.algorithms import ddp_comm_hooks\n\n            amp_type = get_amp_type(distributed_conf.comms_dtype)\n            if amp_type == torch.bfloat16:\n                hook = ddp_comm_hooks.default_hooks.bf16_compress_hook\n                logging.info(\"Enabling bfloat16 grad communication\")\n            else:\n                hook = ddp_comm_hooks.default_hooks.fp16_compress_hook\n                logging.info(\"Enabling fp16 grad communication\")\n            process_group = None\n            self.model.register_comm_hook(process_group, hook)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def _setup_torch_dist_and_backend(self, cuda_conf, distributed_conf) -> None:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = cuda_conf.cudnn_deterministic\n            torch.backends.cudnn.benchmark = cuda_conf.cudnn_benchmark\n            torch.backends.cuda.matmul.allow_tf32 = cuda_conf.allow_tf32\n            torch.backends.cudnn.allow_tf32 = cuda_conf.allow_tf32\n\n        setup_distributed_backend(distributed_conf.backend)\n\n    def _setup_device(self, accelerator):\n        self.local_rank, self.distributed_rank = get_machine_local_and_dist_rank()\n        if accelerator == \"cuda\":\n            self.device = torch.device(\"cuda\", self.local_rank)\n            torch.cuda.set_device(self.local_rank)\n        elif accelerator == \"cpu\":\n            self.device = torch.device(\"cpu\")\n        else:\n            raise ValueError(f\"Unsupported accelerator: {accelerator}\")\n\n    def _setup_ddp_components(self, distributed_conf, accelerator):\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank] if accelerator == \"cuda\" else [],\n            find_unused_parameters=distributed_conf.find_unused_parameters,\n        )\n\n        if distributed_conf.comms_dtype is not None:  # noqa\n\n            from torch.distributed.algorithms import ddp_comm_hooks\n\n            amp_type = get_amp_type(distributed_conf.comms_dtype)\n            if amp_type == torch.bfloat16:\n                hook = ddp_comm_hooks.default_hooks.bf16_compress_hook\n                logging.info(\"Enabling bfloat16 grad communication\")\n            else:\n                hook = ddp_comm_hooks.default_hooks.fp16_compress_hook\n                logging.info(\"Enabling fp16 grad communication\")\n            process_group = None\n            self.model.register_comm_hook(process_group, hook)\n\n    def _move_to_device(self):\n        logging.info(\n            f\"Moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n        self.model.to(self.device)\n\n        if self.loss:\n            copy_data_to_device(self.loss, self.device)\n        if self.scaler:\n            copy_data_to_device(self.scaler, self.device)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.local_rank, self.distributed_rank = get_machine_local_and_dist_rank()\n        if accelerator == \"cuda\":\n            self.device = torch.device(\"cuda\", self.local_rank)\n            torch.cuda.set_device(self.local_rank)\n        elif accelerator == \"cpu\":\n            self.device = torch.device(\"cpu\")\n        else:\n            raise ValueError(f\"Unsupported accelerator: {accelerator}\")\n\n    def _setup_ddp_components(self, distributed_conf, accelerator):\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank] if accelerator == \"cuda\" else [],\n            find_unused_parameters=distributed_conf.find_unused_parameters,\n        )\n\n        if distributed_conf.comms_dtype is not None:  # noqa\n\n            from torch.distributed.algorithms import ddp_comm_hooks\n\n            amp_type = get_amp_type(distributed_conf.comms_dtype)\n            if amp_type == torch.bfloat16:\n                hook = ddp_comm_hooks.default_hooks.bf16_compress_hook\n                logging.info(\"Enabling bfloat16 grad communication\")\n            else:\n                hook = ddp_comm_hooks.default_hooks.fp16_compress_hook\n                logging.info(\"Enabling fp16 grad communication\")\n            process_group = None\n            self.model.register_comm_hook(process_group, hook)\n\n    def _move_to_device(self):\n        logging.info(\n            f\"Moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n        self.model.to(self.device)\n\n        if self.loss:\n            copy_data_to_device(self.loss, self.device)\n        if self.scaler:\n            copy_data_to_device(self.scaler, self.device)\n        if self.metrics:\n            self.metrics.to(self.device)\n\n        logging.info(\n            f\"Done moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n\n    def checkpoint_save(self, epoch):\n\n        if self.distributed_rank != 0:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank] if accelerator == \"cuda\" else [],\n            find_unused_parameters=distributed_conf.find_unused_parameters,\n        )\n\n        if distributed_conf.comms_dtype is not None:  # noqa\n\n            from torch.distributed.algorithms import ddp_comm_hooks\n\n            amp_type = get_amp_type(distributed_conf.comms_dtype)\n            if amp_type == torch.bfloat16:\n                hook = ddp_comm_hooks.default_hooks.bf16_compress_hook\n                logging.info(\"Enabling bfloat16 grad communication\")\n            else:\n                hook = ddp_comm_hooks.default_hooks.fp16_compress_hook\n                logging.info(\"Enabling fp16 grad communication\")\n            process_group = None\n            self.model.register_comm_hook(process_group, hook)\n\n    def _move_to_device(self):\n        logging.info(\n            f\"Moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n        self.model.to(self.device)\n\n        if self.loss:\n            copy_data_to_device(self.loss, self.device)\n        if self.scaler:\n            copy_data_to_device(self.scaler, self.device)\n        if self.metrics:\n            self.metrics.to(self.device)\n\n        logging.info(\n            f\"Done moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n\n    def checkpoint_save(self, epoch):\n\n        if self.distributed_rank != 0:\n            return\n\n        checkpoint_folder = self.checkpoint_conf.save_dir\n        makedir(checkpoint_folder)\n        checkpoint_paths = []\n        checkpoint_paths.append(os.path.join(checkpoint_folder, \"checkpoint.pt\"))\n        if (\n            self.checkpoint_conf.save_freq > 0\n            and int(epoch) % self.checkpoint_conf.save_freq == 0\n        ):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            amp_type = get_amp_type(distributed_conf.comms_dtype)\n            if amp_type == torch.bfloat16:\n                hook = ddp_comm_hooks.default_hooks.bf16_compress_hook\n                logging.info(\"Enabling bfloat16 grad communication\")\n            else:\n                hook = ddp_comm_hooks.default_hooks.fp16_compress_hook\n                logging.info(\"Enabling fp16 grad communication\")\n            process_group = None\n            self.model.register_comm_hook(process_group, hook)\n\n    def _move_to_device(self):\n        logging.info(\n            f\"Moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n        self.model.to(self.device)\n\n        if self.loss:\n            copy_data_to_device(self.loss, self.device)\n        if self.scaler:\n            copy_data_to_device(self.scaler, self.device)\n        if self.metrics:\n            self.metrics.to(self.device)\n\n        logging.info(\n            f\"Done moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n\n    def checkpoint_save(self, epoch):\n\n        if self.distributed_rank != 0:\n            return\n\n        checkpoint_folder = self.checkpoint_conf.save_dir\n        makedir(checkpoint_folder)\n        checkpoint_paths = []\n        checkpoint_paths.append(os.path.join(checkpoint_folder, \"checkpoint.pt\"))\n        if (\n            self.checkpoint_conf.save_freq > 0\n            and int(epoch) % self.checkpoint_conf.save_freq == 0\n        ):\n            checkpoint_paths.append(\n                os.path.join(checkpoint_folder, f\"checkpoint_{int(epoch)}.pt\")\n            )\n\n        checkpoint = {\n            \"model\": self.model.module.state_dict(),\n            \"optimizer\": self.optim.optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"loss\": self.loss.state_dict(),\n            \"steps\": self.steps,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def _move_to_device(self):\n        logging.info(\n            f\"Moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n        self.model.to(self.device)\n\n        if self.loss:\n            copy_data_to_device(self.loss, self.device)\n        if self.scaler:\n            copy_data_to_device(self.scaler, self.device)\n        if self.metrics:\n            self.metrics.to(self.device)\n\n        logging.info(\n            f\"Done moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n\n    def checkpoint_save(self, epoch):\n\n        if self.distributed_rank != 0:\n            return\n\n        checkpoint_folder = self.checkpoint_conf.save_dir\n        makedir(checkpoint_folder)\n        checkpoint_paths = []\n        checkpoint_paths.append(os.path.join(checkpoint_folder, \"checkpoint.pt\"))\n        if (\n            self.checkpoint_conf.save_freq > 0\n            and int(epoch) % self.checkpoint_conf.save_freq == 0\n        ):\n            checkpoint_paths.append(\n                os.path.join(checkpoint_folder, f\"checkpoint_{int(epoch)}.pt\")\n            )\n\n        checkpoint = {\n            \"model\": self.model.module.state_dict(),\n            \"optimizer\": self.optim.optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"loss\": self.loss.state_dict(),\n            \"steps\": self.steps,\n        }\n        if self.optim_conf.amp.enabled:\n            checkpoint[\"scaler\"] = self.scaler.state_dict()\n\n        for checkpoint_path in checkpoint_paths:\n            with g_pathmgr.open(checkpoint_path, \"wb\") as f:\n                torch.save(checkpoint, f)\n\n    def load_checkpoint(self):\n        ckpt_path = get_resume_checkpoint(self.checkpoint_conf.save_dir)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if self.metrics:\n            self.metrics.to(self.device)\n\n        logging.info(\n            f\"Done moving components to device {self.device} and local rank {self.local_rank}.\"\n        )\n\n    def checkpoint_save(self, epoch):\n\n        if self.distributed_rank != 0:\n            return\n\n        checkpoint_folder = self.checkpoint_conf.save_dir\n        makedir(checkpoint_folder)\n        checkpoint_paths = []\n        checkpoint_paths.append(os.path.join(checkpoint_folder, \"checkpoint.pt\"))\n        if (\n            self.checkpoint_conf.save_freq > 0\n            and int(epoch) % self.checkpoint_conf.save_freq == 0\n        ):\n            checkpoint_paths.append(\n                os.path.join(checkpoint_folder, f\"checkpoint_{int(epoch)}.pt\")\n            )\n\n        checkpoint = {\n            \"model\": self.model.module.state_dict(),\n            \"optimizer\": self.optim.optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"loss\": self.loss.state_dict(),\n            \"steps\": self.steps,\n        }\n        if self.optim_conf.amp.enabled:\n            checkpoint[\"scaler\"] = self.scaler.state_dict()\n\n        for checkpoint_path in checkpoint_paths:\n            with g_pathmgr.open(checkpoint_path, \"wb\") as f:\n                torch.save(checkpoint, f)\n\n    def load_checkpoint(self):\n        ckpt_path = get_resume_checkpoint(self.checkpoint_conf.save_dir)\n\n        if ckpt_path is None:\n            # Loading pre-trained model weights\n            model_weight_initializer = instantiate(\n                self.checkpoint_conf.model_weight_initializer\n            )\n            if model_weight_initializer is not None:\n                logging.info(\n                    f\"Loading pretrained checkpoint from {self.checkpoint_conf.model_weight_initializer}\"\n                )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            return\n\n        checkpoint_folder = self.checkpoint_conf.save_dir\n        makedir(checkpoint_folder)\n        checkpoint_paths = []\n        checkpoint_paths.append(os.path.join(checkpoint_folder, \"checkpoint.pt\"))\n        if (\n            self.checkpoint_conf.save_freq > 0\n            and int(epoch) % self.checkpoint_conf.save_freq == 0\n        ):\n            checkpoint_paths.append(\n                os.path.join(checkpoint_folder, f\"checkpoint_{int(epoch)}.pt\")\n            )\n\n        checkpoint = {\n            \"model\": self.model.module.state_dict(),\n            \"optimizer\": self.optim.optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"loss\": self.loss.state_dict(),\n            \"steps\": self.steps,\n        }\n        if self.optim_conf.amp.enabled:\n            checkpoint[\"scaler\"] = self.scaler.state_dict()\n\n        for checkpoint_path in checkpoint_paths:\n            with g_pathmgr.open(checkpoint_path, \"wb\") as f:\n                torch.save(checkpoint, f)\n\n    def load_checkpoint(self):\n        ckpt_path = get_resume_checkpoint(self.checkpoint_conf.save_dir)\n\n        if ckpt_path is None:\n            # Loading pre-trained model weights\n            model_weight_initializer = instantiate(\n                self.checkpoint_conf.model_weight_initializer\n            )\n            if model_weight_initializer is not None:\n                logging.info(\n                    f\"Loading pretrained checkpoint from {self.checkpoint_conf.model_weight_initializer}\"\n                )\n                self.model = model_weight_initializer(model=self.model)\n\n        else:\n            # Resuming from previous training checkpoint\n            logging.info(f\"Resuming training from {ckpt_path}\")\n            with g_pathmgr.open(ckpt_path, \"rb\") as f:\n                checkpoint = torch.load(f, map_location=\"cpu\")\n\n            self.model.load_state_dict(checkpoint[\"model\"], strict=True)\n            self.optim.optimizer.load_state_dict(checkpoint[\"optimizer\"])", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            checkpoint_paths.append(\n                os.path.join(checkpoint_folder, f\"checkpoint_{int(epoch)}.pt\")\n            )\n\n        checkpoint = {\n            \"model\": self.model.module.state_dict(),\n            \"optimizer\": self.optim.optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"loss\": self.loss.state_dict(),\n            \"steps\": self.steps,\n        }\n        if self.optim_conf.amp.enabled:\n            checkpoint[\"scaler\"] = self.scaler.state_dict()\n\n        for checkpoint_path in checkpoint_paths:\n            with g_pathmgr.open(checkpoint_path, \"wb\") as f:\n                torch.save(checkpoint, f)\n\n    def load_checkpoint(self):\n        ckpt_path = get_resume_checkpoint(self.checkpoint_conf.save_dir)\n\n        if ckpt_path is None:\n            # Loading pre-trained model weights\n            model_weight_initializer = instantiate(\n                self.checkpoint_conf.model_weight_initializer\n            )\n            if model_weight_initializer is not None:\n                logging.info(\n                    f\"Loading pretrained checkpoint from {self.checkpoint_conf.model_weight_initializer}\"\n                )\n                self.model = model_weight_initializer(model=self.model)\n\n        else:\n            # Resuming from previous training checkpoint\n            logging.info(f\"Resuming training from {ckpt_path}\")\n            with g_pathmgr.open(ckpt_path, \"rb\") as f:\n                checkpoint = torch.load(f, map_location=\"cpu\")\n\n            self.model.load_state_dict(checkpoint[\"model\"], strict=True)\n            self.optim.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.loss.load_state_dict(checkpoint[\"loss\"], strict=True)\n            self.epoch = checkpoint[\"epoch\"]\n            self.steps = checkpoint[\"steps\"]\n\n            if self.optim_conf.amp.enabled and \"scaler\" in checkpoint:\n                self.scaler.load_state_dict(checkpoint[\"scaler\"])\n\n    def run(self):\n        assert self.mode in [\"train\", \"train_only\", \"val\"]\n        if self.mode == \"train\":", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        }\n        if self.optim_conf.amp.enabled:\n            checkpoint[\"scaler\"] = self.scaler.state_dict()\n\n        for checkpoint_path in checkpoint_paths:\n            with g_pathmgr.open(checkpoint_path, \"wb\") as f:\n                torch.save(checkpoint, f)\n\n    def load_checkpoint(self):\n        ckpt_path = get_resume_checkpoint(self.checkpoint_conf.save_dir)\n\n        if ckpt_path is None:\n            # Loading pre-trained model weights\n            model_weight_initializer = instantiate(\n                self.checkpoint_conf.model_weight_initializer\n            )\n            if model_weight_initializer is not None:\n                logging.info(\n                    f\"Loading pretrained checkpoint from {self.checkpoint_conf.model_weight_initializer}\"\n                )\n                self.model = model_weight_initializer(model=self.model)\n\n        else:\n            # Resuming from previous training checkpoint\n            logging.info(f\"Resuming training from {ckpt_path}\")\n            with g_pathmgr.open(ckpt_path, \"rb\") as f:\n                checkpoint = torch.load(f, map_location=\"cpu\")\n\n            self.model.load_state_dict(checkpoint[\"model\"], strict=True)\n            self.optim.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.loss.load_state_dict(checkpoint[\"loss\"], strict=True)\n            self.epoch = checkpoint[\"epoch\"]\n            self.steps = checkpoint[\"steps\"]\n\n            if self.optim_conf.amp.enabled and \"scaler\" in checkpoint:\n                self.scaler.load_state_dict(checkpoint[\"scaler\"])\n\n    def run(self):\n        assert self.mode in [\"train\", \"train_only\", \"val\"]\n        if self.mode == \"train\":\n            self.run_train()\n            self.run_val()\n        elif self.mode == \"val\":\n            self.run_val()\n        elif self.mode == \"train_only\":\n            self.run_train()\n\n    def _setup_dataloaders(self):\n\n        self.train_dataset = None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        if ckpt_path is None:\n            # Loading pre-trained model weights\n            model_weight_initializer = instantiate(\n                self.checkpoint_conf.model_weight_initializer\n            )\n            if model_weight_initializer is not None:\n                logging.info(\n                    f\"Loading pretrained checkpoint from {self.checkpoint_conf.model_weight_initializer}\"\n                )\n                self.model = model_weight_initializer(model=self.model)\n\n        else:\n            # Resuming from previous training checkpoint\n            logging.info(f\"Resuming training from {ckpt_path}\")\n            with g_pathmgr.open(ckpt_path, \"rb\") as f:\n                checkpoint = torch.load(f, map_location=\"cpu\")\n\n            self.model.load_state_dict(checkpoint[\"model\"], strict=True)\n            self.optim.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.loss.load_state_dict(checkpoint[\"loss\"], strict=True)\n            self.epoch = checkpoint[\"epoch\"]\n            self.steps = checkpoint[\"steps\"]\n\n            if self.optim_conf.amp.enabled and \"scaler\" in checkpoint:\n                self.scaler.load_state_dict(checkpoint[\"scaler\"])\n\n    def run(self):\n        assert self.mode in [\"train\", \"train_only\", \"val\"]\n        if self.mode == \"train\":\n            self.run_train()\n            self.run_val()\n        elif self.mode == \"val\":\n            self.run_val()\n        elif self.mode == \"train_only\":\n            self.run_train()\n\n    def _setup_dataloaders(self):\n\n        self.train_dataset = None\n        self.val_dataset = None\n\n        if self.mode in [\"train\", \"val\"]:\n            self.val_dataset = instantiate(self.data_conf.get(\"val\", None))\n            if self.val_dataset:\n                assert isinstance(\n                    self.val_dataset, (TorchDataset, ConcatDataset)\n                ), f\"Unsuported Val dataloader: {type(self.val_dataset).__name__}\"\n\n        if self.mode in [\"train\", \"train_only\"]:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_285-335"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                self.model = model_weight_initializer(model=self.model)\n\n        else:\n            # Resuming from previous training checkpoint\n            logging.info(f\"Resuming training from {ckpt_path}\")\n            with g_pathmgr.open(ckpt_path, \"rb\") as f:\n                checkpoint = torch.load(f, map_location=\"cpu\")\n\n            self.model.load_state_dict(checkpoint[\"model\"], strict=True)\n            self.optim.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            self.loss.load_state_dict(checkpoint[\"loss\"], strict=True)\n            self.epoch = checkpoint[\"epoch\"]\n            self.steps = checkpoint[\"steps\"]\n\n            if self.optim_conf.amp.enabled and \"scaler\" in checkpoint:\n                self.scaler.load_state_dict(checkpoint[\"scaler\"])\n\n    def run(self):\n        assert self.mode in [\"train\", \"train_only\", \"val\"]\n        if self.mode == \"train\":\n            self.run_train()\n            self.run_val()\n        elif self.mode == \"val\":\n            self.run_val()\n        elif self.mode == \"train_only\":\n            self.run_train()\n\n    def _setup_dataloaders(self):\n\n        self.train_dataset = None\n        self.val_dataset = None\n\n        if self.mode in [\"train\", \"val\"]:\n            self.val_dataset = instantiate(self.data_conf.get(\"val\", None))\n            if self.val_dataset:\n                assert isinstance(\n                    self.val_dataset, (TorchDataset, ConcatDataset)\n                ), f\"Unsuported Val dataloader: {type(self.val_dataset).__name__}\"\n\n        if self.mode in [\"train\", \"train_only\"]:\n            self.train_dataset = instantiate(self.data_conf.train)\n            assert isinstance(\n                self.train_dataset, (TorchDataset, ConcatDataset)\n            ), f\"Unsuported Train dataloader: {type(self.train_dataset).__name__}\"\n\n    def run_train(self):\n        # loop\n        while self.epoch < self.max_epochs:\n\n            outs = self.train_epoch(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_295-345"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.loss.load_state_dict(checkpoint[\"loss\"], strict=True)\n            self.epoch = checkpoint[\"epoch\"]\n            self.steps = checkpoint[\"steps\"]\n\n            if self.optim_conf.amp.enabled and \"scaler\" in checkpoint:\n                self.scaler.load_state_dict(checkpoint[\"scaler\"])\n\n    def run(self):\n        assert self.mode in [\"train\", \"train_only\", \"val\"]\n        if self.mode == \"train\":\n            self.run_train()\n            self.run_val()\n        elif self.mode == \"val\":\n            self.run_val()\n        elif self.mode == \"train_only\":\n            self.run_train()\n\n    def _setup_dataloaders(self):\n\n        self.train_dataset = None\n        self.val_dataset = None\n\n        if self.mode in [\"train\", \"val\"]:\n            self.val_dataset = instantiate(self.data_conf.get(\"val\", None))\n            if self.val_dataset:\n                assert isinstance(\n                    self.val_dataset, (TorchDataset, ConcatDataset)\n                ), f\"Unsuported Val dataloader: {type(self.val_dataset).__name__}\"\n\n        if self.mode in [\"train\", \"train_only\"]:\n            self.train_dataset = instantiate(self.data_conf.train)\n            assert isinstance(\n                self.train_dataset, (TorchDataset, ConcatDataset)\n            ), f\"Unsuported Train dataloader: {type(self.train_dataset).__name__}\"\n\n    def run_train(self):\n        # loop\n        while self.epoch < self.max_epochs:\n\n            outs = self.train_epoch(\n                self.train_dataset.get_loader(epoch=int(self.epoch))\n            )\n            self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n            # log train to text file.\n            if self.distributed_rank == 0:\n                with g_pathmgr.open(\n                    os.path.join(self.logging_conf.log_dir, \"train_stats.json\"),\n                    \"a\",\n                ) as f:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_305-355"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.run_train()\n            self.run_val()\n        elif self.mode == \"val\":\n            self.run_val()\n        elif self.mode == \"train_only\":\n            self.run_train()\n\n    def _setup_dataloaders(self):\n\n        self.train_dataset = None\n        self.val_dataset = None\n\n        if self.mode in [\"train\", \"val\"]:\n            self.val_dataset = instantiate(self.data_conf.get(\"val\", None))\n            if self.val_dataset:\n                assert isinstance(\n                    self.val_dataset, (TorchDataset, ConcatDataset)\n                ), f\"Unsuported Val dataloader: {type(self.val_dataset).__name__}\"\n\n        if self.mode in [\"train\", \"train_only\"]:\n            self.train_dataset = instantiate(self.data_conf.train)\n            assert isinstance(\n                self.train_dataset, (TorchDataset, ConcatDataset)\n            ), f\"Unsuported Train dataloader: {type(self.train_dataset).__name__}\"\n\n    def run_train(self):\n        # loop\n        while self.epoch < self.max_epochs:\n\n            outs = self.train_epoch(\n                self.train_dataset.get_loader(epoch=int(self.epoch))\n            )\n            self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n            # log train to text file.\n            if self.distributed_rank == 0:\n                with g_pathmgr.open(\n                    os.path.join(self.logging_conf.log_dir, \"train_stats.json\"),\n                    \"a\",\n                ) as f:\n                    f.write(json.dumps(outs) + \"\\n\")\n\n            # Run val\n            if self.epoch % self.val_epoch_freq == 0:\n                self.run_val()\n\n            self.epoch += 1\n            self.checkpoint_save(self.epoch)\n\n    def run_val(self):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_315-365"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.val_dataset = None\n\n        if self.mode in [\"train\", \"val\"]:\n            self.val_dataset = instantiate(self.data_conf.get(\"val\", None))\n            if self.val_dataset:\n                assert isinstance(\n                    self.val_dataset, (TorchDataset, ConcatDataset)\n                ), f\"Unsuported Val dataloader: {type(self.val_dataset).__name__}\"\n\n        if self.mode in [\"train\", \"train_only\"]:\n            self.train_dataset = instantiate(self.data_conf.train)\n            assert isinstance(\n                self.train_dataset, (TorchDataset, ConcatDataset)\n            ), f\"Unsuported Train dataloader: {type(self.train_dataset).__name__}\"\n\n    def run_train(self):\n        # loop\n        while self.epoch < self.max_epochs:\n\n            outs = self.train_epoch(\n                self.train_dataset.get_loader(epoch=int(self.epoch))\n            )\n            self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n            # log train to text file.\n            if self.distributed_rank == 0:\n                with g_pathmgr.open(\n                    os.path.join(self.logging_conf.log_dir, \"train_stats.json\"),\n                    \"a\",\n                ) as f:\n                    f.write(json.dumps(outs) + \"\\n\")\n\n            # Run val\n            if self.epoch % self.val_epoch_freq == 0:\n                self.run_val()\n\n            self.epoch += 1\n            self.checkpoint_save(self.epoch)\n\n    def run_val(self):\n\n        if not self.val_dataset:\n            return\n\n        outs = self.val_epoch(self.val_dataset.get_loader(epoch=int(self.epoch)))\n        self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n        if self.distributed_rank == 0:\n            with g_pathmgr.open(\n                os.path.join(self.logging_conf.log_dir, \"val_stats.json\"),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_325-375"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.train_dataset = instantiate(self.data_conf.train)\n            assert isinstance(\n                self.train_dataset, (TorchDataset, ConcatDataset)\n            ), f\"Unsuported Train dataloader: {type(self.train_dataset).__name__}\"\n\n    def run_train(self):\n        # loop\n        while self.epoch < self.max_epochs:\n\n            outs = self.train_epoch(\n                self.train_dataset.get_loader(epoch=int(self.epoch))\n            )\n            self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n            # log train to text file.\n            if self.distributed_rank == 0:\n                with g_pathmgr.open(\n                    os.path.join(self.logging_conf.log_dir, \"train_stats.json\"),\n                    \"a\",\n                ) as f:\n                    f.write(json.dumps(outs) + \"\\n\")\n\n            # Run val\n            if self.epoch % self.val_epoch_freq == 0:\n                self.run_val()\n\n            self.epoch += 1\n            self.checkpoint_save(self.epoch)\n\n    def run_val(self):\n\n        if not self.val_dataset:\n            return\n\n        outs = self.val_epoch(self.val_dataset.get_loader(epoch=int(self.epoch)))\n        self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n        if self.distributed_rank == 0:\n            with g_pathmgr.open(\n                os.path.join(self.logging_conf.log_dir, \"val_stats.json\"),\n                \"a\",\n            ) as f:\n                f.write(json.dumps(outs) + \"\\n\")\n\n    def val_epoch(self, val_loader):\n\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"val\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_335-385"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                self.train_dataset.get_loader(epoch=int(self.epoch))\n            )\n            self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n            # log train to text file.\n            if self.distributed_rank == 0:\n                with g_pathmgr.open(\n                    os.path.join(self.logging_conf.log_dir, \"train_stats.json\"),\n                    \"a\",\n                ) as f:\n                    f.write(json.dumps(outs) + \"\\n\")\n\n            # Run val\n            if self.epoch % self.val_epoch_freq == 0:\n                self.run_val()\n\n            self.epoch += 1\n            self.checkpoint_save(self.epoch)\n\n    def run_val(self):\n\n        if not self.val_dataset:\n            return\n\n        outs = self.val_epoch(self.val_dataset.get_loader(epoch=int(self.epoch)))\n        self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n        if self.distributed_rank == 0:\n            with g_pathmgr.open(\n                os.path.join(self.logging_conf.log_dir, \"val_stats.json\"),\n                \"a\",\n            ) as f:\n                f.write(json.dumps(outs) + \"\\n\")\n\n    def val_epoch(self, val_loader):\n\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"val\"\n\n        iters_per_epoch = len(val_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_345-395"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    f.write(json.dumps(outs) + \"\\n\")\n\n            # Run val\n            if self.epoch % self.val_epoch_freq == 0:\n                self.run_val()\n\n            self.epoch += 1\n            self.checkpoint_save(self.epoch)\n\n    def run_val(self):\n\n        if not self.val_dataset:\n            return\n\n        outs = self.val_epoch(self.val_dataset.get_loader(epoch=int(self.epoch)))\n        self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n        if self.distributed_rank == 0:\n            with g_pathmgr.open(\n                os.path.join(self.logging_conf.log_dir, \"val_stats.json\"),\n                \"a\",\n            ) as f:\n                f.write(json.dumps(outs) + \"\\n\")\n\n    def val_epoch(self, val_loader):\n\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"val\"\n\n        iters_per_epoch = len(val_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values()],\n            prefix=\"Val Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.eval()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_355-405"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        if not self.val_dataset:\n            return\n\n        outs = self.val_epoch(self.val_dataset.get_loader(epoch=int(self.epoch)))\n        self.logger.log_dict(outs, self.epoch)  # Logged only on rank 0\n\n        if self.distributed_rank == 0:\n            with g_pathmgr.open(\n                os.path.join(self.logging_conf.log_dir, \"val_stats.json\"),\n                \"a\",\n            ) as f:\n                f.write(json.dumps(outs) + \"\\n\")\n\n    def val_epoch(self, val_loader):\n\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"val\"\n\n        iters_per_epoch = len(val_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values()],\n            prefix=\"Val Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.eval()\n        if hasattr(self.model.module, \"on_validation_epoch_start\"):\n            self.model.module.on_validation_epoch_start()\n\n        end = time.time()\n\n        limit_val_batches = (\n            iters_per_epoch\n            if self.limit_val_batches is None\n            else self.limit_val_batches\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_365-415"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                \"a\",\n            ) as f:\n                f.write(json.dumps(outs) + \"\\n\")\n\n    def val_epoch(self, val_loader):\n\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"val\"\n\n        iters_per_epoch = len(val_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values()],\n            prefix=\"Val Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.eval()\n        if hasattr(self.model.module, \"on_validation_epoch_start\"):\n            self.model.module.on_validation_epoch_start()\n\n        end = time.time()\n\n        limit_val_batches = (\n            iters_per_epoch\n            if self.limit_val_batches is None\n            else self.limit_val_batches\n        )\n\n        for data_iter, batch in enumerate(val_loader):\n\n            if data_iter > limit_val_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_375-425"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        iters_per_epoch = len(val_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values()],\n            prefix=\"Val Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.eval()\n        if hasattr(self.model.module, \"on_validation_epoch_start\"):\n            self.model.module.on_validation_epoch_start()\n\n        end = time.time()\n\n        limit_val_batches = (\n            iters_per_epoch\n            if self.limit_val_batches is None\n            else self.limit_val_batches\n        )\n\n        for data_iter, batch in enumerate(val_loader):\n\n            if data_iter > limit_val_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            # compute output\n            with torch.no_grad():\n                _, metrics_dict, batch_size = self._step(\n                    batch,\n                    key,\n                    phase_type=phase_type,\n                )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_385-435"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values()],\n            prefix=\"Val Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.eval()\n        if hasattr(self.model.module, \"on_validation_epoch_start\"):\n            self.model.module.on_validation_epoch_start()\n\n        end = time.time()\n\n        limit_val_batches = (\n            iters_per_epoch\n            if self.limit_val_batches is None\n            else self.limit_val_batches\n        )\n\n        for data_iter, batch in enumerate(val_loader):\n\n            if data_iter > limit_val_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            # compute output\n            with torch.no_grad():\n                _, metrics_dict, batch_size = self._step(\n                    batch,\n                    key,\n                    phase_type=phase_type,\n                )\n\n            for k in metrics_dict:\n                metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if torch.cuda.is_available():\n                mem.update(torch.cuda.max_memory_allocated() // 1e9)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_395-445"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if hasattr(self.model.module, \"on_validation_epoch_start\"):\n            self.model.module.on_validation_epoch_start()\n\n        end = time.time()\n\n        limit_val_batches = (\n            iters_per_epoch\n            if self.limit_val_batches is None\n            else self.limit_val_batches\n        )\n\n        for data_iter, batch in enumerate(val_loader):\n\n            if data_iter > limit_val_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            # compute output\n            with torch.no_grad():\n                _, metrics_dict, batch_size = self._step(\n                    batch,\n                    key,\n                    phase_type=phase_type,\n                )\n\n            for k in metrics_dict:\n                metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if torch.cuda.is_available():\n                mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"val\")\n\n        if hasattr(self.model.module, \"on_validation_epoch_end\"):\n            self.model.module.on_validation_epoch_end()\n        return {k: v.avg for k, v in metrics_mts.items()}\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_405-455"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        for data_iter, batch in enumerate(val_loader):\n\n            if data_iter > limit_val_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            # compute output\n            with torch.no_grad():\n                _, metrics_dict, batch_size = self._step(\n                    batch,\n                    key,\n                    phase_type=phase_type,\n                )\n\n            for k in metrics_dict:\n                metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if torch.cuda.is_available():\n                mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"val\")\n\n        if hasattr(self.model.module, \"on_validation_epoch_end\"):\n            self.model.module.on_validation_epoch_end()\n        return {k: v.avg for k, v in metrics_mts.items()}\n\n    def train_epoch(self, train_loader):\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"train\"\n\n        iters_per_epoch = len(train_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_415-465"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            batch = copy_data_to_device(batch, self.device)\n\n            # compute output\n            with torch.no_grad():\n                _, metrics_dict, batch_size = self._step(\n                    batch,\n                    key,\n                    phase_type=phase_type,\n                )\n\n            for k in metrics_dict:\n                metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if torch.cuda.is_available():\n                mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"val\")\n\n        if hasattr(self.model.module, \"on_validation_epoch_end\"):\n            self.model.module.on_validation_epoch_end()\n        return {k: v.avg for k, v in metrics_mts.items()}\n\n    def train_epoch(self, train_loader):\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"train\"\n\n        iters_per_epoch = len(train_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        loss_names = []\n        for key in self.loss.keys():", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_425-475"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            for k in metrics_dict:\n                metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if torch.cuda.is_available():\n                mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"val\")\n\n        if hasattr(self.model.module, \"on_validation_epoch_end\"):\n            self.model.module.on_validation_epoch_end()\n        return {k: v.avg for k, v in metrics_mts.items()}\n\n    def train_epoch(self, train_loader):\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"train\"\n\n        iters_per_epoch = len(train_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        loss_names = []\n        for key in self.loss.keys():\n            loss_names.append(f\"Losses/{phase_type}_{key}_loss\")\n\n        loss_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in loss_names]\n        )\n\n        # TODO: Track optimizer params (LR, WD,) etc.\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values(), *loss_mts.values()],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_435-485"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"val\")\n\n        if hasattr(self.model.module, \"on_validation_epoch_end\"):\n            self.model.module.on_validation_epoch_end()\n        return {k: v.avg for k, v in metrics_mts.items()}\n\n    def train_epoch(self, train_loader):\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"train\"\n\n        iters_per_epoch = len(train_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        loss_names = []\n        for key in self.loss.keys():\n            loss_names.append(f\"Losses/{phase_type}_{key}_loss\")\n\n        loss_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in loss_names]\n        )\n\n        # TODO: Track optimizer params (LR, WD,) etc.\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values(), *loss_mts.values()],\n            prefix=\"Train Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.train()\n\n        end = time.time()\n\n        limit_train_batches = (\n            iters_per_epoch\n            if self.limit_train_batches is None", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_445-495"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def train_epoch(self, train_loader):\n        batch_time = AverageMeter(\"Time\", self.device, \":6.2f\")\n        data_time = AverageMeter(\"Data\", self.device, \":6.2f\")\n        mem = AverageMeter(\"Mem (GB)\", self.device, \":6.1f\")\n        phase_type = \"train\"\n\n        iters_per_epoch = len(train_loader)\n\n        metric_names = []\n        if self.metrics_conf and phase_type in self.metrics_conf:\n            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        loss_names = []\n        for key in self.loss.keys():\n            loss_names.append(f\"Losses/{phase_type}_{key}_loss\")\n\n        loss_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in loss_names]\n        )\n\n        # TODO: Track optimizer params (LR, WD,) etc.\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values(), *loss_mts.values()],\n            prefix=\"Train Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.train()\n\n        end = time.time()\n\n        limit_train_batches = (\n            iters_per_epoch\n            if self.limit_train_batches is None\n            else self.limit_train_batches\n        )\n\n        for data_iter, batch in enumerate(train_loader):\n\n            if data_iter > limit_train_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_455-505"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            for key in self.metrics_conf[phase_type].keys():\n                for name in self.metrics_conf[phase_type][key]:\n                    metric_names.append(f\"Metrics/{phase_type}_{key}/{name}\")\n\n        metrics_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in metric_names]\n        )\n\n        loss_names = []\n        for key in self.loss.keys():\n            loss_names.append(f\"Losses/{phase_type}_{key}_loss\")\n\n        loss_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in loss_names]\n        )\n\n        # TODO: Track optimizer params (LR, WD,) etc.\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values(), *loss_mts.values()],\n            prefix=\"Train Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.train()\n\n        end = time.time()\n\n        limit_train_batches = (\n            iters_per_epoch\n            if self.limit_train_batches is None\n            else self.limit_train_batches\n        )\n\n        for data_iter, batch in enumerate(train_loader):\n\n            if data_iter > limit_train_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            accum_steps = batch.accum_steps\n            chunked_batches = chunk_batch_for_accum_steps(batch, accum_steps)\n\n            self.optim.zero_grad()\n\n            for i, chunked_batch in enumerate(chunked_batches):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_465-515"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            loss_names.append(f\"Losses/{phase_type}_{key}_loss\")\n\n        loss_mts = OrderedDict(\n            [(name, AverageMeter(name, self.device, \":.2e\")) for name in loss_names]\n        )\n\n        # TODO: Track optimizer params (LR, WD,) etc.\n        progress = ProgressMeter(\n            iters_per_epoch,\n            [batch_time, data_time, mem, *metrics_mts.values(), *loss_mts.values()],\n            prefix=\"Train Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.train()\n\n        end = time.time()\n\n        limit_train_batches = (\n            iters_per_epoch\n            if self.limit_train_batches is None\n            else self.limit_train_batches\n        )\n\n        for data_iter, batch in enumerate(train_loader):\n\n            if data_iter > limit_train_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            accum_steps = batch.accum_steps\n            chunked_batches = chunk_batch_for_accum_steps(batch, accum_steps)\n\n            self.optim.zero_grad()\n\n            for i, chunked_batch in enumerate(chunked_batches):\n                ddp_context = (\n                    self.model.no_sync()\n                    if i < accum_steps - 1\n                    else contextlib.nullcontext()\n                )\n\n                with ddp_context:\n                    with torch.cuda.amp.autocast(\n                        enabled=self.optim_conf.amp.enabled,\n                        dtype=get_amp_type(self.optim_conf.amp.amp_dtype),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_475-525"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            prefix=\"Train Epoch: [{}]\".format(self.epoch),\n        )\n\n        self.model.train()\n\n        end = time.time()\n\n        limit_train_batches = (\n            iters_per_epoch\n            if self.limit_train_batches is None\n            else self.limit_train_batches\n        )\n\n        for data_iter, batch in enumerate(train_loader):\n\n            if data_iter > limit_train_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            accum_steps = batch.accum_steps\n            chunked_batches = chunk_batch_for_accum_steps(batch, accum_steps)\n\n            self.optim.zero_grad()\n\n            for i, chunked_batch in enumerate(chunked_batches):\n                ddp_context = (\n                    self.model.no_sync()\n                    if i < accum_steps - 1\n                    else contextlib.nullcontext()\n                )\n\n                with ddp_context:\n                    with torch.cuda.amp.autocast(\n                        enabled=self.optim_conf.amp.enabled,\n                        dtype=get_amp_type(self.optim_conf.amp.amp_dtype),\n                    ):\n                        loss_dict, metrics_dict, batch_size = self._step(\n                            chunked_batch,\n                            key,\n                            phase_type=phase_type,\n                        )\n\n                    assert len(loss_dict) == 1\n                    loss_key, loss = loss_dict.popitem()\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_485-535"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            else self.limit_train_batches\n        )\n\n        for data_iter, batch in enumerate(train_loader):\n\n            if data_iter > limit_train_batches:\n                break\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            accum_steps = batch.accum_steps\n            chunked_batches = chunk_batch_for_accum_steps(batch, accum_steps)\n\n            self.optim.zero_grad()\n\n            for i, chunked_batch in enumerate(chunked_batches):\n                ddp_context = (\n                    self.model.no_sync()\n                    if i < accum_steps - 1\n                    else contextlib.nullcontext()\n                )\n\n                with ddp_context:\n                    with torch.cuda.amp.autocast(\n                        enabled=self.optim_conf.amp.enabled,\n                        dtype=get_amp_type(self.optim_conf.amp.amp_dtype),\n                    ):\n                        loss_dict, metrics_dict, batch_size = self._step(\n                            chunked_batch,\n                            key,\n                            phase_type=phase_type,\n                        )\n\n                    assert len(loss_dict) == 1\n                    loss_key, loss = loss_dict.popitem()\n\n                    if not math.isfinite(loss.item()):\n                        print(\"Loss is {}, stopping training\".format(loss.item()))\n                        sys.exit(1)\n\n                    loss /= accum_steps\n                    self.scaler.scale(loss).backward()\n\n                    for k in metrics_dict:\n                        metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_495-545"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n            key, batch = self._process_batch(batch, phase_type)\n            batch = copy_data_to_device(batch, self.device)\n\n            accum_steps = batch.accum_steps\n            chunked_batches = chunk_batch_for_accum_steps(batch, accum_steps)\n\n            self.optim.zero_grad()\n\n            for i, chunked_batch in enumerate(chunked_batches):\n                ddp_context = (\n                    self.model.no_sync()\n                    if i < accum_steps - 1\n                    else contextlib.nullcontext()\n                )\n\n                with ddp_context:\n                    with torch.cuda.amp.autocast(\n                        enabled=self.optim_conf.amp.enabled,\n                        dtype=get_amp_type(self.optim_conf.amp.amp_dtype),\n                    ):\n                        loss_dict, metrics_dict, batch_size = self._step(\n                            chunked_batch,\n                            key,\n                            phase_type=phase_type,\n                        )\n\n                    assert len(loss_dict) == 1\n                    loss_key, loss = loss_dict.popitem()\n\n                    if not math.isfinite(loss.item()):\n                        print(\"Loss is {}, stopping training\".format(loss.item()))\n                        sys.exit(1)\n\n                    loss /= accum_steps\n                    self.scaler.scale(loss).backward()\n\n                    for k in metrics_dict:\n                        metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n                    loss_mts[loss_key].update(loss.item(), batch_size)\n\n            # compute gradient and do SGD step\n            exact_epoch = self.epoch + float(data_iter) / iters_per_epoch\n\n            self.optim.step_schedulers(float(exact_epoch) / self.max_epochs)\n\n            if self.clip_gradient_partial is not None:\n                self.scaler.unscale_(self.optim.optimizer)\n                self.clip_gradient_partial(parameters=self.model.parameters())", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_505-555"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                ddp_context = (\n                    self.model.no_sync()\n                    if i < accum_steps - 1\n                    else contextlib.nullcontext()\n                )\n\n                with ddp_context:\n                    with torch.cuda.amp.autocast(\n                        enabled=self.optim_conf.amp.enabled,\n                        dtype=get_amp_type(self.optim_conf.amp.amp_dtype),\n                    ):\n                        loss_dict, metrics_dict, batch_size = self._step(\n                            chunked_batch,\n                            key,\n                            phase_type=phase_type,\n                        )\n\n                    assert len(loss_dict) == 1\n                    loss_key, loss = loss_dict.popitem()\n\n                    if not math.isfinite(loss.item()):\n                        print(\"Loss is {}, stopping training\".format(loss.item()))\n                        sys.exit(1)\n\n                    loss /= accum_steps\n                    self.scaler.scale(loss).backward()\n\n                    for k in metrics_dict:\n                        metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n                    loss_mts[loss_key].update(loss.item(), batch_size)\n\n            # compute gradient and do SGD step\n            exact_epoch = self.epoch + float(data_iter) / iters_per_epoch\n\n            self.optim.step_schedulers(float(exact_epoch) / self.max_epochs)\n\n            if self.clip_gradient_partial is not None:\n                self.scaler.unscale_(self.optim.optimizer)\n                self.clip_gradient_partial(parameters=self.model.parameters())\n\n            self.scaler.step(self.optim.optimizer)\n            self.scaler.update()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            mem.update(torch.cuda.max_memory_allocated() // 1e9)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_515-565"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    ):\n                        loss_dict, metrics_dict, batch_size = self._step(\n                            chunked_batch,\n                            key,\n                            phase_type=phase_type,\n                        )\n\n                    assert len(loss_dict) == 1\n                    loss_key, loss = loss_dict.popitem()\n\n                    if not math.isfinite(loss.item()):\n                        print(\"Loss is {}, stopping training\".format(loss.item()))\n                        sys.exit(1)\n\n                    loss /= accum_steps\n                    self.scaler.scale(loss).backward()\n\n                    for k in metrics_dict:\n                        metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n                    loss_mts[loss_key].update(loss.item(), batch_size)\n\n            # compute gradient and do SGD step\n            exact_epoch = self.epoch + float(data_iter) / iters_per_epoch\n\n            self.optim.step_schedulers(float(exact_epoch) / self.max_epochs)\n\n            if self.clip_gradient_partial is not None:\n                self.scaler.unscale_(self.optim.optimizer)\n                self.clip_gradient_partial(parameters=self.model.parameters())\n\n            self.scaler.step(self.optim.optimizer)\n            self.scaler.update()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"train\")\n        out_dict = {k: v.avg for k, v in metrics_mts.items()}\n        for k, v in loss_mts.items():\n            out_dict[k] = v.avg\n        return out_dict\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_525-575"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    if not math.isfinite(loss.item()):\n                        print(\"Loss is {}, stopping training\".format(loss.item()))\n                        sys.exit(1)\n\n                    loss /= accum_steps\n                    self.scaler.scale(loss).backward()\n\n                    for k in metrics_dict:\n                        metrics_mts[k].update(metrics_dict[k].item(), batch_size)\n\n                    loss_mts[loss_key].update(loss.item(), batch_size)\n\n            # compute gradient and do SGD step\n            exact_epoch = self.epoch + float(data_iter) / iters_per_epoch\n\n            self.optim.step_schedulers(float(exact_epoch) / self.max_epochs)\n\n            if self.clip_gradient_partial is not None:\n                self.scaler.unscale_(self.optim.optimizer)\n                self.clip_gradient_partial(parameters=self.model.parameters())\n\n            self.scaler.step(self.optim.optimizer)\n            self.scaler.update()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"train\")\n        out_dict = {k: v.avg for k, v in metrics_mts.items()}\n        for k, v in loss_mts.items():\n            out_dict[k] = v.avg\n        return out_dict\n\n    def _compute_metrics(\n        self, pred: torch.Tensor, label: torch.Tensor, phase_type: str, key: str\n    ) -> Dict[str, torch.Tensor]:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return {}\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)][key]\n        metrics_result = {}\n        for name, metric in metrics_dict.items():\n            metrics_result[f\"Metrics/{phase_type}_{key}/{name}\"] = metric(pred, label)\n        return metrics_result", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_535-585"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    loss_mts[loss_key].update(loss.item(), batch_size)\n\n            # compute gradient and do SGD step\n            exact_epoch = self.epoch + float(data_iter) / iters_per_epoch\n\n            self.optim.step_schedulers(float(exact_epoch) / self.max_epochs)\n\n            if self.clip_gradient_partial is not None:\n                self.scaler.unscale_(self.optim.optimizer)\n                self.clip_gradient_partial(parameters=self.model.parameters())\n\n            self.scaler.step(self.optim.optimizer)\n            self.scaler.update()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"train\")\n        out_dict = {k: v.avg for k, v in metrics_mts.items()}\n        for k, v in loss_mts.items():\n            out_dict[k] = v.avg\n        return out_dict\n\n    def _compute_metrics(\n        self, pred: torch.Tensor, label: torch.Tensor, phase_type: str, key: str\n    ) -> Dict[str, torch.Tensor]:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return {}\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)][key]\n        metrics_result = {}\n        for name, metric in metrics_dict.items():\n            metrics_result[f\"Metrics/{phase_type}_{key}/{name}\"] = metric(pred, label)\n        return metrics_result\n\n    def _reset_metrics(self, phase_type: str) -> None:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)]\n        for k_metric in metrics_dict.values():\n            for metric in k_metric.values():\n                metric.reset()\n\n    def _get_metric_key(self, phase):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_545-595"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n            self.scaler.step(self.optim.optimizer)\n            self.scaler.update()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            mem.update(torch.cuda.max_memory_allocated() // 1e9)\n\n            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"train\")\n        out_dict = {k: v.avg for k, v in metrics_mts.items()}\n        for k, v in loss_mts.items():\n            out_dict[k] = v.avg\n        return out_dict\n\n    def _compute_metrics(\n        self, pred: torch.Tensor, label: torch.Tensor, phase_type: str, key: str\n    ) -> Dict[str, torch.Tensor]:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return {}\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)][key]\n        metrics_result = {}\n        for name, metric in metrics_dict.items():\n            metrics_result[f\"Metrics/{phase_type}_{key}/{name}\"] = metric(pred, label)\n        return metrics_result\n\n    def _reset_metrics(self, phase_type: str) -> None:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)]\n        for k_metric in metrics_dict.values():\n            for metric in k_metric.values():\n                metric.reset()\n\n    def _get_metric_key(self, phase):\n        return f\"{phase}_\"\n\n    def _setup_components(self):\n        logging.info(\"Setting up components: Model, loss, optim, metrics etc.\")\n        self.epoch = 0\n        self.steps = {\"train\": 0, \"val\": 0}\n\n        self.logger = instantiate(self.logging_conf.tensorboard_writer)\n\n        self.model = instantiate(self.model_conf, _convert_=\"all\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_555-605"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            if data_iter % self.logging_conf.log_freq == 0:\n                progress.display(data_iter)\n\n        progress.synchronize()\n        self._reset_metrics(\"train\")\n        out_dict = {k: v.avg for k, v in metrics_mts.items()}\n        for k, v in loss_mts.items():\n            out_dict[k] = v.avg\n        return out_dict\n\n    def _compute_metrics(\n        self, pred: torch.Tensor, label: torch.Tensor, phase_type: str, key: str\n    ) -> Dict[str, torch.Tensor]:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return {}\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)][key]\n        metrics_result = {}\n        for name, metric in metrics_dict.items():\n            metrics_result[f\"Metrics/{phase_type}_{key}/{name}\"] = metric(pred, label)\n        return metrics_result\n\n    def _reset_metrics(self, phase_type: str) -> None:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)]\n        for k_metric in metrics_dict.values():\n            for metric in k_metric.values():\n                metric.reset()\n\n    def _get_metric_key(self, phase):\n        return f\"{phase}_\"\n\n    def _setup_components(self):\n        logging.info(\"Setting up components: Model, loss, optim, metrics etc.\")\n        self.epoch = 0\n        self.steps = {\"train\": 0, \"val\": 0}\n\n        self.logger = instantiate(self.logging_conf.tensorboard_writer)\n\n        self.model = instantiate(self.model_conf, _convert_=\"all\")\n\n        self.loss = {\n            key: wrap_base_loss(el)\n            for (key, el) in instantiate(self.loss_conf, _convert_=\"all\").items()\n        }\n        self.loss = nn.ModuleDict(self.loss)\n\n        self.metrics = nn.ModuleDict()\n        if self.metrics_conf:\n            metrics = instantiate(self.metrics_conf, _convert_=\"all\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_565-615"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def _compute_metrics(\n        self, pred: torch.Tensor, label: torch.Tensor, phase_type: str, key: str\n    ) -> Dict[str, torch.Tensor]:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return {}\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)][key]\n        metrics_result = {}\n        for name, metric in metrics_dict.items():\n            metrics_result[f\"Metrics/{phase_type}_{key}/{name}\"] = metric(pred, label)\n        return metrics_result\n\n    def _reset_metrics(self, phase_type: str) -> None:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)]\n        for k_metric in metrics_dict.values():\n            for metric in k_metric.values():\n                metric.reset()\n\n    def _get_metric_key(self, phase):\n        return f\"{phase}_\"\n\n    def _setup_components(self):\n        logging.info(\"Setting up components: Model, loss, optim, metrics etc.\")\n        self.epoch = 0\n        self.steps = {\"train\": 0, \"val\": 0}\n\n        self.logger = instantiate(self.logging_conf.tensorboard_writer)\n\n        self.model = instantiate(self.model_conf, _convert_=\"all\")\n\n        self.loss = {\n            key: wrap_base_loss(el)\n            for (key, el) in instantiate(self.loss_conf, _convert_=\"all\").items()\n        }\n        self.loss = nn.ModuleDict(self.loss)\n\n        self.metrics = nn.ModuleDict()\n        if self.metrics_conf:\n            metrics = instantiate(self.metrics_conf, _convert_=\"all\")\n            for phase, phase_metrics in metrics.items():\n                self.metrics[self._get_metric_key(phase)] = nn.ModuleDict()\n                for key, metrics in phase_metrics.items():\n                    self.metrics[self._get_metric_key(phase)][key] = nn.ModuleDict(\n                        metrics\n                    )\n\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.optim_conf.amp.enabled)\n\n        # FIXME: grad clip shouldn't be an object", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_575-625"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def _reset_metrics(self, phase_type: str) -> None:\n        if self._get_metric_key(phase_type) not in self.metrics:\n            return\n        metrics_dict = self.metrics[self._get_metric_key(phase_type)]\n        for k_metric in metrics_dict.values():\n            for metric in k_metric.values():\n                metric.reset()\n\n    def _get_metric_key(self, phase):\n        return f\"{phase}_\"\n\n    def _setup_components(self):\n        logging.info(\"Setting up components: Model, loss, optim, metrics etc.\")\n        self.epoch = 0\n        self.steps = {\"train\": 0, \"val\": 0}\n\n        self.logger = instantiate(self.logging_conf.tensorboard_writer)\n\n        self.model = instantiate(self.model_conf, _convert_=\"all\")\n\n        self.loss = {\n            key: wrap_base_loss(el)\n            for (key, el) in instantiate(self.loss_conf, _convert_=\"all\").items()\n        }\n        self.loss = nn.ModuleDict(self.loss)\n\n        self.metrics = nn.ModuleDict()\n        if self.metrics_conf:\n            metrics = instantiate(self.metrics_conf, _convert_=\"all\")\n            for phase, phase_metrics in metrics.items():\n                self.metrics[self._get_metric_key(phase)] = nn.ModuleDict()\n                for key, metrics in phase_metrics.items():\n                    self.metrics[self._get_metric_key(phase)][key] = nn.ModuleDict(\n                        metrics\n                    )\n\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.optim_conf.amp.enabled)\n\n        # FIXME: grad clip shouldn't be an object\n        self.clip_gradient_partial = instantiate(self.optim_conf.gradient_clip)\n\n        logging.info(\"Finished setting up components: Model, loss, optim, metrics etc.\")\n\n        self.checkpoint_conf\n        self.optim_conf\n        self.data_conf\n\n    def _construct_optimizer(self):\n        self.optim = (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_585-635"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        return f\"{phase}_\"\n\n    def _setup_components(self):\n        logging.info(\"Setting up components: Model, loss, optim, metrics etc.\")\n        self.epoch = 0\n        self.steps = {\"train\": 0, \"val\": 0}\n\n        self.logger = instantiate(self.logging_conf.tensorboard_writer)\n\n        self.model = instantiate(self.model_conf, _convert_=\"all\")\n\n        self.loss = {\n            key: wrap_base_loss(el)\n            for (key, el) in instantiate(self.loss_conf, _convert_=\"all\").items()\n        }\n        self.loss = nn.ModuleDict(self.loss)\n\n        self.metrics = nn.ModuleDict()\n        if self.metrics_conf:\n            metrics = instantiate(self.metrics_conf, _convert_=\"all\")\n            for phase, phase_metrics in metrics.items():\n                self.metrics[self._get_metric_key(phase)] = nn.ModuleDict()\n                for key, metrics in phase_metrics.items():\n                    self.metrics[self._get_metric_key(phase)][key] = nn.ModuleDict(\n                        metrics\n                    )\n\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.optim_conf.amp.enabled)\n\n        # FIXME: grad clip shouldn't be an object\n        self.clip_gradient_partial = instantiate(self.optim_conf.gradient_clip)\n\n        logging.info(\"Finished setting up components: Model, loss, optim, metrics etc.\")\n\n        self.checkpoint_conf\n        self.optim_conf\n        self.data_conf\n\n    def _construct_optimizer(self):\n        self.optim = (\n            None\n            if self.optim_conf is None\n            else construct_optimizer(\n                self.model,\n                self.optim_conf.optimizer,\n                self.optim_conf.options,\n                self.optim_conf.param_group_modifiers,\n            )\n        )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_595-645"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        self.loss = {\n            key: wrap_base_loss(el)\n            for (key, el) in instantiate(self.loss_conf, _convert_=\"all\").items()\n        }\n        self.loss = nn.ModuleDict(self.loss)\n\n        self.metrics = nn.ModuleDict()\n        if self.metrics_conf:\n            metrics = instantiate(self.metrics_conf, _convert_=\"all\")\n            for phase, phase_metrics in metrics.items():\n                self.metrics[self._get_metric_key(phase)] = nn.ModuleDict()\n                for key, metrics in phase_metrics.items():\n                    self.metrics[self._get_metric_key(phase)][key] = nn.ModuleDict(\n                        metrics\n                    )\n\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.optim_conf.amp.enabled)\n\n        # FIXME: grad clip shouldn't be an object\n        self.clip_gradient_partial = instantiate(self.optim_conf.gradient_clip)\n\n        logging.info(\"Finished setting up components: Model, loss, optim, metrics etc.\")\n\n        self.checkpoint_conf\n        self.optim_conf\n        self.data_conf\n\n    def _construct_optimizer(self):\n        self.optim = (\n            None\n            if self.optim_conf is None\n            else construct_optimizer(\n                self.model,\n                self.optim_conf.optimizer,\n                self.optim_conf.options,\n                self.optim_conf.param_group_modifiers,\n            )\n        )\n\n    def _process_batch(self, batch, phase_type):\n        assert isinstance(batch, Mapping)\n        assert all(isinstance(v, Sample) for v in batch.values())\n        assert len(batch) == 1\n        return batch.popitem()\n\n    def _step(self, batch: Any, key: str, phase_type: str):\n\n        y_hat = self.model({key: batch}, **batch.model_fwd_kwargs)\n        assert isinstance(y_hat, Mapping)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_605-655"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            for phase, phase_metrics in metrics.items():\n                self.metrics[self._get_metric_key(phase)] = nn.ModuleDict()\n                for key, metrics in phase_metrics.items():\n                    self.metrics[self._get_metric_key(phase)][key] = nn.ModuleDict(\n                        metrics\n                    )\n\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.optim_conf.amp.enabled)\n\n        # FIXME: grad clip shouldn't be an object\n        self.clip_gradient_partial = instantiate(self.optim_conf.gradient_clip)\n\n        logging.info(\"Finished setting up components: Model, loss, optim, metrics etc.\")\n\n        self.checkpoint_conf\n        self.optim_conf\n        self.data_conf\n\n    def _construct_optimizer(self):\n        self.optim = (\n            None\n            if self.optim_conf is None\n            else construct_optimizer(\n                self.model,\n                self.optim_conf.optimizer,\n                self.optim_conf.options,\n                self.optim_conf.param_group_modifiers,\n            )\n        )\n\n    def _process_batch(self, batch, phase_type):\n        assert isinstance(batch, Mapping)\n        assert all(isinstance(v, Sample) for v in batch.values())\n        assert len(batch) == 1\n        return batch.popitem()\n\n    def _step(self, batch: Any, key: str, phase_type: str):\n\n        y_hat = self.model({key: batch}, **batch.model_fwd_kwargs)\n        assert isinstance(y_hat, Mapping)\n        assert len(y_hat) == 1\n        key, y_hat = y_hat.popitem()\n        loss = None\n        batch_size = batch.label.shape[0]\n        loss_str = f\"Losses/{phase_type}_{key}_loss\"\n        if phase_type == \"train\":\n            loss, y_hat = self.loss[key](y_hat, batch)\n            self.logger.log(\n                os.path.join(\"Step\", loss_str),\n                loss,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_615-665"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.clip_gradient_partial = instantiate(self.optim_conf.gradient_clip)\n\n        logging.info(\"Finished setting up components: Model, loss, optim, metrics etc.\")\n\n        self.checkpoint_conf\n        self.optim_conf\n        self.data_conf\n\n    def _construct_optimizer(self):\n        self.optim = (\n            None\n            if self.optim_conf is None\n            else construct_optimizer(\n                self.model,\n                self.optim_conf.optimizer,\n                self.optim_conf.options,\n                self.optim_conf.param_group_modifiers,\n            )\n        )\n\n    def _process_batch(self, batch, phase_type):\n        assert isinstance(batch, Mapping)\n        assert all(isinstance(v, Sample) for v in batch.values())\n        assert len(batch) == 1\n        return batch.popitem()\n\n    def _step(self, batch: Any, key: str, phase_type: str):\n\n        y_hat = self.model({key: batch}, **batch.model_fwd_kwargs)\n        assert isinstance(y_hat, Mapping)\n        assert len(y_hat) == 1\n        key, y_hat = y_hat.popitem()\n        loss = None\n        batch_size = batch.label.shape[0]\n        loss_str = f\"Losses/{phase_type}_{key}_loss\"\n        if phase_type == \"train\":\n            loss, y_hat = self.loss[key](y_hat, batch)\n            self.logger.log(\n                os.path.join(\"Step\", loss_str),\n                loss,\n                self.steps[phase_type],\n            )\n\n        metrics_result = self._compute_metrics(y_hat, batch.label, phase_type, key)\n        self.logger.log_dict(\n            {os.path.join(\"Step\", k): v for k, v in metrics_result.items()},\n            self.steps[phase_type],\n        )\n\n        self.steps[phase_type] += 1", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_625-675"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 677, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            None\n            if self.optim_conf is None\n            else construct_optimizer(\n                self.model,\n                self.optim_conf.optimizer,\n                self.optim_conf.options,\n                self.optim_conf.param_group_modifiers,\n            )\n        )\n\n    def _process_batch(self, batch, phase_type):\n        assert isinstance(batch, Mapping)\n        assert all(isinstance(v, Sample) for v in batch.values())\n        assert len(batch) == 1\n        return batch.popitem()\n\n    def _step(self, batch: Any, key: str, phase_type: str):\n\n        y_hat = self.model({key: batch}, **batch.model_fwd_kwargs)\n        assert isinstance(y_hat, Mapping)\n        assert len(y_hat) == 1\n        key, y_hat = y_hat.popitem()\n        loss = None\n        batch_size = batch.label.shape[0]\n        loss_str = f\"Losses/{phase_type}_{key}_loss\"\n        if phase_type == \"train\":\n            loss, y_hat = self.loss[key](y_hat, batch)\n            self.logger.log(\n                os.path.join(\"Step\", loss_str),\n                loss,\n                self.steps[phase_type],\n            )\n\n        metrics_result = self._compute_metrics(y_hat, batch.label, phase_type, key)\n        self.logger.log_dict(\n            {os.path.join(\"Step\", k): v for k, v in metrics_result.items()},\n            self.steps[phase_type],\n        )\n\n        self.steps[phase_type] += 1\n\n        return {loss_str: loss}, metrics_result, batch_size", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_635-677"}
{"title": "facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "trainer", "omnivision_trainer.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 677, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def _process_batch(self, batch, phase_type):\n        assert isinstance(batch, Mapping)\n        assert all(isinstance(v, Sample) for v in batch.values())\n        assert len(batch) == 1\n        return batch.popitem()\n\n    def _step(self, batch: Any, key: str, phase_type: str):\n\n        y_hat = self.model({key: batch}, **batch.model_fwd_kwargs)\n        assert isinstance(y_hat, Mapping)\n        assert len(y_hat) == 1\n        key, y_hat = y_hat.popitem()\n        loss = None\n        batch_size = batch.label.shape[0]\n        loss_str = f\"Losses/{phase_type}_{key}_loss\"\n        if phase_type == \"train\":\n            loss, y_hat = self.loss[key](y_hat, batch)\n            self.logger.log(\n                os.path.join(\"Step\", loss_str),\n                loss,\n                self.steps[phase_type],\n            )\n\n        metrics_result = self._compute_metrics(y_hat, batch.label, phase_type, key)\n        self.logger.log_dict(\n            {os.path.join(\"Step\", k): v for k, v in metrics_result.items()},\n            self.steps[phase_type],\n        )\n\n        self.steps[phase_type] += 1\n\n        return {loss_str: loss}, metrics_result, batch_size", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-trainer-omnivision_trainer.py_645-677"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Copied from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport logging\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom iopath.common.file_io import g_pathmgr\n\nfrom .distributed import broadcast_object, is_primary\n\n\n# constants:\nCHECKPOINT_FILE = \"checkpoint.torch\"\nCPU_DEVICE = torch.device(\"cpu\")\nGPU_DEVICE = torch.device(\"cuda\")\n\n\ndef load_and_broadcast_checkpoint_list(\n    checkpoint_paths: List[str], device: torch.device = CPU_DEVICE", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Copied from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport logging\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom iopath.common.file_io import g_pathmgr\n\nfrom .distributed import broadcast_object, is_primary\n\n\n# constants:\nCHECKPOINT_FILE = \"checkpoint.torch\"\nCPU_DEVICE = torch.device(\"cpu\")\nGPU_DEVICE = torch.device(\"cuda\")\n\n\ndef load_and_broadcast_checkpoint_list(\n    checkpoint_paths: List[str], device: torch.device = CPU_DEVICE\n):\n    if is_primary():\n        for path in checkpoint_paths:\n            checkpoint = load_checkpoint(path, device)\n            if checkpoint is not None:\n                break\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_paths}\")\n    return broadcast_object(checkpoint)\n\nAST=Module(Import(alias)ImportFrom(aliasaliasalias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))arg(Attribute(Name(Load)Load))Name(Load))If(Call(Name(Load))For(Name(Store)Name(Load)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))If(Compare(Name(Load)IsNotConstant)Break))Assign(Name(Store)Constant))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Return(Call(Name(Load)Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Copied from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport logging\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom iopath.common.file_io import g_pathmgr\n\nfrom .distributed import broadcast_object, is_primary\n\n\n# constants:\nCHECKPOINT_FILE = \"checkpoint.torch\"\nCPU_DEVICE = torch.device(\"cpu\")\nGPU_DEVICE = torch.device(\"cuda\")\n\n\ndef load_and_broadcast_checkpoint_list(\n    checkpoint_paths: List[str], device: torch.device = CPU_DEVICE\n):\n    if is_primary():\n        for path in checkpoint_paths:\n            checkpoint = load_checkpoint(path, device)\n            if checkpoint is not None:\n                break\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_paths}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_and_broadcast_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint on primary and broadcasts it to all replicas.\n\n    This is a collective operation which needs to be run in sync on all replicas.\n\n    See :func:`load_checkpoint` for the arguments.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#\n# Copied from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport logging\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom iopath.common.file_io import g_pathmgr\n\nfrom .distributed import broadcast_object, is_primary\n\n\n# constants:\nCHECKPOINT_FILE = \"checkpoint.torch\"\nCPU_DEVICE = torch.device(\"cpu\")\nGPU_DEVICE = torch.device(\"cuda\")\n\n\ndef load_and_broadcast_checkpoint_list(\n    checkpoint_paths: List[str], device: torch.device = CPU_DEVICE\n):\n    if is_primary():\n        for path in checkpoint_paths:\n            checkpoint = load_checkpoint(path, device)\n            if checkpoint is not None:\n                break\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_paths}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_and_broadcast_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint on primary and broadcasts it to all replicas.\n\n    This is a collective operation which needs to be run in sync on all replicas.\n\n    See :func:`load_checkpoint` for the arguments.\n    \"\"\"\n    if is_primary():\n        checkpoint = load_checkpoint(checkpoint_path, device)\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_path}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_checkpoint(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n# constants:\nCHECKPOINT_FILE = \"checkpoint.torch\"\nCPU_DEVICE = torch.device(\"cpu\")\nGPU_DEVICE = torch.device(\"cuda\")\n\n\ndef load_and_broadcast_checkpoint_list(\n    checkpoint_paths: List[str], device: torch.device = CPU_DEVICE\n):\n    if is_primary():\n        for path in checkpoint_paths:\n            checkpoint = load_checkpoint(path, device)\n            if checkpoint is not None:\n                break\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_paths}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_and_broadcast_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint on primary and broadcasts it to all replicas.\n\n    This is a collective operation which needs to be run in sync on all replicas.\n\n    See :func:`load_checkpoint` for the arguments.\n    \"\"\"\n    if is_primary():\n        checkpoint = load_checkpoint(checkpoint_path, device)\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_path}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint from the specified checkpoint path.\n\n    Args:\n        checkpoint_path: The path to load the checkpoint from. Can be a file or a\n            directory. If it is a directory, the checkpoint is loaded from\n            :py:data:`CHECKPOINT_FILE` inside the directory.\n        device: device to load the checkpoint to\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "):\n    if is_primary():\n        for path in checkpoint_paths:\n            checkpoint = load_checkpoint(path, device)\n            if checkpoint is not None:\n                break\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_paths}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_and_broadcast_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint on primary and broadcasts it to all replicas.\n\n    This is a collective operation which needs to be run in sync on all replicas.\n\n    See :func:`load_checkpoint` for the arguments.\n    \"\"\"\n    if is_primary():\n        checkpoint = load_checkpoint(checkpoint_path, device)\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_path}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint from the specified checkpoint path.\n\n    Args:\n        checkpoint_path: The path to load the checkpoint from. Can be a file or a\n            directory. If it is a directory, the checkpoint is loaded from\n            :py:data:`CHECKPOINT_FILE` inside the directory.\n        device: device to load the checkpoint to\n\n    Returns:\n        The checkpoint, if it exists, or None.\n    \"\"\"\n    if not checkpoint_path:\n        return None\n\n    assert device is not None, \"Please specify what device to load checkpoint on\"\n    assert device.type in [\"cpu\", \"cuda\"], f\"Unknown device: {device}\"\n    if device.type == \"cuda\":\n        assert torch.cuda.is_available()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef load_and_broadcast_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint on primary and broadcasts it to all replicas.\n\n    This is a collective operation which needs to be run in sync on all replicas.\n\n    See :func:`load_checkpoint` for the arguments.\n    \"\"\"\n    if is_primary():\n        checkpoint = load_checkpoint(checkpoint_path, device)\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_path}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint from the specified checkpoint path.\n\n    Args:\n        checkpoint_path: The path to load the checkpoint from. Can be a file or a\n            directory. If it is a directory, the checkpoint is loaded from\n            :py:data:`CHECKPOINT_FILE` inside the directory.\n        device: device to load the checkpoint to\n\n    Returns:\n        The checkpoint, if it exists, or None.\n    \"\"\"\n    if not checkpoint_path:\n        return None\n\n    assert device is not None, \"Please specify what device to load checkpoint on\"\n    assert device.type in [\"cpu\", \"cuda\"], f\"Unknown device: {device}\"\n    if device.type == \"cuda\":\n        assert torch.cuda.is_available()\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint path {checkpoint_path} not found\")\n        return None\n    if g_pathmgr.isdir(checkpoint_path):\n        checkpoint_path = f\"{checkpoint_path.rstrip('/')}/{CHECKPOINT_FILE}\"\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint file {checkpoint_path} not found.\")\n        return None\n\nAST=Module(FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load))Name(Load))Expr(Constant)If(Call(Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Constant))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Return(Call(Name(Load)Name(Load)))Subscript(Name(Load)Name(Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Attribute(Name(Load)Load))Name(Load))Expr(Constant)If(UnaryOp(NotName(Load))Return(Constant))Assert(Compare(Name(Load)IsNotConstant)Constant)Assert(Compare(Attribute(Name(Load)Load)InList(ConstantConstantLoad))JoinedStr(ConstantFormattedValue(Name(Load))))If(Compare(Attribute(Name(Load)Load)EqConstant)Assert(Call(Attribute(Attribute(Name(Load)Load)Load))))If(UnaryOp(NotCall(Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load))Constant)))Return(Constant))If(Call(Attribute(Name(Load)Load)Name(Load))Assign(Name(Store)JoinedStr(FormattedValue(Call(Attribute(Name(Load)Load)Constant))ConstantFormattedValue(Name(Load)))))If(UnaryOp(NotCall(Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)JoinedStr(ConstantFormattedValue(Name(Load))Constant)))Return(Constant))Subscript(Name(Load)Name(Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 93, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    if is_primary():\n        checkpoint = load_checkpoint(checkpoint_path, device)\n    else:\n        checkpoint = None\n    logging.info(f\"Broadcasting checkpoint loaded from {checkpoint_path}\")\n    return broadcast_object(checkpoint)\n\n\ndef load_checkpoint(\n    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint from the specified checkpoint path.\n\n    Args:\n        checkpoint_path: The path to load the checkpoint from. Can be a file or a\n            directory. If it is a directory, the checkpoint is loaded from\n            :py:data:`CHECKPOINT_FILE` inside the directory.\n        device: device to load the checkpoint to\n\n    Returns:\n        The checkpoint, if it exists, or None.\n    \"\"\"\n    if not checkpoint_path:\n        return None\n\n    assert device is not None, \"Please specify what device to load checkpoint on\"\n    assert device.type in [\"cpu\", \"cuda\"], f\"Unknown device: {device}\"\n    if device.type == \"cuda\":\n        assert torch.cuda.is_available()\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint path {checkpoint_path} not found\")\n        return None\n    if g_pathmgr.isdir(checkpoint_path):\n        checkpoint_path = f\"{checkpoint_path.rstrip('/')}/{CHECKPOINT_FILE}\"\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint file {checkpoint_path} not found.\")\n        return None\n\n    logging.info(f\"Attempting to load checkpoint from {checkpoint_path}\")\n    # load model on specified device and not on saved device for model and return\n    # the checkpoint\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=device)\n    logging.info(f\"Loaded checkpoint from {checkpoint_path}\")\n    return checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_45-93"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 93, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    checkpoint_path: str, device: torch.device = CPU_DEVICE\n) -> Optional[Dict]:\n    \"\"\"Loads a checkpoint from the specified checkpoint path.\n\n    Args:\n        checkpoint_path: The path to load the checkpoint from. Can be a file or a\n            directory. If it is a directory, the checkpoint is loaded from\n            :py:data:`CHECKPOINT_FILE` inside the directory.\n        device: device to load the checkpoint to\n\n    Returns:\n        The checkpoint, if it exists, or None.\n    \"\"\"\n    if not checkpoint_path:\n        return None\n\n    assert device is not None, \"Please specify what device to load checkpoint on\"\n    assert device.type in [\"cpu\", \"cuda\"], f\"Unknown device: {device}\"\n    if device.type == \"cuda\":\n        assert torch.cuda.is_available()\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint path {checkpoint_path} not found\")\n        return None\n    if g_pathmgr.isdir(checkpoint_path):\n        checkpoint_path = f\"{checkpoint_path.rstrip('/')}/{CHECKPOINT_FILE}\"\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint file {checkpoint_path} not found.\")\n        return None\n\n    logging.info(f\"Attempting to load checkpoint from {checkpoint_path}\")\n    # load model on specified device and not on saved device for model and return\n    # the checkpoint\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=device)\n    logging.info(f\"Loaded checkpoint from {checkpoint_path}\")\n    return checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_55-93"}
{"title": "facebookresearch_omnivore-omnivision-utils-checkpoint.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "checkpoint.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 93, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Returns:\n        The checkpoint, if it exists, or None.\n    \"\"\"\n    if not checkpoint_path:\n        return None\n\n    assert device is not None, \"Please specify what device to load checkpoint on\"\n    assert device.type in [\"cpu\", \"cuda\"], f\"Unknown device: {device}\"\n    if device.type == \"cuda\":\n        assert torch.cuda.is_available()\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint path {checkpoint_path} not found\")\n        return None\n    if g_pathmgr.isdir(checkpoint_path):\n        checkpoint_path = f\"{checkpoint_path.rstrip('/')}/{CHECKPOINT_FILE}\"\n\n    if not g_pathmgr.exists(checkpoint_path):\n        logging.warning(f\"Checkpoint file {checkpoint_path} not found.\")\n        return None\n\n    logging.info(f\"Attempting to load checkpoint from {checkpoint_path}\")\n    # load model on specified device and not on saved device for model and return\n    # the checkpoint\n    with g_pathmgr.open(checkpoint_path, \"rb\") as f:\n        checkpoint = torch.load(f, map_location=device)\n    logging.info(f\"Loaded checkpoint from {checkpoint_path}\")\n    return checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-checkpoint.py_65-93"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport getpass\nimport json\nimport logging\nimport os.path\nimport pickle\nfrom multiprocessing import shared_memory\nfrom typing import Any, List, Tuple, Union\n\nimport numpy as np\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.utils.distributed import (\n    barrier,\n    broadcast_object,\n    is_local_primary,\n    is_torch_dataloader_worker,\n)\nfrom PIL import Image\n\n\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport getpass\nimport json\nimport logging\nimport os.path\nimport pickle\nfrom multiprocessing import shared_memory\nfrom typing import Any, List, Tuple, Union\n\nimport numpy as np\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.utils.distributed import (\n    barrier,\n    broadcast_object,\n    is_local_primary,\n    is_torch_dataloader_worker,\n)\nfrom PIL import Image\n\n\nclass IdentityTransform:\n    def __call__(self, x: Any) -> Any:\n        return x\n\n\n# copied from vissl.data.data_helper\ndef get_mean_image(crop_size: Union[Tuple, int]):\n    \"\"\"\n    Helper function that returns a gray PIL image of the size specified by user.\n    Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport getpass\nimport json\nimport logging\nimport os.path\nimport pickle\nfrom multiprocessing import shared_memory\nfrom typing import Any, List, Tuple, Union\n\nimport numpy as np\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.utils.distributed import (\n    barrier,\n    broadcast_object,\n    is_local_primary,\n    is_torch_dataloader_worker,\n)\nfrom PIL import Image\n\n\nclass IdentityTransform:\n    def __call__(self, x: Any) -> Any:\n        return x\n\n\n# copied from vissl.data.data_helper\ndef get_mean_image(crop_size: Union[Tuple, int]):\n    \"\"\"\n    Helper function that returns a gray PIL image of the size specified by user.\n    Args:\n        crop_size (tuple, or int): used to generate (crop_size[0] x crop_size[1] x 3) image\n            in the case of a tuple of (crop_size, crop_size, 3) image in case of int.\n    Returns:\n        img: PIL Image\n    \"\"\"\n    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    img = Image.fromarray(\n        128 * np.ones((crop_size[0], crop_size[1], 3), dtype=np.uint8)\n    )\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ClassDef(FunctionDef(arguments(argarg(Name(Load)))Return(Name(Load))Name(Load)))FunctionDef(arguments(arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)))Expr(Constant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Tuple(Name(Load)Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(ConstantMultCall(Attribute(Name(Load)Load)Tuple(Subscript(Name(Load)ConstantLoad)Subscript(Name(Load)ConstantLoad)ConstantLoad)keyword(Attribute(Name(Load)Load))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport getpass\nimport json\nimport logging\nimport os.path\nimport pickle\nfrom multiprocessing import shared_memory\nfrom typing import Any, List, Tuple, Union\n\nimport numpy as np\nfrom iopath.common.file_io import g_pathmgr\nfrom omnivision.utils.distributed import (\n    barrier,\n    broadcast_object,\n    is_local_primary,\n    is_torch_dataloader_worker,\n)\nfrom PIL import Image\n\n\nclass IdentityTransform:\n    def __call__(self, x: Any) -> Any:\n        return x\n\n\n# copied from vissl.data.data_helper\ndef get_mean_image(crop_size: Union[Tuple, int]):\n    \"\"\"\n    Helper function that returns a gray PIL image of the size specified by user.\n    Args:\n        crop_size (tuple, or int): used to generate (crop_size[0] x crop_size[1] x 3) image\n            in the case of a tuple of (crop_size, crop_size, 3) image in case of int.\n    Returns:\n        img: PIL Image\n    \"\"\"\n    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    img = Image.fromarray(\n        128 * np.ones((crop_size[0], crop_size[1], 3), dtype=np.uint8)\n    )\n    return img\n\n\ndef list_of_paths_to_path(path_list: List[str]):\n    path_exists = False\n    for idx, path in enumerate(path_list):\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n    if path_exists is False:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from iopath.common.file_io import g_pathmgr\nfrom omnivision.utils.distributed import (\n    barrier,\n    broadcast_object,\n    is_local_primary,\n    is_torch_dataloader_worker,\n)\nfrom PIL import Image\n\n\nclass IdentityTransform:\n    def __call__(self, x: Any) -> Any:\n        return x\n\n\n# copied from vissl.data.data_helper\ndef get_mean_image(crop_size: Union[Tuple, int]):\n    \"\"\"\n    Helper function that returns a gray PIL image of the size specified by user.\n    Args:\n        crop_size (tuple, or int): used to generate (crop_size[0] x crop_size[1] x 3) image\n            in the case of a tuple of (crop_size, crop_size, 3) image in case of int.\n    Returns:\n        img: PIL Image\n    \"\"\"\n    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    img = Image.fromarray(\n        128 * np.ones((crop_size[0], crop_size[1], 3), dtype=np.uint8)\n    )\n    return img\n\n\ndef list_of_paths_to_path(path_list: List[str]):\n    path_exists = False\n    for idx, path in enumerate(path_list):\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n    if path_exists is False:\n        path = None\n    return path_exists, path, idx\n\n\ndef pickle_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = pickle.load(fh)\n    return data\n\n\n\nAST=Module(ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ClassDef(FunctionDef(arguments(argarg(Name(Load)))Return(Name(Load))Name(Load)))FunctionDef(arguments(arg(Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)))Expr(Constant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Tuple(Name(Load)Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(ConstantMultCall(Attribute(Name(Load)Load)Tuple(Subscript(Name(Load)ConstantLoad)Subscript(Name(Load)ConstantLoad)ConstantLoad)keyword(Attribute(Name(Load)Load))))))Return(Name(Load)))FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load)))Assign(Name(Store)Constant)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load))If(Call(Attribute(Name(Load)Load)Name(Load))Assign(Name(Store)Constant)Break))If(Compare(Name(Load)IsConstant)Assign(Name(Store)Constant))Return(Tuple(Name(Load)Name(Load)Name(Load)Load)))FunctionDef(arguments(arg)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class IdentityTransform:\n    def __call__(self, x: Any) -> Any:\n        return x\n\n\n# copied from vissl.data.data_helper\ndef get_mean_image(crop_size: Union[Tuple, int]):\n    \"\"\"\n    Helper function that returns a gray PIL image of the size specified by user.\n    Args:\n        crop_size (tuple, or int): used to generate (crop_size[0] x crop_size[1] x 3) image\n            in the case of a tuple of (crop_size, crop_size, 3) image in case of int.\n    Returns:\n        img: PIL Image\n    \"\"\"\n    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    img = Image.fromarray(\n        128 * np.ones((crop_size[0], crop_size[1], 3), dtype=np.uint8)\n    )\n    return img\n\n\ndef list_of_paths_to_path(path_list: List[str]):\n    path_exists = False\n    for idx, path in enumerate(path_list):\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n    if path_exists is False:\n        path = None\n    return path_exists, path, idx\n\n\ndef pickle_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = pickle.load(fh)\n    return data\n\n\ndef numpy_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = np.load(fh)\n    return data\n\n\nFILE_EXT_TO_HANDLER = {\".pkl\": pickle_load, \".npy\": numpy_load}\n\n\nclass FileLoader:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        crop_size (tuple, or int): used to generate (crop_size[0] x crop_size[1] x 3) image\n            in the case of a tuple of (crop_size, crop_size, 3) image in case of int.\n    Returns:\n        img: PIL Image\n    \"\"\"\n    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    img = Image.fromarray(\n        128 * np.ones((crop_size[0], crop_size[1], 3), dtype=np.uint8)\n    )\n    return img\n\n\ndef list_of_paths_to_path(path_list: List[str]):\n    path_exists = False\n    for idx, path in enumerate(path_list):\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n    if path_exists is False:\n        path = None\n    return path_exists, path, idx\n\n\ndef pickle_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = pickle.load(fh)\n    return data\n\n\ndef numpy_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = np.load(fh)\n    return data\n\n\nFILE_EXT_TO_HANDLER = {\".pkl\": pickle_load, \".npy\": numpy_load}\n\n\nclass FileLoader:\n    @staticmethod\n    def load(path_list: List[str], file_handler=None, return_idx=True):\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n        if file_handler is None:\n            _, ext = os.path.splitext(path)\n            file_handler = FILE_EXT_TO_HANDLER[ext]\n        arr = file_handler(path)\n        if return_idx:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return img\n\n\ndef list_of_paths_to_path(path_list: List[str]):\n    path_exists = False\n    for idx, path in enumerate(path_list):\n        if g_pathmgr.exists(path):\n            path_exists = True\n            break\n    if path_exists is False:\n        path = None\n    return path_exists, path, idx\n\n\ndef pickle_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = pickle.load(fh)\n    return data\n\n\ndef numpy_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = np.load(fh)\n    return data\n\n\nFILE_EXT_TO_HANDLER = {\".pkl\": pickle_load, \".npy\": numpy_load}\n\n\nclass FileLoader:\n    @staticmethod\n    def load(path_list: List[str], file_handler=None, return_idx=True):\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n        if file_handler is None:\n            _, ext = os.path.splitext(path)\n            file_handler = FILE_EXT_TO_HANDLER[ext]\n        arr = file_handler(path)\n        if return_idx:\n            return arr, idx\n        return arr\n\n\nclass SharedMemoryNumpyLoader:\n    \"\"\"\n    WARN: A referenced to this object needs to be preserved till\n    the returned np array is being used. This uses collective\n    operations.\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        path = None\n    return path_exists, path, idx\n\n\ndef pickle_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = pickle.load(fh)\n    return data\n\n\ndef numpy_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = np.load(fh)\n    return data\n\n\nFILE_EXT_TO_HANDLER = {\".pkl\": pickle_load, \".npy\": numpy_load}\n\n\nclass FileLoader:\n    @staticmethod\n    def load(path_list: List[str], file_handler=None, return_idx=True):\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n        if file_handler is None:\n            _, ext = os.path.splitext(path)\n            file_handler = FILE_EXT_TO_HANDLER[ext]\n        arr = file_handler(path)\n        if return_idx:\n            return arr, idx\n        return arr\n\n\nclass SharedMemoryNumpyLoader:\n    \"\"\"\n    WARN: A referenced to this object needs to be preserved till\n    the returned np array is being used. This uses collective\n    operations.\n    \"\"\"\n\n    def __init__(self):\n        self.sm = None\n        self.sm_name = None\n\n    def load(self, path_list: List[str]) -> np.ndarray:\n        \"\"\"Attempts to load data from a list of paths. Each element is tried (in order)\n        until a file that exists is found. That file is then used to read the data.\n        \"\"\"\n        if self.sm is not None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def numpy_load(path):\n    with g_pathmgr.open(path, \"rb\") as fh:\n        data = np.load(fh)\n    return data\n\n\nFILE_EXT_TO_HANDLER = {\".pkl\": pickle_load, \".npy\": numpy_load}\n\n\nclass FileLoader:\n    @staticmethod\n    def load(path_list: List[str], file_handler=None, return_idx=True):\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n        if file_handler is None:\n            _, ext = os.path.splitext(path)\n            file_handler = FILE_EXT_TO_HANDLER[ext]\n        arr = file_handler(path)\n        if return_idx:\n            return arr, idx\n        return arr\n\n\nclass SharedMemoryNumpyLoader:\n    \"\"\"\n    WARN: A referenced to this object needs to be preserved till\n    the returned np array is being used. This uses collective\n    operations.\n    \"\"\"\n\n    def __init__(self):\n        self.sm = None\n        self.sm_name = None\n\n    def load(self, path_list: List[str]) -> np.ndarray:\n        \"\"\"Attempts to load data from a list of paths. Each element is tried (in order)\n        until a file that exists is found. That file is then used to read the data.\n        \"\"\"\n        if self.sm is not None:\n            raise RuntimeError(\"Cannot load multiple objects with the same loader\")\n\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n\n        self.sm_name = (\n            \"\".join([x if x.isalnum() else \"_\" for x in path]) + f\"_{getpass.getuser()}\"\n        )\n\nAST=Module(FunctionDef(arguments(arg)With(withitem(Call(Attribute(Name(Load)Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load)))Assign(Name(Store)Dict(ConstantConstantName(Load)Name(Load)))ClassDef(FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))argargConstantConstant)Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))If(UnaryOp(NotName(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))))If(Compare(Name(Load)IsConstant)Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Subscript(Name(Load)Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))If(Name(Load)Return(Tuple(Name(Load)Name(Load)Load)))Return(Name(Load))Name(Load)))ClassDef(Expr(Constant)FunctionDef(arguments(arg)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant))FunctionDef(arguments(argarg(Subscript(Name(Load)Name(Load)Load)))Expr(Constant)If(Compare(Attribute(Name(Load)Load)IsNotConstant)Raise(Call(Name(Load)Constant)))Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))If(UnaryOp(NotName(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))))Assign(Attribute(Name(Load)Store)BinOp(Call(Attribute(ConstantLoad)ListComp(IfExp(Call(Attribute(Name(Load)Load))Name(Load)Constant)comprehension(Name(Store)Name(Load))))AddJoinedStr(ConstantFormattedValue(Call(Attribute(Name(Load)Load))))))Attribute(Name(Load)Load))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    @staticmethod\n    def load(path_list: List[str], file_handler=None, return_idx=True):\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n        if file_handler is None:\n            _, ext = os.path.splitext(path)\n            file_handler = FILE_EXT_TO_HANDLER[ext]\n        arr = file_handler(path)\n        if return_idx:\n            return arr, idx\n        return arr\n\n\nclass SharedMemoryNumpyLoader:\n    \"\"\"\n    WARN: A referenced to this object needs to be preserved till\n    the returned np array is being used. This uses collective\n    operations.\n    \"\"\"\n\n    def __init__(self):\n        self.sm = None\n        self.sm_name = None\n\n    def load(self, path_list: List[str]) -> np.ndarray:\n        \"\"\"Attempts to load data from a list of paths. Each element is tried (in order)\n        until a file that exists is found. That file is then used to read the data.\n        \"\"\"\n        if self.sm is not None:\n            raise RuntimeError(\"Cannot load multiple objects with the same loader\")\n\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n\n        self.sm_name = (\n            \"\".join([x if x.isalnum() else \"_\" for x in path]) + f\"_{getpass.getuser()}\"\n        )\n\n        # we only read from local rank 0 parent process on a machine\n        # all other GPU parent processes and dataloaders read from shared memory\n        if is_local_primary() and not is_torch_dataloader_worker():\n            # this is the local rank 0 process\n            arr = load_file(path)\n            assert isinstance(\n                arr, np.ndarray\n            ), f\"arr is not an ndarray. found {type(arr)}\"\n            logging.info(f\"Moving data files to shared memory: {self.sm_name}\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            return arr, idx\n        return arr\n\n\nclass SharedMemoryNumpyLoader:\n    \"\"\"\n    WARN: A referenced to this object needs to be preserved till\n    the returned np array is being used. This uses collective\n    operations.\n    \"\"\"\n\n    def __init__(self):\n        self.sm = None\n        self.sm_name = None\n\n    def load(self, path_list: List[str]) -> np.ndarray:\n        \"\"\"Attempts to load data from a list of paths. Each element is tried (in order)\n        until a file that exists is found. That file is then used to read the data.\n        \"\"\"\n        if self.sm is not None:\n            raise RuntimeError(\"Cannot load multiple objects with the same loader\")\n\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n\n        self.sm_name = (\n            \"\".join([x if x.isalnum() else \"_\" for x in path]) + f\"_{getpass.getuser()}\"\n        )\n\n        # we only read from local rank 0 parent process on a machine\n        # all other GPU parent processes and dataloaders read from shared memory\n        if is_local_primary() and not is_torch_dataloader_worker():\n            # this is the local rank 0 process\n            arr = load_file(path)\n            assert isinstance(\n                arr, np.ndarray\n            ), f\"arr is not an ndarray. found {type(arr)}\"\n            logging.info(f\"Moving data files to shared memory: {self.sm_name}\")\n            try:\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            except FileExistsError:\n                logging.info(\n                    \"Shared memory already exists, closing it out and recreating\"\n                )\n                sm_old = shared_memory.SharedMemory(name=self.sm_name, create=False)\n                sm_old.close()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def __init__(self):\n        self.sm = None\n        self.sm_name = None\n\n    def load(self, path_list: List[str]) -> np.ndarray:\n        \"\"\"Attempts to load data from a list of paths. Each element is tried (in order)\n        until a file that exists is found. That file is then used to read the data.\n        \"\"\"\n        if self.sm is not None:\n            raise RuntimeError(\"Cannot load multiple objects with the same loader\")\n\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n\n        self.sm_name = (\n            \"\".join([x if x.isalnum() else \"_\" for x in path]) + f\"_{getpass.getuser()}\"\n        )\n\n        # we only read from local rank 0 parent process on a machine\n        # all other GPU parent processes and dataloaders read from shared memory\n        if is_local_primary() and not is_torch_dataloader_worker():\n            # this is the local rank 0 process\n            arr = load_file(path)\n            assert isinstance(\n                arr, np.ndarray\n            ), f\"arr is not an ndarray. found {type(arr)}\"\n            logging.info(f\"Moving data files to shared memory: {self.sm_name}\")\n            try:\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            except FileExistsError:\n                logging.info(\n                    \"Shared memory already exists, closing it out and recreating\"\n                )\n                sm_old = shared_memory.SharedMemory(name=self.sm_name, create=False)\n                sm_old.close()\n                sm_old.unlink()\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            sm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=sm.buf)\n            sm_arr[:] = arr[:]\n            # barrier for all (non-dataloader) proceses to ensure the data is\n            # available on all GPUs\n            barrier()\n            broadcast_object(sm_arr.shape)  # arr_shape", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            raise RuntimeError(\"Cannot load multiple objects with the same loader\")\n\n        path_exists, path, idx = list_of_paths_to_path(path_list)\n\n        if not path_exists:\n            raise ValueError(f\"No path exists in {path_list}\")\n\n        self.sm_name = (\n            \"\".join([x if x.isalnum() else \"_\" for x in path]) + f\"_{getpass.getuser()}\"\n        )\n\n        # we only read from local rank 0 parent process on a machine\n        # all other GPU parent processes and dataloaders read from shared memory\n        if is_local_primary() and not is_torch_dataloader_worker():\n            # this is the local rank 0 process\n            arr = load_file(path)\n            assert isinstance(\n                arr, np.ndarray\n            ), f\"arr is not an ndarray. found {type(arr)}\"\n            logging.info(f\"Moving data files to shared memory: {self.sm_name}\")\n            try:\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            except FileExistsError:\n                logging.info(\n                    \"Shared memory already exists, closing it out and recreating\"\n                )\n                sm_old = shared_memory.SharedMemory(name=self.sm_name, create=False)\n                sm_old.close()\n                sm_old.unlink()\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            sm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=sm.buf)\n            sm_arr[:] = arr[:]\n            # barrier for all (non-dataloader) proceses to ensure the data is\n            # available on all GPUs\n            barrier()\n            broadcast_object(sm_arr.shape)  # arr_shape\n            broadcast_object(sm_arr.dtype)  # arr_type\n        else:\n            if not is_torch_dataloader_worker():\n                # parent process on a GPU which isn't local rank 0; wait for barrier\n                barrier()\n                arr_shape = broadcast_object(None)\n                arr_dtype = broadcast_object(None)\n            logging.info(f\"Loading data files from shared memory: {self.sm_name}\")\n            sm = shared_memory.SharedMemory(name=self.sm_name, create=False)\n            sm_arr = np.ndarray(shape=arr_shape, dtype=arr_dtype, buffer=sm.buf)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # we only read from local rank 0 parent process on a machine\n        # all other GPU parent processes and dataloaders read from shared memory\n        if is_local_primary() and not is_torch_dataloader_worker():\n            # this is the local rank 0 process\n            arr = load_file(path)\n            assert isinstance(\n                arr, np.ndarray\n            ), f\"arr is not an ndarray. found {type(arr)}\"\n            logging.info(f\"Moving data files to shared memory: {self.sm_name}\")\n            try:\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            except FileExistsError:\n                logging.info(\n                    \"Shared memory already exists, closing it out and recreating\"\n                )\n                sm_old = shared_memory.SharedMemory(name=self.sm_name, create=False)\n                sm_old.close()\n                sm_old.unlink()\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            sm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=sm.buf)\n            sm_arr[:] = arr[:]\n            # barrier for all (non-dataloader) proceses to ensure the data is\n            # available on all GPUs\n            barrier()\n            broadcast_object(sm_arr.shape)  # arr_shape\n            broadcast_object(sm_arr.dtype)  # arr_type\n        else:\n            if not is_torch_dataloader_worker():\n                # parent process on a GPU which isn't local rank 0; wait for barrier\n                barrier()\n                arr_shape = broadcast_object(None)\n                arr_dtype = broadcast_object(None)\n            logging.info(f\"Loading data files from shared memory: {self.sm_name}\")\n            sm = shared_memory.SharedMemory(name=self.sm_name, create=False)\n            sm_arr = np.ndarray(shape=arr_shape, dtype=arr_dtype, buffer=sm.buf)\n        # need to keep a reference to the shared memory otherwise it will get\n        # garbage collected and result in a segfault\n        self.sm = sm\n        return sm_arr, idx\n\n    def __del__(self):\n        # FIXME: this doesn't seem to be working on the FAIR cluster\n        if self.sm is None:\n            return\n        self.sm.close()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            try:\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            except FileExistsError:\n                logging.info(\n                    \"Shared memory already exists, closing it out and recreating\"\n                )\n                sm_old = shared_memory.SharedMemory(name=self.sm_name, create=False)\n                sm_old.close()\n                sm_old.unlink()\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            sm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=sm.buf)\n            sm_arr[:] = arr[:]\n            # barrier for all (non-dataloader) proceses to ensure the data is\n            # available on all GPUs\n            barrier()\n            broadcast_object(sm_arr.shape)  # arr_shape\n            broadcast_object(sm_arr.dtype)  # arr_type\n        else:\n            if not is_torch_dataloader_worker():\n                # parent process on a GPU which isn't local rank 0; wait for barrier\n                barrier()\n                arr_shape = broadcast_object(None)\n                arr_dtype = broadcast_object(None)\n            logging.info(f\"Loading data files from shared memory: {self.sm_name}\")\n            sm = shared_memory.SharedMemory(name=self.sm_name, create=False)\n            sm_arr = np.ndarray(shape=arr_shape, dtype=arr_dtype, buffer=sm.buf)\n        # need to keep a reference to the shared memory otherwise it will get\n        # garbage collected and result in a segfault\n        self.sm = sm\n        return sm_arr, idx\n\n    def __del__(self):\n        # FIXME: this doesn't seem to be working on the FAIR cluster\n        if self.sm is None:\n            return\n        self.sm.close()\n        if is_local_primary() and not is_torch_dataloader_worker():\n            logging.info(f\"Unlinking shared memory: {self.sm_name}\")\n            self.sm.unlink()\n\n\n# Copied from vissl.utils.io\ndef load_file(filename, mmap_mode=None):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                sm_old.unlink()\n                sm = shared_memory.SharedMemory(\n                    name=self.sm_name, create=True, size=arr.nbytes\n                )\n            sm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=sm.buf)\n            sm_arr[:] = arr[:]\n            # barrier for all (non-dataloader) proceses to ensure the data is\n            # available on all GPUs\n            barrier()\n            broadcast_object(sm_arr.shape)  # arr_shape\n            broadcast_object(sm_arr.dtype)  # arr_type\n        else:\n            if not is_torch_dataloader_worker():\n                # parent process on a GPU which isn't local rank 0; wait for barrier\n                barrier()\n                arr_shape = broadcast_object(None)\n                arr_dtype = broadcast_object(None)\n            logging.info(f\"Loading data files from shared memory: {self.sm_name}\")\n            sm = shared_memory.SharedMemory(name=self.sm_name, create=False)\n            sm_arr = np.ndarray(shape=arr_shape, dtype=arr_dtype, buffer=sm.buf)\n        # need to keep a reference to the shared memory otherwise it will get\n        # garbage collected and result in a segfault\n        self.sm = sm\n        return sm_arr, idx\n\n    def __del__(self):\n        # FIXME: this doesn't seem to be working on the FAIR cluster\n        if self.sm is None:\n            return\n        self.sm.close()\n        if is_local_primary() and not is_torch_dataloader_worker():\n            logging.info(f\"Unlinking shared memory: {self.sm_name}\")\n            self.sm.unlink()\n\n\n# Copied from vissl.utils.io\ndef load_file(filename, mmap_mode=None):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    logging.info(f\"Loading data from file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            broadcast_object(sm_arr.dtype)  # arr_type\n        else:\n            if not is_torch_dataloader_worker():\n                # parent process on a GPU which isn't local rank 0; wait for barrier\n                barrier()\n                arr_shape = broadcast_object(None)\n                arr_dtype = broadcast_object(None)\n            logging.info(f\"Loading data files from shared memory: {self.sm_name}\")\n            sm = shared_memory.SharedMemory(name=self.sm_name, create=False)\n            sm_arr = np.ndarray(shape=arr_shape, dtype=arr_dtype, buffer=sm.buf)\n        # need to keep a reference to the shared memory otherwise it will get\n        # garbage collected and result in a segfault\n        self.sm = sm\n        return sm_arr, idx\n\n    def __del__(self):\n        # FIXME: this doesn't seem to be working on the FAIR cluster\n        if self.sm is None:\n            return\n        self.sm.close()\n        if is_local_primary() and not is_torch_dataloader_worker():\n            logging.info(f\"Unlinking shared memory: {self.sm_name}\")\n            self.sm.unlink()\n\n\n# Copied from vissl.utils.io\ndef load_file(filename, mmap_mode=None):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    logging.info(f\"Loading data from file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\", mmap_mode=mmap_mode)\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without PathManager\"\n                )\n                data = np.load(filename, encoding=\"latin1\", mmap_mode=mmap_mode)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # need to keep a reference to the shared memory otherwise it will get\n        # garbage collected and result in a segfault\n        self.sm = sm\n        return sm_arr, idx\n\n    def __del__(self):\n        # FIXME: this doesn't seem to be working on the FAIR cluster\n        if self.sm is None:\n            return\n        self.sm.close()\n        if is_local_primary() and not is_torch_dataloader_worker():\n            logging.info(f\"Unlinking shared memory: {self.sm_name}\")\n            self.sm.unlink()\n\n\n# Copied from vissl.utils.io\ndef load_file(filename, mmap_mode=None):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    logging.info(f\"Loading data from file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\", mmap_mode=mmap_mode)\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without PathManager\"\n                )\n                data = np.load(filename, encoding=\"latin1\", mmap_mode=mmap_mode)\n                logging.info(\"Successfully loaded without PathManager\")\n            except Exception:\n                logging.info(\"Could not mmap without PathManager. Trying without mmap\")\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\")\n        else:\n            with g_pathmgr.open(filename, \"rb\") as fopen:\n                data = np.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".json\":\n        with g_pathmgr.open(filename, \"r\") as fopen:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        if is_local_primary() and not is_torch_dataloader_worker():\n            logging.info(f\"Unlinking shared memory: {self.sm_name}\")\n            self.sm.unlink()\n\n\n# Copied from vissl.utils.io\ndef load_file(filename, mmap_mode=None):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    logging.info(f\"Loading data from file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\", mmap_mode=mmap_mode)\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without PathManager\"\n                )\n                data = np.load(filename, encoding=\"latin1\", mmap_mode=mmap_mode)\n                logging.info(\"Successfully loaded without PathManager\")\n            except Exception:\n                logging.info(\"Could not mmap without PathManager. Trying without mmap\")\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\")\n        else:\n            with g_pathmgr.open(filename, \"rb\") as fopen:\n                data = np.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".json\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = json.loads(fopen.read())\n    else:\n        raise Exception(f\"Reading from {file_ext} is not supported yet\")\n    return data\n\n\ndef load_file_from_list(file_list, mmap_mode=None):\n    for path in file_list:\n        if g_pathmgr.exists(path):\n            return load_file(path, mmap_mode)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 217, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    logging.info(f\"Loading data from file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\", mmap_mode=mmap_mode)\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without PathManager\"\n                )\n                data = np.load(filename, encoding=\"latin1\", mmap_mode=mmap_mode)\n                logging.info(\"Successfully loaded without PathManager\")\n            except Exception:\n                logging.info(\"Could not mmap without PathManager. Trying without mmap\")\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\")\n        else:\n            with g_pathmgr.open(filename, \"rb\") as fopen:\n                data = np.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".json\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = json.loads(fopen.read())\n    else:\n        raise Exception(f\"Reading from {file_ext} is not supported yet\")\n    return data\n\n\ndef load_file_from_list(file_list, mmap_mode=None):\n    for path in file_list:\n        if g_pathmgr.exists(path):\n            return load_file(path, mmap_mode)\n            break\n    raise Exception(f\"None of the paths exist in {file_list}\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_175-217"}
{"title": "facebookresearch_omnivore-omnivision-utils-data.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "data.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 217, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\", mmap_mode=mmap_mode)\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without PathManager\"\n                )\n                data = np.load(filename, encoding=\"latin1\", mmap_mode=mmap_mode)\n                logging.info(\"Successfully loaded without PathManager\")\n            except Exception:\n                logging.info(\"Could not mmap without PathManager. Trying without mmap\")\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, encoding=\"latin1\")\n        else:\n            with g_pathmgr.open(filename, \"rb\") as fopen:\n                data = np.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".json\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = json.loads(fopen.read())\n    else:\n        raise Exception(f\"Reading from {file_ext} is not supported yet\")\n    return data\n\n\ndef load_file_from_list(file_list, mmap_mode=None):\n    for path in file_list:\n        if g_pathmgr.exists(path):\n            return load_file(path, mmap_mode)\n            break\n    raise Exception(f\"None of the paths exist in {file_list}\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-data.py_185-217"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/distributed_util.py\n\nimport io\nimport os\nimport tempfile\nfrom typing import Any, Callable, List, Tuple\n\nimport torch\nimport torch.distributed as dist\n\n\n_PRIMARY_RANK = 0\n\n\ndef is_local_primary():\n    return int(os.getenv(\"LOCAL_RANK\")) == 0\n\n\ndef is_local_primary_cuda():\n    assert dist.is_initialized()\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasalias)Import(alias)Import(alias)Assign(Name(Store)Constant)FunctionDef(argumentsReturn(Compare(Call(Name(Load)Call(Attribute(Name(Load)Load)Constant))EqConstant)))FunctionDef(argumentsAssert(Call(Attribute(Name(Load)Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/distributed_util.py\n\nimport io\nimport os\nimport tempfile\nfrom typing import Any, Callable, List, Tuple\n\nimport torch\nimport torch.distributed as dist\n\n\n_PRIMARY_RANK = 0\n\n\ndef is_local_primary():\n    return int(os.getenv(\"LOCAL_RANK\")) == 0\n\n\ndef is_local_primary_cuda():\n    assert dist.is_initialized()\n    assert torch.cuda.is_available()\n    return torch.cuda.current_device() == 0\n\n\ndef is_torch_dataloader_worker():\n    return torch.utils.data.get_worker_info() is not None\n\n\ndef convert_to_distributed_tensor(tensor: torch.Tensor) -> Tuple[torch.Tensor, str]:\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/distributed_util.py\n\nimport io\nimport os\nimport tempfile\nfrom typing import Any, Callable, List, Tuple\n\nimport torch\nimport torch.distributed as dist\n\n\n_PRIMARY_RANK = 0\n\n\ndef is_local_primary():\n    return int(os.getenv(\"LOCAL_RANK\")) == 0\n\n\ndef is_local_primary_cuda():\n    assert dist.is_initialized()\n    assert torch.cuda.is_available()\n    return torch.cuda.current_device() == 0\n\n\ndef is_torch_dataloader_worker():\n    return torch.utils.data.get_worker_info() is not None\n\n\ndef convert_to_distributed_tensor(tensor: torch.Tensor) -> Tuple[torch.Tensor, str]:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This helper function converts to the correct\n    device and returns the tensor + original device.\n    \"\"\"\n    orig_device = \"cpu\" if not tensor.is_cuda else \"gpu\"\n    if (\n        torch.distributed.is_available()\n        and torch.distributed.get_backend() == torch.distributed.Backend.NCCL\n        and not tensor.is_cuda\n    ):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/distributed_util.py\n\nimport io\nimport os\nimport tempfile\nfrom typing import Any, Callable, List, Tuple\n\nimport torch\nimport torch.distributed as dist\n\n\n_PRIMARY_RANK = 0\n\n\ndef is_local_primary():\n    return int(os.getenv(\"LOCAL_RANK\")) == 0\n\n\ndef is_local_primary_cuda():\n    assert dist.is_initialized()\n    assert torch.cuda.is_available()\n    return torch.cuda.current_device() == 0\n\n\ndef is_torch_dataloader_worker():\n    return torch.utils.data.get_worker_info() is not None\n\n\ndef convert_to_distributed_tensor(tensor: torch.Tensor) -> Tuple[torch.Tensor, str]:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This helper function converts to the correct\n    device and returns the tensor + original device.\n    \"\"\"\n    orig_device = \"cpu\" if not tensor.is_cuda else \"gpu\"\n    if (\n        torch.distributed.is_available()\n        and torch.distributed.get_backend() == torch.distributed.Backend.NCCL\n        and not tensor.is_cuda\n    ):\n        tensor = tensor.cuda()\n    return (tensor, orig_device)\n\n\ndef convert_to_normal_tensor(tensor: torch.Tensor, orig_device: str) -> torch.Tensor:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This converts the tensor back to original device.\n    \"\"\"\n    if tensor.is_cuda and orig_device == \"cpu\":", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n_PRIMARY_RANK = 0\n\n\ndef is_local_primary():\n    return int(os.getenv(\"LOCAL_RANK\")) == 0\n\n\ndef is_local_primary_cuda():\n    assert dist.is_initialized()\n    assert torch.cuda.is_available()\n    return torch.cuda.current_device() == 0\n\n\ndef is_torch_dataloader_worker():\n    return torch.utils.data.get_worker_info() is not None\n\n\ndef convert_to_distributed_tensor(tensor: torch.Tensor) -> Tuple[torch.Tensor, str]:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This helper function converts to the correct\n    device and returns the tensor + original device.\n    \"\"\"\n    orig_device = \"cpu\" if not tensor.is_cuda else \"gpu\"\n    if (\n        torch.distributed.is_available()\n        and torch.distributed.get_backend() == torch.distributed.Backend.NCCL\n        and not tensor.is_cuda\n    ):\n        tensor = tensor.cuda()\n    return (tensor, orig_device)\n\n\ndef convert_to_normal_tensor(tensor: torch.Tensor, orig_device: str) -> torch.Tensor:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This converts the tensor back to original device.\n    \"\"\"\n    if tensor.is_cuda and orig_device == \"cpu\":\n        tensor = tensor.cpu()\n    return tensor\n\n\ndef is_distributed_training_run() -> bool:\n    return (\n        torch.distributed.is_available()\n        and torch.distributed.is_initialized()\n        and (torch.distributed.get_world_size() > 1)\n    )\n\nAST=Module(Assign(Name(Store)Constant)FunctionDef(argumentsReturn(Compare(Call(Name(Load)Call(Attribute(Name(Load)Load)Constant))EqConstant)))FunctionDef(argumentsAssert(Call(Attribute(Name(Load)Load)))Assert(Call(Attribute(Attribute(Name(Load)Load)Load)))Return(Compare(Call(Attribute(Attribute(Name(Load)Load)Load))EqConstant)))FunctionDef(argumentsReturn(Compare(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load))IsNotConstant)))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)IfExp(UnaryOp(NotAttribute(Name(Load)Load))ConstantConstant))If(BoolOp(AndCall(Attribute(Attribute(Name(Load)Load)Load))Compare(Call(Attribute(Attribute(Name(Load)Load)Load))EqAttribute(Attribute(Attribute(Name(Load)Load)Load)Load))UnaryOp(NotAttribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load))))Return(Tuple(Name(Load)Name(Load)Load))Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Name(Load)Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Name(Load)))Expr(Constant)If(BoolOp(AndAttribute(Name(Load)Load)Compare(Name(Load)EqConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load))))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(argumentsReturn(BoolOp(AndCall(Attribute(Attribute(Name(Load)Load)Load))Call(Attribute(Attribute(Name(Load)Load)Load))Compare(Call(Attribute(Attribute(Name(Load)Load)Load))GtConstant)))Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    assert torch.cuda.is_available()\n    return torch.cuda.current_device() == 0\n\n\ndef is_torch_dataloader_worker():\n    return torch.utils.data.get_worker_info() is not None\n\n\ndef convert_to_distributed_tensor(tensor: torch.Tensor) -> Tuple[torch.Tensor, str]:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This helper function converts to the correct\n    device and returns the tensor + original device.\n    \"\"\"\n    orig_device = \"cpu\" if not tensor.is_cuda else \"gpu\"\n    if (\n        torch.distributed.is_available()\n        and torch.distributed.get_backend() == torch.distributed.Backend.NCCL\n        and not tensor.is_cuda\n    ):\n        tensor = tensor.cuda()\n    return (tensor, orig_device)\n\n\ndef convert_to_normal_tensor(tensor: torch.Tensor, orig_device: str) -> torch.Tensor:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This converts the tensor back to original device.\n    \"\"\"\n    if tensor.is_cuda and orig_device == \"cpu\":\n        tensor = tensor.cpu()\n    return tensor\n\n\ndef is_distributed_training_run() -> bool:\n    return (\n        torch.distributed.is_available()\n        and torch.distributed.is_initialized()\n        and (torch.distributed.get_world_size() > 1)\n    )\n\n\ndef is_primary() -> bool:\n    \"\"\"\n    Returns True if this is rank 0 of a distributed training job OR if it is\n    a single trainer job. Otherwise False.\n    \"\"\"\n    return get_rank() == _PRIMARY_RANK\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This helper function converts to the correct\n    device and returns the tensor + original device.\n    \"\"\"\n    orig_device = \"cpu\" if not tensor.is_cuda else \"gpu\"\n    if (\n        torch.distributed.is_available()\n        and torch.distributed.get_backend() == torch.distributed.Backend.NCCL\n        and not tensor.is_cuda\n    ):\n        tensor = tensor.cuda()\n    return (tensor, orig_device)\n\n\ndef convert_to_normal_tensor(tensor: torch.Tensor, orig_device: str) -> torch.Tensor:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This converts the tensor back to original device.\n    \"\"\"\n    if tensor.is_cuda and orig_device == \"cpu\":\n        tensor = tensor.cpu()\n    return tensor\n\n\ndef is_distributed_training_run() -> bool:\n    return (\n        torch.distributed.is_available()\n        and torch.distributed.is_initialized()\n        and (torch.distributed.get_world_size() > 1)\n    )\n\n\ndef is_primary() -> bool:\n    \"\"\"\n    Returns True if this is rank 0 of a distributed training job OR if it is\n    a single trainer job. Otherwise False.\n    \"\"\"\n    return get_rank() == _PRIMARY_RANK\n\n\ndef all_reduce_mean(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing mean reduction\n    of tensor over all processes.\n    \"\"\"\n    return all_reduce_op(\n        tensor,\n        torch.distributed.ReduceOp.SUM,\n        lambda t: t / torch.distributed.get_world_size(),\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        tensor = tensor.cuda()\n    return (tensor, orig_device)\n\n\ndef convert_to_normal_tensor(tensor: torch.Tensor, orig_device: str) -> torch.Tensor:\n    \"\"\"\n    For some backends, such as NCCL, communication only works if the\n    tensor is on the GPU. This converts the tensor back to original device.\n    \"\"\"\n    if tensor.is_cuda and orig_device == \"cpu\":\n        tensor = tensor.cpu()\n    return tensor\n\n\ndef is_distributed_training_run() -> bool:\n    return (\n        torch.distributed.is_available()\n        and torch.distributed.is_initialized()\n        and (torch.distributed.get_world_size() > 1)\n    )\n\n\ndef is_primary() -> bool:\n    \"\"\"\n    Returns True if this is rank 0 of a distributed training job OR if it is\n    a single trainer job. Otherwise False.\n    \"\"\"\n    return get_rank() == _PRIMARY_RANK\n\n\ndef all_reduce_mean(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing mean reduction\n    of tensor over all processes.\n    \"\"\"\n    return all_reduce_op(\n        tensor,\n        torch.distributed.ReduceOp.SUM,\n        lambda t: t / torch.distributed.get_world_size(),\n    )\n\n\ndef all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing sum\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.SUM)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        tensor = tensor.cpu()\n    return tensor\n\n\ndef is_distributed_training_run() -> bool:\n    return (\n        torch.distributed.is_available()\n        and torch.distributed.is_initialized()\n        and (torch.distributed.get_world_size() > 1)\n    )\n\n\ndef is_primary() -> bool:\n    \"\"\"\n    Returns True if this is rank 0 of a distributed training job OR if it is\n    a single trainer job. Otherwise False.\n    \"\"\"\n    return get_rank() == _PRIMARY_RANK\n\n\ndef all_reduce_mean(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing mean reduction\n    of tensor over all processes.\n    \"\"\"\n    return all_reduce_op(\n        tensor,\n        torch.distributed.ReduceOp.SUM,\n        lambda t: t / torch.distributed.get_world_size(),\n    )\n\n\ndef all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing sum\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.SUM)\n\n\ndef all_reduce_min(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MIN)\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef is_primary() -> bool:\n    \"\"\"\n    Returns True if this is rank 0 of a distributed training job OR if it is\n    a single trainer job. Otherwise False.\n    \"\"\"\n    return get_rank() == _PRIMARY_RANK\n\n\ndef all_reduce_mean(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing mean reduction\n    of tensor over all processes.\n    \"\"\"\n    return all_reduce_op(\n        tensor,\n        torch.distributed.ReduceOp.SUM,\n        lambda t: t / torch.distributed.get_world_size(),\n    )\n\n\ndef all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing sum\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.SUM)\n\n\ndef all_reduce_min(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MIN)\n\n\ndef all_reduce_max(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MAX)\n\n\ndef all_reduce_op(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def all_reduce_mean(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing mean reduction\n    of tensor over all processes.\n    \"\"\"\n    return all_reduce_op(\n        tensor,\n        torch.distributed.ReduceOp.SUM,\n        lambda t: t / torch.distributed.get_world_size(),\n    )\n\n\ndef all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing sum\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.SUM)\n\n\ndef all_reduce_min(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MIN)\n\n\ndef all_reduce_max(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MAX)\n\n\ndef all_reduce_op(\n    tensor: torch.Tensor,\n    op: torch.distributed.ReduceOp,\n    after_op_func: Callable[[torch.Tensor], torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing sum\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.SUM)\n\n\ndef all_reduce_min(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MIN)\n\n\ndef all_reduce_max(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MAX)\n\n\ndef all_reduce_op(\n    tensor: torch.Tensor,\n    op: torch.distributed.ReduceOp,\n    after_op_func: Callable[[torch.Tensor], torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.all_reduce(tensor, op)\n        if after_op_func is not None:\n            tensor = after_op_func(tensor)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef gather_tensors_from_all(tensor: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef all_reduce_min(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MIN)\n\n\ndef all_reduce_max(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MAX)\n\n\ndef all_reduce_op(\n    tensor: torch.Tensor,\n    op: torch.distributed.ReduceOp,\n    after_op_func: Callable[[torch.Tensor], torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.all_reduce(tensor, op)\n        if after_op_func is not None:\n            tensor = after_op_func(tensor)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef gather_tensors_from_all(tensor: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"\n    Wrapper over torch.distributed.all_gather for performing\n    'gather' of 'tensor' over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if tensor.ndim == 0:\n        # 0 dim tensors cannot be gathered. so unsqueeze\n        tensor = tensor.unsqueeze(0)\n\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)Return(Call(Name(Load)Name(Load)Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)Return(Call(Name(Load)Name(Load)Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Tuple(List(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Load)Load))Constant)Expr(Constant)If(Call(Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Name(Load)))If(Compare(Name(Load)IsNotConstant)Assign(Name(Store)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load))))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant)))If(Call(Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load))))Subscript(Name(Load)Attribute(Name(Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def all_reduce_max(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing min\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    return all_reduce_op(tensor, torch.distributed.ReduceOp.MAX)\n\n\ndef all_reduce_op(\n    tensor: torch.Tensor,\n    op: torch.distributed.ReduceOp,\n    after_op_func: Callable[[torch.Tensor], torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.all_reduce(tensor, op)\n        if after_op_func is not None:\n            tensor = after_op_func(tensor)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef gather_tensors_from_all(tensor: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"\n    Wrapper over torch.distributed.all_gather for performing\n    'gather' of 'tensor' over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if tensor.ndim == 0:\n        # 0 dim tensors cannot be gathered. so unsqueeze\n        tensor = tensor.unsqueeze(0)\n\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        gathered_tensors = [\n            torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(gathered_tensors, tensor)\n        gathered_tensors = [\n            convert_to_normal_tensor(_tensor, orig_device)\n            for _tensor in gathered_tensors\n        ]\n    else:\n        gathered_tensors = [tensor]\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)Return(Call(Name(Load)Name(Load)Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Attribute(Name(Load)Load)Load))arg(Subscript(Name(Load)Tuple(List(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Load)Load))Constant)Expr(Constant)If(Call(Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Name(Load)))If(Compare(Name(Load)IsNotConstant)Assign(Name(Store)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load))))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load)))Expr(Constant)If(Compare(Attribute(Name(Load)Load)EqConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant)))If(Call(Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Assign(Name(Store)ListComp(Call(Attribute(Name(Load)Load)Name(Load))comprehension(Name(Store)Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load))))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Name(Load)))Assign(Name(Store)ListComp(Call(Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load))))Assign(Name(Store)List(Name(Load)Load)))Subscript(Name(Load)Attribute(Name(Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    tensor: torch.Tensor,\n    op: torch.distributed.ReduceOp,\n    after_op_func: Callable[[torch.Tensor], torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.all_reduce for performing\n    reduction of tensor over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.all_reduce(tensor, op)\n        if after_op_func is not None:\n            tensor = after_op_func(tensor)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef gather_tensors_from_all(tensor: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"\n    Wrapper over torch.distributed.all_gather for performing\n    'gather' of 'tensor' over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if tensor.ndim == 0:\n        # 0 dim tensors cannot be gathered. so unsqueeze\n        tensor = tensor.unsqueeze(0)\n\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        gathered_tensors = [\n            torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(gathered_tensors, tensor)\n        gathered_tensors = [\n            convert_to_normal_tensor(_tensor, orig_device)\n            for _tensor in gathered_tensors\n        ]\n    else:\n        gathered_tensors = [tensor]\n\n    return gathered_tensors\n\n\ndef gather_from_all(tensor: torch.Tensor) -> torch.Tensor:\n    gathered_tensors = gather_tensors_from_all(tensor)\n    gathered_tensor = torch.cat(gathered_tensors, 0)\n    return gathered_tensor\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.all_reduce(tensor, op)\n        if after_op_func is not None:\n            tensor = after_op_func(tensor)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef gather_tensors_from_all(tensor: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"\n    Wrapper over torch.distributed.all_gather for performing\n    'gather' of 'tensor' over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if tensor.ndim == 0:\n        # 0 dim tensors cannot be gathered. so unsqueeze\n        tensor = tensor.unsqueeze(0)\n\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        gathered_tensors = [\n            torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(gathered_tensors, tensor)\n        gathered_tensors = [\n            convert_to_normal_tensor(_tensor, orig_device)\n            for _tensor in gathered_tensors\n        ]\n    else:\n        gathered_tensors = [tensor]\n\n    return gathered_tensors\n\n\ndef gather_from_all(tensor: torch.Tensor) -> torch.Tensor:\n    gathered_tensors = gather_tensors_from_all(tensor)\n    gathered_tensor = torch.cat(gathered_tensors, 0)\n    return gathered_tensor\n\n\ndef broadcast(tensor: torch.Tensor, src: int = 0) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.broadcast for broadcasting a tensor from the source\n    to all processes in both distributed / non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.broadcast(tensor, src)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Wrapper over torch.distributed.all_gather for performing\n    'gather' of 'tensor' over all processes in both distributed /\n    non-distributed scenarios.\n    \"\"\"\n    if tensor.ndim == 0:\n        # 0 dim tensors cannot be gathered. so unsqueeze\n        tensor = tensor.unsqueeze(0)\n\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        gathered_tensors = [\n            torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(gathered_tensors, tensor)\n        gathered_tensors = [\n            convert_to_normal_tensor(_tensor, orig_device)\n            for _tensor in gathered_tensors\n        ]\n    else:\n        gathered_tensors = [tensor]\n\n    return gathered_tensors\n\n\ndef gather_from_all(tensor: torch.Tensor) -> torch.Tensor:\n    gathered_tensors = gather_tensors_from_all(tensor)\n    gathered_tensor = torch.cat(gathered_tensors, 0)\n    return gathered_tensor\n\n\ndef broadcast(tensor: torch.Tensor, src: int = 0) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.broadcast for broadcasting a tensor from the source\n    to all processes in both distributed / non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.broadcast(tensor, src)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef barrier() -> None:\n    \"\"\"\n    Wrapper over torch.distributed.barrier, returns without waiting\n    if the distributed process group is not initialized instead of throwing error.\n    \"\"\"\n    if not torch.distributed.is_available() or not torch.distributed.is_initialized():\n        return\n    torch.distributed.barrier()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        gathered_tensors = [\n            torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(gathered_tensors, tensor)\n        gathered_tensors = [\n            convert_to_normal_tensor(_tensor, orig_device)\n            for _tensor in gathered_tensors\n        ]\n    else:\n        gathered_tensors = [tensor]\n\n    return gathered_tensors\n\n\ndef gather_from_all(tensor: torch.Tensor) -> torch.Tensor:\n    gathered_tensors = gather_tensors_from_all(tensor)\n    gathered_tensor = torch.cat(gathered_tensors, 0)\n    return gathered_tensor\n\n\ndef broadcast(tensor: torch.Tensor, src: int = 0) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.broadcast for broadcasting a tensor from the source\n    to all processes in both distributed / non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.broadcast(tensor, src)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef barrier() -> None:\n    \"\"\"\n    Wrapper over torch.distributed.barrier, returns without waiting\n    if the distributed process group is not initialized instead of throwing error.\n    \"\"\"\n    if not torch.distributed.is_available() or not torch.distributed.is_initialized():\n        return\n    torch.distributed.barrier()\n\n\ndef get_world_size() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting worldsize in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    return gathered_tensors\n\n\ndef gather_from_all(tensor: torch.Tensor) -> torch.Tensor:\n    gathered_tensors = gather_tensors_from_all(tensor)\n    gathered_tensor = torch.cat(gathered_tensors, 0)\n    return gathered_tensor\n\n\ndef broadcast(tensor: torch.Tensor, src: int = 0) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.broadcast for broadcasting a tensor from the source\n    to all processes in both distributed / non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.broadcast(tensor, src)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef barrier() -> None:\n    \"\"\"\n    Wrapper over torch.distributed.barrier, returns without waiting\n    if the distributed process group is not initialized instead of throwing error.\n    \"\"\"\n    if not torch.distributed.is_available() or not torch.distributed.is_initialized():\n        return\n    torch.distributed.barrier()\n\n\ndef get_world_size() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting worldsize in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 1\n    )\n\n\ndef get_rank() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting rank in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def broadcast(tensor: torch.Tensor, src: int = 0) -> torch.Tensor:\n    \"\"\"\n    Wrapper over torch.distributed.broadcast for broadcasting a tensor from the source\n    to all processes in both distributed / non-distributed scenarios.\n    \"\"\"\n    if is_distributed_training_run():\n        tensor, orig_device = convert_to_distributed_tensor(tensor)\n        torch.distributed.broadcast(tensor, src)\n        tensor = convert_to_normal_tensor(tensor, orig_device)\n    return tensor\n\n\ndef barrier() -> None:\n    \"\"\"\n    Wrapper over torch.distributed.barrier, returns without waiting\n    if the distributed process group is not initialized instead of throwing error.\n    \"\"\"\n    if not torch.distributed.is_available() or not torch.distributed.is_initialized():\n        return\n    torch.distributed.barrier()\n\n\ndef get_world_size() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting worldsize in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 1\n    )\n\n\ndef get_rank() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting rank in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_rank()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 0\n    )\n\n\ndef broadcast_object(obj: Any, src: int = _PRIMARY_RANK, use_disk: bool = True) -> Any:\n    \"\"\"Broadcast an object from a source to all workers.\n\n    Args:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef barrier() -> None:\n    \"\"\"\n    Wrapper over torch.distributed.barrier, returns without waiting\n    if the distributed process group is not initialized instead of throwing error.\n    \"\"\"\n    if not torch.distributed.is_available() or not torch.distributed.is_initialized():\n        return\n    torch.distributed.barrier()\n\n\ndef get_world_size() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting worldsize in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 1\n    )\n\n\ndef get_rank() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting rank in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_rank()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 0\n    )\n\n\ndef broadcast_object(obj: Any, src: int = _PRIMARY_RANK, use_disk: bool = True) -> Any:\n    \"\"\"Broadcast an object from a source to all workers.\n\n    Args:\n        obj: Object to broadcast, must be serializable\n        src: Source rank for broadcast (default is primary)\n        use_disk: If enabled, removes redundant CPU memory copies by writing to\n            disk\n    \"\"\"\n    # Either broadcast from primary to the fleet (default),\n    # or use the src setting as the original rank\n    if get_rank() == src:\n        # Emit data\n        buffer = io.BytesIO()\n\nAST=Module(FunctionDef(argumentsExpr(Constant)If(BoolOp(OrUnaryOp(NotCall(Attribute(Attribute(Name(Load)Load)Load)))UnaryOp(NotCall(Attribute(Attribute(Name(Load)Load)Load))))Return)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)))Constant)FunctionDef(argumentsExpr(Constant)Return(IfExp(BoolOp(AndCall(Attribute(Attribute(Name(Load)Load)Load))Call(Attribute(Attribute(Name(Load)Load)Load)))Call(Attribute(Attribute(Name(Load)Load)Load))Constant))Name(Load))FunctionDef(argumentsExpr(Constant)Return(IfExp(BoolOp(AndCall(Attribute(Attribute(Name(Load)Load)Load))Call(Attribute(Attribute(Name(Load)Load)Load)))Call(Attribute(Attribute(Name(Load)Load)Load))Constant))Name(Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Expr(Constant)If(Compare(Call(Name(Load))EqName(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load))))Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_world_size() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting worldsize in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_world_size()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 1\n    )\n\n\ndef get_rank() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting rank in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_rank()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 0\n    )\n\n\ndef broadcast_object(obj: Any, src: int = _PRIMARY_RANK, use_disk: bool = True) -> Any:\n    \"\"\"Broadcast an object from a source to all workers.\n\n    Args:\n        obj: Object to broadcast, must be serializable\n        src: Source rank for broadcast (default is primary)\n        use_disk: If enabled, removes redundant CPU memory copies by writing to\n            disk\n    \"\"\"\n    # Either broadcast from primary to the fleet (default),\n    # or use the src setting as the original rank\n    if get_rank() == src:\n        # Emit data\n        buffer = io.BytesIO()\n        torch.save(obj, buffer)\n        data_view = buffer.getbuffer()\n        length_tensor = torch.LongTensor([len(data_view)])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.ByteTensor(data_view)\n        data_tensor = broadcast(data_tensor, src=src)\n    else:\n        # Fetch from the source\n        length_tensor = torch.LongTensor([0])\n        length_tensor = broadcast(length_tensor, src=src)\n\nAST=Module(FunctionDef(argumentsExpr(Constant)Return(IfExp(BoolOp(AndCall(Attribute(Attribute(Name(Load)Load)Load))Call(Attribute(Attribute(Name(Load)Load)Load)))Call(Attribute(Attribute(Name(Load)Load)Load))Constant))Name(Load))FunctionDef(argumentsExpr(Constant)Return(IfExp(BoolOp(AndCall(Attribute(Attribute(Name(Load)Load)Load))Call(Attribute(Attribute(Name(Load)Load)Load)))Call(Attribute(Attribute(Name(Load)Load)Load))Constant))Name(Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load))Name(Load)Constant)Expr(Constant)If(Compare(Call(Name(Load))EqName(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Call(Name(Load)Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(ConstantLoad)))Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Name(Load)))))Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        else 1\n    )\n\n\ndef get_rank() -> int:\n    \"\"\"\n    Simple wrapper for correctly getting rank in both distributed\n    / non-distributed settings\n    \"\"\"\n    return (\n        torch.distributed.get_rank()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 0\n    )\n\n\ndef broadcast_object(obj: Any, src: int = _PRIMARY_RANK, use_disk: bool = True) -> Any:\n    \"\"\"Broadcast an object from a source to all workers.\n\n    Args:\n        obj: Object to broadcast, must be serializable\n        src: Source rank for broadcast (default is primary)\n        use_disk: If enabled, removes redundant CPU memory copies by writing to\n            disk\n    \"\"\"\n    # Either broadcast from primary to the fleet (default),\n    # or use the src setting as the original rank\n    if get_rank() == src:\n        # Emit data\n        buffer = io.BytesIO()\n        torch.save(obj, buffer)\n        data_view = buffer.getbuffer()\n        length_tensor = torch.LongTensor([len(data_view)])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.ByteTensor(data_view)\n        data_tensor = broadcast(data_tensor, src=src)\n    else:\n        # Fetch from the source\n        length_tensor = torch.LongTensor([0])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.empty([length_tensor.item()], dtype=torch.uint8)\n        data_tensor = broadcast(data_tensor, src=src)\n        if use_disk:\n            with tempfile.TemporaryFile(\"r+b\") as f:\n                f.write(data_tensor.numpy())\n                # remove reference to the data tensor and hope that Python garbage\n                # collects it\n                del data_tensor\n                f.seek(0)\n                obj = torch.load(f)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 249, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        torch.distributed.get_rank()\n        if torch.distributed.is_available() and torch.distributed.is_initialized()\n        else 0\n    )\n\n\ndef broadcast_object(obj: Any, src: int = _PRIMARY_RANK, use_disk: bool = True) -> Any:\n    \"\"\"Broadcast an object from a source to all workers.\n\n    Args:\n        obj: Object to broadcast, must be serializable\n        src: Source rank for broadcast (default is primary)\n        use_disk: If enabled, removes redundant CPU memory copies by writing to\n            disk\n    \"\"\"\n    # Either broadcast from primary to the fleet (default),\n    # or use the src setting as the original rank\n    if get_rank() == src:\n        # Emit data\n        buffer = io.BytesIO()\n        torch.save(obj, buffer)\n        data_view = buffer.getbuffer()\n        length_tensor = torch.LongTensor([len(data_view)])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.ByteTensor(data_view)\n        data_tensor = broadcast(data_tensor, src=src)\n    else:\n        # Fetch from the source\n        length_tensor = torch.LongTensor([0])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.empty([length_tensor.item()], dtype=torch.uint8)\n        data_tensor = broadcast(data_tensor, src=src)\n        if use_disk:\n            with tempfile.TemporaryFile(\"r+b\") as f:\n                f.write(data_tensor.numpy())\n                # remove reference to the data tensor and hope that Python garbage\n                # collects it\n                del data_tensor\n                f.seek(0)\n                obj = torch.load(f)\n        else:\n            buffer = io.BytesIO(data_tensor.numpy())\n            obj = torch.load(buffer)\n    return obj", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_205-249"}
{"title": "facebookresearch_omnivore-omnivision-utils-distributed.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "distributed.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 249, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        obj: Object to broadcast, must be serializable\n        src: Source rank for broadcast (default is primary)\n        use_disk: If enabled, removes redundant CPU memory copies by writing to\n            disk\n    \"\"\"\n    # Either broadcast from primary to the fleet (default),\n    # or use the src setting as the original rank\n    if get_rank() == src:\n        # Emit data\n        buffer = io.BytesIO()\n        torch.save(obj, buffer)\n        data_view = buffer.getbuffer()\n        length_tensor = torch.LongTensor([len(data_view)])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.ByteTensor(data_view)\n        data_tensor = broadcast(data_tensor, src=src)\n    else:\n        # Fetch from the source\n        length_tensor = torch.LongTensor([0])\n        length_tensor = broadcast(length_tensor, src=src)\n        data_tensor = torch.empty([length_tensor.item()], dtype=torch.uint8)\n        data_tensor = broadcast(data_tensor, src=src)\n        if use_disk:\n            with tempfile.TemporaryFile(\"r+b\") as f:\n                f.write(data_tensor.numpy())\n                # remove reference to the data tensor and hope that Python garbage\n                # collects it\n                del data_tensor\n                f.seek(0)\n                obj = torch.load(f)\n        else:\n            buffer = io.BytesIO(data_tensor.numpy())\n            obj = torch.load(buffer)\n    return obj", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-distributed.py_215-249"}
{"title": "facebookresearch_omnivore-omnivision-utils-generic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "generic.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport torch\n\n\ndef convert_to_one_hot(targets: torch.Tensor, classes) -> torch.Tensor:\n    \"\"\"\n    This function converts target class indices to one-hot vectors,\n    given the number of classes.\n    \"\"\"\n    assert (\n        torch.max(targets).item() < classes\n    ), \"Class Index must be less than number of classes\"\n    one_hot_targets = torch.zeros(\n        (targets.shape[0], classes), dtype=torch.long, device=targets.device\n    )\n    one_hot_targets.scatter_(1, targets.long(), 1)\n    return one_hot_targets\n\n\n\nAST=Module(Import(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg)Expr(Constant)Assert(Compare(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load))LtName(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Subscript(Attribute(Name(Load)Load)ConstantLoad)Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load))Constant))Return(Name(Load))Attribute(Name(Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-generic.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-utils-generic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "generic.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport torch\n\n\ndef convert_to_one_hot(targets: torch.Tensor, classes) -> torch.Tensor:\n    \"\"\"\n    This function converts target class indices to one-hot vectors,\n    given the number of classes.\n    \"\"\"\n    assert (\n        torch.max(targets).item() < classes\n    ), \"Class Index must be less than number of classes\"\n    one_hot_targets = torch.zeros(\n        (targets.shape[0], classes), dtype=torch.long, device=targets.device\n    )\n    one_hot_targets.scatter_(1, targets.long(), 1)\n    return one_hot_targets\n\n\ndef maybe_convert_to_one_hot(\n    target: torch.Tensor, model_output: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    This function infers whether target is integer or 0/1 encoded\n    and converts it to 0/1 encoding if necessary.\n    \"\"\"\n    target_shape_list = list(target.size())\n\n    if len(target_shape_list) == 1 or (", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-generic.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-utils-generic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "generic.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport torch\n\n\ndef convert_to_one_hot(targets: torch.Tensor, classes) -> torch.Tensor:\n    \"\"\"\n    This function converts target class indices to one-hot vectors,\n    given the number of classes.\n    \"\"\"\n    assert (\n        torch.max(targets).item() < classes\n    ), \"Class Index must be less than number of classes\"\n    one_hot_targets = torch.zeros(\n        (targets.shape[0], classes), dtype=torch.long, device=targets.device\n    )\n    one_hot_targets.scatter_(1, targets.long(), 1)\n    return one_hot_targets\n\n\ndef maybe_convert_to_one_hot(\n    target: torch.Tensor, model_output: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    This function infers whether target is integer or 0/1 encoded\n    and converts it to 0/1 encoding if necessary.\n    \"\"\"\n    target_shape_list = list(target.size())\n\n    if len(target_shape_list) == 1 or (\n        len(target_shape_list) == 2 and target_shape_list[1] == 1\n    ):\n        target = convert_to_one_hot(target.view(-1, 1), model_output.shape[1])\n\n    # target are not necessarily hard 0/1 encoding. It can be soft\n    # (i.e. fractional) in some cases, such as mixup label\n    assert (\n        target.shape == model_output.shape\n    ), \"Target must of the same shape as model_output.\"\n\n\nAST=Module(Import(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg)Expr(Constant)Assert(Compare(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load))LtName(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Subscript(Attribute(Name(Load)Load)ConstantLoad)Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load))Constant))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))If(BoolOp(OrCompare(Call(Name(Load)Name(Load))EqConstant)BoolOp(AndCompare(Call(Name(Load)Name(Load))EqConstant)Compare(Subscript(Name(Load)ConstantLoad)EqConstant)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load)UnaryOp(USubConstant)Constant)Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assert(Compare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load))Constant)Attribute(Name(Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-generic.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-utils-generic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "generic.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Modified from https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/generic/util.py\n\nimport torch\n\n\ndef convert_to_one_hot(targets: torch.Tensor, classes) -> torch.Tensor:\n    \"\"\"\n    This function converts target class indices to one-hot vectors,\n    given the number of classes.\n    \"\"\"\n    assert (\n        torch.max(targets).item() < classes\n    ), \"Class Index must be less than number of classes\"\n    one_hot_targets = torch.zeros(\n        (targets.shape[0], classes), dtype=torch.long, device=targets.device\n    )\n    one_hot_targets.scatter_(1, targets.long(), 1)\n    return one_hot_targets\n\n\ndef maybe_convert_to_one_hot(\n    target: torch.Tensor, model_output: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    This function infers whether target is integer or 0/1 encoded\n    and converts it to 0/1 encoding if necessary.\n    \"\"\"\n    target_shape_list = list(target.size())\n\n    if len(target_shape_list) == 1 or (\n        len(target_shape_list) == 2 and target_shape_list[1] == 1\n    ):\n        target = convert_to_one_hot(target.view(-1, 1), model_output.shape[1])\n\n    # target are not necessarily hard 0/1 encoding. It can be soft\n    # (i.e. fractional) in some cases, such as mixup label\n    assert (\n        target.shape == model_output.shape\n    ), \"Target must of the same shape as model_output.\"\n\n    return target\n\n\ndef is_on_gpu(model: torch.nn.Module) -> bool:\n    \"\"\"\n    Returns True if all parameters of a model live on the GPU.\n    \"\"\"\n    assert isinstance(model, torch.nn.Module)\n    on_gpu = True\n    has_params = False\n\nAST=Module(Import(alias)FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg)Expr(Constant)Assert(Compare(Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load))Load))LtName(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Subscript(Attribute(Name(Load)Load)ConstantLoad)Name(Load)Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)ConstantCall(Attribute(Name(Load)Load))Constant))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))If(BoolOp(OrCompare(Call(Name(Load)Name(Load))EqConstant)BoolOp(AndCompare(Call(Name(Load)Name(Load))EqConstant)Compare(Subscript(Name(Load)ConstantLoad)EqConstant)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load)UnaryOp(USubConstant)Constant)Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assert(Compare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load))Constant)Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Attribute(Name(Load)Load)Load)))Expr(Constant)Assert(Call(Name(Load)Name(Load)Attribute(Attribute(Name(Load)Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-generic.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-utils-generic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "generic.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 60, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    assert (\n        torch.max(targets).item() < classes\n    ), \"Class Index must be less than number of classes\"\n    one_hot_targets = torch.zeros(\n        (targets.shape[0], classes), dtype=torch.long, device=targets.device\n    )\n    one_hot_targets.scatter_(1, targets.long(), 1)\n    return one_hot_targets\n\n\ndef maybe_convert_to_one_hot(\n    target: torch.Tensor, model_output: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    This function infers whether target is integer or 0/1 encoded\n    and converts it to 0/1 encoding if necessary.\n    \"\"\"\n    target_shape_list = list(target.size())\n\n    if len(target_shape_list) == 1 or (\n        len(target_shape_list) == 2 and target_shape_list[1] == 1\n    ):\n        target = convert_to_one_hot(target.view(-1, 1), model_output.shape[1])\n\n    # target are not necessarily hard 0/1 encoding. It can be soft\n    # (i.e. fractional) in some cases, such as mixup label\n    assert (\n        target.shape == model_output.shape\n    ), \"Target must of the same shape as model_output.\"\n\n    return target\n\n\ndef is_on_gpu(model: torch.nn.Module) -> bool:\n    \"\"\"\n    Returns True if all parameters of a model live on the GPU.\n    \"\"\"\n    assert isinstance(model, torch.nn.Module)\n    on_gpu = True\n    has_params = False\n    for param in model.parameters():\n        has_params = True\n        if not param.data.is_cuda:\n            on_gpu = False\n    return has_params and on_gpu", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-generic.py_15-60"}
{"title": "facebookresearch_omnivore-omnivision-utils-generic.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "generic.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 60, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def maybe_convert_to_one_hot(\n    target: torch.Tensor, model_output: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    This function infers whether target is integer or 0/1 encoded\n    and converts it to 0/1 encoding if necessary.\n    \"\"\"\n    target_shape_list = list(target.size())\n\n    if len(target_shape_list) == 1 or (\n        len(target_shape_list) == 2 and target_shape_list[1] == 1\n    ):\n        target = convert_to_one_hot(target.view(-1, 1), model_output.shape[1])\n\n    # target are not necessarily hard 0/1 encoding. It can be soft\n    # (i.e. fractional) in some cases, such as mixup label\n    assert (\n        target.shape == model_output.shape\n    ), \"Target must of the same shape as model_output.\"\n\n    return target\n\n\ndef is_on_gpu(model: torch.nn.Module) -> bool:\n    \"\"\"\n    Returns True if all parameters of a model live on the GPU.\n    \"\"\"\n    assert isinstance(model, torch.nn.Module)\n    on_gpu = True\n    has_params = False\n    for param in model.parameters():\n        has_params = True\n        if not param.data.is_cuda:\n            on_gpu = False\n    return has_params and on_gpu\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Name(Load)Load))arg(Attribute(Name(Load)Load)))Expr(Constant)Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))If(BoolOp(OrCompare(Call(Name(Load)Name(Load))EqConstant)BoolOp(AndCompare(Call(Name(Load)Name(Load))EqConstant)Compare(Subscript(Name(Load)ConstantLoad)EqConstant)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load)UnaryOp(USubConstant)Constant)Subscript(Attribute(Name(Load)Load)ConstantLoad))))Assert(Compare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load))Constant)Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Attribute(Attribute(Name(Load)Load)Load)))Expr(Constant)Assert(Call(Name(Load)Name(Load)Attribute(Attribute(Name(Load)Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Constant)For(Name(Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Constant)If(UnaryOp(NotAttribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Constant)))Return(BoolOp(AndName(Load)Name(Load)))Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-generic.py_25-60"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport os\nimport tempfile\nfrom contextlib import contextmanager\nfrom typing import Callable\n\nimport torch\nimport torch.distributed as dist\n\n\n@contextmanager\ndef with_temp_files(count: int):\n    \"\"\"\n    Context manager to create temporary files and remove them\n    after at the end of the context\n    \"\"\"\n    if count == 1:\n        fd, file_name = tempfile.mkstemp()\n        yield file_name\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)If(Compare(Name(Load)EqConstant)Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))Expr(Yield(Name(Load))))Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport os\nimport tempfile\nfrom contextlib import contextmanager\nfrom typing import Callable\n\nimport torch\nimport torch.distributed as dist\n\n\n@contextmanager\ndef with_temp_files(count: int):\n    \"\"\"\n    Context manager to create temporary files and remove them\n    after at the end of the context\n    \"\"\"\n    if count == 1:\n        fd, file_name = tempfile.mkstemp()\n        yield file_name\n        os.close(fd)\n    else:\n        temp_files = [tempfile.mkstemp() for _ in range(count)]\n        yield [t[1] for t in temp_files]\n        for t in temp_files:\n            os.close(t[0])\n\n\ndef gpu_test(gpu_count: int = 1):\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport os\nimport tempfile\nfrom contextlib import contextmanager\nfrom typing import Callable\n\nimport torch\nimport torch.distributed as dist\n\n\n@contextmanager\ndef with_temp_files(count: int):\n    \"\"\"\n    Context manager to create temporary files and remove them\n    after at the end of the context\n    \"\"\"\n    if count == 1:\n        fd, file_name = tempfile.mkstemp()\n        yield file_name\n        os.close(fd)\n    else:\n        temp_files = [tempfile.mkstemp() for _ in range(count)]\n        yield [t[1] for t in temp_files]\n        for t in temp_files:\n            os.close(t[0])\n\n\ndef gpu_test(gpu_count: int = 1):\n    \"\"\"\n    Annotation for GPU tests, skipping the test if the\n    required amount of GPU is not available\n    \"\"\"\n\n    def gpu_test_decorator(test_function: Callable):\n        @functools.wraps(test_function)\n        def wrapped_test(*args, **kwargs):\n            if torch.cuda.device_count() >= gpu_count:\n                return test_function(*args, **kwargs)\n\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)If(Compare(Name(Load)EqConstant)Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))Expr(Yield(Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)ListComp(Call(Attribute(Name(Load)Load))comprehension(Name(Store)Call(Name(Load)Name(Load)))))Expr(Yield(ListComp(Subscript(Name(Load)ConstantLoad)comprehension(Name(Store)Name(Load)))))For(Name(Store)Name(Load)Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))))Name(Load))FunctionDef(arguments(arg(Name(Load))Constant)Expr(Constant)FunctionDef(arguments(arg(Name(Load)))FunctionDef(arguments(argarg)If(Compare(Call(Attribute(Attribute(Name(Load)Load)Load))GtEName(Load))Return(Call(Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))))Call(Attribute(Name(Load)Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nimport functools\nimport os\nimport tempfile\nfrom contextlib import contextmanager\nfrom typing import Callable\n\nimport torch\nimport torch.distributed as dist\n\n\n@contextmanager\ndef with_temp_files(count: int):\n    \"\"\"\n    Context manager to create temporary files and remove them\n    after at the end of the context\n    \"\"\"\n    if count == 1:\n        fd, file_name = tempfile.mkstemp()\n        yield file_name\n        os.close(fd)\n    else:\n        temp_files = [tempfile.mkstemp() for _ in range(count)]\n        yield [t[1] for t in temp_files]\n        for t in temp_files:\n            os.close(t[0])\n\n\ndef gpu_test(gpu_count: int = 1):\n    \"\"\"\n    Annotation for GPU tests, skipping the test if the\n    required amount of GPU is not available\n    \"\"\"\n\n    def gpu_test_decorator(test_function: Callable):\n        @functools.wraps(test_function)\n        def wrapped_test(*args, **kwargs):\n            if torch.cuda.device_count() >= gpu_count:\n                return test_function(*args, **kwargs)\n\n        return wrapped_test\n\n    return gpu_test_decorator\n\n\ndef init_distributed_on_file(world_size: int, gpu_id: int, sync_file: str):\n    \"\"\"\n    Init the process group need to do distributed training, by syncing\n    the different workers on a file.\n    \"\"\"\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)FunctionDef(arguments(arg(Name(Load)))Expr(Constant)If(Compare(Name(Load)EqConstant)Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))Expr(Yield(Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)ListComp(Call(Attribute(Name(Load)Load))comprehension(Name(Store)Call(Name(Load)Name(Load)))))Expr(Yield(ListComp(Subscript(Name(Load)ConstantLoad)comprehension(Name(Store)Name(Load)))))For(Name(Store)Name(Load)Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))))Name(Load))FunctionDef(arguments(arg(Name(Load))Constant)Expr(Constant)FunctionDef(arguments(arg(Name(Load)))FunctionDef(arguments(argarg)If(Compare(Call(Attribute(Attribute(Name(Load)Load)Load))GtEName(Load))Return(Call(Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))))Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 62, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n@contextmanager\ndef with_temp_files(count: int):\n    \"\"\"\n    Context manager to create temporary files and remove them\n    after at the end of the context\n    \"\"\"\n    if count == 1:\n        fd, file_name = tempfile.mkstemp()\n        yield file_name\n        os.close(fd)\n    else:\n        temp_files = [tempfile.mkstemp() for _ in range(count)]\n        yield [t[1] for t in temp_files]\n        for t in temp_files:\n            os.close(t[0])\n\n\ndef gpu_test(gpu_count: int = 1):\n    \"\"\"\n    Annotation for GPU tests, skipping the test if the\n    required amount of GPU is not available\n    \"\"\"\n\n    def gpu_test_decorator(test_function: Callable):\n        @functools.wraps(test_function)\n        def wrapped_test(*args, **kwargs):\n            if torch.cuda.device_count() >= gpu_count:\n                return test_function(*args, **kwargs)\n\n        return wrapped_test\n\n    return gpu_test_decorator\n\n\ndef init_distributed_on_file(world_size: int, gpu_id: int, sync_file: str):\n    \"\"\"\n    Init the process group need to do distributed training, by syncing\n    the different workers on a file.\n    \"\"\"\n    torch.cuda.set_device(gpu_id)\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=\"file://\" + sync_file,\n        world_size=world_size,\n        rank=gpu_id,\n    )\n\nAST=Module(FunctionDef(arguments(arg(Name(Load)))Expr(Constant)If(Compare(Name(Load)EqConstant)Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))Expr(Yield(Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)ListComp(Call(Attribute(Name(Load)Load))comprehension(Name(Store)Call(Name(Load)Name(Load)))))Expr(Yield(ListComp(Subscript(Name(Load)ConstantLoad)comprehension(Name(Store)Name(Load)))))For(Name(Store)Name(Load)Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))))Name(Load))FunctionDef(arguments(arg(Name(Load))Constant)Expr(Constant)FunctionDef(arguments(arg(Name(Load)))FunctionDef(arguments(argarg)If(Compare(Call(Attribute(Attribute(Name(Load)Load)Load))GtEName(Load))Return(Call(Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))))Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)keyword(Constant)keyword(BinOp(ConstantAddName(Load)))keyword(Name(Load))keyword(Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_15-62"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 62, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        os.close(fd)\n    else:\n        temp_files = [tempfile.mkstemp() for _ in range(count)]\n        yield [t[1] for t in temp_files]\n        for t in temp_files:\n            os.close(t[0])\n\n\ndef gpu_test(gpu_count: int = 1):\n    \"\"\"\n    Annotation for GPU tests, skipping the test if the\n    required amount of GPU is not available\n    \"\"\"\n\n    def gpu_test_decorator(test_function: Callable):\n        @functools.wraps(test_function)\n        def wrapped_test(*args, **kwargs):\n            if torch.cuda.device_count() >= gpu_count:\n                return test_function(*args, **kwargs)\n\n        return wrapped_test\n\n    return gpu_test_decorator\n\n\ndef init_distributed_on_file(world_size: int, gpu_id: int, sync_file: str):\n    \"\"\"\n    Init the process group need to do distributed training, by syncing\n    the different workers on a file.\n    \"\"\"\n    torch.cuda.set_device(gpu_id)\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=\"file://\" + sync_file,\n        world_size=world_size,\n        rank=gpu_id,\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_25-62"}
{"title": "facebookresearch_omnivore-omnivision-utils-testing.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "testing.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 62, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Annotation for GPU tests, skipping the test if the\n    required amount of GPU is not available\n    \"\"\"\n\n    def gpu_test_decorator(test_function: Callable):\n        @functools.wraps(test_function)\n        def wrapped_test(*args, **kwargs):\n            if torch.cuda.device_count() >= gpu_count:\n                return test_function(*args, **kwargs)\n\n        return wrapped_test\n\n    return gpu_test_decorator\n\n\ndef init_distributed_on_file(world_size: int, gpu_id: int, sync_file: str):\n    \"\"\"\n    Init the process group need to do distributed training, by syncing\n    the different workers on a file.\n    \"\"\"\n    torch.cuda.set_device(gpu_id)\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=\"file://\" + sync_file,\n        world_size=world_size,\n        rank=gpu_id,\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-testing.py_35-62"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from,\n# TnT  - https://github.com/pytorch/tnt/blob/master/torchtnt/utils/device.py\n# SLIP - https://github.com/facebookresearch/SLIP\n\nimport atexit\nimport functools\nimport logging\nimport os\nimport random\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Mapping, Protocol, runtime_checkable\n\nimport hydra\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_0-25"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from,\n# TnT  - https://github.com/pytorch/tnt/blob/master/torchtnt/utils/device.py\n# SLIP - https://github.com/facebookresearch/SLIP\n\nimport atexit\nimport functools\nimport logging\nimport os\nimport random\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Mapping, Protocol, runtime_checkable\n\nimport hydra\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\n\ndef register_omegaconf_resolvers():\n    OmegaConf.register_new_resolver(\"get_method\", hydra.utils.get_method)\n    OmegaConf.register_new_resolver(\"get_class\", hydra.utils.get_class)\n    OmegaConf.register_new_resolver(\"times\", lambda x, y: x * y)\n    OmegaConf.register_new_resolver(\"divide\", lambda x, y: x / y)\n    OmegaConf.register_new_resolver(\"range\", lambda x: list(range(x)))\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(argumentsExpr(Call(Attribute(Name(Load)Load)ConstantAttribute(Attribute(Name(Load)Load)Load)))Expr(Call(Attribute(Name(Load)Load)ConstantAttribute(Attribute(Name(Load)Load)Load)))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(argarg)BinOp(Name(Load)MultName(Load)))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(argarg)BinOp(Name(Load)DivName(Load)))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(arg)Call(Name(Load)Call(Name(Load)Name(Load))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_0-35"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Code modified from,\n# TnT  - https://github.com/pytorch/tnt/blob/master/torchtnt/utils/device.py\n# SLIP - https://github.com/facebookresearch/SLIP\n\nimport atexit\nimport functools\nimport logging\nimport os\nimport random\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Mapping, Protocol, runtime_checkable\n\nimport hydra\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\n\ndef register_omegaconf_resolvers():\n    OmegaConf.register_new_resolver(\"get_method\", hydra.utils.get_method)\n    OmegaConf.register_new_resolver(\"get_class\", hydra.utils.get_class)\n    OmegaConf.register_new_resolver(\"times\", lambda x, y: x * y)\n    OmegaConf.register_new_resolver(\"divide\", lambda x, y: x / y)\n    OmegaConf.register_new_resolver(\"range\", lambda x: list(range(x)))\n    OmegaConf.register_new_resolver(\"int\", lambda x: int(x))\n\n\ndef setup_distributed_backend(backend):\n    \"\"\"\n    Initialize torch.distributed and set the CUDA device.\n    Expects environment variables to be set as per\n    https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization\n    along with the environ variable \"LOCAL_RANK\" which is used to set the CUDA device.\n    This is run inside a new process, so the cfg is reset and must be set explicitly.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_0-45"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n# Code modified from,\n# TnT  - https://github.com/pytorch/tnt/blob/master/torchtnt/utils/device.py\n# SLIP - https://github.com/facebookresearch/SLIP\n\nimport atexit\nimport functools\nimport logging\nimport os\nimport random\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Mapping, Protocol, runtime_checkable\n\nimport hydra\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\n\ndef register_omegaconf_resolvers():\n    OmegaConf.register_new_resolver(\"get_method\", hydra.utils.get_method)\n    OmegaConf.register_new_resolver(\"get_class\", hydra.utils.get_class)\n    OmegaConf.register_new_resolver(\"times\", lambda x, y: x * y)\n    OmegaConf.register_new_resolver(\"divide\", lambda x, y: x / y)\n    OmegaConf.register_new_resolver(\"range\", lambda x: list(range(x)))\n    OmegaConf.register_new_resolver(\"int\", lambda x: int(x))\n\n\ndef setup_distributed_backend(backend):\n    \"\"\"\n    Initialize torch.distributed and set the CUDA device.\n    Expects environment variables to be set as per\n    https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization\n    along with the environ variable \"LOCAL_RANK\" which is used to set the CUDA device.\n    This is run inside a new process, so the cfg is reset and must be set explicitly.\n    \"\"\"\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.distributed.init_process_group(backend=backend)\n\n\ndef get_machine_local_and_dist_rank():\n    \"\"\"\n    Get the distributed and local rank of the current gpu.\n    \"\"\"\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", None))\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasaliasaliasalias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(argumentsExpr(Call(Attribute(Name(Load)Load)ConstantAttribute(Attribute(Name(Load)Load)Load)))Expr(Call(Attribute(Name(Load)Load)ConstantAttribute(Attribute(Name(Load)Load)Load)))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(argarg)BinOp(Name(Load)MultName(Load)))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(argarg)BinOp(Name(Load)DivName(Load)))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(arg)Call(Name(Load)Call(Name(Load)Name(Load))))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(arg)Call(Name(Load)Name(Load))))))FunctionDef(arguments(arg)Expr(Constant)Assign(Name(Store)Call(Name(Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load)))))FunctionDef(argumentsExpr(Constant)Assign(Name(Store)Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)ConstantConstant)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_5-55"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import sys\nfrom collections import defaultdict\nfrom dataclasses import fields, is_dataclass\nfrom typing import Any, Mapping, Protocol, runtime_checkable\n\nimport hydra\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\n\ndef register_omegaconf_resolvers():\n    OmegaConf.register_new_resolver(\"get_method\", hydra.utils.get_method)\n    OmegaConf.register_new_resolver(\"get_class\", hydra.utils.get_class)\n    OmegaConf.register_new_resolver(\"times\", lambda x, y: x * y)\n    OmegaConf.register_new_resolver(\"divide\", lambda x, y: x / y)\n    OmegaConf.register_new_resolver(\"range\", lambda x: list(range(x)))\n    OmegaConf.register_new_resolver(\"int\", lambda x: int(x))\n\n\ndef setup_distributed_backend(backend):\n    \"\"\"\n    Initialize torch.distributed and set the CUDA device.\n    Expects environment variables to be set as per\n    https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization\n    along with the environ variable \"LOCAL_RANK\" which is used to set the CUDA device.\n    This is run inside a new process, so the cfg is reset and must be set explicitly.\n    \"\"\"\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.distributed.init_process_group(backend=backend)\n\n\ndef get_machine_local_and_dist_rank():\n    \"\"\"\n    Get the distributed and local rank of the current gpu.\n    \"\"\"\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", None))\n    distributed_rank = int(os.environ.get(\"RANK\", None))\n    assert (\n        local_rank is not None and distributed_rank is not None\n    ), \"Please the set the RANK and LOCAL_RANK environment variables.\"\n    return local_rank, distributed_rank\n\n\ndef print_cfg(cfg):\n    \"\"\"\n    Supports printing both Hydra DictConfig and also the AttrDict config", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_15-65"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from iopath.common.file_io import g_pathmgr\nfrom omegaconf import OmegaConf\n\n\ndef register_omegaconf_resolvers():\n    OmegaConf.register_new_resolver(\"get_method\", hydra.utils.get_method)\n    OmegaConf.register_new_resolver(\"get_class\", hydra.utils.get_class)\n    OmegaConf.register_new_resolver(\"times\", lambda x, y: x * y)\n    OmegaConf.register_new_resolver(\"divide\", lambda x, y: x / y)\n    OmegaConf.register_new_resolver(\"range\", lambda x: list(range(x)))\n    OmegaConf.register_new_resolver(\"int\", lambda x: int(x))\n\n\ndef setup_distributed_backend(backend):\n    \"\"\"\n    Initialize torch.distributed and set the CUDA device.\n    Expects environment variables to be set as per\n    https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization\n    along with the environ variable \"LOCAL_RANK\" which is used to set the CUDA device.\n    This is run inside a new process, so the cfg is reset and must be set explicitly.\n    \"\"\"\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.distributed.init_process_group(backend=backend)\n\n\ndef get_machine_local_and_dist_rank():\n    \"\"\"\n    Get the distributed and local rank of the current gpu.\n    \"\"\"\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", None))\n    distributed_rank = int(os.environ.get(\"RANK\", None))\n    assert (\n        local_rank is not None and distributed_rank is not None\n    ), \"Please the set the RANK and LOCAL_RANK environment variables.\"\n    return local_rank, distributed_rank\n\n\ndef print_cfg(cfg):\n    \"\"\"\n    Supports printing both Hydra DictConfig and also the AttrDict config\n    \"\"\"\n    logging.info(\"Training with config:\")\n    logging.info(OmegaConf.to_yaml(cfg))\n\n\ndef set_seeds(seed_value, max_epochs, dist_rank):\n    \"\"\"\n    Set the python random, numpy and torch seed for each gpu. Also set the CUDA\n    seeds if the CUDA is available. This ensures deterministic nature of the training.\n    \"\"\"\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)FunctionDef(argumentsExpr(Call(Attribute(Name(Load)Load)ConstantAttribute(Attribute(Name(Load)Load)Load)))Expr(Call(Attribute(Name(Load)Load)ConstantAttribute(Attribute(Name(Load)Load)Load)))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(argarg)BinOp(Name(Load)MultName(Load)))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(argarg)BinOp(Name(Load)DivName(Load)))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(arg)Call(Name(Load)Call(Name(Load)Name(Load))))))Expr(Call(Attribute(Name(Load)Load)ConstantLambda(arguments(arg)Call(Name(Load)Name(Load))))))FunctionDef(arguments(arg)Expr(Constant)Assign(Name(Store)Call(Name(Load)Subscript(Attribute(Name(Load)Load)ConstantLoad)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)keyword(Name(Load)))))FunctionDef(argumentsExpr(Constant)Assign(Name(Store)Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)ConstantConstant)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Attribute(Name(Load)Load)Load)ConstantConstant)))Assert(BoolOp(AndCompare(Name(Load)IsNotConstant)Compare(Name(Load)IsNotConstant))Constant)Return(Tuple(Name(Load)Name(Load)Load)))FunctionDef(arguments(arg)Expr(Constant)Expr(Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argargarg)Expr(Constant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_25-75"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    OmegaConf.register_new_resolver(\"int\", lambda x: int(x))\n\n\ndef setup_distributed_backend(backend):\n    \"\"\"\n    Initialize torch.distributed and set the CUDA device.\n    Expects environment variables to be set as per\n    https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization\n    along with the environ variable \"LOCAL_RANK\" which is used to set the CUDA device.\n    This is run inside a new process, so the cfg is reset and must be set explicitly.\n    \"\"\"\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.distributed.init_process_group(backend=backend)\n\n\ndef get_machine_local_and_dist_rank():\n    \"\"\"\n    Get the distributed and local rank of the current gpu.\n    \"\"\"\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", None))\n    distributed_rank = int(os.environ.get(\"RANK\", None))\n    assert (\n        local_rank is not None and distributed_rank is not None\n    ), \"Please the set the RANK and LOCAL_RANK environment variables.\"\n    return local_rank, distributed_rank\n\n\ndef print_cfg(cfg):\n    \"\"\"\n    Supports printing both Hydra DictConfig and also the AttrDict config\n    \"\"\"\n    logging.info(\"Training with config:\")\n    logging.info(OmegaConf.to_yaml(cfg))\n\n\ndef set_seeds(seed_value, max_epochs, dist_rank):\n    \"\"\"\n    Set the python random, numpy and torch seed for each gpu. Also set the CUDA\n    seeds if the CUDA is available. This ensures deterministic nature of the training.\n    \"\"\"\n    # Since in the pytorch sampler, we increment the seed by 1 for every epoch.\n    seed_value = (seed_value + dist_rank) * max_epochs\n    logging.info(f\"MACHINE SEED: {seed_value}\")\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_35-85"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.distributed.init_process_group(backend=backend)\n\n\ndef get_machine_local_and_dist_rank():\n    \"\"\"\n    Get the distributed and local rank of the current gpu.\n    \"\"\"\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", None))\n    distributed_rank = int(os.environ.get(\"RANK\", None))\n    assert (\n        local_rank is not None and distributed_rank is not None\n    ), \"Please the set the RANK and LOCAL_RANK environment variables.\"\n    return local_rank, distributed_rank\n\n\ndef print_cfg(cfg):\n    \"\"\"\n    Supports printing both Hydra DictConfig and also the AttrDict config\n    \"\"\"\n    logging.info(\"Training with config:\")\n    logging.info(OmegaConf.to_yaml(cfg))\n\n\ndef set_seeds(seed_value, max_epochs, dist_rank):\n    \"\"\"\n    Set the python random, numpy and torch seed for each gpu. Also set the CUDA\n    seeds if the CUDA is available. This ensures deterministic nature of the training.\n    \"\"\"\n    # Since in the pytorch sampler, we increment the seed by 1 for every epoch.\n    seed_value = (seed_value + dist_rank) * max_epochs\n    logging.info(f\"MACHINE SEED: {seed_value}\")\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_45-95"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    distributed_rank = int(os.environ.get(\"RANK\", None))\n    assert (\n        local_rank is not None and distributed_rank is not None\n    ), \"Please the set the RANK and LOCAL_RANK environment variables.\"\n    return local_rank, distributed_rank\n\n\ndef print_cfg(cfg):\n    \"\"\"\n    Supports printing both Hydra DictConfig and also the AttrDict config\n    \"\"\"\n    logging.info(\"Training with config:\")\n    logging.info(OmegaConf.to_yaml(cfg))\n\n\ndef set_seeds(seed_value, max_epochs, dist_rank):\n    \"\"\"\n    Set the python random, numpy and torch seed for each gpu. Also set the CUDA\n    seeds if the CUDA is available. This ensures deterministic nature of the training.\n    \"\"\"\n    # Since in the pytorch sampler, we increment the seed by 1 for every epoch.\n    seed_value = (seed_value + dist_rank) * max_epochs\n    logging.info(f\"MACHINE SEED: {seed_value}\")\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_55-105"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    logging.info(\"Training with config:\")\n    logging.info(OmegaConf.to_yaml(cfg))\n\n\ndef set_seeds(seed_value, max_epochs, dist_rank):\n    \"\"\"\n    Set the python random, numpy and torch seed for each gpu. Also set the CUDA\n    seeds if the CUDA is available. This ensures deterministic nature of the training.\n    \"\"\"\n    # Since in the pytorch sampler, we increment the seed by 1 for every epoch.\n    seed_value = (seed_value + dist_rank) * max_epochs\n    logging.info(f\"MACHINE SEED: {seed_value}\")\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_amp_type(amp_type: str):\n\n    assert amp_type in [\"bfloat16\", \"float16\"], \"Invalid Amp type.\"\n\n    if amp_type == \"bfloat16\":\n        return torch.bfloat16\n    else:\n        return torch.float16", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_65-115"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # Since in the pytorch sampler, we increment the seed by 1 for every epoch.\n    seed_value = (seed_value + dist_rank) * max_epochs\n    logging.info(f\"MACHINE SEED: {seed_value}\")\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_amp_type(amp_type: str):\n\n    assert amp_type in [\"bfloat16\", \"float16\"], \"Invalid Amp type.\"\n\n    if amp_type == \"bfloat16\":\n        return torch.bfloat16\n    else:\n        return torch.float16\n\n\n@runtime_checkable\nclass _CopyableData(Protocol):\n    def to(self, device: torch.device, *args: Any, **kwargs: Any):\n        \"\"\"Copy data to the specified device\"\"\"\n        ...\n\n\ndef _is_named_tuple(x) -> bool:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_75-125"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_amp_type(amp_type: str):\n\n    assert amp_type in [\"bfloat16\", \"float16\"], \"Invalid Amp type.\"\n\n    if amp_type == \"bfloat16\":\n        return torch.bfloat16\n    else:\n        return torch.float16\n\n\n@runtime_checkable\nclass _CopyableData(Protocol):\n    def to(self, device: torch.device, *args: Any, **kwargs: Any):\n        \"\"\"Copy data to the specified device\"\"\"\n        ...\n\n\ndef _is_named_tuple(x) -> bool:\n    return isinstance(x, tuple) and hasattr(x, \"_asdict\") and hasattr(x, \"_fields\")\n\n\ndef copy_data_to_device(data, device: torch.device, *args: Any, **kwargs: Any):\n    \"\"\"Function that recursively copies data to a torch.device.\n\n    Args:\n        data: The data to copy to device\n        device: The device to which the data should be copied\n        args: positional arguments that will be passed to the `to` call", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_85-135"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_amp_type(amp_type: str):\n\n    assert amp_type in [\"bfloat16\", \"float16\"], \"Invalid Amp type.\"\n\n    if amp_type == \"bfloat16\":\n        return torch.bfloat16\n    else:\n        return torch.float16\n\n\n@runtime_checkable\nclass _CopyableData(Protocol):\n    def to(self, device: torch.device, *args: Any, **kwargs: Any):\n        \"\"\"Copy data to the specified device\"\"\"\n        ...\n\n\ndef _is_named_tuple(x) -> bool:\n    return isinstance(x, tuple) and hasattr(x, \"_asdict\") and hasattr(x, \"_fields\")\n\n\ndef copy_data_to_device(data, device: torch.device, *args: Any, **kwargs: Any):\n    \"\"\"Function that recursively copies data to a torch.device.\n\n    Args:\n        data: The data to copy to device\n        device: The device to which the data should be copied\n        args: positional arguments that will be passed to the `to` call\n        kwargs: keyword arguments that will be passed to the `to` call\n\n    Returns:\n        The data on the correct device\n    \"\"\"\n\n    if _is_named_tuple(data):\n        return type(data)(\n            **copy_data_to_device(data._asdict(), device, *args, **kwargs)\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_95-145"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_amp_type(amp_type: str):\n\n    assert amp_type in [\"bfloat16\", \"float16\"], \"Invalid Amp type.\"\n\n    if amp_type == \"bfloat16\":\n        return torch.bfloat16\n    else:\n        return torch.float16\n\n\n@runtime_checkable\nclass _CopyableData(Protocol):\n    def to(self, device: torch.device, *args: Any, **kwargs: Any):\n        \"\"\"Copy data to the specified device\"\"\"\n        ...\n\n\ndef _is_named_tuple(x) -> bool:\n    return isinstance(x, tuple) and hasattr(x, \"_asdict\") and hasattr(x, \"_fields\")\n\n\ndef copy_data_to_device(data, device: torch.device, *args: Any, **kwargs: Any):\n    \"\"\"Function that recursively copies data to a torch.device.\n\n    Args:\n        data: The data to copy to device\n        device: The device to which the data should be copied\n        args: positional arguments that will be passed to the `to` call\n        kwargs: keyword arguments that will be passed to the `to` call\n\n    Returns:\n        The data on the correct device\n    \"\"\"\n\n    if _is_named_tuple(data):\n        return type(data)(\n            **copy_data_to_device(data._asdict(), device, *args, **kwargs)\n        )\n    elif isinstance(data, (list, tuple)):\n        return type(data)(copy_data_to_device(e, device, *args, **kwargs) for e in data)\n    elif isinstance(data, defaultdict):\n        return type(data)(\n            data.default_factory,\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            },\n        )\n\nAST=Module(FunctionDef(arguments(arg(Name(Load)))Assert(Compare(Name(Load)InList(ConstantConstantLoad))Constant)If(Compare(Name(Load)EqConstant)Return(Attribute(Name(Load)Load))Return(Attribute(Name(Load)Load))))ClassDef(Name(Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)Expr(Constant))Name(Load))FunctionDef(arguments(arg)Return(BoolOp(AndCall(Name(Load)Name(Load)Name(Load))Call(Name(Load)Name(Load)Constant)Call(Name(Load)Name(Load)Constant)))Name(Load))FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Name(Load))arg(Name(Load)))Expr(Constant)If(Call(Name(Load)Name(Load))Return(Call(Call(Name(Load)Name(Load))keyword(Call(Name(Load)Call(Attribute(Name(Load)Load))Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))))If(Call(Name(Load)Name(Load)Tuple(Name(Load)Name(Load)Load))Return(Call(Call(Name(Load)Name(Load))GeneratorExp(Call(Name(Load)Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))comprehension(Name(Store)Name(Load)))))If(Call(Name(Load)Name(Load)Name(Load))Return(Call(Call(Name(Load)Name(Load))Attribute(Name(Load)Load)DictComp(Name(Load)Call(Name(Load)Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)))))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_105-155"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n@runtime_checkable\nclass _CopyableData(Protocol):\n    def to(self, device: torch.device, *args: Any, **kwargs: Any):\n        \"\"\"Copy data to the specified device\"\"\"\n        ...\n\n\ndef _is_named_tuple(x) -> bool:\n    return isinstance(x, tuple) and hasattr(x, \"_asdict\") and hasattr(x, \"_fields\")\n\n\ndef copy_data_to_device(data, device: torch.device, *args: Any, **kwargs: Any):\n    \"\"\"Function that recursively copies data to a torch.device.\n\n    Args:\n        data: The data to copy to device\n        device: The device to which the data should be copied\n        args: positional arguments that will be passed to the `to` call\n        kwargs: keyword arguments that will be passed to the `to` call\n\n    Returns:\n        The data on the correct device\n    \"\"\"\n\n    if _is_named_tuple(data):\n        return type(data)(\n            **copy_data_to_device(data._asdict(), device, *args, **kwargs)\n        )\n    elif isinstance(data, (list, tuple)):\n        return type(data)(copy_data_to_device(e, device, *args, **kwargs) for e in data)\n    elif isinstance(data, defaultdict):\n        return type(data)(\n            data.default_factory,\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            },\n        )\n    elif isinstance(data, Mapping):\n        return type(data)(\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            }\n        )\n    elif is_dataclass(data) and not isinstance(data, type):\n        new_data_class = type(data)(\n            **{", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_115-165"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return isinstance(x, tuple) and hasattr(x, \"_asdict\") and hasattr(x, \"_fields\")\n\n\ndef copy_data_to_device(data, device: torch.device, *args: Any, **kwargs: Any):\n    \"\"\"Function that recursively copies data to a torch.device.\n\n    Args:\n        data: The data to copy to device\n        device: The device to which the data should be copied\n        args: positional arguments that will be passed to the `to` call\n        kwargs: keyword arguments that will be passed to the `to` call\n\n    Returns:\n        The data on the correct device\n    \"\"\"\n\n    if _is_named_tuple(data):\n        return type(data)(\n            **copy_data_to_device(data._asdict(), device, *args, **kwargs)\n        )\n    elif isinstance(data, (list, tuple)):\n        return type(data)(copy_data_to_device(e, device, *args, **kwargs) for e in data)\n    elif isinstance(data, defaultdict):\n        return type(data)(\n            data.default_factory,\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            },\n        )\n    elif isinstance(data, Mapping):\n        return type(data)(\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            }\n        )\n    elif is_dataclass(data) and not isinstance(data, type):\n        new_data_class = type(data)(\n            **{\n                field.name: copy_data_to_device(\n                    getattr(data, field.name), device, *args, **kwargs\n                )\n                for field in fields(data)\n                if field.init\n            }\n        )\n        for field in fields(data):\n            if not field.init:\n                setattr(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_125-175"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        kwargs: keyword arguments that will be passed to the `to` call\n\n    Returns:\n        The data on the correct device\n    \"\"\"\n\n    if _is_named_tuple(data):\n        return type(data)(\n            **copy_data_to_device(data._asdict(), device, *args, **kwargs)\n        )\n    elif isinstance(data, (list, tuple)):\n        return type(data)(copy_data_to_device(e, device, *args, **kwargs) for e in data)\n    elif isinstance(data, defaultdict):\n        return type(data)(\n            data.default_factory,\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            },\n        )\n    elif isinstance(data, Mapping):\n        return type(data)(\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            }\n        )\n    elif is_dataclass(data) and not isinstance(data, type):\n        new_data_class = type(data)(\n            **{\n                field.name: copy_data_to_device(\n                    getattr(data, field.name), device, *args, **kwargs\n                )\n                for field in fields(data)\n                if field.init\n            }\n        )\n        for field in fields(data):\n            if not field.init:\n                setattr(\n                    new_data_class,\n                    field.name,\n                    copy_data_to_device(\n                        getattr(data, field.name), device, *args, **kwargs\n                    ),\n                )\n        return new_data_class\n    elif isinstance(data, _CopyableData):\n        return data.to(device, *args, **kwargs)\n    return data", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_135-185"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif isinstance(data, (list, tuple)):\n        return type(data)(copy_data_to_device(e, device, *args, **kwargs) for e in data)\n    elif isinstance(data, defaultdict):\n        return type(data)(\n            data.default_factory,\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            },\n        )\n    elif isinstance(data, Mapping):\n        return type(data)(\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            }\n        )\n    elif is_dataclass(data) and not isinstance(data, type):\n        new_data_class = type(data)(\n            **{\n                field.name: copy_data_to_device(\n                    getattr(data, field.name), device, *args, **kwargs\n                )\n                for field in fields(data)\n                if field.init\n            }\n        )\n        for field in fields(data):\n            if not field.init:\n                setattr(\n                    new_data_class,\n                    field.name,\n                    copy_data_to_device(\n                        getattr(data, field.name), device, *args, **kwargs\n                    ),\n                )\n        return new_data_class\n    elif isinstance(data, _CopyableData):\n        return data.to(device, *args, **kwargs)\n    return data\n\n\ndef move_optimizer_state_to_device(\n    optimizer: torch.optim.Optimizer, device: torch.device\n) -> torch.optim.Optimizer:\n    optimizer.state = copy_data_to_device(optimizer.state, device)\n    return optimizer\n\n\nclass AverageMeter(object):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_145-195"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    elif isinstance(data, Mapping):\n        return type(data)(\n            {\n                k: copy_data_to_device(v, device, *args, **kwargs)\n                for k, v in data.items()\n            }\n        )\n    elif is_dataclass(data) and not isinstance(data, type):\n        new_data_class = type(data)(\n            **{\n                field.name: copy_data_to_device(\n                    getattr(data, field.name), device, *args, **kwargs\n                )\n                for field in fields(data)\n                if field.init\n            }\n        )\n        for field in fields(data):\n            if not field.init:\n                setattr(\n                    new_data_class,\n                    field.name,\n                    copy_data_to_device(\n                        getattr(data, field.name), device, *args, **kwargs\n                    ),\n                )\n        return new_data_class\n    elif isinstance(data, _CopyableData):\n        return data.to(device, *args, **kwargs)\n    return data\n\n\ndef move_optimizer_state_to_device(\n    optimizer: torch.optim.Optimizer, device: torch.device\n) -> torch.optim.Optimizer:\n    optimizer.state = copy_data_to_device(optimizer.state, device)\n    return optimizer\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, name, device, fmt=\":f\"):\n        self.name = name\n        self.fmt = fmt\n        self.device = device\n        self.reset()\n\n    def reset(self):\n        self.val = 0", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_155-205"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                field.name: copy_data_to_device(\n                    getattr(data, field.name), device, *args, **kwargs\n                )\n                for field in fields(data)\n                if field.init\n            }\n        )\n        for field in fields(data):\n            if not field.init:\n                setattr(\n                    new_data_class,\n                    field.name,\n                    copy_data_to_device(\n                        getattr(data, field.name), device, *args, **kwargs\n                    ),\n                )\n        return new_data_class\n    elif isinstance(data, _CopyableData):\n        return data.to(device, *args, **kwargs)\n    return data\n\n\ndef move_optimizer_state_to_device(\n    optimizer: torch.optim.Optimizer, device: torch.device\n) -> torch.optim.Optimizer:\n    optimizer.state = copy_data_to_device(optimizer.state, device)\n    return optimizer\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, name, device, fmt=\":f\"):\n        self.name = name\n        self.fmt = fmt\n        self.device = device\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self._allow_updates = True\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_165-215"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    new_data_class,\n                    field.name,\n                    copy_data_to_device(\n                        getattr(data, field.name), device, *args, **kwargs\n                    ),\n                )\n        return new_data_class\n    elif isinstance(data, _CopyableData):\n        return data.to(device, *args, **kwargs)\n    return data\n\n\ndef move_optimizer_state_to_device(\n    optimizer: torch.optim.Optimizer, device: torch.device\n) -> torch.optim.Optimizer:\n    optimizer.state = copy_data_to_device(optimizer.state, device)\n    return optimizer\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, name, device, fmt=\":f\"):\n        self.name = name\n        self.fmt = fmt\n        self.device = device\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self._allow_updates = True\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def synchronize(self):\n        assert self._allow_updates, \"Please reset the meter to allow synchronization.\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor(\n            [self.sum, self.count], dtype=torch.float64, device=self.device\n        )\n        dist.barrier()\n        dist.all_reduce(t)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_175-225"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef move_optimizer_state_to_device(\n    optimizer: torch.optim.Optimizer, device: torch.device\n) -> torch.optim.Optimizer:\n    optimizer.state = copy_data_to_device(optimizer.state, device)\n    return optimizer\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, name, device, fmt=\":f\"):\n        self.name = name\n        self.fmt = fmt\n        self.device = device\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self._allow_updates = True\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def synchronize(self):\n        assert self._allow_updates, \"Please reset the meter to allow synchronization.\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor(\n            [self.sum, self.count], dtype=torch.float64, device=self.device\n        )\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.sum = int(t[0])\n        self.count = t[1]\n        self.avg = self.sum / self.count if self.count > 0 else np.nan\n        self._allow_updates = False\n\n    def __str__(self):\n        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n        return fmtstr.format(**self.__dict__)\n\n\nAST=Module(FunctionDef(arguments(arg(Attribute(Attribute(Name(Load)Load)Load))arg(Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Attribute(Name(Load)Load)Name(Load)))Return(Name(Load))Attribute(Attribute(Name(Load)Load)Load))ClassDef(Name(Load)Expr(Constant)FunctionDef(arguments(argargargargConstant)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Expr(Call(Attribute(Name(Load)Load))))FunctionDef(arguments(arg)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Constant))FunctionDef(arguments(argargargConstant)Assign(Attribute(Name(Load)Store)Name(Load))AugAssign(Attribute(Name(Load)Store)AddBinOp(Name(Load)MultName(Load)))AugAssign(Attribute(Name(Load)Store)AddName(Load))Assign(Attribute(Name(Load)Store)BinOp(Attribute(Name(Load)Load)DivAttribute(Name(Load)Load))))FunctionDef(arguments(arg)Assert(Attribute(Name(Load)Load)Constant)If(UnaryOp(NotCall(Name(Load)))Return)Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)keyword(Attribute(Name(Load)Load))keyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Subscript(Name(Load)ConstantLoad)))Assign(Attribute(Name(Load)Store)Subscript(Name(Load)ConstantLoad))Assign(Attribute(Name(Load)Store)IfExp(Compare(Attribute(Name(Load)Load)GtConstant)BinOp(Attribute(Name(Load)Load)DivAttribute(Name(Load)Load))Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)Constant))FunctionDef(arguments(arg)Assign(Name(Store)BinOp(BinOp(BinOp(BinOp(ConstantAddAttribute(Name(Load)Load))AddConstant)AddAttribute(Name(Load)Load))AddConstant))Return(Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_185-235"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, name, device, fmt=\":f\"):\n        self.name = name\n        self.fmt = fmt\n        self.device = device\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self._allow_updates = True\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def synchronize(self):\n        assert self._allow_updates, \"Please reset the meter to allow synchronization.\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor(\n            [self.sum, self.count], dtype=torch.float64, device=self.device\n        )\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.sum = int(t[0])\n        self.count = t[1]\n        self.avg = self.sum / self.count if self.count > 0 else np.nan\n        self._allow_updates = False\n\n    def __str__(self):\n        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_195-245"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self._allow_updates = True\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def synchronize(self):\n        assert self._allow_updates, \"Please reset the meter to allow synchronization.\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor(\n            [self.sum, self.count], dtype=torch.float64, device=self.device\n        )\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.sum = int(t[0])\n        self.count = t[1]\n        self.avg = self.sum / self.count if self.count > 0 else np.nan\n        self._allow_updates = False\n\n    def __str__(self):\n        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        logging.info(\"\\t\".join(entries))\n\n    def synchronize(self):\n        for meter in self.meters:\n            meter.synchronize()\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \"{:\" + str(num_digits) + \"d}\"\n        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_205-255"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def synchronize(self):\n        assert self._allow_updates, \"Please reset the meter to allow synchronization.\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor(\n            [self.sum, self.count], dtype=torch.float64, device=self.device\n        )\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.sum = int(t[0])\n        self.count = t[1]\n        self.avg = self.sum / self.count if self.count > 0 else np.nan\n        self._allow_updates = False\n\n    def __str__(self):\n        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        logging.info(\"\\t\".join(entries))\n\n    def synchronize(self):\n        for meter in self.meters:\n            meter.synchronize()\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \"{:\" + str(num_digits) + \"d}\"\n        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n\n\ndef get_resume_checkpoint(checkpoint_save_dir):\n\n    if not g_pathmgr.isdir(checkpoint_save_dir):\n        return None\n    ckpt_file = os.path.join(checkpoint_save_dir, \"checkpoint.pt\")\n    if not g_pathmgr.isfile(ckpt_file):\n        return None\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_215-265"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        t = t.tolist()\n        self.sum = int(t[0])\n        self.count = t[1]\n        self.avg = self.sum / self.count if self.count > 0 else np.nan\n        self._allow_updates = False\n\n    def __str__(self):\n        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        logging.info(\"\\t\".join(entries))\n\n    def synchronize(self):\n        for meter in self.meters:\n            meter.synchronize()\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \"{:\" + str(num_digits) + \"d}\"\n        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n\n\ndef get_resume_checkpoint(checkpoint_save_dir):\n\n    if not g_pathmgr.isdir(checkpoint_save_dir):\n        return None\n    ckpt_file = os.path.join(checkpoint_save_dir, \"checkpoint.pt\")\n    if not g_pathmgr.isfile(ckpt_file):\n        return None\n\n    return ckpt_file\n\n\n# TODO: Move this to a separate logging file.\n\n\ndef setup_logging(name, output_dir=None, rank=0):\n    \"\"\"\n    Setup various logging streams: stdout and file handlers.\n    For file handlers, we only setup for the master gpu.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_225-275"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        logging.info(\"\\t\".join(entries))\n\n    def synchronize(self):\n        for meter in self.meters:\n            meter.synchronize()\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \"{:\" + str(num_digits) + \"d}\"\n        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n\n\ndef get_resume_checkpoint(checkpoint_save_dir):\n\n    if not g_pathmgr.isdir(checkpoint_save_dir):\n        return None\n    ckpt_file = os.path.join(checkpoint_save_dir, \"checkpoint.pt\")\n    if not g_pathmgr.isfile(ckpt_file):\n        return None\n\n    return ckpt_file\n\n\n# TODO: Move this to a separate logging file.\n\n\ndef setup_logging(name, output_dir=None, rank=0):\n    \"\"\"\n    Setup various logging streams: stdout and file handlers.\n    For file handlers, we only setup for the master gpu.\n    \"\"\"\n    # get the filename if we want to log to the file as well\n    log_filename = None\n    if output_dir:\n        makedir(output_dir)\n        if rank == 0:\n            log_filename = f\"{output_dir}/log.txt\"\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\nAST=Module(ClassDef(Name(Load)FunctionDef(arguments(argargargargConstant)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)List(BinOp(Attribute(Name(Load)Load)AddCall(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Load))AugAssign(Name(Store)AddListComp(Call(Name(Load)Name(Load))comprehension(Name(Store)Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(ConstantLoad)Name(Load)))))FunctionDef(arguments(arg)For(Name(Store)Attribute(Name(Load)Load)Expr(Call(Attribute(Name(Load)Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Call(Name(Load)BinOp(Name(Load)FloorDivConstant))))Assign(Name(Store)BinOp(BinOp(ConstantAddCall(Name(Load)Name(Load)))AddConstant))Return(BinOp(BinOp(BinOp(BinOp(ConstantAddName(Load))AddConstant)AddCall(Attribute(Name(Load)Load)Name(Load)))AddConstant))))FunctionDef(arguments(arg)If(UnaryOp(NotCall(Attribute(Name(Load)Load)Name(Load)))Return(Constant))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Constant))If(UnaryOp(NotCall(Attribute(Name(Load)Load)Name(Load)))Return(Constant))Return(Name(Load)))FunctionDef(arguments(argargargConstantConstant)Expr(Constant)Assign(Name(Store)Constant)If(Name(Load)Expr(Call(Name(Load)Name(Load)))If(Compare(Name(Load)EqConstant)Assign(Name(Store)JoinedStr(FormattedValue(Name(Load))Constant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_235-285"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        logging.info(\"\\t\".join(entries))\n\n    def synchronize(self):\n        for meter in self.meters:\n            meter.synchronize()\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \"{:\" + str(num_digits) + \"d}\"\n        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n\n\ndef get_resume_checkpoint(checkpoint_save_dir):\n\n    if not g_pathmgr.isdir(checkpoint_save_dir):\n        return None\n    ckpt_file = os.path.join(checkpoint_save_dir, \"checkpoint.pt\")\n    if not g_pathmgr.isfile(ckpt_file):\n        return None\n\n    return ckpt_file\n\n\n# TODO: Move this to a separate logging file.\n\n\ndef setup_logging(name, output_dir=None, rank=0):\n    \"\"\"\n    Setup various logging streams: stdout and file handlers.\n    For file handlers, we only setup for the master gpu.\n    \"\"\"\n    # get the filename if we want to log to the file as well\n    log_filename = None\n    if output_dir:\n        makedir(output_dir)\n        if rank == 0:\n            log_filename = f\"{output_dir}/log.txt\"\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    # create formatter\n    FORMAT = \"%(levelname)s %(asctime)s %(filename)s:%(lineno)4d: %(message)s\"\n    formatter = logging.Formatter(FORMAT)\n\n    # clean up any pre-existing handlers\n    for h in logger.handlers:\n        logger.removeHandler(h)\n    logger.root.handlers = []\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_245-295"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_resume_checkpoint(checkpoint_save_dir):\n\n    if not g_pathmgr.isdir(checkpoint_save_dir):\n        return None\n    ckpt_file = os.path.join(checkpoint_save_dir, \"checkpoint.pt\")\n    if not g_pathmgr.isfile(ckpt_file):\n        return None\n\n    return ckpt_file\n\n\n# TODO: Move this to a separate logging file.\n\n\ndef setup_logging(name, output_dir=None, rank=0):\n    \"\"\"\n    Setup various logging streams: stdout and file handlers.\n    For file handlers, we only setup for the master gpu.\n    \"\"\"\n    # get the filename if we want to log to the file as well\n    log_filename = None\n    if output_dir:\n        makedir(output_dir)\n        if rank == 0:\n            log_filename = f\"{output_dir}/log.txt\"\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    # create formatter\n    FORMAT = \"%(levelname)s %(asctime)s %(filename)s:%(lineno)4d: %(message)s\"\n    formatter = logging.Formatter(FORMAT)\n\n    # clean up any pre-existing handlers\n    for h in logger.handlers:\n        logger.removeHandler(h)\n    logger.root.handlers = []\n\n    # setup the console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # we log to file as well if user wants\n    if log_filename and rank == 0:\n        file_handler = logging.StreamHandler(_cached_log_stream(log_filename))\n        file_handler.setLevel(logging.DEBUG)\n\nAST=Module(FunctionDef(arguments(arg)If(UnaryOp(NotCall(Attribute(Name(Load)Load)Name(Load)))Return(Constant))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Constant))If(UnaryOp(NotCall(Attribute(Name(Load)Load)Name(Load)))Return(Constant))Return(Name(Load)))FunctionDef(arguments(argargargConstantConstant)Expr(Constant)Assign(Name(Store)Constant)If(Name(Load)Expr(Call(Name(Load)Name(Load)))If(Compare(Name(Load)EqConstant)Assign(Name(Store)JoinedStr(FormattedValue(Name(Load))Constant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Assign(Name(Store)Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))For(Name(Store)Attribute(Name(Load)Load)Expr(Call(Attribute(Name(Load)Load)Name(Load))))Assign(Attribute(Attribute(Name(Load)Load)Store)List(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)))If(BoolOp(AndName(Load)Compare(Name(Load)EqConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))))Expr(Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_255-305"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return ckpt_file\n\n\n# TODO: Move this to a separate logging file.\n\n\ndef setup_logging(name, output_dir=None, rank=0):\n    \"\"\"\n    Setup various logging streams: stdout and file handlers.\n    For file handlers, we only setup for the master gpu.\n    \"\"\"\n    # get the filename if we want to log to the file as well\n    log_filename = None\n    if output_dir:\n        makedir(output_dir)\n        if rank == 0:\n            log_filename = f\"{output_dir}/log.txt\"\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    # create formatter\n    FORMAT = \"%(levelname)s %(asctime)s %(filename)s:%(lineno)4d: %(message)s\"\n    formatter = logging.Formatter(FORMAT)\n\n    # clean up any pre-existing handlers\n    for h in logger.handlers:\n        logger.removeHandler(h)\n    logger.root.handlers = []\n\n    # setup the console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # we log to file as well if user wants\n    if log_filename and rank == 0:\n        file_handler = logging.StreamHandler(_cached_log_stream(log_filename))\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    logging.root = logger\n\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_265-315"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    # get the filename if we want to log to the file as well\n    log_filename = None\n    if output_dir:\n        makedir(output_dir)\n        if rank == 0:\n            log_filename = f\"{output_dir}/log.txt\"\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    # create formatter\n    FORMAT = \"%(levelname)s %(asctime)s %(filename)s:%(lineno)4d: %(message)s\"\n    formatter = logging.Formatter(FORMAT)\n\n    # clean up any pre-existing handlers\n    for h in logger.handlers:\n        logger.removeHandler(h)\n    logger.root.handlers = []\n\n    # setup the console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # we log to file as well if user wants\n    if log_filename and rank == 0:\n        file_handler = logging.StreamHandler(_cached_log_stream(log_filename))\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    logging.root = logger\n\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):\n    # we tune the buffering value so that the logs are updated\n    # frequently.\n    log_buffer_kb = 10 * 1024  # 10KB\n    io = g_pathmgr.open(filename, mode=\"a\", buffering=log_buffer_kb)\n    atexit.register(io.close)\n    return io\n\n\ndef shutdown_logging():\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_275-325"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 331, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    # create formatter\n    FORMAT = \"%(levelname)s %(asctime)s %(filename)s:%(lineno)4d: %(message)s\"\n    formatter = logging.Formatter(FORMAT)\n\n    # clean up any pre-existing handlers\n    for h in logger.handlers:\n        logger.removeHandler(h)\n    logger.root.handlers = []\n\n    # setup the console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # we log to file as well if user wants\n    if log_filename and rank == 0:\n        file_handler = logging.StreamHandler(_cached_log_stream(log_filename))\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    logging.root = logger\n\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):\n    # we tune the buffering value so that the logs are updated\n    # frequently.\n    log_buffer_kb = 10 * 1024  # 10KB\n    io = g_pathmgr.open(filename, mode=\"a\", buffering=log_buffer_kb)\n    atexit.register(io.close)\n    return io\n\n\ndef shutdown_logging():\n    \"\"\"\n    After training is done, we ensure to shut down all the logger streams.\n    \"\"\"\n    logging.info(\"Shutting down loggers...\")\n    handlers = logging.root.handlers\n    for handler in handlers:\n        handler.close()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_285-331"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 331, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    # setup the console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # we log to file as well if user wants\n    if log_filename and rank == 0:\n        file_handler = logging.StreamHandler(_cached_log_stream(log_filename))\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    logging.root = logger\n\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):\n    # we tune the buffering value so that the logs are updated\n    # frequently.\n    log_buffer_kb = 10 * 1024  # 10KB\n    io = g_pathmgr.open(filename, mode=\"a\", buffering=log_buffer_kb)\n    atexit.register(io.close)\n    return io\n\n\ndef shutdown_logging():\n    \"\"\"\n    After training is done, we ensure to shut down all the logger streams.\n    \"\"\"\n    logging.info(\"Shutting down loggers...\")\n    handlers = logging.root.handlers\n    for handler in handlers:\n        handler.close()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_295-331"}
{"title": "facebookresearch_omnivore-omnivision-utils-train.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivision", "utils", "train.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 331, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    logging.root = logger\n\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):\n    # we tune the buffering value so that the logs are updated\n    # frequently.\n    log_buffer_kb = 10 * 1024  # 10KB\n    io = g_pathmgr.open(filename, mode=\"a\", buffering=log_buffer_kb)\n    atexit.register(io.close)\n    return io\n\n\ndef shutdown_logging():\n    \"\"\"\n    After training is done, we ensure to shut down all the logger streams.\n    \"\"\"\n    logging.info(\"Shutting down loggers...\")\n    handlers = logging.root.handlers\n    for handler in handlers:\n        handler.close()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivision-utils-train.py_305-331"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nDownload the weights in ./checkpoints and ImageNet 1K ID to class mappings beforehand\nwget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nwget https://dl.fbaipublicfiles.com/omnivore/sunrgbd_classnames.json\n\"\"\"\nimport json\nfrom pathlib import Path\n\nimport cog\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom models.omnivore_model import get_all_heads, OmnivoreModel\nfrom models.swin_transformer_3d import SwinTransformer3D\nfrom PIL import Image\nfrom transforms import DepthNorm\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_0-25"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nDownload the weights in ./checkpoints and ImageNet 1K ID to class mappings beforehand\nwget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nwget https://dl.fbaipublicfiles.com/omnivore/sunrgbd_classnames.json\n\"\"\"\nimport json\nfrom pathlib import Path\n\nimport cog\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom models.omnivore_model import get_all_heads, OmnivoreModel\nfrom models.swin_transformer_3d import SwinTransformer3D\nfrom PIL import Image\nfrom transforms import DepthNorm\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"checkpoints/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"checkpoints/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"checkpoints/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"checkpoints/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"checkpoints/swinL_In21k_checkpoint.torch\",\n}\n\n\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_0-35"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nDownload the weights in ./checkpoints and ImageNet 1K ID to class mappings beforehand\nwget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nwget https://dl.fbaipublicfiles.com/omnivore/sunrgbd_classnames.json\n\"\"\"\nimport json\nfrom pathlib import Path\n\nimport cog\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom models.omnivore_model import get_all_heads, OmnivoreModel\nfrom models.swin_transformer_3d import SwinTransformer3D\nfrom PIL import Image\nfrom transforms import DepthNorm\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"checkpoints/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"checkpoints/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"checkpoints/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"checkpoints/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"checkpoints/swinL_In21k_checkpoint.torch\",\n}\n\n\nclass Predictor(cog.Predictor):\n    def setup(self):\n        self.device = \"cuda:0\"\n        self.models = {\n            \"omnivore_swinT\": omnivore_swinT(),\n            \"omnivore_swinS\": omnivore_swinS(),\n            \"omnivore_swinB\": omnivore_swinB(),\n            \"omnivore_swinB_in21k\": omnivore_swinB(\n                checkpoint_name=\"omnivore_swinB_in21k\"\n            ),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_0-45"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nDownload the weights in ./checkpoints and ImageNet 1K ID to class mappings beforehand\nwget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nwget https://dl.fbaipublicfiles.com/omnivore/sunrgbd_classnames.json\n\"\"\"\nimport json\nfrom pathlib import Path\n\nimport cog\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom models.omnivore_model import get_all_heads, OmnivoreModel\nfrom models.swin_transformer_3d import SwinTransformer3D\nfrom PIL import Image\nfrom transforms import DepthNorm\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"checkpoints/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"checkpoints/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"checkpoints/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"checkpoints/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"checkpoints/swinL_In21k_checkpoint.torch\",\n}\n\n\nclass Predictor(cog.Predictor):\n    def setup(self):\n        self.device = \"cuda:0\"\n        self.models = {\n            \"omnivore_swinT\": omnivore_swinT(),\n            \"omnivore_swinS\": omnivore_swinS(),\n            \"omnivore_swinB\": omnivore_swinB(),\n            \"omnivore_swinB_in21k\": omnivore_swinB(\n                checkpoint_name=\"omnivore_swinB_in21k\"\n            ),\n            \"omnivore_swinL_in21k\": omnivore_swinL(\n                checkpoint_name=\"omnivore_swinL_in21k\"\n            ),\n        }\n\n        with open(\"imagenet_class_index.json\", \"r\") as f:\n            imagenet_classnames = json.load(f)\n\n        # Create an id to label name mapping\n        self.imagenet_id_to_classname = {}\n\nAST=Module(Expr(Constant)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)Dict(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Attribute(Name(Load)Store)Constant)Assign(Attribute(Name(Load)Store)Dict(ConstantConstantConstantConstantConstantCall(Name(Load))Call(Name(Load))Call(Name(Load))Call(Name(Load)keyword(Constant))Call(Name(Load)keyword(Constant))))With(withitem(Call(Name(Load)ConstantConstant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))Assign(Attribute(Name(Load)Store)Dict))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_5-55"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import cog\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom models.omnivore_model import get_all_heads, OmnivoreModel\nfrom models.swin_transformer_3d import SwinTransformer3D\nfrom PIL import Image\nfrom transforms import DepthNorm\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"checkpoints/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"checkpoints/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"checkpoints/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"checkpoints/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"checkpoints/swinL_In21k_checkpoint.torch\",\n}\n\n\nclass Predictor(cog.Predictor):\n    def setup(self):\n        self.device = \"cuda:0\"\n        self.models = {\n            \"omnivore_swinT\": omnivore_swinT(),\n            \"omnivore_swinS\": omnivore_swinS(),\n            \"omnivore_swinB\": omnivore_swinB(),\n            \"omnivore_swinB_in21k\": omnivore_swinB(\n                checkpoint_name=\"omnivore_swinB_in21k\"\n            ),\n            \"omnivore_swinL_in21k\": omnivore_swinL(\n                checkpoint_name=\"omnivore_swinL_in21k\"\n            ),\n        }\n\n        with open(\"imagenet_class_index.json\", \"r\") as f:\n            imagenet_classnames = json.load(f)\n\n        # Create an id to label name mapping\n        self.imagenet_id_to_classname = {}\n        for k, v in imagenet_classnames.items():\n            self.imagenet_id_to_classname[k] = v[1]\n\n        with open(\"sunrgbd_classnames.json\", \"r\") as f:\n            self.sunrgbd_id_to_classname = json.load(f)\n\n        self.image_transform = T.Compose(\n            [\n                T.Resize(224),\n                T.CenterCrop(224),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_15-65"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"checkpoints/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"checkpoints/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"checkpoints/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"checkpoints/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"checkpoints/swinL_In21k_checkpoint.torch\",\n}\n\n\nclass Predictor(cog.Predictor):\n    def setup(self):\n        self.device = \"cuda:0\"\n        self.models = {\n            \"omnivore_swinT\": omnivore_swinT(),\n            \"omnivore_swinS\": omnivore_swinS(),\n            \"omnivore_swinB\": omnivore_swinB(),\n            \"omnivore_swinB_in21k\": omnivore_swinB(\n                checkpoint_name=\"omnivore_swinB_in21k\"\n            ),\n            \"omnivore_swinL_in21k\": omnivore_swinL(\n                checkpoint_name=\"omnivore_swinL_in21k\"\n            ),\n        }\n\n        with open(\"imagenet_class_index.json\", \"r\") as f:\n            imagenet_classnames = json.load(f)\n\n        # Create an id to label name mapping\n        self.imagenet_id_to_classname = {}\n        for k, v in imagenet_classnames.items():\n            self.imagenet_id_to_classname[k] = v[1]\n\n        with open(\"sunrgbd_classnames.json\", \"r\") as f:\n            self.sunrgbd_id_to_classname = json.load(f)\n\n        self.image_transform = T.Compose(\n            [\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        self.rgbd_transform = T.Compose(\n            [\n                DepthNorm(max_depth=75.0, clamp_max_before_scale=True),\n                T.Resize(224),\n                T.CenterCrop(224),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_25-75"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class Predictor(cog.Predictor):\n    def setup(self):\n        self.device = \"cuda:0\"\n        self.models = {\n            \"omnivore_swinT\": omnivore_swinT(),\n            \"omnivore_swinS\": omnivore_swinS(),\n            \"omnivore_swinB\": omnivore_swinB(),\n            \"omnivore_swinB_in21k\": omnivore_swinB(\n                checkpoint_name=\"omnivore_swinB_in21k\"\n            ),\n            \"omnivore_swinL_in21k\": omnivore_swinL(\n                checkpoint_name=\"omnivore_swinL_in21k\"\n            ),\n        }\n\n        with open(\"imagenet_class_index.json\", \"r\") as f:\n            imagenet_classnames = json.load(f)\n\n        # Create an id to label name mapping\n        self.imagenet_id_to_classname = {}\n        for k, v in imagenet_classnames.items():\n            self.imagenet_id_to_classname[k] = v[1]\n\n        with open(\"sunrgbd_classnames.json\", \"r\") as f:\n            self.sunrgbd_id_to_classname = json.load(f)\n\n        self.image_transform = T.Compose(\n            [\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        self.rgbd_transform = T.Compose(\n            [\n                DepthNorm(max_depth=75.0, clamp_max_before_scale=True),\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.Normalize(\n                    mean=[0.485, 0.456, 0.406, 0.0418],\n                    std=[0.229, 0.224, 0.225, 0.0295],\n                ),\n            ]\n        )\n\n    @cog.input(\n        \"image\",\n        type=Path,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_35-85"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            \"omnivore_swinL_in21k\": omnivore_swinL(\n                checkpoint_name=\"omnivore_swinL_in21k\"\n            ),\n        }\n\n        with open(\"imagenet_class_index.json\", \"r\") as f:\n            imagenet_classnames = json.load(f)\n\n        # Create an id to label name mapping\n        self.imagenet_id_to_classname = {}\n        for k, v in imagenet_classnames.items():\n            self.imagenet_id_to_classname[k] = v[1]\n\n        with open(\"sunrgbd_classnames.json\", \"r\") as f:\n            self.sunrgbd_id_to_classname = json.load(f)\n\n        self.image_transform = T.Compose(\n            [\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        self.rgbd_transform = T.Compose(\n            [\n                DepthNorm(max_depth=75.0, clamp_max_before_scale=True),\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.Normalize(\n                    mean=[0.485, 0.456, 0.406, 0.0418],\n                    std=[0.229, 0.224, 0.225, 0.0295],\n                ),\n            ]\n        )\n\n    @cog.input(\n        \"image\",\n        type=Path,\n        help=\"input image\",\n    )\n    @cog.input(\n        \"model_name\",\n        type=str,\n        default=\"omnivore_swinB\",\n        options=[\n            \"omnivore_swinB\",\n            \"omnivore_swinT\",\n            \"omnivore_swinS\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_45-95"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for k, v in imagenet_classnames.items():\n            self.imagenet_id_to_classname[k] = v[1]\n\n        with open(\"sunrgbd_classnames.json\", \"r\") as f:\n            self.sunrgbd_id_to_classname = json.load(f)\n\n        self.image_transform = T.Compose(\n            [\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        self.rgbd_transform = T.Compose(\n            [\n                DepthNorm(max_depth=75.0, clamp_max_before_scale=True),\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.Normalize(\n                    mean=[0.485, 0.456, 0.406, 0.0418],\n                    std=[0.229, 0.224, 0.225, 0.0295],\n                ),\n            ]\n        )\n\n    @cog.input(\n        \"image\",\n        type=Path,\n        help=\"input image\",\n    )\n    @cog.input(\n        \"model_name\",\n        type=str,\n        default=\"omnivore_swinB\",\n        options=[\n            \"omnivore_swinB\",\n            \"omnivore_swinT\",\n            \"omnivore_swinS\",\n            \"omnivore_swinB_in21k\",\n            \"omnivore_swinL_in21k\",\n        ],\n        help=\"Choose a model\",\n    )\n    @cog.input(\n        \"topk\",\n        type=int,\n        min=1,\n        max=10,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_55-105"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        self.rgbd_transform = T.Compose(\n            [\n                DepthNorm(max_depth=75.0, clamp_max_before_scale=True),\n                T.Resize(224),\n                T.CenterCrop(224),\n                T.Normalize(\n                    mean=[0.485, 0.456, 0.406, 0.0418],\n                    std=[0.229, 0.224, 0.225, 0.0295],\n                ),\n            ]\n        )\n\n    @cog.input(\n        \"image\",\n        type=Path,\n        help=\"input image\",\n    )\n    @cog.input(\n        \"model_name\",\n        type=str,\n        default=\"omnivore_swinB\",\n        options=[\n            \"omnivore_swinB\",\n            \"omnivore_swinT\",\n            \"omnivore_swinS\",\n            \"omnivore_swinB_in21k\",\n            \"omnivore_swinL_in21k\",\n        ],\n        help=\"Choose a model\",\n    )\n    @cog.input(\n        \"topk\",\n        type=int,\n        min=1,\n        max=10,\n        default=5,\n        help=\"Choose top k predictions to return.\",\n    )\n    def predict(self, image, model_name, topk):\n        model = self.models[model_name]\n        model.to(self.device)\n        model.eval()\n\n        image = Image.open(str(image)).convert(\"RGB\")\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_65-115"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                T.Normalize(\n                    mean=[0.485, 0.456, 0.406, 0.0418],\n                    std=[0.229, 0.224, 0.225, 0.0295],\n                ),\n            ]\n        )\n\n    @cog.input(\n        \"image\",\n        type=Path,\n        help=\"input image\",\n    )\n    @cog.input(\n        \"model_name\",\n        type=str,\n        default=\"omnivore_swinB\",\n        options=[\n            \"omnivore_swinB\",\n            \"omnivore_swinT\",\n            \"omnivore_swinS\",\n            \"omnivore_swinB_in21k\",\n            \"omnivore_swinL_in21k\",\n        ],\n        help=\"Choose a model\",\n    )\n    @cog.input(\n        \"topk\",\n        type=int,\n        min=1,\n        max=10,\n        default=5,\n        help=\"Choose top k predictions to return.\",\n    )\n    def predict(self, image, model_name, topk):\n        model = self.models[model_name]\n        model.to(self.device)\n        model.eval()\n\n        image = Image.open(str(image)).convert(\"RGB\")\n\n        # Inference with Images\n        image = self.image_transform(image)\n\n        # The model expects inputs of shape: B x C x T x H x W\n        image = image[None, :, None, ...]\n        image = image.to(self.device)\n        prediction = model(image, input_type=\"image\")\n        prediction = F.softmax(prediction, dim=1)\n        pred_classes = prediction.topk(k=5).indices\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_75-125"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        help=\"input image\",\n    )\n    @cog.input(\n        \"model_name\",\n        type=str,\n        default=\"omnivore_swinB\",\n        options=[\n            \"omnivore_swinB\",\n            \"omnivore_swinT\",\n            \"omnivore_swinS\",\n            \"omnivore_swinB_in21k\",\n            \"omnivore_swinL_in21k\",\n        ],\n        help=\"Choose a model\",\n    )\n    @cog.input(\n        \"topk\",\n        type=int,\n        min=1,\n        max=10,\n        default=5,\n        help=\"Choose top k predictions to return.\",\n    )\n    def predict(self, image, model_name, topk):\n        model = self.models[model_name]\n        model.to(self.device)\n        model.eval()\n\n        image = Image.open(str(image)).convert(\"RGB\")\n\n        # Inference with Images\n        image = self.image_transform(image)\n\n        # The model expects inputs of shape: B x C x T x H x W\n        image = image[None, :, None, ...]\n        image = image.to(self.device)\n        prediction = model(image, input_type=\"image\")\n        prediction = F.softmax(prediction, dim=1)\n        pred_classes = prediction.topk(k=5).indices\n\n        pred_class_names = [\n            self.imagenet_id_to_classname[str(i.item())] for i in pred_classes[0]\n        ]\n        return f\"Top {topk} predicted labels: %s\" % \", \".join(pred_class_names)\n\n\ndef omnivore_base(trunk, head_dim_in=1024, checkpoint_name=\"omnivore_swinB\"):\n\n    heads = get_all_heads(dim_in=head_dim_in)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_85-135"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            \"omnivore_swinB_in21k\",\n            \"omnivore_swinL_in21k\",\n        ],\n        help=\"Choose a model\",\n    )\n    @cog.input(\n        \"topk\",\n        type=int,\n        min=1,\n        max=10,\n        default=5,\n        help=\"Choose top k predictions to return.\",\n    )\n    def predict(self, image, model_name, topk):\n        model = self.models[model_name]\n        model.to(self.device)\n        model.eval()\n\n        image = Image.open(str(image)).convert(\"RGB\")\n\n        # Inference with Images\n        image = self.image_transform(image)\n\n        # The model expects inputs of shape: B x C x T x H x W\n        image = image[None, :, None, ...]\n        image = image.to(self.device)\n        prediction = model(image, input_type=\"image\")\n        prediction = F.softmax(prediction, dim=1)\n        pred_classes = prediction.topk(k=5).indices\n\n        pred_class_names = [\n            self.imagenet_id_to_classname[str(i.item())] for i in pred_classes[0]\n        ]\n        return f\"Top {topk} predicted labels: %s\" % \", \".join(pred_class_names)\n\n\ndef omnivore_base(trunk, head_dim_in=1024, checkpoint_name=\"omnivore_swinB\"):\n\n    heads = get_all_heads(dim_in=head_dim_in)\n\n    path = CHECKPOINT_PATHS[checkpoint_name]\n    # All models are loaded onto CPU by default\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    trunk.load_state_dict(checkpoint[\"trunk\"])\n\n    heads.load_state_dict(checkpoint[\"heads\"])\n\n    return OmnivoreModel(trunk=trunk, heads=heads)\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_95-145"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        default=5,\n        help=\"Choose top k predictions to return.\",\n    )\n    def predict(self, image, model_name, topk):\n        model = self.models[model_name]\n        model.to(self.device)\n        model.eval()\n\n        image = Image.open(str(image)).convert(\"RGB\")\n\n        # Inference with Images\n        image = self.image_transform(image)\n\n        # The model expects inputs of shape: B x C x T x H x W\n        image = image[None, :, None, ...]\n        image = image.to(self.device)\n        prediction = model(image, input_type=\"image\")\n        prediction = F.softmax(prediction, dim=1)\n        pred_classes = prediction.topk(k=5).indices\n\n        pred_class_names = [\n            self.imagenet_id_to_classname[str(i.item())] for i in pred_classes[0]\n        ]\n        return f\"Top {topk} predicted labels: %s\" % \", \".join(pred_class_names)\n\n\ndef omnivore_base(trunk, head_dim_in=1024, checkpoint_name=\"omnivore_swinB\"):\n\n    heads = get_all_heads(dim_in=head_dim_in)\n\n    path = CHECKPOINT_PATHS[checkpoint_name]\n    # All models are loaded onto CPU by default\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    trunk.load_state_dict(checkpoint[\"trunk\"])\n\n    heads.load_state_dict(checkpoint[\"heads\"])\n\n    return OmnivoreModel(trunk=trunk, heads=heads)\n\n\ndef omnivore_swinB(checkpoint_name=\"omnivore_swinB\"):\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_105-155"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # Inference with Images\n        image = self.image_transform(image)\n\n        # The model expects inputs of shape: B x C x T x H x W\n        image = image[None, :, None, ...]\n        image = image.to(self.device)\n        prediction = model(image, input_type=\"image\")\n        prediction = F.softmax(prediction, dim=1)\n        pred_classes = prediction.topk(k=5).indices\n\n        pred_class_names = [\n            self.imagenet_id_to_classname[str(i.item())] for i in pred_classes[0]\n        ]\n        return f\"Top {topk} predicted labels: %s\" % \", \".join(pred_class_names)\n\n\ndef omnivore_base(trunk, head_dim_in=1024, checkpoint_name=\"omnivore_swinB\"):\n\n    heads = get_all_heads(dim_in=head_dim_in)\n\n    path = CHECKPOINT_PATHS[checkpoint_name]\n    # All models are loaded onto CPU by default\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    trunk.load_state_dict(checkpoint[\"trunk\"])\n\n    heads.load_state_dict(checkpoint[\"heads\"])\n\n    return OmnivoreModel(trunk=trunk, heads=heads)\n\n\ndef omnivore_swinB(checkpoint_name=\"omnivore_swinB\"):\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        checkpoint_name=checkpoint_name,\n    )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_115-165"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        pred_class_names = [\n            self.imagenet_id_to_classname[str(i.item())] for i in pred_classes[0]\n        ]\n        return f\"Top {topk} predicted labels: %s\" % \", \".join(pred_class_names)\n\n\ndef omnivore_base(trunk, head_dim_in=1024, checkpoint_name=\"omnivore_swinB\"):\n\n    heads = get_all_heads(dim_in=head_dim_in)\n\n    path = CHECKPOINT_PATHS[checkpoint_name]\n    # All models are loaded onto CPU by default\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    trunk.load_state_dict(checkpoint[\"trunk\"])\n\n    heads.load_state_dict(checkpoint[\"heads\"])\n\n    return OmnivoreModel(trunk=trunk, heads=heads)\n\n\ndef omnivore_swinB(checkpoint_name=\"omnivore_swinB\"):\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinS():\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_125-175"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    path = CHECKPOINT_PATHS[checkpoint_name]\n    # All models are loaded onto CPU by default\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    trunk.load_state_dict(checkpoint[\"trunk\"])\n\n    heads.load_state_dict(checkpoint[\"heads\"])\n\n    return OmnivoreModel(trunk=trunk, heads=heads)\n\n\ndef omnivore_swinB(checkpoint_name=\"omnivore_swinB\"):\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinS():\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinS\",\n    )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_135-185"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def omnivore_swinB(checkpoint_name=\"omnivore_swinB\"):\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinS():\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT():\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_145-195"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinS():\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT():\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinT\",\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_155-205"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef omnivore_swinS():\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT():\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef omnivore_swinL(checkpoint_name=\"\"):\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_165-215"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT():\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef omnivore_swinL(checkpoint_name=\"\"):\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_175-225"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 228, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef omnivore_swinT():\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef omnivore_swinL(checkpoint_name=\"\"):\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1536,  # 192*8\n        checkpoint_name=checkpoint_name,\n    )\n\nAST=Module(FunctionDef(argumentsAssign(Name(Store)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(List(ConstantConstantConstantConstantLoad))keyword(List(ConstantConstantConstantConstantLoad))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)))Return(Call(Name(Load)keyword(Name(Load))keyword(Constant)keyword(Constant))))FunctionDef(arguments(argConstant)Assert(Compare(Name(Load)NotEqConstant)Constant)Assign(Name(Store)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(List(ConstantConstantConstantConstantLoad))keyword(List(ConstantConstantConstantConstantLoad))keyword(Tuple(ConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)))Return(Call(Name(Load)keyword(Name(Load))keyword(Constant)keyword(Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_185-228"}
{"title": "facebookresearch_omnivore-omnivore-predict.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "predict.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 228, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef omnivore_swinL(checkpoint_name=\"\"):\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n    )\n\n    return omnivore_base(\n        trunk=trunk,\n        head_dim_in=1536,  # 192*8\n        checkpoint_name=checkpoint_name,\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-predict.py_195-228"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport math\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\n\n\nclass DepthNorm(nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_0-25"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport math\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\n\n\nclass DepthNorm(nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_0-35"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport math\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\n\n\nclass DepthNorm(nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def forward(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_0-45"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# LICENSE file in the root directory of this source tree.\n\n\nimport math\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\n\n\nclass DepthNorm(nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def forward(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Expr(Call(Attribute(Call(Name(Load))Load)))If(Compare(Name(Load)LtConstant)Raise(Call(Name(Load)BinOp(ConstantModName(Load)))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Attribute(Name(Load)Load))If(Compare(Name(Load)NotEqConstant)Assign(Name(Store)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))))Raise(Call(Name(Load)Name(Load))))Assign(Name(Store)Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))Assign(Name(Store)Subscript(Name(Load)Tuple(Slice(ConstantConstant)ConstantLoad)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_5-55"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class DepthNorm(nn.Module):\n    \"\"\"\n    Normalize the depth channel: in an RGBD input of shape (4, H, W),\n    only the last channel is modified.\n    The depth channel is also clamped at 0.0. The Midas depth prediction\n    model outputs inverse depth maps - negative values correspond\n    to distances far away so can be clamped at 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def forward(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img\n\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Constant)Expr(Call(Attribute(Call(Name(Load))Load)))If(Compare(Name(Load)LtConstant)Raise(Call(Name(Load)BinOp(ConstantModName(Load)))))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg(Attribute(Name(Load)Load)))Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Attribute(Name(Load)Load))If(Compare(Name(Load)NotEqConstant)Assign(Name(Store)JoinedStr(ConstantFormattedValue(Attribute(Name(Load)Load))))Raise(Call(Name(Load)Name(Load))))Assign(Name(Store)Subscript(Name(Load)Tuple(Slice(Constant)ConstantLoad)Load))Assign(Name(Store)Subscript(Name(Load)Tuple(Slice(ConstantConstant)ConstantLoad)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))))If(Attribute(Name(Load)Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load)))))AugAssign(Name(Store)DivAttribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Name(Load)Name(Load)Load)keyword(Constant)))Return(Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_15-65"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self,\n        max_depth: float,\n        clamp_max_before_scale: bool = False,\n        min_depth: float = 0.01,\n    ):\n        \"\"\"\n        Args:\n            max_depth (float): The max value of depth for the dataset\n            clamp_max (bool): Whether to clamp to max_depth or to divide by max_depth\n        \"\"\"\n        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def forward(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, stride: int = 8):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_25-75"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        super().__init__()\n        if max_depth < 0.0:\n            raise ValueError(\"max_depth must be > 0; got %.2f\" % max_depth)\n        self.max_depth = max_depth\n        self.clamp_max_before_scale = clamp_max_before_scale\n        self.min_depth = min_depth\n\n    def forward(self, image: torch.Tensor):\n        C, H, W = image.shape\n        if C != 4:\n            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, stride: int = 8):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(0, video.size(1) - self.frames + 1, self.stride):\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_35-85"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            err_msg = (\n                f\"This transform is for 4 channel RGBD input only; got {image.shape}\"\n            )\n            raise ValueError(err_msg)\n        color_img = image[:3, ...]  # (3, H, W)\n        depth_img = image[3:4, ...]  # (1, H, W)\n\n        # Clamp to 0.0 to prevent negative depth values\n        depth_img = depth_img.clamp(min=self.min_depth)\n\n        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, stride: int = 8):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(0, video.size(1) - self.frames + 1, self.stride):\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_45-95"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        # divide by max_depth\n        if self.clamp_max_before_scale:\n            depth_img = depth_img.clamp(max=self.max_depth)\n\n        depth_img /= self.max_depth\n\n        img = torch.cat([color_img, depth_img], dim=0)\n        return img\n\n\nclass TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, stride: int = 8):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(0, video.size(1) - self.frames + 1, self.stride):\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_55-105"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class TemporalCrop(nn.Module):\n    \"\"\"\n    Convert the video into smaller clips temporally.\n    \"\"\"\n\n    def __init__(self, frames_per_clip: int = 8, stride: int = 8):\n        super().__init__()\n        self.frames = frames_per_clip\n        self.stride = stride\n\n    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(0, video.size(1) - self.frames + 1, self.stride):\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assert(Compare(Attribute(Name(Load)Load)EqConstant)Constant)Assign(Name(Store)List(Load))For(Name(Store)Call(Name(Load)ConstantBinOp(BinOp(Call(Attribute(Name(Load)Load)Constant)SubAttribute(Name(Load)Load))AddConstant)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)Tuple(SliceSlice(Name(Load)BinOp(Name(Load)AddAttribute(Name(Load)Load)))ConstantLoad)Load))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argarg(Name(Load))arg(Name(Load))ConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)List(ConstantLoad))Raise(Call(Name(Load)Constant)))))FunctionDef(arguments(argarg(Subscript(Name(Load)Attribute(Name(Load)Load)Load)))Expr(Constant)Assert(Call(Name(Load)Name(Load)Name(Load))Constant))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_65-115"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def forward(self, video):\n        assert video.ndim == 4, \"Must be (C, T, H, W)\"\n        res = []\n        for start in range(0, video.size(1) - self.frames + 1, self.stride):\n            res.append(video[:, start : start + self.frames, ...])\n        return res\n\n\nclass SpatialCrop(nn.Module):\n    \"\"\"\n    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        return res\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_75-125"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Convert the video into 3 smaller clips spatially. Must be used after the\n        temporal crops to get spatial crops, and should be used with\n        -2 in the spatial crop at the slowfast augmentation stage (so full\n        frames are passed in here). Will return a larger list with the\n        3x spatial crops as well. It's useful for 3x4 testing (eg in SwinT)\n        or 3x10 testing in SlowFast etc.\n    \"\"\"\n\n    def __init__(self, crop_size: int = 224, num_crops: int = 3):\n        super().__init__()\n        self.crop_size = crop_size\n        if num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        return res\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_85-135"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.crop_size = crop_size\n        if num_crops == 3:\n            self.crops_to_ext = [0, 1, 2]\n        elif num_crops == 1:\n            self.crops_to_ext = [1]\n        else:\n            raise NotImplementedError(\n                \"Nothing else supported yet, \"\n                \"slowfast only takes 0, 1, 2 as arguments\"\n            )\n\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        return res\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_95-145"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def forward(self, videos: Sequence[torch.Tensor]):\n        \"\"\"\n        Args:\n            videos: A list of C, T, H, W videos.\n        Returns:\n            videos: A list with 3x the number of elements. Each video converted\n                to C, T, H', W' by spatial cropping.\n        \"\"\"\n        assert isinstance(videos, list), \"Must be a list of videos after temporal crops\"\n        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        return res\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_105-155"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        assert all([video.ndim == 4 for video in videos]), \"Must be (C,T,H,W)\"\n        res = []\n        for video in videos:\n            for spatial_idx in self.crops_to_ext:\n                res.append(uniform_crop(video, self.crop_size, spatial_idx)[0])\n        return res\n\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_115-165"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_125-175"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes\n\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_135-185"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_145-195"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 196, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_155-196"}
{"title": "facebookresearch_omnivore-omnivore-transforms.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "transforms.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 196, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n    cropped_boxes = crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-transforms.py_165-196"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Any, Optional, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\nfrom omnivision.models.swin_transformer import SwinTransformer3D\n\n\ndef get_all_heads(dim_in: int = 1024) -> nn.Module:\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in),\n            \"rgbd\": get_sunrgbd_head(dim_in),\n            \"video\": get_kinetics_head(dim_in),\n        }\n    )\n\nAST=Module(ImportFrom(aliasaliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Name(Load))Call(Name(Load)Name(Load))Call(Name(Load)Name(Load)))))Attribute(Name(Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_0-25"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Any, Optional, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\nfrom omnivision.models.swin_transformer import SwinTransformer3D\n\n\ndef get_all_heads(dim_in: int = 1024) -> nn.Module:\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in),\n            \"rgbd\": get_sunrgbd_head(dim_in),\n            \"video\": get_kinetics_head(dim_in),\n        }\n    )\n    return heads\n\n\ndef get_imagenet_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=1000, bias=True)\n    return head\n\n\ndef get_sunrgbd_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=19, bias=True)\n\nAST=Module(ImportFrom(aliasaliasalias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Name(Load))Call(Name(Load)Name(Load))Call(Name(Load)Name(Load)))))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)))Attribute(Name(Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_0-35"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Any, Optional, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\nfrom omnivision.models.swin_transformer import SwinTransformer3D\n\n\ndef get_all_heads(dim_in: int = 1024) -> nn.Module:\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in),\n            \"rgbd\": get_sunrgbd_head(dim_in),\n            \"video\": get_kinetics_head(dim_in),\n        }\n    )\n    return heads\n\n\ndef get_imagenet_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=1000, bias=True)\n    return head\n\n\ndef get_sunrgbd_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=19, bias=True)\n    return head\n\n\ndef get_kinetics_head(dim_in: int = 1024, num_classes: int = 400) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=num_classes, bias=True)\n    return nn.Sequential(nn.Dropout(p=0.5), head)\n\n\nclass OmnivoreModel(nn.Module):\n    def __init__(self, trunk: nn.Module, heads: Union[nn.ModuleDict, nn.Module]):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_0-45"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# LICENSE file in the root directory of this source tree.\n\n\nfrom typing import Any, Optional, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\nfrom omnivision.models.swin_transformer import SwinTransformer3D\n\n\ndef get_all_heads(dim_in: int = 1024) -> nn.Module:\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in),\n            \"rgbd\": get_sunrgbd_head(dim_in),\n            \"video\": get_kinetics_head(dim_in),\n        }\n    )\n    return heads\n\n\ndef get_imagenet_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=1000, bias=True)\n    return head\n\n\ndef get_sunrgbd_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=19, bias=True)\n    return head\n\n\ndef get_kinetics_head(dim_in: int = 1024, num_classes: int = 400) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=num_classes, bias=True)\n    return nn.Sequential(nn.Dropout(p=0.5), head)\n\n\nclass OmnivoreModel(nn.Module):\n    def __init__(self, trunk: nn.Module, heads: Union[nn.ModuleDict, nn.Module]):\n        super().__init__()\n        self.trunk = trunk\n        self.heads = heads\n        self.types = [\"image\", \"video\", \"rgbd\"]\n        self.multimodal_model = False\n        if isinstance(heads, nn.ModuleDict):\n            self.multimodal_model = True\n            assert all([n in heads for n in self.types]), \"All heads must be provided\"\n\n    def forward(self, x: torch.Tensor, input_type: Optional[str] = None):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_5-55"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_all_heads(dim_in: int = 1024) -> nn.Module:\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in),\n            \"rgbd\": get_sunrgbd_head(dim_in),\n            \"video\": get_kinetics_head(dim_in),\n        }\n    )\n    return heads\n\n\ndef get_imagenet_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=1000, bias=True)\n    return head\n\n\ndef get_sunrgbd_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=19, bias=True)\n    return head\n\n\ndef get_kinetics_head(dim_in: int = 1024, num_classes: int = 400) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=num_classes, bias=True)\n    return nn.Sequential(nn.Dropout(p=0.5), head)\n\n\nclass OmnivoreModel(nn.Module):\n    def __init__(self, trunk: nn.Module, heads: Union[nn.ModuleDict, nn.Module]):\n        super().__init__()\n        self.trunk = trunk\n        self.heads = heads\n        self.types = [\"image\", \"video\", \"rgbd\"]\n        self.multimodal_model = False\n        if isinstance(heads, nn.ModuleDict):\n            self.multimodal_model = True\n            assert all([n in heads for n in self.types]), \"All heads must be provided\"\n\n    def forward(self, x: torch.Tensor, input_type: Optional[str] = None):\n        \"\"\"\n        Args:\n            x: input to the model of shape 1 x C x T x H x W\n            input_type: Optional[str] one of [\"image\", \"video\", \"rgbd\"]\n                if self.multimodal_model is True\n        Returns:\n            preds: tensor of shape (1, num_classes)\n        \"\"\"\n        assert x.ndim == 5\n        features = self.trunk(x)\n\nAST=Module(FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Dict(ConstantConstantConstantCall(Name(Load)Name(Load))Call(Name(Load)Name(Load))Call(Name(Load)Name(Load)))))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)keyword(Constant)))Return(Name(Load))Attribute(Name(Load)Load))FunctionDef(arguments(arg(Name(Load))arg(Name(Load))ConstantConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Name(Load))keyword(Constant)))Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant))Name(Load)))Attribute(Name(Load)Load))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Tuple(Attribute(Name(Load)Load)Attribute(Name(Load)Load)Load)Load)))Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)List(ConstantConstantConstantLoad))Assign(Attribute(Name(Load)Store)Constant)If(Call(Name(Load)Name(Load)Attribute(Name(Load)Load))Assign(Attribute(Name(Load)Store)Constant)Assert(Call(Name(Load)ListComp(Compare(Name(Load)InName(Load))comprehension(Name(Store)Attribute(Name(Load)Load))))Constant)))FunctionDef(arguments(argarg(Attribute(Name(Load)Load))arg(Subscript(Name(Load)Name(Load)Load))Constant)Expr(Constant)Assert(Compare(Attribute(Name(Load)Load)EqConstant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_15-65"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return heads\n\n\ndef get_imagenet_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=1000, bias=True)\n    return head\n\n\ndef get_sunrgbd_head(dim_in: int = 1024) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=19, bias=True)\n    return head\n\n\ndef get_kinetics_head(dim_in: int = 1024, num_classes: int = 400) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=num_classes, bias=True)\n    return nn.Sequential(nn.Dropout(p=0.5), head)\n\n\nclass OmnivoreModel(nn.Module):\n    def __init__(self, trunk: nn.Module, heads: Union[nn.ModuleDict, nn.Module]):\n        super().__init__()\n        self.trunk = trunk\n        self.heads = heads\n        self.types = [\"image\", \"video\", \"rgbd\"]\n        self.multimodal_model = False\n        if isinstance(heads, nn.ModuleDict):\n            self.multimodal_model = True\n            assert all([n in heads for n in self.types]), \"All heads must be provided\"\n\n    def forward(self, x: torch.Tensor, input_type: Optional[str] = None):\n        \"\"\"\n        Args:\n            x: input to the model of shape 1 x C x T x H x W\n            input_type: Optional[str] one of [\"image\", \"video\", \"rgbd\"]\n                if self.multimodal_model is True\n        Returns:\n            preds: tensor of shape (1, num_classes)\n        \"\"\"\n        assert x.ndim == 5\n        features = self.trunk(x)\n        head = self.heads\n        if self.multimodal_model:\n            assert input_type in self.types, \"unsupported input type\"\n            head = head[input_type]\n        return head(features)\n\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinS_checkpoint.torch\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_25-75"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return head\n\n\ndef get_kinetics_head(dim_in: int = 1024, num_classes: int = 400) -> nn.Module:\n    head = nn.Linear(in_features=dim_in, out_features=num_classes, bias=True)\n    return nn.Sequential(nn.Dropout(p=0.5), head)\n\n\nclass OmnivoreModel(nn.Module):\n    def __init__(self, trunk: nn.Module, heads: Union[nn.ModuleDict, nn.Module]):\n        super().__init__()\n        self.trunk = trunk\n        self.heads = heads\n        self.types = [\"image\", \"video\", \"rgbd\"]\n        self.multimodal_model = False\n        if isinstance(heads, nn.ModuleDict):\n            self.multimodal_model = True\n            assert all([n in heads for n in self.types]), \"All heads must be provided\"\n\n    def forward(self, x: torch.Tensor, input_type: Optional[str] = None):\n        \"\"\"\n        Args:\n            x: input to the model of shape 1 x C x T x H x W\n            input_type: Optional[str] one of [\"image\", \"video\", \"rgbd\"]\n                if self.multimodal_model is True\n        Returns:\n            preds: tensor of shape (1, num_classes)\n        \"\"\"\n        assert x.ndim == 5\n        features = self.trunk(x)\n        head = self.heads\n        if self.multimodal_model:\n            assert input_type in self.types, \"unsupported input type\"\n            head = head[input_type]\n        return head(features)\n\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinL_In21k_checkpoint.torch\",\n    \"omnivore_swinB_epic\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_epic_checkpoint.torch\",\n}\n\n\ndef _omnivore_base(\n    trunk: nn.Module,\n    heads: Optional[Union[nn.Module, nn.ModuleDict]] = None,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_35-85"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        super().__init__()\n        self.trunk = trunk\n        self.heads = heads\n        self.types = [\"image\", \"video\", \"rgbd\"]\n        self.multimodal_model = False\n        if isinstance(heads, nn.ModuleDict):\n            self.multimodal_model = True\n            assert all([n in heads for n in self.types]), \"All heads must be provided\"\n\n    def forward(self, x: torch.Tensor, input_type: Optional[str] = None):\n        \"\"\"\n        Args:\n            x: input to the model of shape 1 x C x T x H x W\n            input_type: Optional[str] one of [\"image\", \"video\", \"rgbd\"]\n                if self.multimodal_model is True\n        Returns:\n            preds: tensor of shape (1, num_classes)\n        \"\"\"\n        assert x.ndim == 5\n        features = self.trunk(x)\n        head = self.heads\n        if self.multimodal_model:\n            assert input_type in self.types, \"unsupported input type\"\n            head = head[input_type]\n        return head(features)\n\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinL_In21k_checkpoint.torch\",\n    \"omnivore_swinB_epic\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_epic_checkpoint.torch\",\n}\n\n\ndef _omnivore_base(\n    trunk: nn.Module,\n    heads: Optional[Union[nn.Module, nn.ModuleDict]] = None,\n    head_dim_in: int = 1024,\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n) -> nn.Module:\n    \"\"\"\n    Load and initialize the specified Omnivore\n    model trunk (and optionally heads).\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_45-95"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        \"\"\"\n        Args:\n            x: input to the model of shape 1 x C x T x H x W\n            input_type: Optional[str] one of [\"image\", \"video\", \"rgbd\"]\n                if self.multimodal_model is True\n        Returns:\n            preds: tensor of shape (1, num_classes)\n        \"\"\"\n        assert x.ndim == 5\n        features = self.trunk(x)\n        head = self.heads\n        if self.multimodal_model:\n            assert input_type in self.types, \"unsupported input type\"\n            head = head[input_type]\n        return head(features)\n\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinL_In21k_checkpoint.torch\",\n    \"omnivore_swinB_epic\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_epic_checkpoint.torch\",\n}\n\n\ndef _omnivore_base(\n    trunk: nn.Module,\n    heads: Optional[Union[nn.Module, nn.ModuleDict]] = None,\n    head_dim_in: int = 1024,\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n) -> nn.Module:\n    \"\"\"\n    Load and initialize the specified Omnivore\n    model trunk (and optionally heads).\n\n    Args:\n        trunk: nn.Module of the SwinTransformer3D trunk\n        heads: Provide the heads module if using a custom\n            model. If not provided image/video/rgbd heads are\n            added corresponding to the omnivore base model.\n        head_dim_in: Only needs to be set if heads = None.\n            The dim is used for the default base model heads.\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_55-105"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        head = self.heads\n        if self.multimodal_model:\n            assert input_type in self.types, \"unsupported input type\"\n            head = head[input_type]\n        return head(features)\n\n\nCHECKPOINT_PATHS = {\n    \"omnivore_swinT\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinT_checkpoint.torch\",\n    \"omnivore_swinS\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinS_checkpoint.torch\",\n    \"omnivore_swinB\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinL_In21k_checkpoint.torch\",\n    \"omnivore_swinB_epic\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_epic_checkpoint.torch\",\n}\n\n\ndef _omnivore_base(\n    trunk: nn.Module,\n    heads: Optional[Union[nn.Module, nn.ModuleDict]] = None,\n    head_dim_in: int = 1024,\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n) -> nn.Module:\n    \"\"\"\n    Load and initialize the specified Omnivore\n    model trunk (and optionally heads).\n\n    Args:\n        trunk: nn.Module of the SwinTransformer3D trunk\n        heads: Provide the heads module if using a custom\n            model. If not provided image/video/rgbd heads are\n            added corresponding to the omnivore base model.\n        head_dim_in: Only needs to be set if heads = None.\n            The dim is used for the default base model heads.\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the full Omnivore model\n    \"\"\"\n    if load_heads and heads is None:\n        # Get heads\n        heads = get_all_heads(dim_in=head_dim_in)\n\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_65-115"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"omnivore_swinB\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_checkpoint.torch\",\n    \"omnivore_swinB_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_In21k_checkpoint.torch\",\n    \"omnivore_swinL_in21k\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinL_In21k_checkpoint.torch\",\n    \"omnivore_swinB_epic\": \"https://dl.fbaipublicfiles.com/omnivore/models/swinB_epic_checkpoint.torch\",\n}\n\n\ndef _omnivore_base(\n    trunk: nn.Module,\n    heads: Optional[Union[nn.Module, nn.ModuleDict]] = None,\n    head_dim_in: int = 1024,\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n) -> nn.Module:\n    \"\"\"\n    Load and initialize the specified Omnivore\n    model trunk (and optionally heads).\n\n    Args:\n        trunk: nn.Module of the SwinTransformer3D trunk\n        heads: Provide the heads module if using a custom\n            model. If not provided image/video/rgbd heads are\n            added corresponding to the omnivore base model.\n        head_dim_in: Only needs to be set if heads = None.\n            The dim is used for the default base model heads.\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the full Omnivore model\n    \"\"\"\n    if load_heads and heads is None:\n        # Get heads\n        heads = get_all_heads(dim_in=head_dim_in)\n\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n\n        # All models are loaded onto CPU by default\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        trunk.load_state_dict(checkpoint[\"trunk\"])\n\n        if load_heads:\n            heads.load_state_dict(checkpoint[\"heads\"])\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_75-125"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    head_dim_in: int = 1024,\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n) -> nn.Module:\n    \"\"\"\n    Load and initialize the specified Omnivore\n    model trunk (and optionally heads).\n\n    Args:\n        trunk: nn.Module of the SwinTransformer3D trunk\n        heads: Provide the heads module if using a custom\n            model. If not provided image/video/rgbd heads are\n            added corresponding to the omnivore base model.\n        head_dim_in: Only needs to be set if heads = None.\n            The dim is used for the default base model heads.\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the full Omnivore model\n    \"\"\"\n    if load_heads and heads is None:\n        # Get heads\n        heads = get_all_heads(dim_in=head_dim_in)\n\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n\n        # All models are loaded onto CPU by default\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        trunk.load_state_dict(checkpoint[\"trunk\"])\n\n        if load_heads:\n            heads.load_state_dict(checkpoint[\"heads\"])\n\n    if load_heads:\n        model = OmnivoreModel(trunk=trunk, heads=heads)\n    else:\n        model = trunk\n\n    return model\n\n\ndef omnivore_swinB_epic(\n    progress: bool = True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_85-135"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    Args:\n        trunk: nn.Module of the SwinTransformer3D trunk\n        heads: Provide the heads module if using a custom\n            model. If not provided image/video/rgbd heads are\n            added corresponding to the omnivore base model.\n        head_dim_in: Only needs to be set if heads = None.\n            The dim is used for the default base model heads.\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the full Omnivore model\n    \"\"\"\n    if load_heads and heads is None:\n        # Get heads\n        heads = get_all_heads(dim_in=head_dim_in)\n\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n\n        # All models are loaded onto CPU by default\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        trunk.load_state_dict(checkpoint[\"trunk\"])\n\n        if load_heads:\n            heads.load_state_dict(checkpoint[\"heads\"])\n\n    if load_heads:\n        model = OmnivoreModel(trunk=trunk, heads=heads)\n    else:\n        model = trunk\n\n    return model\n\n\ndef omnivore_swinB_epic(\n    progress: bool = True,\n    checkpoint_name: str = \"omnivore_swinB_epic\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore swin B model trained on EPIC-KITCHENS-100 dataset\n\n    Args:\n        progress: print progress of loading checkpoint\n\n    Returns:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_95-145"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    Returns:\n        model: nn.Module of the full Omnivore model\n    \"\"\"\n    if load_heads and heads is None:\n        # Get heads\n        heads = get_all_heads(dim_in=head_dim_in)\n\n    if pretrained:\n        path = CHECKPOINT_PATHS[checkpoint_name]\n\n        # All models are loaded onto CPU by default\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        trunk.load_state_dict(checkpoint[\"trunk\"])\n\n        if load_heads:\n            heads.load_state_dict(checkpoint[\"heads\"])\n\n    if load_heads:\n        model = OmnivoreModel(trunk=trunk, heads=heads)\n    else:\n        model = trunk\n\n    return model\n\n\ndef omnivore_swinB_epic(\n    progress: bool = True,\n    checkpoint_name: str = \"omnivore_swinB_epic\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore swin B model trained on EPIC-KITCHENS-100 dataset\n\n    Args:\n        progress: print progress of loading checkpoint\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_105-155"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # All models are loaded onto CPU by default\n        checkpoint = load_state_dict_from_url(\n            path, progress=progress, map_location=\"cpu\"\n        )\n        trunk.load_state_dict(checkpoint[\"trunk\"])\n\n        if load_heads:\n            heads.load_state_dict(checkpoint[\"heads\"])\n\n    if load_heads:\n        model = OmnivoreModel(trunk=trunk, heads=heads)\n    else:\n        model = trunk\n\n    return model\n\n\ndef omnivore_swinB_epic(\n    progress: bool = True,\n    checkpoint_name: str = \"omnivore_swinB_epic\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore swin B model trained on EPIC-KITCHENS-100 dataset\n\n    Args:\n        progress: print progress of loading checkpoint\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.4,\n        patch_norm=True,\n        **kwargs,\n    )\n\n    heads = nn.Sequential(\n        nn.Dropout(p=0.5), nn.Linear(in_features=1024, out_features=3806, bias=True)\n    )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_115-165"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    if load_heads:\n        model = OmnivoreModel(trunk=trunk, heads=heads)\n    else:\n        model = trunk\n\n    return model\n\n\ndef omnivore_swinB_epic(\n    progress: bool = True,\n    checkpoint_name: str = \"omnivore_swinB_epic\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore swin B model trained on EPIC-KITCHENS-100 dataset\n\n    Args:\n        progress: print progress of loading checkpoint\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.4,\n        patch_norm=True,\n        **kwargs,\n    )\n\n    heads = nn.Sequential(\n        nn.Dropout(p=0.5), nn.Linear(in_features=1024, out_features=3806, bias=True)\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=True,\n        load_heads=True,\n        checkpoint_name=checkpoint_name,\n        heads=heads,\n    )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_125-175"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    checkpoint_name: str = \"omnivore_swinB_epic\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore swin B model trained on EPIC-KITCHENS-100 dataset\n\n    Args:\n        progress: print progress of loading checkpoint\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.4,\n        patch_norm=True,\n        **kwargs,\n    )\n\n    heads = nn.Sequential(\n        nn.Dropout(p=0.5), nn.Linear(in_features=1024, out_features=3806, bias=True)\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=True,\n        load_heads=True,\n        checkpoint_name=checkpoint_name,\n        heads=heads,\n    )\n\n\ndef omnivore_swinB(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin B patch (2,4,4) window (1,6,7,7)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_135-185"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.4,\n        patch_norm=True,\n        **kwargs,\n    )\n\n    heads = nn.Sequential(\n        nn.Dropout(p=0.5), nn.Linear(in_features=1024, out_features=3806, bias=True)\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=True,\n        load_heads=True,\n        checkpoint_name=checkpoint_name,\n        heads=heads,\n    )\n\n\ndef omnivore_swinB(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin B patch (2,4,4) window (1,6,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_145-195"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        window_size=(16, 7, 7),\n        drop_path_rate=0.4,\n        patch_norm=True,\n        **kwargs,\n    )\n\n    heads = nn.Sequential(\n        nn.Dropout(p=0.5), nn.Linear(in_features=1024, out_features=3806, bias=True)\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=True,\n        load_heads=True,\n        checkpoint_name=checkpoint_name,\n        heads=heads,\n    )\n\n\ndef omnivore_swinB(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin B patch (2,4,4) window (1,6,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_155-205"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=True,\n        load_heads=True,\n        checkpoint_name=checkpoint_name,\n        heads=heads,\n    )\n\n\ndef omnivore_swinB(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin B patch (2,4,4) window (1,6,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_165-215"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\ndef omnivore_swinB(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"omnivore_swinB\",\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin B patch (2,4,4) window (1,6,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinB_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_175-225"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinB_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore Swin B model pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        progress: print progress of loading checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_185-235"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=128,\n        depths=[2, 2, 18, 2],\n        num_heads=[4, 8, 16, 32],\n        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinB_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore Swin B model pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    return omnivore_swinB(\n        pretrained=pretrained,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_195-245"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        window_size=(16, 7, 7),\n        drop_path_rate=0.3,  # TODO: set this based on the final models\n        patch_norm=True,  # Make this the default value?\n        depth_mode=\"summed_rgb_d_tokens\",  # Make this the default value?\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=1024,  # embed_dim * 8 = 128*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinB_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore Swin B model pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    return omnivore_swinB(\n        pretrained=pretrained,\n        load_heads=load_heads,\n        progress=progress,\n        checkpoint_name=\"omnivore_swinB_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinS(\n    pretrained: bool = True,\n    progress: bool = True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_205-255"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinB_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore Swin B model pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    return omnivore_swinB(\n        pretrained=pretrained,\n        load_heads=load_heads,\n        progress=progress,\n        checkpoint_name=\"omnivore_swinB_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinS(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin S patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_215-265"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore Swin B model pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    return omnivore_swinB(\n        pretrained=pretrained,\n        load_heads=load_heads,\n        progress=progress,\n        checkpoint_name=\"omnivore_swinB_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinS(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin S patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_225-275"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    return omnivore_swinB(\n        pretrained=pretrained,\n        load_heads=load_heads,\n        progress=progress,\n        checkpoint_name=\"omnivore_swinB_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinS(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin S patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_235-285"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        load_heads=load_heads,\n        progress=progress,\n        checkpoint_name=\"omnivore_swinB_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinS(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin S patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinS\",\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_245-295"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin S patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin T patch (2,4,4) window (8,7,7)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_255-305"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin T patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_265-315"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 18, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin T patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_275-325"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinS\",\n    )\n\n\ndef omnivore_swinT(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin T patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_285-335"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef omnivore_swinT(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Omnivore model trunk: Swin T patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef _omnivore_swinL(\n    pretrained: bool = True,\n    progress: bool = True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_295-345"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef _omnivore_swinL(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"\",\n    heads: Optional[nn.ModuleDict] = None,\n    **kwargs: Any,\n) -> nn.Module:\n    \"\"\"\n    Omnivore model trunk: Swin L patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_305-355"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        model: nn.Module of the omnivore model\n    \"\"\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=96,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 24],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef _omnivore_swinL(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"\",\n    heads: Optional[nn.ModuleDict] = None,\n    **kwargs: Any,\n) -> nn.Module:\n    \"\"\"\n    Omnivore model trunk: Swin L patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_315-365"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        window_size=(8, 7, 7),\n        drop_path_rate=0.2,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        head_dim_in=768,  # 96*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef _omnivore_swinL(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"\",\n    heads: Optional[nn.ModuleDict] = None,\n    **kwargs: Any,\n) -> nn.Module:\n    \"\"\"\n    Omnivore model trunk: Swin L patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_325-375"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinT\",\n    )\n\n\ndef _omnivore_swinL(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    checkpoint_name: str = \"\",\n    heads: Optional[nn.ModuleDict] = None,\n    **kwargs: Any,\n) -> nn.Module:\n    \"\"\"\n    Omnivore model trunk: Swin L patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        heads=heads,\n        head_dim_in=1536,  # 192*8", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_335-385"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    load_heads: bool = True,\n    checkpoint_name: str = \"\",\n    heads: Optional[nn.ModuleDict] = None,\n    **kwargs: Any,\n) -> nn.Module:\n    \"\"\"\n    Omnivore model trunk: Swin L patch (2,4,4) window (8,7,7)\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        heads=heads,\n        head_dim_in=1536,  # 192*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinL_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_345-395"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            Imagenet 1k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        heads=heads,\n        head_dim_in=1536,  # 192*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinL_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        pretrained: if True loads weights from model trained on", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_355-405"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    assert checkpoint_name != \"\", \"checkpoint_name must be provided\"\n\n    # Only specify the non default values\n    trunk = SwinTransformer3D(\n        pretrained2d=False,\n        patch_size=(2, 4, 4),\n        embed_dim=192,\n        depths=[2, 2, 18, 2],\n        num_heads=[6, 12, 24, 48],\n        window_size=(8, 7, 7),\n        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        heads=heads,\n        head_dim_in=1536,  # 192*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinL_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet1k, Imagenet 21k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n    return _omnivore_swinL(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_365-415"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        drop_path_rate=0.3,\n        patch_norm=True,\n        depth_mode=\"summed_rgb_d_tokens\",\n        **kwargs,\n    )\n\n    return _omnivore_base(\n        trunk=trunk,\n        heads=heads,\n        head_dim_in=1536,  # 192*8\n        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinL_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet1k, Imagenet 21k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinL_kinetics600(\n    pretrained: bool = True,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_375-425"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        progress=progress,\n        pretrained=pretrained,\n        load_heads=load_heads,\n        checkpoint_name=checkpoint_name,\n    )\n\n\ndef omnivore_swinL_imagenet21k(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet1k, Imagenet 21k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinL_kinetics600(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 trained with Kinetics 600\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 600, SUN RGBD.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_385-435"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 pretrained on Imagenet 1k, Imagenet 21k,\n    Kinetics 400, SUN RGBD. By default the pretrained\n    weights will be loaded.\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet1k, Imagenet 21k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinL_kinetics600(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 trained with Kinetics 600\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 600, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    heads = nn.ModuleDict(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_395-445"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            Imagenet1k, Imagenet 21k, Kinetics 400, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinL_kinetics600(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 trained with Kinetics 600\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 600, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in=1536),\n            \"rgbd\": get_sunrgbd_head(dim_in=1536),\n            \"video\": get_kinetics_head(dim_in=1536, num_classes=600),\n        }\n    )\n\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_405-455"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 460, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_in21k\",\n        **kwargs,\n    )\n\n\ndef omnivore_swinL_kinetics600(\n    pretrained: bool = True,\n    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 trained with Kinetics 600\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 600, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in=1536),\n            \"rgbd\": get_sunrgbd_head(dim_in=1536),\n            \"video\": get_kinetics_head(dim_in=1536, num_classes=600),\n        }\n    )\n\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_kinetics600\",\n        heads=heads,\n        **kwargs,\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_415-460"}
{"title": "facebookresearch_omnivore-omnivore-models-omnivore_model.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "omnivore_model.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 460, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    progress: bool = True,\n    load_heads: bool = True,\n    **kwargs: Any,\n) -> nn.Module:\n    r\"\"\"\n    Swin L patch 244 window 877 trained with Kinetics 600\n\n    Args:\n        pretrained: if True loads weights from model trained on\n            Imagenet 1k, Kinetics 600, SUN RGBD.\n        progress: print progress of loading checkpoint\n        load_heads: if True, loads the 3 heads, one each for\n            image/video/rgbd prediction. If False loads only the\n            trunk.\n\n    Returns:\n        model: nn.Module of the omnivore model\n    \"\"\"\n\n    heads = nn.ModuleDict(\n        {\n            \"image\": get_imagenet_head(dim_in=1536),\n            \"rgbd\": get_sunrgbd_head(dim_in=1536),\n            \"video\": get_kinetics_head(dim_in=1536, num_classes=600),\n        }\n    )\n\n    return _omnivore_swinL(\n        pretrained=pretrained,\n        progress=progress,\n        load_heads=load_heads,\n        checkpoint_name=\"omnivore_swinL_kinetics600\",\n        heads=heads,\n        **kwargs,\n    )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-omnivore_model.py_425-460"}
{"title": "facebookresearch_omnivore-omnivore-models-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 17, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}, {"fpath_tuple": ["facebookresearch_omnivore", "omnivore", "models", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 17, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom .omnivore_model import (\n    omnivore_swinB,\n    omnivore_swinB_epic,\n    omnivore_swinB_imagenet21k,\n    omnivore_swinL_imagenet21k,\n    omnivore_swinL_kinetics600,\n    omnivore_swinS,\n    omnivore_swinT,\n)\n\nAST=Module(ImportFrom(aliasaliasaliasaliasaliasaliasalias))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-omnivore-models-__init__.py_0-17"}
{"title": "facebookresearch_omnivore-temp_build-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "temp_build", "setup.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"omnivore\",\n    version=\"0.0\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-temp_build-setup.py_0-25"}
{"title": "facebookresearch_omnivore-temp_build-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "temp_build", "setup.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"omnivore\",\n    version=\"0.0\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-temp_build-setup.py_0-35"}
{"title": "facebookresearch_omnivore-temp_build-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "temp_build", "setup.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 43, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"omnivore\",\n    version=\"0.0\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",\n            \"ufmt==2.0.0b2\",\n            \"usort==1.0.2\",\n            \"libcst==0.4.1\",\n            ##################################\n        ],\n    },\n    packages=find_packages(exclude=(\"scripts\", \"tests\")),\n)\n\nAST=Module(ImportFrom(aliasalias)Expr(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))keyword(Constant)keyword(List(Load))keyword(Dict(ConstantList(ConstantConstantConstantConstantConstantLoad)))keyword(Call(Name(Load)keyword(Tuple(ConstantConstantLoad)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-temp_build-setup.py_0-43"}
{"title": "facebookresearch_omnivore-temp_build-setup.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "temp_build", "setup.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 43, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "setup(\n    name=\"omnivore\",\n    version=\"0.0\",\n    author=\"FAIR\",\n    url=\"https://github.com/facebookresearch/omnivore\",\n    install_requires=[\n        \"einops\",\n        \"timm\",\n        \"ftfy\",\n        \"regex\",\n        \"torchmetrics\",\n        \"torchaudio>=0.9.0\",\n        \"hydra-core\",\n        \"submitit>=1.4.4\",\n        \"pytorchvideo>=0.1.5\",\n        \"fvcore\",\n        \"opencv-python\",\n        \"tensorboard==2.9.1\",\n        \"torch>=1.12\",\n        \"torchvision>=0.13\",\n    ],\n    license=\"CC BY-NC 4.0\",\n    tests_require=[],\n    extras_require={\n        \"dev\": [\n            \"sphinx\",\n            ##################################\n            # Formatter settings based on\n            # `pyfmt -V`\n            \"black==22.3.0\",\n            \"ufmt==2.0.0b2\",\n            \"usort==1.0.2\",\n            \"libcst==0.4.1\",\n            ##################################\n        ],\n    },\n    packages=find_packages(exclude=(\"scripts\", \"tests\")),\n)\n\nAST=Module(Expr(Call(Name(Load)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(List(ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantLoad))keyword(Constant)keyword(List(Load))keyword(Dict(ConstantList(ConstantConstantConstantConstantConstantLoad)))keyword(Call(Name(Load)keyword(Tuple(ConstantConstantLoad)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-temp_build-setup.py_5-43"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport os\nimport tempfile\nimport unittest\nfrom pathlib import Path\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import OmegaConf\nfrom omnivision.model.checkpoint_utils import (\n    load_checkpoint_and_apply_kernels,\n    load_state_dict_into_model,\n)\nfrom tests.util import SimpleNet\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_checkpoint\"\n\n\nclass TestCheckpointLoaderConf(unittest.TestCase):\n    def test_simple_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)Assign(Name(Store)BinOp(Attribute(Call(Name(Load)Name(Load))Load)DivConstant))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_0-25"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport os\nimport tempfile\nimport unittest\nfrom pathlib import Path\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import OmegaConf\nfrom omnivision.model.checkpoint_utils import (\n    load_checkpoint_and_apply_kernels,\n    load_state_dict_into_model,\n)\nfrom tests.util import SimpleNet\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_checkpoint\"\n\n\nclass TestCheckpointLoaderConf(unittest.TestCase):\n    def test_simple_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_0-35"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport os\nimport tempfile\nimport unittest\nfrom pathlib import Path\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import OmegaConf\nfrom omnivision.model.checkpoint_utils import (\n    load_checkpoint_and_apply_kernels,\n    load_state_dict_into_model,\n)\nfrom tests.util import SimpleNet\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_checkpoint\"\n\n\nclass TestCheckpointLoaderConf(unittest.TestCase):\n    def test_simple_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(checkpoint_path=ckpt_path)\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in range(4):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_0-45"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from pathlib import Path\n\nimport hydra\nimport torch\nimport torch.nn as nn\nfrom omegaconf import OmegaConf\nfrom omnivision.model.checkpoint_utils import (\n    load_checkpoint_and_apply_kernels,\n    load_state_dict_into_model,\n)\nfrom tests.util import SimpleNet\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_checkpoint\"\n\n\nclass TestCheckpointLoaderConf(unittest.TestCase):\n    def test_simple_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(checkpoint_path=ckpt_path)\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in range(4):\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_filter_model(self):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_5-55"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from tests.util import SimpleNet\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_checkpoint\"\n\n\nclass TestCheckpointLoaderConf(unittest.TestCase):\n    def test_simple_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(checkpoint_path=ckpt_path)\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in range(4):\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n\nAST=Module(ImportFrom(alias)Assign(Name(Store)BinOp(Attribute(Call(Name(Load)Name(Load))Load)DivConstant))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))For(Name(Store)Call(Name(Load)Constant)Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load)))))With(withitem(Call(Attribute(Name(Load)Load))Name(Store))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)Constant))Assign(Name(Store)Dict(ConstantCall(Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load))))For(Name(Store)Call(Name(Load)Constant)Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))))))FunctionDef(arguments(arg)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))For(Name(Store)Call(Name(Load)Constant)Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))Call(Attribute(Attribute(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))))Load)Load))))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_15-65"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(checkpoint_path=ckpt_path)\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in range(4):\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_25-75"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(checkpoint_path=ckpt_path)\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in range(4):\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_35-85"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1, 2]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_45-95"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1, 2]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_55-105"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1, 2]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_exclude_filter_model(self):\n\n        num_layers = 4", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_65-115"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1, 2]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_75-125"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1, 2]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_85-135"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_95-145"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 3]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_105-155"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 3]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [1, 2]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_115-165"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 3]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [1, 2]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_125-175"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 3]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [1, 2]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_135-185"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 3]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [1, 2]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_145-195"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [1, 2]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_155-205"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_include_exclude_filter_model(self):\n\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n        model_ckpt = SimpleNet(2, num_layers, 0.0)\n\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_165-215"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        for i in range(4):\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 2, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_175-225"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptIncludeKernel\",\n                    \"key_pattern\": [\"layer_[1,2]*\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 2, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_remap_with_repeat_filter_and_exclude_model(self):\n\n        num_layers = 4", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_185-235"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    \"key_pattern\": [\"layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 2, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_remap_with_repeat_filter_and_exclude_model(self):\n\n        num_layers = 4\n        ref_val = 1.0\n        model = SimpleNet(2, num_layers, ref_val)\n\n        class ComplexNetRepeat(nn.Module):\n            def __init__(self):\n                super(ComplexNetRepeat, self).__init__()\n\n                self.complex_layer_0 = SimpleNet(\n                    2, num_layers, init_val=2.0\n                )  # output will be 6 dims", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_195-245"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [1]:\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 2, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_remap_with_repeat_filter_and_exclude_model(self):\n\n        num_layers = 4\n        ref_val = 1.0\n        model = SimpleNet(2, num_layers, ref_val)\n\n        class ComplexNetRepeat(nn.Module):\n            def __init__(self):\n                super(ComplexNetRepeat, self).__init__()\n\n                self.complex_layer_0 = SimpleNet(\n                    2, num_layers, init_val=2.0\n                )  # output will be 6 dims\n                self.complex_layer_1 = SimpleNet(\n                    2, num_layers, init_val=3.0\n                )  # output will be 10 dims\n\n            def forward(self, x):\n                x = self.complex_layer_0(x)\n                x = self.complex_layer_1(x)\n                return x\n\n        model_ckpt = ComplexNetRepeat()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_205-255"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        for i in [0, 2, 3]:\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_remap_with_repeat_filter_and_exclude_model(self):\n\n        num_layers = 4\n        ref_val = 1.0\n        model = SimpleNet(2, num_layers, ref_val)\n\n        class ComplexNetRepeat(nn.Module):\n            def __init__(self):\n                super(ComplexNetRepeat, self).__init__()\n\n                self.complex_layer_0 = SimpleNet(\n                    2, num_layers, init_val=2.0\n                )  # output will be 6 dims\n                self.complex_layer_1 = SimpleNet(\n                    2, num_layers, init_val=3.0\n                )  # output will be 10 dims\n\n            def forward(self, x):\n                x = self.complex_layer_0(x)\n                x = self.complex_layer_1(x)\n                return x\n\n        model_ckpt = ComplexNetRepeat()\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"*\"],\n                    \"source_pattern\": \"\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"complex_layer_0.\", \"complex_layer_1.\"],\n                },", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_215-265"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n    def test_remap_with_repeat_filter_and_exclude_model(self):\n\n        num_layers = 4\n        ref_val = 1.0\n        model = SimpleNet(2, num_layers, ref_val)\n\n        class ComplexNetRepeat(nn.Module):\n            def __init__(self):\n                super(ComplexNetRepeat, self).__init__()\n\n                self.complex_layer_0 = SimpleNet(\n                    2, num_layers, init_val=2.0\n                )  # output will be 6 dims\n                self.complex_layer_1 = SimpleNet(\n                    2, num_layers, init_val=3.0\n                )  # output will be 10 dims\n\n            def forward(self, x):\n                x = self.complex_layer_0(x)\n                x = self.complex_layer_1(x)\n                return x\n\n        model_ckpt = ComplexNetRepeat()\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"*\"],\n                    \"source_pattern\": \"\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"complex_layer_0.\", \"complex_layer_1.\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"*layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_225-275"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        ref_val = 1.0\n        model = SimpleNet(2, num_layers, ref_val)\n\n        class ComplexNetRepeat(nn.Module):\n            def __init__(self):\n                super(ComplexNetRepeat, self).__init__()\n\n                self.complex_layer_0 = SimpleNet(\n                    2, num_layers, init_val=2.0\n                )  # output will be 6 dims\n                self.complex_layer_1 = SimpleNet(\n                    2, num_layers, init_val=3.0\n                )  # output will be 10 dims\n\n            def forward(self, x):\n                x = self.complex_layer_0(x)\n                x = self.complex_layer_1(x)\n                return x\n\n        model_ckpt = ComplexNetRepeat()\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"*\"],\n                    \"source_pattern\": \"\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"complex_layer_0.\", \"complex_layer_1.\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"*layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 1, 3]:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_235-285"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                self.complex_layer_1 = SimpleNet(\n                    2, num_layers, init_val=3.0\n                )  # output will be 10 dims\n\n            def forward(self, x):\n                x = self.complex_layer_0(x)\n                x = self.complex_layer_1(x)\n                return x\n\n        model_ckpt = ComplexNetRepeat()\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"*\"],\n                    \"source_pattern\": \"\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"complex_layer_0.\", \"complex_layer_1.\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"*layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 1, 3]:\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").weight.mean().item(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_245-295"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"*\"],\n                    \"source_pattern\": \"\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"complex_layer_0.\", \"complex_layer_1.\"],\n                },\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"*layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 1, 3]:\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n\n    def test_ckpt_with_remap_model(self):\n\n        num_layers = 4", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_255-305"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptExcludeKernel\",\n                    \"key_pattern\": [\"*layer_2*\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 1, 3]:\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n\n    def test_ckpt_with_remap_model(self):\n\n        num_layers = 4\n\n        class BaseModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.model = SimpleNet(2, num_layers, 2.0)\n\n            def forward(self, x):\n                return self.model(x)\n\n        model = BaseModule()", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_265-315"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(\n                ckpt_st_dict, model_ckpt, strict=False\n            )\n\n        for i in [0, 1, 3]:\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n\n    def test_ckpt_with_remap_model(self):\n\n        num_layers = 4\n\n        class BaseModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.model = SimpleNet(2, num_layers, 2.0)\n\n            def forward(self, x):\n                return self.model(x)\n\n        model = BaseModule()\n        model_ckpt = SimpleNet(2, num_layers, 1.0)\n\n        for i in [0, 1, 2, 3]:\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_275-325"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_0, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").weight.mean().item(),\n                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n\n    def test_ckpt_with_remap_model(self):\n\n        num_layers = 4\n\n        class BaseModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.model = SimpleNet(2, num_layers, 2.0)\n\n            def forward(self, x):\n                return self.model(x)\n\n        model = BaseModule()\n        model_ckpt = SimpleNet(2, num_layers, 1.0)\n\n        for i in [0, 1, 2, 3]:\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"model*\"],\n                    \"source_pattern\": \"model.\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"\"],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_285-335"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                ref_val,\n            )\n            self.assertEqual(\n                getattr(model_ckpt.complex_layer_1, f\"layer_{i}\").bias.mean().item(),\n                ref_val,\n            )\n\n    def test_ckpt_with_remap_model(self):\n\n        num_layers = 4\n\n        class BaseModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.model = SimpleNet(2, num_layers, 2.0)\n\n            def forward(self, x):\n                return self.model(x)\n\n        model = BaseModule()\n        model_ckpt = SimpleNet(2, num_layers, 1.0)\n\n        for i in [0, 1, 2, 3]:\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"model*\"],\n                    \"source_pattern\": \"model.\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_295-345"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n        class BaseModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.model = SimpleNet(2, num_layers, 2.0)\n\n            def forward(self, x):\n                return self.model(x)\n\n        model = BaseModule()\n        model_ckpt = SimpleNet(2, num_layers, 1.0)\n\n        for i in [0, 1, 2, 3]:\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"model*\"],\n                    \"source_pattern\": \"model.\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in [0, 1, 2, 3]:\n            self.assertEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_305-355"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 358, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        model_ckpt = SimpleNet(2, num_layers, 1.0)\n\n        for i in [0, 1, 2, 3]:\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertNotEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"model*\"],\n                    \"source_pattern\": \"model.\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in [0, 1, 2, 3]:\n            self.assertEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_315-358"}
{"title": "facebookresearch_omnivore-tests-test_checkpoint_loading.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_checkpoint_loading.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 358, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n\n        # add include, exclude filter\n        filter_conf = OmegaConf.create(\n            [\n                {\n                    \"_target_\": \"omnivision.model.checkpoint_utils.CkptRenameWithCopyKernel\",\n                    \"key_pattern\": [\"model*\"],\n                    \"source_pattern\": \"model.\",  # Note: Only First occurence in target is replaced\n                    \"target_patterns\": [\"\"],\n                },\n            ]\n        )\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            ckpt_path = os.path.join(tmpdirname, \"test.ckpt\")\n            ckpt = {\"state_dict\": model.state_dict()}\n            torch.save(ckpt, ckpt_path)\n            ckpt_st_dict = load_checkpoint_and_apply_kernels(\n                checkpoint_path=ckpt_path,\n                checkpoint_kernels=[hydra.utils.instantiate(f) for f in filter_conf],\n            )\n            model_ckpt = load_state_dict_into_model(ckpt_st_dict, model_ckpt)\n\n        for i in [0, 1, 2, 3]:\n            self.assertEqual(\n                getattr(model.model, f\"layer_{i}\").weight.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").weight.mean(),\n            )\n            self.assertEqual(\n                getattr(model.model, f\"layer_{i}\").bias.mean(),\n                getattr(model_ckpt, f\"layer_{i}\").bias.mean(),\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_checkpoint_loading.py_325-358"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport unittest\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\nfrom omnivision.model.model_wrappers import MIMOHeadWrapper\nfrom tests.util import SimpleNet\n\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_model_wrapper\"\n\n\nclass TestMIMOHeadWrapper(unittest.TestCase):\n    def test_single_input_single_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_0-25"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport unittest\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\nfrom omnivision.model.model_wrappers import MIMOHeadWrapper\nfrom tests.util import SimpleNet\n\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_model_wrapper\"\n\n\nclass TestMIMOHeadWrapper(unittest.TestCase):\n    def test_single_input_single_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            }\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Assign(Name(Store)BinOp(Attribute(Call(Name(Load)Name(Load))Load)DivConstant))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))Assign(Name(Store)List(Dict(ConstantConstantConstantConstantCall(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))ConstantConstantConstant)Load))Assign(Name(Store)List(Dict(ConstantConstantConstantList(ConstantLoad))Load))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Dict(ConstantCall(Name(Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)Constant)))))Assign(Name(Store)Call(Name(Load)Name(Load))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_0-35"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport unittest\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\nfrom omnivision.model.model_wrappers import MIMOHeadWrapper\nfrom tests.util import SimpleNet\n\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_model_wrapper\"\n\n\nclass TestMIMOHeadWrapper(unittest.TestCase):\n    def test_single_input_single_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            }\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n\n    def test_single_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_0-45"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import torch\nimport torch.nn as nn\nfrom omnivision.data.api import VisionSample\nfrom omnivision.model.model_wrappers import MIMOHeadWrapper\nfrom tests.util import SimpleNet\n\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs_model_wrapper\"\n\n\nclass TestMIMOHeadWrapper(unittest.TestCase):\n    def test_single_input_single_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            }\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n\n    def test_single_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_5-55"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "class TestMIMOHeadWrapper(unittest.TestCase):\n    def test_single_input_single_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            }\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n\n    def test_single_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))Assign(Name(Store)List(Dict(ConstantConstantConstantConstantCall(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))ConstantConstantConstant)Load))Assign(Name(Store)List(Dict(ConstantConstantConstantList(ConstantLoad))Load))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Dict(ConstantCall(Name(Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)Constant)))))Assign(Name(Store)Call(Name(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)Call(Attribute(Name(Load)Load)))Set(Constant)))Expr(Call(Attribute(Name(Load)Load)Subscript(Attribute(Subscript(Name(Load)ConstantLoad)Load)ConstantLoad)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Subscript(Attribute(Subscript(Name(Load)ConstantLoad)Load)ConstantLoad)Constant)))FunctionDef(arguments(arg)Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Call(Name(Load)ConstantName(Load)Constant))Assign(Name(Store)List(Dict(ConstantConstantConstantConstantCall(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))ConstantConstantConstant)Dict(ConstantConstantConstantConstantCall(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))ConstantConstantConstant)Load))Assign(Name(Store)List(Dict(ConstantConstantConstantList(ConstantLoad))Load))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Dict(ConstantCall(Name(Load)keyword(Call(Attribute(Name(Load)Load)Name(Load)Constant)))))Assign(Name(Store)Call(Name(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)Call(Attribute(Name(Load)Load)))Set(ConstantConstant))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_15-65"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            }\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n\n    def test_single_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n    def test_multi_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_25-75"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.assertSetEqual(set(out.keys()), {\"out_1\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n\n    def test_single_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n    def test_multi_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_35-85"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n    def test_multi_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_3\",\n            },\n        ]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_45-95"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        out = model(inp)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n    def test_multi_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_3\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # run the same tests twice to make sure there are no internal state issues\n        for _ in range(2):\n            inp = dict(**inp_1, **inp_2)\n            out = model(inp)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_55-105"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n    def test_multi_input_multi_output(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_3\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # run the same tests twice to make sure there are no internal state issues\n        for _ in range(2):\n            inp = dict(**inp_1, **inp_2)\n            out = model(inp)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\", \"out_3\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n            out = model(inp_1)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_65-115"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_3\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # run the same tests twice to make sure there are no internal state issues\n        for _ in range(2):\n            inp = dict(**inp_1, **inp_2)\n            out = model(inp)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\", \"out_3\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n            out = model(inp_1)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n            out = model(inp_2)\n            self.assertSetEqual(set(out.keys()), {\"out_3\"})\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_75-125"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_3\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # run the same tests twice to make sure there are no internal state issues\n        for _ in range(2):\n            inp = dict(**inp_1, **inp_2)\n            out = model(inp)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\", \"out_3\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n            out = model(inp_1)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n            out = model(inp_2)\n            self.assertSetEqual(set(out.keys()), {\"out_3\"})\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n    def test_multi_input_multi_output_same_output_key(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_85-135"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # run the same tests twice to make sure there are no internal state issues\n        for _ in range(2):\n            inp = dict(**inp_1, **inp_2)\n            out = model(inp)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\", \"out_3\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n            out = model(inp_1)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n            out = model(inp_2)\n            self.assertSetEqual(set(out.keys()), {\"out_3\"})\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n    def test_multi_input_multi_output_same_output_key(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_95-145"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\", \"out_3\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n            out = model(inp_1)\n            self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n            out = model(inp_2)\n            self.assertSetEqual(set(out.keys()), {\"out_3\"})\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n    def test_multi_input_multi_output_same_output_key(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_105-155"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_1\"].shape[1], 10)\n            self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n            out = model(inp_2)\n            self.assertSetEqual(set(out.keys()), {\"out_3\"})\n            self.assertEqual(out[\"out_3\"].shape[0], batch_size)\n            self.assertEqual(out[\"out_3\"].shape[1], 3)\n\n    def test_multi_input_multi_output_same_output_key(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # second and third head will produce the same output key (out_2)\n        # this should raise\n        inp = dict(**inp_1, **inp_2)\n        with self.assertRaises(Exception):\n            out = model(inp)\n\n        out = model(inp_1)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_115-165"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 173, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def test_multi_input_multi_output_same_output_key(self):\n        batch_size = 8\n        num_layers = 4\n        model = SimpleNet(2, num_layers, 1.0)\n\n        heads = [\n            {\n                \"head\": nn.Linear(in_features=4, out_features=10),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # second and third head will produce the same output key (out_2)\n        # this should raise\n        inp = dict(**inp_1, **inp_2)\n        with self.assertRaises(Exception):\n            out = model(inp)\n\n        out = model(inp_1)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n        out = model(inp_2)\n        self.assertSetEqual(set(out.keys()), {\"out_2\"})\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 3)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_125-173"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 173, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                \"output_key\": \"out_1\",\n            },\n            {\n                \"head\": nn.Linear(in_features=4, out_features=2),\n                \"fork_module\": \"layer_1\",\n                \"input_key\": \"input_1\",\n                \"output_key\": \"out_2\",\n            },\n            {\n                \"head\": nn.Linear(in_features=5, out_features=3),\n                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # second and third head will produce the same output key (out_2)\n        # this should raise\n        inp = dict(**inp_1, **inp_2)\n        with self.assertRaises(Exception):\n            out = model(inp)\n\n        out = model(inp_1)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n        out = model(inp_2)\n        self.assertSetEqual(set(out.keys()), {\"out_2\"})\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 3)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_135-173"}
{"title": "facebookresearch_omnivore-tests-test_model_wrappers.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_model_wrappers.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 173, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                \"fork_module\": \"layer_2\",\n                \"input_key\": \"input_2\",\n                \"output_key\": \"out_2\",\n            },\n        ]\n        trunk_fields = [{\"input_key\": None, \"args\": [\"vision\"]}]\n        model = MIMOHeadWrapper(model, heads, trunk_fields)\n\n        inp_1 = {\"input_1\": VisionSample(vision=torch.rand(batch_size, 2))}\n        inp_2 = {\"input_2\": VisionSample(vision=torch.rand(batch_size, 2))}\n\n        # second and third head will produce the same output key (out_2)\n        # this should raise\n        inp = dict(**inp_1, **inp_2)\n        with self.assertRaises(Exception):\n            out = model(inp)\n\n        out = model(inp_1)\n        self.assertSetEqual(set(out.keys()), {\"out_1\", \"out_2\"})\n        self.assertEqual(out[\"out_1\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_1\"].shape[1], 10)\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 2)\n\n        out = model(inp_2)\n        self.assertSetEqual(set(out.keys()), {\"out_2\"})\n        self.assertEqual(out[\"out_2\"].shape[0], batch_size)\n        self.assertEqual(out[\"out_2\"].shape[1], 3)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_model_wrappers.py_145-173"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright Meta Platforms, Inc. and affiliates. Confidential and proprietary.\nimport unittest\nfrom pathlib import Path\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import OmegaConf\nfrom omnivision.optim import construct_optimizer, OmniOptimizer\nfrom omnivision.trainer.omnivision_trainer import OmnivisionOptimConf\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs\"\n\n\nclass MiniNet(nn.Module):\n    def __init__(self):\n        super(MiniNet, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n        nn.init.constant_(self.fc1.weight, 1.0)\n        nn.init.constant_(self.fc1.bias, 2.0)\n\n        nn.init.constant_(self.fc2.weight, 3.0)\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)Assign(Name(Store)BinOp(Attribute(Call(Name(Load)Name(Load))Load)DivConstant))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ConstantConstant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_0-25"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright Meta Platforms, Inc. and affiliates. Confidential and proprietary.\nimport unittest\nfrom pathlib import Path\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import OmegaConf\nfrom omnivision.optim import construct_optimizer, OmniOptimizer\nfrom omnivision.trainer.omnivision_trainer import OmnivisionOptimConf\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs\"\n\n\nclass MiniNet(nn.Module):\n    def __init__(self):\n        super(MiniNet, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n        nn.init.constant_(self.fc1.weight, 1.0)\n        nn.init.constant_(self.fc1.bias, 2.0)\n\n        nn.init.constant_(self.fc2.weight, 3.0)\n        nn.init.constant_(self.fc2.bias, 4.0)\n\n        nn.init.constant_(self.fc3.weight, 5.0)\n        nn.init.constant_(self.fc3.bias, 6.0)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nAST=Module(Import(alias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)Assign(Name(Store)BinOp(Attribute(Call(Name(Load)Name(Load))Load)DivConstant))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load)Name(Load)Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ConstantConstant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load)Constant)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_0-35"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright Meta Platforms, Inc. and affiliates. Confidential and proprietary.\nimport unittest\nfrom pathlib import Path\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import OmegaConf\nfrom omnivision.optim import construct_optimizer, OmniOptimizer\nfrom omnivision.trainer.omnivision_trainer import OmnivisionOptimConf\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs\"\n\n\nclass MiniNet(nn.Module):\n    def __init__(self):\n        super(MiniNet, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n        nn.init.constant_(self.fc1.weight, 1.0)\n        nn.init.constant_(self.fc1.bias, 2.0)\n\n        nn.init.constant_(self.fc2.weight, 3.0)\n        nn.init.constant_(self.fc2.bias, 4.0)\n\n        nn.init.constant_(self.fc3.weight, 5.0)\n        nn.init.constant_(self.fc3.bias, 6.0)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass ComplexNet(nn.Module):\n    def __init__(self):\n        super(ComplexNet, self).__init__()\n        self.conv1d = nn.Conv1d(5, 10, 2, groups=5)\n        self.mn1 = MiniNet()\n        self.bn1 = nn.BatchNorm1d(1)\n\n    def forward(self, x):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_0-45"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "import torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import OmegaConf\nfrom omnivision.optim import construct_optimizer, OmniOptimizer\nfrom omnivision.trainer.omnivision_trainer import OmnivisionOptimConf\n\nCONFIG_FOLDER = Path(__file__).parent / \"configs\"\n\n\nclass MiniNet(nn.Module):\n    def __init__(self):\n        super(MiniNet, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n        nn.init.constant_(self.fc1.weight, 1.0)\n        nn.init.constant_(self.fc1.bias, 2.0)\n\n        nn.init.constant_(self.fc2.weight, 3.0)\n        nn.init.constant_(self.fc2.bias, 4.0)\n\n        nn.init.constant_(self.fc3.weight, 5.0)\n        nn.init.constant_(self.fc3.bias, 6.0)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass ComplexNet(nn.Module):\n    def __init__(self):\n        super(ComplexNet, self).__init__()\n        self.conv1d = nn.Conv1d(5, 10, 2, groups=5)\n        self.mn1 = MiniNet()\n        self.bn1 = nn.BatchNorm1d(1)\n\n    def forward(self, x):\n        x = self.conv1d(x)\n        x = self.mn1(x)\n        x = self.bn1(x)\n        return x\n\n\nclass TestSchedulerConf(unittest.TestCase):\n    def _check_valid(self, param_groups, expected_values):\n        for param_group in param_groups:\n            self.assertTrue(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_5-55"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self):\n        super(MiniNet, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 5)\n        self.fc3 = nn.Linear(5, 1)\n\n        nn.init.constant_(self.fc1.weight, 1.0)\n        nn.init.constant_(self.fc1.bias, 2.0)\n\n        nn.init.constant_(self.fc2.weight, 3.0)\n        nn.init.constant_(self.fc2.bias, 4.0)\n\n        nn.init.constant_(self.fc3.weight, 5.0)\n        nn.init.constant_(self.fc3.bias, 6.0)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass ComplexNet(nn.Module):\n    def __init__(self):\n        super(ComplexNet, self).__init__()\n        self.conv1d = nn.Conv1d(5, 10, 2, groups=5)\n        self.mn1 = MiniNet()\n        self.bn1 = nn.BatchNorm1d(1)\n\n    def forward(self, x):\n        x = self.conv1d(x)\n        x = self.mn1(x)\n        x = self.bn1(x)\n        return x\n\n\nclass TestSchedulerConf(unittest.TestCase):\n    def _check_valid(self, param_groups, expected_values):\n        for param_group in param_groups:\n            self.assertTrue(\n                set([\"lr\", \"weight_decay\", \"params\"]).issubset(set(param_group.keys()))\n            )\n            lr = np.round(param_group[\"lr\"], 2)\n            wd = np.round(param_group[\"weight_decay\"], 2)\n            self.assertEqual(expected_values[lr][wd], set(param_group[\"params\"]))\n\n    def test_scheduler_base(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_base.yaml\")\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_15-65"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        nn.init.constant_(self.fc2.bias, 4.0)\n\n        nn.init.constant_(self.fc3.weight, 5.0)\n        nn.init.constant_(self.fc3.bias, 6.0)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass ComplexNet(nn.Module):\n    def __init__(self):\n        super(ComplexNet, self).__init__()\n        self.conv1d = nn.Conv1d(5, 10, 2, groups=5)\n        self.mn1 = MiniNet()\n        self.bn1 = nn.BatchNorm1d(1)\n\n    def forward(self, x):\n        x = self.conv1d(x)\n        x = self.mn1(x)\n        x = self.bn1(x)\n        return x\n\n\nclass TestSchedulerConf(unittest.TestCase):\n    def _check_valid(self, param_groups, expected_values):\n        for param_group in param_groups:\n            self.assertTrue(\n                set([\"lr\", \"weight_decay\", \"params\"]).issubset(set(param_group.keys()))\n            )\n            lr = np.round(param_group[\"lr\"], 2)\n            wd = np.round(param_group[\"weight_decay\"], 2)\n            self.assertEqual(expected_values[lr][wd], set(param_group[\"params\"]))\n\n    def test_scheduler_base(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_base.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_25-75"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\nclass ComplexNet(nn.Module):\n    def __init__(self):\n        super(ComplexNet, self).__init__()\n        self.conv1d = nn.Conv1d(5, 10, 2, groups=5)\n        self.mn1 = MiniNet()\n        self.bn1 = nn.BatchNorm1d(1)\n\n    def forward(self, x):\n        x = self.conv1d(x)\n        x = self.mn1(x)\n        x = self.bn1(x)\n        return x\n\n\nclass TestSchedulerConf(unittest.TestCase):\n    def _check_valid(self, param_groups, expected_values):\n        for param_group in param_groups:\n            self.assertTrue(\n                set([\"lr\", \"weight_decay\", \"params\"]).issubset(set(param_group.keys()))\n            )\n            lr = np.round(param_group[\"lr\"], 2)\n            wd = np.round(param_group[\"weight_decay\"], 2)\n            self.assertEqual(expected_values[lr][wd], set(param_group[\"params\"]))\n\n    def test_scheduler_base(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_base.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_35-85"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        x = self.conv1d(x)\n        x = self.mn1(x)\n        x = self.bn1(x)\n        return x\n\n\nclass TestSchedulerConf(unittest.TestCase):\n    def _check_valid(self, param_groups, expected_values):\n        for param_group in param_groups:\n            self.assertTrue(\n                set([\"lr\", \"weight_decay\", \"params\"]).issubset(set(param_group.keys()))\n            )\n            lr = np.round(param_group[\"lr\"], 2)\n            wd = np.round(param_group[\"weight_decay\"], 2)\n            self.assertEqual(expected_values[lr][wd], set(param_group[\"params\"]))\n\n    def test_scheduler_base(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_base.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_45-95"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                set([\"lr\", \"weight_decay\", \"params\"]).issubset(set(param_group.keys()))\n            )\n            lr = np.round(param_group[\"lr\"], 2)\n            wd = np.round(param_group[\"weight_decay\"], 2)\n            self.assertEqual(expected_values[lr][wd], set(param_group[\"params\"]))\n\n    def test_scheduler_base(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_base.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_55-105"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_unspecified_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unspecified_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_65-115"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_unspecified_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unspecified_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n        expected_values = {\n            0.2: {0.0: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.9: {\n                0.4: set([mini_net.fc2.weight]),\n                0.0: set([mini_net.fc3.weight, mini_net.fc2.bias, mini_net.fc3.bias]),\n            },", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_75-125"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_unspecified_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unspecified_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n        expected_values = {\n            0.2: {0.0: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.9: {\n                0.4: set([mini_net.fc2.weight]),\n                0.0: set([mini_net.fc3.weight, mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_85-135"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_unspecified_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unspecified_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n        expected_values = {\n            0.2: {0.0: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.9: {\n                0.4: set([mini_net.fc2.weight]),\n                0.0: set([mini_net.fc3.weight, mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_95-145"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_unspecified_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unspecified_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n        expected_values = {\n            0.2: {0.0: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.9: {\n                0.4: set([mini_net.fc2.weight]),\n                0.0: set([mini_net.fc3.weight, mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_105-155"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n        expected_values = {\n            0.2: {0.0: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.9: {\n                0.4: set([mini_net.fc2.weight]),\n                0.0: set([mini_net.fc3.weight, mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_non_constant(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_linear.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_115-165"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_basic_param_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_basic_param_module.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_non_constant(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_linear.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_125-175"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight, mini_net.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_non_constant(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_linear.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n        # check values at init are same for step where = 0\n        optimizer.step(0.0)\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.25)\n        # Update the LR (key in expected_values) to correspond to 0.25 of training\n        lr_0_25 = np.round((1 - 0.25) * 0.2, 2)\n        expected_values[lr_0_25] = expected_values[0.2]\n        del expected_values[0.2]", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_135-185"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                        mini_net.fc2.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.weight,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_non_constant(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_linear.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n        # check values at init are same for step where = 0\n        optimizer.step(0.0)\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.25)\n        # Update the LR (key in expected_values) to correspond to 0.25 of training\n        lr_0_25 = np.round((1 - 0.25) * 0.2, 2)\n        expected_values[lr_0_25] = expected_values[0.2]\n        del expected_values[0.2]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.5)\n        # Update the LR (key in expected_values) to correspond to 0.5 of training\n        lr_0_5 = np.round((1 - 0.5) * 0.2, 2)\n        expected_values[lr_0_5] = expected_values[lr_0_25]\n        del expected_values[lr_0_25]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_defaults(self) -> None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_145-195"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n    def test_scheduler_non_constant(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_linear.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n        # check values at init are same for step where = 0\n        optimizer.step(0.0)\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.25)\n        # Update the LR (key in expected_values) to correspond to 0.25 of training\n        lr_0_25 = np.round((1 - 0.25) * 0.2, 2)\n        expected_values[lr_0_25] = expected_values[0.2]\n        del expected_values[0.2]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.5)\n        # Update the LR (key in expected_values) to correspond to 0.5 of training\n        lr_0_5 = np.round((1 - 0.5) * 0.2, 2)\n        expected_values[lr_0_5] = expected_values[lr_0_25]\n        del expected_values[lr_0_25]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 6)\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_155-205"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n        # check values at init are same for step where = 0\n        optimizer.step(0.0)\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.25)\n        # Update the LR (key in expected_values) to correspond to 0.25 of training\n        lr_0_25 = np.round((1 - 0.25) * 0.2, 2)\n        expected_values[lr_0_25] = expected_values[0.2]\n        del expected_values[0.2]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.5)\n        # Update the LR (key in expected_values) to correspond to 0.5 of training\n        lr_0_5 = np.round((1 - 0.5) * 0.2, 2)\n        expected_values[lr_0_5] = expected_values[lr_0_25]\n        del expected_values[lr_0_25]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 6)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {0.4: set([mini_net.fc3.weight]), 0.5: set([mini_net.fc3.bias])},\n            0.6: {0.4: set([mini_net.fc2.weight]), 0.5: set([mini_net.fc2.bias])},\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_multiple.yaml\")", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_165-215"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n        # check values at init are same for step where = 0\n        optimizer.step(0.0)\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.25)\n        # Update the LR (key in expected_values) to correspond to 0.25 of training\n        lr_0_25 = np.round((1 - 0.25) * 0.2, 2)\n        expected_values[lr_0_25] = expected_values[0.2]\n        del expected_values[0.2]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.5)\n        # Update the LR (key in expected_values) to correspond to 0.5 of training\n        lr_0_5 = np.round((1 - 0.5) * 0.2, 2)\n        expected_values[lr_0_5] = expected_values[lr_0_25]\n        del expected_values[lr_0_25]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 6)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {0.4: set([mini_net.fc3.weight]), 0.5: set([mini_net.fc3.bias])},\n            0.6: {0.4: set([mini_net.fc2.weight]), 0.5: set([mini_net.fc2.bias])},\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_multiple.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_175-225"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n        optimizer.step(0.5)\n        # Update the LR (key in expected_values) to correspond to 0.5 of training\n        lr_0_5 = np.round((1 - 0.5) * 0.2, 2)\n        expected_values[lr_0_5] = expected_values[lr_0_25]\n        del expected_values[lr_0_25]\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 6)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {0.4: set([mini_net.fc3.weight]), 0.5: set([mini_net.fc3.bias])},\n            0.6: {0.4: set([mini_net.fc2.weight]), 0.5: set([mini_net.fc2.bias])},\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_multiple.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed1.yaml\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_185-235"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 6)\n\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {0.4: set([mini_net.fc3.weight]), 0.5: set([mini_net.fc3.bias])},\n            0.6: {0.4: set([mini_net.fc2.weight]), 0.5: set([mini_net.fc2.bias])},\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_multiple.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_195-245"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {0.4: set([mini_net.fc3.weight]), 0.5: set([mini_net.fc3.bias])},\n            0.6: {0.4: set([mini_net.fc2.weight]), 0.5: set([mini_net.fc2.bias])},\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_with_multiple.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_205-255"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 4)\n        expected_values = {\n            0.2: {0.4: set([mini_net.fc1.weight]), 0.5: set([mini_net.fc1.bias])},\n            0.3: {\n                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n            },\n            0.8: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_215-265"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                0.4: set([mini_net.fc2.weight, mini_net.fc3.weight]),\n                0.5: set([mini_net.fc2.bias, mini_net.fc3.bias]),\n            },\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n            },\n            0.8: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed2.yaml\"\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_225-275"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n            },\n            0.8: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_235-285"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            0.2: {0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias])},\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n            },\n            0.8: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                )\n            },\n            0.3: {", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_245-295"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            },\n            0.8: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                )\n            },\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias]),\n            },", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_255-305"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                )\n            },\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module3(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed3.yaml\"\n            )\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_265-315"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 3)\n\n        expected_values = {\n            0.2: {\n                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                )\n            },\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module3(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed3.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.3: {\n                0.4: set(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_275-325"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                0.5: set(\n                    [\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                )\n            },\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module3(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed3.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set(\n                    [\n                        mini_net.mn1.fc1.weight,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_285-335"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set([mini_net.mn1.fc1.weight, mini_net.mn1.fc1.bias]),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module3(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed3.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set(\n                    [\n                        mini_net.mn1.fc1.weight,\n                        mini_net.mn1.fc1.bias,\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_295-345"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_scheduler_complex_param_module3(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_mixed3.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set(\n                    [\n                        mini_net.mn1.fc1.weight,\n                        mini_net.mn1.fc1.bias,\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_invalid_scheduler_multiple_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"invalid_scheduler_multiple_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*one scheduler.*option.*default.*\"\n        ):", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_305-355"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mini_net = ComplexNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 2)\n\n        expected_values = {\n            0.3: {\n                0.4: set(\n                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set(\n                    [\n                        mini_net.mn1.fc1.weight,\n                        mini_net.mn1.fc1.bias,\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_invalid_scheduler_multiple_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"invalid_scheduler_multiple_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*one scheduler.*option.*default.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"invalid_scheduler_overlapping_groups.yaml\"\n            )\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_315-365"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                    [\n                        mini_net.bn1.weight,\n                        mini_net.bn1.bias,\n                        mini_net.conv1d.weight,\n                        mini_net.conv1d.bias,\n                    ]\n                ),\n                0.5: set(\n                    [\n                        mini_net.mn1.fc1.weight,\n                        mini_net.mn1.fc1.bias,\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_invalid_scheduler_multiple_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"invalid_scheduler_multiple_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*one scheduler.*option.*default.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"invalid_scheduler_overlapping_groups.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid1.yaml\"", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_325-375"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "                        mini_net.mn1.fc1.bias,\n                        mini_net.mn1.fc2.weight,\n                        mini_net.mn1.fc2.bias,\n                        mini_net.mn1.fc3.weight,\n                        mini_net.mn1.fc3.bias,\n                    ]\n                ),\n            },\n        }\n\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_invalid_scheduler_multiple_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"invalid_scheduler_multiple_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*one scheduler.*option.*default.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"invalid_scheduler_overlapping_groups.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module2(self) -> None:\n        conf = OmnivisionOptimConf(", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_335-385"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        self._check_valid(optimizer.optimizer.param_groups, expected_values)\n\n    def test_invalid_scheduler_multiple_defaults(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"invalid_scheduler_multiple_defaults.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*one scheduler.*option.*default.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"invalid_scheduler_overlapping_groups.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_345-395"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"invalid_scheduler_overlapping_groups.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_nonexistent_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_param_module_non_existent.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*option.*lr.*BatchNorm1d.*not match.*class.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_355-405"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module1(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid1.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_nonexistent_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_param_module_non_existent.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*option.*lr.*BatchNorm1d.*not match.*class.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unused_param_names.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_365-415"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_overlapping_groups_module2(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_nonexistent_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_param_module_non_existent.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*option.*lr.*BatchNorm1d.*not match.*class.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unused_param_names.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_unused_param_names_multiple.yaml\"\n            )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_375-425"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_complex_param_module_invalid2.yaml\"\n            )\n        )\n        mini_net = ComplexNet()\n        with self.assertRaisesRegex(AssertionError, \".*param.groups.*disjoint.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_scheduler_nonexistent_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_param_module_non_existent.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*option.*lr.*BatchNorm1d.*not match.*class.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unused_param_names.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_unused_param_names_multiple.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_385-435"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def test_invalid_scheduler_nonexistent_module(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_param_module_non_existent.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError, \".*option.*lr.*BatchNorm1d.*not match.*class.*\"\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unused_param_names.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_unused_param_names_multiple.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_option(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_invalid_option.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*wd.*not found.*SGD.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_395-445"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            )\n\n    def test_unused_param_groups(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_unused_param_names.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_unused_param_names_multiple.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_option(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_invalid_option.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*wd.*not found.*SGD.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_scheduler_only_default(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_only_default.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 1)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_405-455"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_unused_param_groups_multiple(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(\n                CONFIG_FOLDER / \"scheduler_unused_param_names_multiple.yaml\"\n            )\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_option(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_invalid_option.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*wd.*not found.*SGD.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_scheduler_only_default(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_only_default.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 1)\n\n        expected_values = {\n            0.2: {\n                0.4: set(\n                    [\n                        mini_net.fc1.weight,\n                        mini_net.fc1.bias,\n                        mini_net.fc2.weight,\n                        mini_net.fc3.weight,\n                        mini_net.fc2.bias,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_415-465"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 471, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(\n            AssertionError,\n            \".*option.*lr.*fc4\\*.*not match.*\",\n        ):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_invalid_option(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_invalid_option.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*wd.*not found.*SGD.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_scheduler_only_default(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_only_default.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 1)\n\n        expected_values = {\n            0.2: {\n                0.4: set(\n                    [\n                        mini_net.fc1.weight,\n                        mini_net.fc1.bias,\n                        mini_net.fc2.weight,\n                        mini_net.fc3.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            }\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_425-471"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 471, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def test_invalid_option(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_invalid_option.yaml\")\n        )\n        mini_net = MiniNet()\n        with self.assertRaisesRegex(AssertionError, \".*wd.*not found.*SGD.*\"):\n            construct_optimizer(\n                mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n            )\n\n    def test_scheduler_only_default(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_only_default.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 1)\n\n        expected_values = {\n            0.2: {\n                0.4: set(\n                    [\n                        mini_net.fc1.weight,\n                        mini_net.fc1.bias,\n                        mini_net.fc2.weight,\n                        mini_net.fc3.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            }\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_435-471"}
{"title": "facebookresearch_omnivore-tests-test_scheduler.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "test_scheduler.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 471, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def test_scheduler_only_default(self) -> None:\n        conf = OmnivisionOptimConf(\n            **OmegaConf.load(CONFIG_FOLDER / \"scheduler_only_default.yaml\")\n        )\n        mini_net = MiniNet()\n        optimizer = construct_optimizer(\n            mini_net, conf.optimizer, conf.options, conf.param_group_modifiers\n        )\n        self.assertIsInstance(optimizer, OmniOptimizer)\n        self.assertEqual(len(optimizer.optimizer.param_groups), 1)\n\n        expected_values = {\n            0.2: {\n                0.4: set(\n                    [\n                        mini_net.fc1.weight,\n                        mini_net.fc1.bias,\n                        mini_net.fc2.weight,\n                        mini_net.fc3.weight,\n                        mini_net.fc2.bias,\n                        mini_net.fc3.bias,\n                    ]\n                ),\n            }\n        }\n        self._check_valid(optimizer.optimizer.param_groups, expected_values)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-test_scheduler.py_445-471"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\nimport csv\nimport os\nfrom functools import wraps\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Callable, Dict, List, Optional\nfrom unittest import mock\n\nimport torch\nimport torch.nn as nn\nimport torchvision.io as io\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self, inp_dim, num_layers, init_val=0.0):\n        super().__init__()\n\n        tmp_inp_dim = inp_dim\n        self.num_layers = num_layers\n        for i in range(num_layers):\n            setattr(self, f\"layer_{i}\", nn.Linear(tmp_inp_dim, tmp_inp_dim + 1))\n            layer = getattr(self, f\"layer_{i}\")\n            nn.init.constant_(layer.weight, init_val)\n            nn.init.constant_(layer.bias, init_val)\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))For(Name(Store)Call(Name(Load)Name(Load))Expr(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)AddConstant))))Assign(Name(Store)Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Name(Load)))))))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_0-25"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\nimport csv\nimport os\nfrom functools import wraps\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Callable, Dict, List, Optional\nfrom unittest import mock\n\nimport torch\nimport torch.nn as nn\nimport torchvision.io as io\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self, inp_dim, num_layers, init_val=0.0):\n        super().__init__()\n\n        tmp_inp_dim = inp_dim\n        self.num_layers = num_layers\n        for i in range(num_layers):\n            setattr(self, f\"layer_{i}\", nn.Linear(tmp_inp_dim, tmp_inp_dim + 1))\n            layer = getattr(self, f\"layer_{i}\")\n            nn.init.constant_(layer.weight, init_val)\n            nn.init.constant_(layer.bias, init_val)\n            tmp_inp_dim += 1\n\n    def forward(self, x):\n        for i in range(self.num_layers):\n            layer = getattr(self, f\"layer_{i}\")\n            x = layer(x)\n        return x\n\n\ndef create_small_kinetics_dataset(root_dir: str) -> None:", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_0-35"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\nimport csv\nimport os\nfrom functools import wraps\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Callable, Dict, List, Optional\nfrom unittest import mock\n\nimport torch\nimport torch.nn as nn\nimport torchvision.io as io\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self, inp_dim, num_layers, init_val=0.0):\n        super().__init__()\n\n        tmp_inp_dim = inp_dim\n        self.num_layers = num_layers\n        for i in range(num_layers):\n            setattr(self, f\"layer_{i}\", nn.Linear(tmp_inp_dim, tmp_inp_dim + 1))\n            layer = getattr(self, f\"layer_{i}\")\n            nn.init.constant_(layer.weight, init_val)\n            nn.init.constant_(layer.bias, init_val)\n            tmp_inp_dim += 1\n\n    def forward(self, x):\n        for i in range(self.num_layers):\n            layer = getattr(self, f\"layer_{i}\")\n            x = layer(x)\n        return x\n\n\ndef create_small_kinetics_dataset(root_dir: str) -> None:\n    \"\"\"\n    A test utility function to create a small Kinetics like dataset\n\n    Args:\n        root_dir(str): The directory to create the dataset in.\n        Typically, a temporary directory is used.\n    \"\"\"\n    video_codec = \"libx264rgb\"\n    options = {\"crf\": \"0\"}\n    height: int = 250\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(alias)Import(alias)Import(alias)Import(alias)ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))For(Name(Store)Call(Name(Load)Name(Load))Expr(Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)AddConstant))))Assign(Name(Store)Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)Name(Load)))AugAssign(Name(Store)AddConstant)))FunctionDef(arguments(argarg)For(Name(Store)Call(Name(Load)Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))))Assign(Name(Store)Call(Name(Load)Name(Load))))Return(Name(Load))))FunctionDef(arguments(arg(Name(Load)))Expr(Constant)Assign(Name(Store)Constant)Assign(Name(Store)Dict(ConstantConstant))AnnAssign(Name(Store)Name(Load)Constant)Constant))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_0-45"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "from tempfile import TemporaryDirectory\nfrom typing import Any, Callable, Dict, List, Optional\nfrom unittest import mock\n\nimport torch\nimport torch.nn as nn\nimport torchvision.io as io\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self, inp_dim, num_layers, init_val=0.0):\n        super().__init__()\n\n        tmp_inp_dim = inp_dim\n        self.num_layers = num_layers\n        for i in range(num_layers):\n            setattr(self, f\"layer_{i}\", nn.Linear(tmp_inp_dim, tmp_inp_dim + 1))\n            layer = getattr(self, f\"layer_{i}\")\n            nn.init.constant_(layer.weight, init_val)\n            nn.init.constant_(layer.bias, init_val)\n            tmp_inp_dim += 1\n\n    def forward(self, x):\n        for i in range(self.num_layers):\n            layer = getattr(self, f\"layer_{i}\")\n            x = layer(x)\n        return x\n\n\ndef create_small_kinetics_dataset(root_dir: str) -> None:\n    \"\"\"\n    A test utility function to create a small Kinetics like dataset\n\n    Args:\n        root_dir(str): The directory to create the dataset in.\n        Typically, a temporary directory is used.\n    \"\"\"\n    video_codec = \"libx264rgb\"\n    options = {\"crf\": \"0\"}\n    height: int = 250\n    width: int = 250\n    num_frames = 20\n    fps = 5\n    data = create_dummy_video_frames(num_frames, height, width)\n\n    train_data = [\n        [\"a.mp4\", \"308\"],\n        [\"b.mp4\", \"298\"],\n        [\"c.mp4\", \"240\"],\n        [\"d.mp4\", \"363\"],", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_5-55"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def __init__(self, inp_dim, num_layers, init_val=0.0):\n        super().__init__()\n\n        tmp_inp_dim = inp_dim\n        self.num_layers = num_layers\n        for i in range(num_layers):\n            setattr(self, f\"layer_{i}\", nn.Linear(tmp_inp_dim, tmp_inp_dim + 1))\n            layer = getattr(self, f\"layer_{i}\")\n            nn.init.constant_(layer.weight, init_val)\n            nn.init.constant_(layer.bias, init_val)\n            tmp_inp_dim += 1\n\n    def forward(self, x):\n        for i in range(self.num_layers):\n            layer = getattr(self, f\"layer_{i}\")\n            x = layer(x)\n        return x\n\n\ndef create_small_kinetics_dataset(root_dir: str) -> None:\n    \"\"\"\n    A test utility function to create a small Kinetics like dataset\n\n    Args:\n        root_dir(str): The directory to create the dataset in.\n        Typically, a temporary directory is used.\n    \"\"\"\n    video_codec = \"libx264rgb\"\n    options = {\"crf\": \"0\"}\n    height: int = 250\n    width: int = 250\n    num_frames = 20\n    fps = 5\n    data = create_dummy_video_frames(num_frames, height, width)\n\n    train_data = [\n        [\"a.mp4\", \"308\"],\n        [\"b.mp4\", \"298\"],\n        [\"c.mp4\", \"240\"],\n        [\"d.mp4\", \"363\"],\n    ]\n\n    val_data = [\n        [\"a.mp4\", \"151\"],\n    ]\n\n    for i in range(4):\n        io.write_video(\n            os.path.join(root_dir, train_data[i][0]),\n            data,", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_15-65"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            tmp_inp_dim += 1\n\n    def forward(self, x):\n        for i in range(self.num_layers):\n            layer = getattr(self, f\"layer_{i}\")\n            x = layer(x)\n        return x\n\n\ndef create_small_kinetics_dataset(root_dir: str) -> None:\n    \"\"\"\n    A test utility function to create a small Kinetics like dataset\n\n    Args:\n        root_dir(str): The directory to create the dataset in.\n        Typically, a temporary directory is used.\n    \"\"\"\n    video_codec = \"libx264rgb\"\n    options = {\"crf\": \"0\"}\n    height: int = 250\n    width: int = 250\n    num_frames = 20\n    fps = 5\n    data = create_dummy_video_frames(num_frames, height, width)\n\n    train_data = [\n        [\"a.mp4\", \"308\"],\n        [\"b.mp4\", \"298\"],\n        [\"c.mp4\", \"240\"],\n        [\"d.mp4\", \"363\"],\n    ]\n\n    val_data = [\n        [\"a.mp4\", \"151\"],\n    ]\n\n    for i in range(4):\n        io.write_video(\n            os.path.join(root_dir, train_data[i][0]),\n            data,\n            fps=fps,\n            video_codec=video_codec,\n            options=options,\n        )\n\n    train_file = os.path.join(root_dir, \"train.csv\")\n    write_single_csv_file(train_file, train_data)\n\n    val_file = os.path.join(root_dir, \"val.csv\")\n    write_single_csv_file(val_file, val_data)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_25-75"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    \"\"\"\n    A test utility function to create a small Kinetics like dataset\n\n    Args:\n        root_dir(str): The directory to create the dataset in.\n        Typically, a temporary directory is used.\n    \"\"\"\n    video_codec = \"libx264rgb\"\n    options = {\"crf\": \"0\"}\n    height: int = 250\n    width: int = 250\n    num_frames = 20\n    fps = 5\n    data = create_dummy_video_frames(num_frames, height, width)\n\n    train_data = [\n        [\"a.mp4\", \"308\"],\n        [\"b.mp4\", \"298\"],\n        [\"c.mp4\", \"240\"],\n        [\"d.mp4\", \"363\"],\n    ]\n\n    val_data = [\n        [\"a.mp4\", \"151\"],\n    ]\n\n    for i in range(4):\n        io.write_video(\n            os.path.join(root_dir, train_data[i][0]),\n            data,\n            fps=fps,\n            video_codec=video_codec,\n            options=options,\n        )\n\n    train_file = os.path.join(root_dir, \"train.csv\")\n    write_single_csv_file(train_file, train_data)\n\n    val_file = os.path.join(root_dir, \"val.csv\")\n    write_single_csv_file(val_file, val_data)\n\n\n# pyre-fixme[2]: Parameter annotation cannot contain `Any`.\ndef write_single_csv_file(file_name: str, data: List[Any]) -> None:\n    with open(file_name, \"w+\", newline=\"\") as csvfile:\n        data_writer = csv.writer(\n            # pyre-fixme[6]: Expected `_Writer` for 1st param but got `TextIOWrapper`.\n            csvfile,\n            delimiter=\" \",\n        )", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_35-85"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    width: int = 250\n    num_frames = 20\n    fps = 5\n    data = create_dummy_video_frames(num_frames, height, width)\n\n    train_data = [\n        [\"a.mp4\", \"308\"],\n        [\"b.mp4\", \"298\"],\n        [\"c.mp4\", \"240\"],\n        [\"d.mp4\", \"363\"],\n    ]\n\n    val_data = [\n        [\"a.mp4\", \"151\"],\n    ]\n\n    for i in range(4):\n        io.write_video(\n            os.path.join(root_dir, train_data[i][0]),\n            data,\n            fps=fps,\n            video_codec=video_codec,\n            options=options,\n        )\n\n    train_file = os.path.join(root_dir, \"train.csv\")\n    write_single_csv_file(train_file, train_data)\n\n    val_file = os.path.join(root_dir, \"val.csv\")\n    write_single_csv_file(val_file, val_data)\n\n\n# pyre-fixme[2]: Parameter annotation cannot contain `Any`.\ndef write_single_csv_file(file_name: str, data: List[Any]) -> None:\n    with open(file_name, \"w+\", newline=\"\") as csvfile:\n        data_writer = csv.writer(\n            # pyre-fixme[6]: Expected `_Writer` for 1st param but got `TextIOWrapper`.\n            csvfile,\n            delimiter=\" \",\n        )\n        for row in data:\n            data_writer.writerow(row)\n\n\n# pyre-fixme[3]\ndef create_dummy_video_frames(num_frames: int, height: int, width: int):\n    y, x = torch.meshgrid(torch.linspace(-2, 2, height), torch.linspace(-2, 2, width))\n    data = []\n    for i in range(num_frames):\n        xc = float(i) / num_frames", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_45-95"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    ]\n\n    val_data = [\n        [\"a.mp4\", \"151\"],\n    ]\n\n    for i in range(4):\n        io.write_video(\n            os.path.join(root_dir, train_data[i][0]),\n            data,\n            fps=fps,\n            video_codec=video_codec,\n            options=options,\n        )\n\n    train_file = os.path.join(root_dir, \"train.csv\")\n    write_single_csv_file(train_file, train_data)\n\n    val_file = os.path.join(root_dir, \"val.csv\")\n    write_single_csv_file(val_file, val_data)\n\n\n# pyre-fixme[2]: Parameter annotation cannot contain `Any`.\ndef write_single_csv_file(file_name: str, data: List[Any]) -> None:\n    with open(file_name, \"w+\", newline=\"\") as csvfile:\n        data_writer = csv.writer(\n            # pyre-fixme[6]: Expected `_Writer` for 1st param but got `TextIOWrapper`.\n            csvfile,\n            delimiter=\" \",\n        )\n        for row in data:\n            data_writer.writerow(row)\n\n\n# pyre-fixme[3]\ndef create_dummy_video_frames(num_frames: int, height: int, width: int):\n    y, x = torch.meshgrid(torch.linspace(-2, 2, height), torch.linspace(-2, 2, width))\n    data = []\n    for i in range(num_frames):\n        xc = float(i) / num_frames\n        yc = 1 - float(i) / (2 * num_frames)\n        d = torch.exp(-((x - xc) ** 2 + (y - yc) ** 2) / 2) * 255\n        data.append(d.unsqueeze(2).repeat(1, 1, 3).byte())\n    return torch.stack(data, 0)\n\n\ndef run_locally(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator to run unittest locally.\"\"\"\n\n    @wraps(func)", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_55-105"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "            fps=fps,\n            video_codec=video_codec,\n            options=options,\n        )\n\n    train_file = os.path.join(root_dir, \"train.csv\")\n    write_single_csv_file(train_file, train_data)\n\n    val_file = os.path.join(root_dir, \"val.csv\")\n    write_single_csv_file(val_file, val_data)\n\n\n# pyre-fixme[2]: Parameter annotation cannot contain `Any`.\ndef write_single_csv_file(file_name: str, data: List[Any]) -> None:\n    with open(file_name, \"w+\", newline=\"\") as csvfile:\n        data_writer = csv.writer(\n            # pyre-fixme[6]: Expected `_Writer` for 1st param but got `TextIOWrapper`.\n            csvfile,\n            delimiter=\" \",\n        )\n        for row in data:\n            data_writer.writerow(row)\n\n\n# pyre-fixme[3]\ndef create_dummy_video_frames(num_frames: int, height: int, width: int):\n    y, x = torch.meshgrid(torch.linspace(-2, 2, height), torch.linspace(-2, 2, width))\n    data = []\n    for i in range(num_frames):\n        xc = float(i) / num_frames\n        yc = 1 - float(i) / (2 * num_frames)\n        d = torch.exp(-((x - xc) ** 2 + (y - yc) ** 2) / 2) * 255\n        data.append(d.unsqueeze(2).repeat(1, 1, 3).byte())\n    return torch.stack(data, 0)\n\n\ndef run_locally(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator to run unittest locally.\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):  # pyre-ignore[2,3]\n        with mock.patch(\n            \"torch.distributed.is_available\",\n            return_value=False,\n        ):\n            return func(*args, **kwargs)\n\n    return wrapper\n\n", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_65-115"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\n# pyre-fixme[2]: Parameter annotation cannot contain `Any`.\ndef write_single_csv_file(file_name: str, data: List[Any]) -> None:\n    with open(file_name, \"w+\", newline=\"\") as csvfile:\n        data_writer = csv.writer(\n            # pyre-fixme[6]: Expected `_Writer` for 1st param but got `TextIOWrapper`.\n            csvfile,\n            delimiter=\" \",\n        )\n        for row in data:\n            data_writer.writerow(row)\n\n\n# pyre-fixme[3]\ndef create_dummy_video_frames(num_frames: int, height: int, width: int):\n    y, x = torch.meshgrid(torch.linspace(-2, 2, height), torch.linspace(-2, 2, width))\n    data = []\n    for i in range(num_frames):\n        xc = float(i) / num_frames\n        yc = 1 - float(i) / (2 * num_frames)\n        d = torch.exp(-((x - xc) ** 2 + (y - yc) ** 2) / 2) * 255\n        data.append(d.unsqueeze(2).repeat(1, 1, 3).byte())\n    return torch.stack(data, 0)\n\n\ndef run_locally(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator to run unittest locally.\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):  # pyre-ignore[2,3]\n        with mock.patch(\n            \"torch.distributed.is_available\",\n            return_value=False,\n        ):\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef tempdir(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator for creating a tempory directory that\n    is cleaned up after function execution.\"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):  # pyre-ignore[2,3]\n        with TemporaryDirectory() as temp:\n            return func(self, temp, *args, **kwargs)\n\n    return wrapper\n\nAST=Module(FunctionDef(arguments(arg(Name(Load))arg(Subscript(Name(Load)Name(Load)Load)))With(withitem(Call(Name(Load)Name(Load)Constantkeyword(Constant))Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))For(Name(Store)Name(Load)Expr(Call(Attribute(Name(Load)Load)Name(Load)))))Constant)FunctionDef(arguments(arg(Name(Load))arg(Name(Load))arg(Name(Load)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)UnaryOp(USubConstant)ConstantName(Load))Call(Attribute(Name(Load)Load)UnaryOp(USubConstant)ConstantName(Load))))Assign(Name(Store)List(Load))For(Name(Store)Call(Name(Load)Name(Load))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load))DivName(Load)))Assign(Name(Store)BinOp(ConstantSubBinOp(Call(Name(Load)Name(Load))DivBinOp(ConstantMultName(Load)))))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)BinOp(UnaryOp(USubBinOp(BinOp(BinOp(Name(Load)SubName(Load))PowConstant)AddBinOp(BinOp(Name(Load)SubName(Load))PowConstant)))DivConstant))MultConstant))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Call(Attribute(Call(Attribute(Name(Load)Load)Constant)Load)ConstantConstantConstant)Load)))))Return(Call(Attribute(Name(Load)Load)Name(Load)Constant)))FunctionDef(arguments(arg(Name(Load)))Expr(Constant)FunctionDef(arguments(argarg)With(withitem(Call(Attribute(Name(Load)Load)Constantkeyword(Constant)))Return(Call(Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))))Call(Name(Load)Name(Load)))Return(Name(Load))Name(Load))FunctionDef(arguments(arg(Name(Load)))Expr(Constant)FunctionDef(arguments(argargarg)With(withitem(Call(Name(Load))Name(Store))Return(Call(Name(Load)Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))))Call(Name(Load)Name(Load)))Return(Name(Load))Name(Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_75-125"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        for row in data:\n            data_writer.writerow(row)\n\n\n# pyre-fixme[3]\ndef create_dummy_video_frames(num_frames: int, height: int, width: int):\n    y, x = torch.meshgrid(torch.linspace(-2, 2, height), torch.linspace(-2, 2, width))\n    data = []\n    for i in range(num_frames):\n        xc = float(i) / num_frames\n        yc = 1 - float(i) / (2 * num_frames)\n        d = torch.exp(-((x - xc) ** 2 + (y - yc) ** 2) / 2) * 255\n        data.append(d.unsqueeze(2).repeat(1, 1, 3).byte())\n    return torch.stack(data, 0)\n\n\ndef run_locally(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator to run unittest locally.\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):  # pyre-ignore[2,3]\n        with mock.patch(\n            \"torch.distributed.is_available\",\n            return_value=False,\n        ):\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef tempdir(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator for creating a tempory directory that\n    is cleaned up after function execution.\"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):  # pyre-ignore[2,3]\n        with TemporaryDirectory() as temp:\n            return func(self, temp, *args, **kwargs)\n\n    return wrapper\n\n\ndef get_mock_init_trainer_params(\n    overrides: Optional[Dict[str, Any]] = None,\n) -> Callable[..., Dict[str, Any]]:\n    \"\"\"\n    Order of trainer_params setting in unit test:\n      - First call original function, which sets params from config\n      - Then override some params to disable logger and checkpoint\n      - Apply any test-specific overrides.", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_85-135"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "        yc = 1 - float(i) / (2 * num_frames)\n        d = torch.exp(-((x - xc) ** 2 + (y - yc) ** 2) / 2) * 255\n        data.append(d.unsqueeze(2).repeat(1, 1, 3).byte())\n    return torch.stack(data, 0)\n\n\ndef run_locally(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator to run unittest locally.\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):  # pyre-ignore[2,3]\n        with mock.patch(\n            \"torch.distributed.is_available\",\n            return_value=False,\n        ):\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef tempdir(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator for creating a tempory directory that\n    is cleaned up after function execution.\"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):  # pyre-ignore[2,3]\n        with TemporaryDirectory() as temp:\n            return func(self, temp, *args, **kwargs)\n\n    return wrapper\n\n\ndef get_mock_init_trainer_params(\n    overrides: Optional[Dict[str, Any]] = None,\n) -> Callable[..., Dict[str, Any]]:\n    \"\"\"\n    Order of trainer_params setting in unit test:\n      - First call original function, which sets params from config\n      - Then override some params to disable logger and checkpoint\n      - Apply any test-specific overrides.\n    \"\"\"\n\n    def mock_init_trainer_params(\n        original: Callable[..., Dict[str, Any]],\n    ) -> Dict[str, Any]:\n        trainer_params = original()\n\n        trainer_params[\"logger\"] = False\n        trainer_params[\"enable_checkpointing\"] = False\n        trainer_params[\"fast_dev_run\"] = True", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_95-145"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 152, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "    def wrapper(*args, **kwargs):  # pyre-ignore[2,3]\n        with mock.patch(\n            \"torch.distributed.is_available\",\n            return_value=False,\n        ):\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef tempdir(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator for creating a tempory directory that\n    is cleaned up after function execution.\"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):  # pyre-ignore[2,3]\n        with TemporaryDirectory() as temp:\n            return func(self, temp, *args, **kwargs)\n\n    return wrapper\n\n\ndef get_mock_init_trainer_params(\n    overrides: Optional[Dict[str, Any]] = None,\n) -> Callable[..., Dict[str, Any]]:\n    \"\"\"\n    Order of trainer_params setting in unit test:\n      - First call original function, which sets params from config\n      - Then override some params to disable logger and checkpoint\n      - Apply any test-specific overrides.\n    \"\"\"\n\n    def mock_init_trainer_params(\n        original: Callable[..., Dict[str, Any]],\n    ) -> Dict[str, Any]:\n        trainer_params = original()\n\n        trainer_params[\"logger\"] = False\n        trainer_params[\"enable_checkpointing\"] = False\n        trainer_params[\"fast_dev_run\"] = True\n\n        if overrides:\n            trainer_params.update(overrides)\n\n        return trainer_params\n\n    return mock_init_trainer_params", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_105-152"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 152, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "def tempdir(func: Callable) -> Callable:  # pyre-ignore[24]\n    \"\"\"A decorator for creating a tempory directory that\n    is cleaned up after function execution.\"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):  # pyre-ignore[2,3]\n        with TemporaryDirectory() as temp:\n            return func(self, temp, *args, **kwargs)\n\n    return wrapper\n\n\ndef get_mock_init_trainer_params(\n    overrides: Optional[Dict[str, Any]] = None,\n) -> Callable[..., Dict[str, Any]]:\n    \"\"\"\n    Order of trainer_params setting in unit test:\n      - First call original function, which sets params from config\n      - Then override some params to disable logger and checkpoint\n      - Apply any test-specific overrides.\n    \"\"\"\n\n    def mock_init_trainer_params(\n        original: Callable[..., Dict[str, Any]],\n    ) -> Dict[str, Any]:\n        trainer_params = original()\n\n        trainer_params[\"logger\"] = False\n        trainer_params[\"enable_checkpointing\"] = False\n        trainer_params[\"fast_dev_run\"] = True\n\n        if overrides:\n            trainer_params.update(overrides)\n\n        return trainer_params\n\n    return mock_init_trainer_params\n\nAST=Module(FunctionDef(arguments(arg(Name(Load)))Expr(Constant)FunctionDef(arguments(argargarg)With(withitem(Call(Name(Load))Name(Store))Return(Call(Name(Load)Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load)))))Call(Name(Load)Name(Load)))Return(Name(Load))Name(Load))FunctionDef(arguments(arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))Constant)Expr(Constant)FunctionDef(arguments(arg(Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load)Load)))Assign(Name(Store)Call(Name(Load)))Assign(Subscript(Name(Load)ConstantStore)Constant)Assign(Subscript(Name(Load)ConstantStore)Constant)Assign(Subscript(Name(Load)ConstantStore)Constant)If(Name(Load)Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))Return(Name(Load))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_115-152"}
{"title": "facebookresearch_omnivore-tests-util.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "util.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 152, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "\n\ndef get_mock_init_trainer_params(\n    overrides: Optional[Dict[str, Any]] = None,\n) -> Callable[..., Dict[str, Any]]:\n    \"\"\"\n    Order of trainer_params setting in unit test:\n      - First call original function, which sets params from config\n      - Then override some params to disable logger and checkpoint\n      - Apply any test-specific overrides.\n    \"\"\"\n\n    def mock_init_trainer_params(\n        original: Callable[..., Dict[str, Any]],\n    ) -> Dict[str, Any]:\n        trainer_params = original()\n\n        trainer_params[\"logger\"] = False\n        trainer_params[\"enable_checkpointing\"] = False\n        trainer_params[\"fast_dev_run\"] = True\n\n        if overrides:\n            trainer_params.update(overrides)\n\n        return trainer_params\n\n    return mock_init_trainer_params\n\nAST=Module(FunctionDef(arguments(arg(Subscript(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load))Constant)Expr(Constant)FunctionDef(arguments(arg(Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load)Load)))Assign(Name(Store)Call(Name(Load)))Assign(Subscript(Name(Load)ConstantStore)Constant)Assign(Subscript(Name(Load)ConstantStore)Constant)Assign(Subscript(Name(Load)ConstantStore)Constant)If(Name(Load)Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load))Subscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load))Return(Name(Load))Subscript(Name(Load)Tuple(ConstantSubscript(Name(Load)Tuple(Name(Load)Name(Load)Load)Load)Load)Load)))", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-util.py_125-152"}
{"title": "facebookresearch_omnivore-tests-__init__.py", "metadata": [{"fpath_tuple": ["facebookresearch_omnivore", "tests", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "facebookresearch_omnivore", "slice_size": 5}], "contents": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nAST=Module", "id": "facebookresearch_omnivore_facebookresearch_omnivore-tests-__init__.py_0-1"}
