{"title": "lucidrains_imagen-pytorch-setup.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "setup.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from setuptools import setup, find_packages\nexec(open('imagen_pytorch/version.py').read())\n\nsetup(\n  name = 'imagen-pytorch',\n  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'imagen_pytorch = imagen_pytorch.cli:main',\n      'imagen = imagen_pytorch.cli:imagen'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'Imagen - unprecedented photorealism \u00d7 deep level of language understanding',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/imagen-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'text-to-image',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-setup.py_0-25"}
{"title": "lucidrains_imagen-pytorch-setup.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "setup.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from setuptools import setup, find_packages\nexec(open('imagen_pytorch/version.py').read())\n\nsetup(\n  name = 'imagen-pytorch',\n  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'imagen_pytorch = imagen_pytorch.cli:main',\n      'imagen = imagen_pytorch.cli:imagen'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'Imagen - unprecedented photorealism \u00d7 deep level of language understanding',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/imagen-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'text-to-image',\n    'denoising-diffusion'\n  ],\n  install_requires=[\n    'accelerate==0.21.0',\n    'beartype',\n    'click',\n    'datasets',\n    'einops>=0.6',\n    'einops-exts',\n    'ema-pytorch>=0.0.3',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-setup.py_0-35"}
{"title": "lucidrains_imagen-pytorch-setup.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "setup.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from setuptools import setup, find_packages\nexec(open('imagen_pytorch/version.py').read())\n\nsetup(\n  name = 'imagen-pytorch',\n  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'imagen_pytorch = imagen_pytorch.cli:main',\n      'imagen = imagen_pytorch.cli:imagen'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'Imagen - unprecedented photorealism \u00d7 deep level of language understanding',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/imagen-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'text-to-image',\n    'denoising-diffusion'\n  ],\n  install_requires=[\n    'accelerate==0.21.0',\n    'beartype',\n    'click',\n    'datasets',\n    'einops>=0.6',\n    'einops-exts',\n    'ema-pytorch>=0.0.3',\n    'fsspec',\n    'kornia',\n    'lion-pytorch',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic',\n    'pytorch-lightning',\n    'pytorch-warmup',\n    'sentencepiece',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-setup.py_0-45"}
{"title": "lucidrains_imagen-pytorch-setup.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "setup.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'imagen_pytorch = imagen_pytorch.cli:main',\n      'imagen = imagen_pytorch.cli:imagen'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'Imagen - unprecedented photorealism \u00d7 deep level of language understanding',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/imagen-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'text-to-image',\n    'denoising-diffusion'\n  ],\n  install_requires=[\n    'accelerate==0.21.0',\n    'beartype',\n    'click',\n    'datasets',\n    'einops>=0.6',\n    'einops-exts',\n    'ema-pytorch>=0.0.3',\n    'fsspec',\n    'kornia',\n    'lion-pytorch',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic',\n    'pytorch-lightning',\n    'pytorch-warmup',\n    'sentencepiece',\n    'torch>=1.6',\n    'torchvision',\n    'transformers',\n    'triton',\n    'tqdm'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-setup.py_5-55"}
{"title": "lucidrains_imagen-pytorch-setup.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "setup.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 59, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "  description = 'Imagen - unprecedented photorealism \u00d7 deep level of language understanding',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/imagen-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'text-to-image',\n    'denoising-diffusion'\n  ],\n  install_requires=[\n    'accelerate==0.21.0',\n    'beartype',\n    'click',\n    'datasets',\n    'einops>=0.6',\n    'einops-exts',\n    'ema-pytorch>=0.0.3',\n    'fsspec',\n    'kornia',\n    'lion-pytorch',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic',\n    'pytorch-lightning',\n    'pytorch-warmup',\n    'sentencepiece',\n    'torch>=1.6',\n    'torchvision',\n    'transformers',\n    'triton',\n    'tqdm'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-setup.py_15-59"}
{"title": "lucidrains_imagen-pytorch-setup.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "setup.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 59, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    'denoising-diffusion'\n  ],\n  install_requires=[\n    'accelerate==0.21.0',\n    'beartype',\n    'click',\n    'datasets',\n    'einops>=0.6',\n    'einops-exts',\n    'ema-pytorch>=0.0.3',\n    'fsspec',\n    'kornia',\n    'lion-pytorch',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic',\n    'pytorch-lightning',\n    'pytorch-warmup',\n    'sentencepiece',\n    'torch>=1.6',\n    'torchvision',\n    'transformers',\n    'triton',\n    'tqdm'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-setup.py_25-59"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import click\nimport torch\nfrom pathlib import Path\nimport pkgutil\n\nfrom imagen_pytorch import load_imagen_from_checkpoint\nfrom imagen_pytorch.version import __version__\nfrom imagen_pytorch.data import Collator\nfrom imagen_pytorch.utils import safeget\nfrom imagen_pytorch import ImagenTrainer, ElucidatedImagenConfig, ImagenConfig\nfrom datasets import load_dataset\n\nimport json\n\ndef exists(val):\n    return val is not None\n\ndef simple_slugify(text, max_length = 255):\n    return text.replace('-', '_').replace(',', '').replace(' ', '_').replace('|', '--').strip('-_')[:max_length]\n\ndef main():\n    pass\n\n@click.group()\ndef imagen():", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import click\nimport torch\nfrom pathlib import Path\nimport pkgutil\n\nfrom imagen_pytorch import load_imagen_from_checkpoint\nfrom imagen_pytorch.version import __version__\nfrom imagen_pytorch.data import Collator\nfrom imagen_pytorch.utils import safeget\nfrom imagen_pytorch import ImagenTrainer, ElucidatedImagenConfig, ImagenConfig\nfrom datasets import load_dataset\n\nimport json\n\ndef exists(val):\n    return val is not None\n\ndef simple_slugify(text, max_length = 255):\n    return text.replace('-', '_').replace(',', '').replace(' ', '_').replace('|', '--').strip('-_')[:max_length]\n\ndef main():\n    pass\n\n@click.group()\ndef imagen():\n    pass\n\n@imagen.command(help = 'Sample from the Imagen model checkpoint')\n@click.option('--model', default = './imagen.pt', help = 'path to trained Imagen model')\n@click.option('--cond_scale', default = 5, help = 'conditioning scale (classifier free guidance) in decoder')\n@click.option('--load_ema', default = True, help = 'load EMA version of unets if available')\n@click.argument('text')\ndef sample(\n    model,\n    cond_scale,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import click\nimport torch\nfrom pathlib import Path\nimport pkgutil\n\nfrom imagen_pytorch import load_imagen_from_checkpoint\nfrom imagen_pytorch.version import __version__\nfrom imagen_pytorch.data import Collator\nfrom imagen_pytorch.utils import safeget\nfrom imagen_pytorch import ImagenTrainer, ElucidatedImagenConfig, ImagenConfig\nfrom datasets import load_dataset\n\nimport json\n\ndef exists(val):\n    return val is not None\n\ndef simple_slugify(text, max_length = 255):\n    return text.replace('-', '_').replace(',', '').replace(' ', '_').replace('|', '--').strip('-_')[:max_length]\n\ndef main():\n    pass\n\n@click.group()\ndef imagen():\n    pass\n\n@imagen.command(help = 'Sample from the Imagen model checkpoint')\n@click.option('--model', default = './imagen.pt', help = 'path to trained Imagen model')\n@click.option('--cond_scale', default = 5, help = 'conditioning scale (classifier free guidance) in decoder')\n@click.option('--load_ema', default = True, help = 'load EMA version of unets if available')\n@click.argument('text')\ndef sample(\n    model,\n    cond_scale,\n    load_ema,\n    text\n):\n    model_path = Path(model)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'model not found at {full_model_path}'\n    loaded = torch.load(str(model_path))\n\n    # get version\n\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Import(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargConstant)Return(Subscript(Call(Attribute(Call(Attribute(Call(Attribute(Call(Attribute(Call(Attribute(Name(Load)Load)ConstantConstant)Load)ConstantConstant)Load)ConstantConstant)Load)ConstantConstant)Load)Constant)Slice(Name(Load))Load)))FunctionDef(argumentsPass)FunctionDef(argumentsPassCall(Attribute(Name(Load)Load)))FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constant)))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch import load_imagen_from_checkpoint\nfrom imagen_pytorch.version import __version__\nfrom imagen_pytorch.data import Collator\nfrom imagen_pytorch.utils import safeget\nfrom imagen_pytorch import ImagenTrainer, ElucidatedImagenConfig, ImagenConfig\nfrom datasets import load_dataset\n\nimport json\n\ndef exists(val):\n    return val is not None\n\ndef simple_slugify(text, max_length = 255):\n    return text.replace('-', '_').replace(',', '').replace(' ', '_').replace('|', '--').strip('-_')[:max_length]\n\ndef main():\n    pass\n\n@click.group()\ndef imagen():\n    pass\n\n@imagen.command(help = 'Sample from the Imagen model checkpoint')\n@click.option('--model', default = './imagen.pt', help = 'path to trained Imagen model')\n@click.option('--cond_scale', default = 5, help = 'conditioning scale (classifier free guidance) in decoder')\n@click.option('--load_ema', default = True, help = 'load EMA version of unets if available')\n@click.argument('text')\ndef sample(\n    model,\n    cond_scale,\n    load_ema,\n    text\n):\n    model_path = Path(model)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'model not found at {full_model_path}'\n    loaded = torch.load(str(model_path))\n\n    # get version\n\n    version = safeget(loaded, 'version')\n    print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n\n    # get imagen parameters and type\n\n    imagen = load_imagen_from_checkpoint(str(model_path), load_ema_if_available = load_ema)\n    imagen.cuda()\n\n    # generate image\n\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Import(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargConstant)Return(Subscript(Call(Attribute(Call(Attribute(Call(Attribute(Call(Attribute(Call(Attribute(Name(Load)Load)ConstantConstant)Load)ConstantConstant)Load)ConstantConstant)Load)ConstantConstant)Load)Constant)Slice(Name(Load))Load)))FunctionDef(argumentsPass)FunctionDef(argumentsPassCall(Attribute(Name(Load)Load)))FunctionDef(arguments(argargargarg)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Expr(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Name(Load))ConstantFormattedValue(Name(Load)))))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Name(Load))keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constant)))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return val is not None\n\ndef simple_slugify(text, max_length = 255):\n    return text.replace('-', '_').replace(',', '').replace(' ', '_').replace('|', '--').strip('-_')[:max_length]\n\ndef main():\n    pass\n\n@click.group()\ndef imagen():\n    pass\n\n@imagen.command(help = 'Sample from the Imagen model checkpoint')\n@click.option('--model', default = './imagen.pt', help = 'path to trained Imagen model')\n@click.option('--cond_scale', default = 5, help = 'conditioning scale (classifier free guidance) in decoder')\n@click.option('--load_ema', default = True, help = 'load EMA version of unets if available')\n@click.argument('text')\ndef sample(\n    model,\n    cond_scale,\n    load_ema,\n    text\n):\n    model_path = Path(model)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'model not found at {full_model_path}'\n    loaded = torch.load(str(model_path))\n\n    # get version\n\n    version = safeget(loaded, 'version')\n    print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n\n    # get imagen parameters and type\n\n    imagen = load_imagen_from_checkpoint(str(model_path), load_ema_if_available = load_ema)\n    imagen.cuda()\n\n    # generate image\n\n    pil_image = imagen.sample(text, cond_scale = cond_scale, return_pil_images = True)\n\n    image_path = f'./{simple_slugify(text)}.png'\n    pil_image[0].save(image_path)\n\n    print(f'image saved to {str(image_path)}')\n    return\n\n@imagen.command(help = 'Generate a config for the Imagen model')\n@click.option('--path', default = './imagen_config.json', help = 'Path to the Imagen model config')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    pass\n\n@imagen.command(help = 'Sample from the Imagen model checkpoint')\n@click.option('--model', default = './imagen.pt', help = 'path to trained Imagen model')\n@click.option('--cond_scale', default = 5, help = 'conditioning scale (classifier free guidance) in decoder')\n@click.option('--load_ema', default = True, help = 'load EMA version of unets if available')\n@click.argument('text')\ndef sample(\n    model,\n    cond_scale,\n    load_ema,\n    text\n):\n    model_path = Path(model)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'model not found at {full_model_path}'\n    loaded = torch.load(str(model_path))\n\n    # get version\n\n    version = safeget(loaded, 'version')\n    print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n\n    # get imagen parameters and type\n\n    imagen = load_imagen_from_checkpoint(str(model_path), load_ema_if_available = load_ema)\n    imagen.cuda()\n\n    # generate image\n\n    pil_image = imagen.sample(text, cond_scale = cond_scale, return_pil_images = True)\n\n    image_path = f'./{simple_slugify(text)}.png'\n    pil_image[0].save(image_path)\n\n    print(f'image saved to {str(image_path)}')\n    return\n\n@imagen.command(help = 'Generate a config for the Imagen model')\n@click.option('--path', default = './imagen_config.json', help = 'Path to the Imagen model config')\ndef config(\n    path\n):\n    data = pkgutil.get_data(__name__, 'default_config.json').decode(\"utf-8\") \n    with open(path, 'w') as f:\n        f.write(data)\n\n@imagen.command(help = 'Train the Imagen model')\n@click.option('--config', default = './imagen_config.json', help = 'Path to the Imagen model config')\n@click.option('--unet', default = 1, help = 'Unet to train', type = click.IntRange(1, 3, False, True, True))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    load_ema,\n    text\n):\n    model_path = Path(model)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'model not found at {full_model_path}'\n    loaded = torch.load(str(model_path))\n\n    # get version\n\n    version = safeget(loaded, 'version')\n    print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n\n    # get imagen parameters and type\n\n    imagen = load_imagen_from_checkpoint(str(model_path), load_ema_if_available = load_ema)\n    imagen.cuda()\n\n    # generate image\n\n    pil_image = imagen.sample(text, cond_scale = cond_scale, return_pil_images = True)\n\n    image_path = f'./{simple_slugify(text)}.png'\n    pil_image[0].save(image_path)\n\n    print(f'image saved to {str(image_path)}')\n    return\n\n@imagen.command(help = 'Generate a config for the Imagen model')\n@click.option('--path', default = './imagen_config.json', help = 'Path to the Imagen model config')\ndef config(\n    path\n):\n    data = pkgutil.get_data(__name__, 'default_config.json').decode(\"utf-8\") \n    with open(path, 'w') as f:\n        f.write(data)\n\n@imagen.command(help = 'Train the Imagen model')\n@click.option('--config', default = './imagen_config.json', help = 'Path to the Imagen model config')\n@click.option('--unet', default = 1, help = 'Unet to train', type = click.IntRange(1, 3, False, True, True))\n@click.option('--epoches', default = 1000, help = 'Amount of epoches to train for')\n@click.option('--text', required = False, help = 'Text to sample with between epoches', type=str)\n@click.option('--valid', is_flag = False, flag_value=50, default = 0, help = 'Do validation between epoches', show_default = True)\ndef train(\n    config,\n    unet,\n    epoches,\n    text,\n    valid\n):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    version = safeget(loaded, 'version')\n    print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n\n    # get imagen parameters and type\n\n    imagen = load_imagen_from_checkpoint(str(model_path), load_ema_if_available = load_ema)\n    imagen.cuda()\n\n    # generate image\n\n    pil_image = imagen.sample(text, cond_scale = cond_scale, return_pil_images = True)\n\n    image_path = f'./{simple_slugify(text)}.png'\n    pil_image[0].save(image_path)\n\n    print(f'image saved to {str(image_path)}')\n    return\n\n@imagen.command(help = 'Generate a config for the Imagen model')\n@click.option('--path', default = './imagen_config.json', help = 'Path to the Imagen model config')\ndef config(\n    path\n):\n    data = pkgutil.get_data(__name__, 'default_config.json').decode(\"utf-8\") \n    with open(path, 'w') as f:\n        f.write(data)\n\n@imagen.command(help = 'Train the Imagen model')\n@click.option('--config', default = './imagen_config.json', help = 'Path to the Imagen model config')\n@click.option('--unet', default = 1, help = 'Unet to train', type = click.IntRange(1, 3, False, True, True))\n@click.option('--epoches', default = 1000, help = 'Amount of epoches to train for')\n@click.option('--text', required = False, help = 'Text to sample with between epoches', type=str)\n@click.option('--valid', is_flag = False, flag_value=50, default = 0, help = 'Do validation between epoches', show_default = True)\ndef train(\n    config,\n    unet,\n    epoches,\n    text,\n    valid\n):\n    # check config path\n\n    config_path = Path(config)\n    full_config_path = str(config_path.resolve())\n    assert config_path.exists(), f'config not found at {full_config_path}'\n    \n    with open(config_path, 'r') as f:\n        config_data = json.loads(f.read())\n\n    assert 'checkpoint_path' in config_data, 'checkpoint path not found in config'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    pil_image = imagen.sample(text, cond_scale = cond_scale, return_pil_images = True)\n\n    image_path = f'./{simple_slugify(text)}.png'\n    pil_image[0].save(image_path)\n\n    print(f'image saved to {str(image_path)}')\n    return\n\n@imagen.command(help = 'Generate a config for the Imagen model')\n@click.option('--path', default = './imagen_config.json', help = 'Path to the Imagen model config')\ndef config(\n    path\n):\n    data = pkgutil.get_data(__name__, 'default_config.json').decode(\"utf-8\") \n    with open(path, 'w') as f:\n        f.write(data)\n\n@imagen.command(help = 'Train the Imagen model')\n@click.option('--config', default = './imagen_config.json', help = 'Path to the Imagen model config')\n@click.option('--unet', default = 1, help = 'Unet to train', type = click.IntRange(1, 3, False, True, True))\n@click.option('--epoches', default = 1000, help = 'Amount of epoches to train for')\n@click.option('--text', required = False, help = 'Text to sample with between epoches', type=str)\n@click.option('--valid', is_flag = False, flag_value=50, default = 0, help = 'Do validation between epoches', show_default = True)\ndef train(\n    config,\n    unet,\n    epoches,\n    text,\n    valid\n):\n    # check config path\n\n    config_path = Path(config)\n    full_config_path = str(config_path.resolve())\n    assert config_path.exists(), f'config not found at {full_config_path}'\n    \n    with open(config_path, 'r') as f:\n        config_data = json.loads(f.read())\n\n    assert 'checkpoint_path' in config_data, 'checkpoint path not found in config'\n    \n    model_path = Path(config_data['checkpoint_path'])\n    full_model_path = str(model_path.resolve())\n    \n    # setup imagen config\n\n    imagen_config_klass = ElucidatedImagenConfig if config_data['type'] == 'elucidated' else ImagenConfig\n    imagen = imagen_config_klass(**config_data['imagen']).create()\n\n    trainer = ImagenTrainer(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def config(\n    path\n):\n    data = pkgutil.get_data(__name__, 'default_config.json').decode(\"utf-8\") \n    with open(path, 'w') as f:\n        f.write(data)\n\n@imagen.command(help = 'Train the Imagen model')\n@click.option('--config', default = './imagen_config.json', help = 'Path to the Imagen model config')\n@click.option('--unet', default = 1, help = 'Unet to train', type = click.IntRange(1, 3, False, True, True))\n@click.option('--epoches', default = 1000, help = 'Amount of epoches to train for')\n@click.option('--text', required = False, help = 'Text to sample with between epoches', type=str)\n@click.option('--valid', is_flag = False, flag_value=50, default = 0, help = 'Do validation between epoches', show_default = True)\ndef train(\n    config,\n    unet,\n    epoches,\n    text,\n    valid\n):\n    # check config path\n\n    config_path = Path(config)\n    full_config_path = str(config_path.resolve())\n    assert config_path.exists(), f'config not found at {full_config_path}'\n    \n    with open(config_path, 'r') as f:\n        config_data = json.loads(f.read())\n\n    assert 'checkpoint_path' in config_data, 'checkpoint path not found in config'\n    \n    model_path = Path(config_data['checkpoint_path'])\n    full_model_path = str(model_path.resolve())\n    \n    # setup imagen config\n\n    imagen_config_klass = ElucidatedImagenConfig if config_data['type'] == 'elucidated' else ImagenConfig\n    imagen = imagen_config_klass(**config_data['imagen']).create()\n\n    trainer = ImagenTrainer(\n    imagen = imagen,\n        **config_data['trainer']\n    )\n\n    # load pt\n    if model_path.exists():\n        loaded = torch.load(str(model_path))\n        version = safeget(loaded, 'version')\n        print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n        trainer.load(model_path)\n\nAST=Module(FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Call(Attribute(Name(Load)Load)Name(Load)Constant)Load)Constant))With(withitem(Call(Name(Load)Name(Load)Constant)Name(Store))Expr(Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argargargargarg)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))With(withitem(Call(Name(Load)Name(Load)Constant)Name(Store))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)))))Assert(Compare(ConstantInName(Load))Constant)Assign(Name(Store)Call(Name(Load)Subscript(Name(Load)ConstantLoad)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assign(Name(Store)IfExp(Compare(Subscript(Name(Load)ConstantLoad)EqConstant)Name(Load)Name(Load)))Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Subscript(Name(Load)ConstantLoad)))Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))keyword(Subscript(Name(Load)ConstantLoad))))If(Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Expr(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))ConstantFormattedValue(Name(Load))ConstantFormattedValue(Name(Load)))))Expr(Call(Attribute(Name(Load)Load)Name(Load))))Call(Attribute(Name(Load)Load)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant)keyword(Call(Attribute(Name(Load)Load)ConstantConstantConstantConstantConstant)))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant)keyword(Name(Load)))Call(Attribute(Name(Load)Load)Constantkeyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "@click.option('--epoches', default = 1000, help = 'Amount of epoches to train for')\n@click.option('--text', required = False, help = 'Text to sample with between epoches', type=str)\n@click.option('--valid', is_flag = False, flag_value=50, default = 0, help = 'Do validation between epoches', show_default = True)\ndef train(\n    config,\n    unet,\n    epoches,\n    text,\n    valid\n):\n    # check config path\n\n    config_path = Path(config)\n    full_config_path = str(config_path.resolve())\n    assert config_path.exists(), f'config not found at {full_config_path}'\n    \n    with open(config_path, 'r') as f:\n        config_data = json.loads(f.read())\n\n    assert 'checkpoint_path' in config_data, 'checkpoint path not found in config'\n    \n    model_path = Path(config_data['checkpoint_path'])\n    full_model_path = str(model_path.resolve())\n    \n    # setup imagen config\n\n    imagen_config_klass = ElucidatedImagenConfig if config_data['type'] == 'elucidated' else ImagenConfig\n    imagen = imagen_config_klass(**config_data['imagen']).create()\n\n    trainer = ImagenTrainer(\n    imagen = imagen,\n        **config_data['trainer']\n    )\n\n    # load pt\n    if model_path.exists():\n        loaded = torch.load(str(model_path))\n        version = safeget(loaded, 'version')\n        print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n        trainer.load(model_path)\n        \n    if torch.cuda.is_available():\n        trainer = trainer.cuda()\n\n    size = config_data['imagen']['image_sizes'][unet-1]\n\n    max_batch_size = config_data['max_batch_size'] if 'max_batch_size' in config_data else 1\n\n    channels = 'RGB'\n    if 'channels' in config_data['imagen']:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    # check config path\n\n    config_path = Path(config)\n    full_config_path = str(config_path.resolve())\n    assert config_path.exists(), f'config not found at {full_config_path}'\n    \n    with open(config_path, 'r') as f:\n        config_data = json.loads(f.read())\n\n    assert 'checkpoint_path' in config_data, 'checkpoint path not found in config'\n    \n    model_path = Path(config_data['checkpoint_path'])\n    full_model_path = str(model_path.resolve())\n    \n    # setup imagen config\n\n    imagen_config_klass = ElucidatedImagenConfig if config_data['type'] == 'elucidated' else ImagenConfig\n    imagen = imagen_config_klass(**config_data['imagen']).create()\n\n    trainer = ImagenTrainer(\n    imagen = imagen,\n        **config_data['trainer']\n    )\n\n    # load pt\n    if model_path.exists():\n        loaded = torch.load(str(model_path))\n        version = safeget(loaded, 'version')\n        print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n        trainer.load(model_path)\n        \n    if torch.cuda.is_available():\n        trainer = trainer.cuda()\n\n    size = config_data['imagen']['image_sizes'][unet-1]\n\n    max_batch_size = config_data['max_batch_size'] if 'max_batch_size' in config_data else 1\n\n    channels = 'RGB'\n    if 'channels' in config_data['imagen']:\n        assert config_data['imagen']['channels'] > 0 and config_data['imagen']['channels'] < 5, 'Imagen only support 1 to 4 channels L, LA, RGB, RGBA'\n        if config_data['imagen']['channels'] == 4:\n            channels = 'RGBA' # Color with alpha\n        elif config_data['imagen']['channels'] == 2:\n            channels == 'LA' # Luminance (Greyscale) with alpha\n        elif config_data['imagen']['channels'] == 1:\n            channels = 'L' # Luminance (Greyscale)\n\n\n    assert 'batch_size' in config_data['dataset'], 'A batch_size is required in the config file'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    \n    model_path = Path(config_data['checkpoint_path'])\n    full_model_path = str(model_path.resolve())\n    \n    # setup imagen config\n\n    imagen_config_klass = ElucidatedImagenConfig if config_data['type'] == 'elucidated' else ImagenConfig\n    imagen = imagen_config_klass(**config_data['imagen']).create()\n\n    trainer = ImagenTrainer(\n    imagen = imagen,\n        **config_data['trainer']\n    )\n\n    # load pt\n    if model_path.exists():\n        loaded = torch.load(str(model_path))\n        version = safeget(loaded, 'version')\n        print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n        trainer.load(model_path)\n        \n    if torch.cuda.is_available():\n        trainer = trainer.cuda()\n\n    size = config_data['imagen']['image_sizes'][unet-1]\n\n    max_batch_size = config_data['max_batch_size'] if 'max_batch_size' in config_data else 1\n\n    channels = 'RGB'\n    if 'channels' in config_data['imagen']:\n        assert config_data['imagen']['channels'] > 0 and config_data['imagen']['channels'] < 5, 'Imagen only support 1 to 4 channels L, LA, RGB, RGBA'\n        if config_data['imagen']['channels'] == 4:\n            channels = 'RGBA' # Color with alpha\n        elif config_data['imagen']['channels'] == 2:\n            channels == 'LA' # Luminance (Greyscale) with alpha\n        elif config_data['imagen']['channels'] == 1:\n            channels = 'L' # Luminance (Greyscale)\n\n\n    assert 'batch_size' in config_data['dataset'], 'A batch_size is required in the config file'\n    \n    # load and add train dataset and valid dataset\n    ds = load_dataset(config_data['dataset_name'])\n    trainer.add_train_dataset(\n        ds = ds['train'],\n        collate_fn = Collator(\n            image_size = size,\n            image_label = config_data['image_label'],\n            text_label = config_data['text_label'],\n            url_label = config_data['url_label'],", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_95-145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    imagen = imagen,\n        **config_data['trainer']\n    )\n\n    # load pt\n    if model_path.exists():\n        loaded = torch.load(str(model_path))\n        version = safeget(loaded, 'version')\n        print(f'loading Imagen from {full_model_path}, saved at version {version} - current package version is {__version__}')\n        trainer.load(model_path)\n        \n    if torch.cuda.is_available():\n        trainer = trainer.cuda()\n\n    size = config_data['imagen']['image_sizes'][unet-1]\n\n    max_batch_size = config_data['max_batch_size'] if 'max_batch_size' in config_data else 1\n\n    channels = 'RGB'\n    if 'channels' in config_data['imagen']:\n        assert config_data['imagen']['channels'] > 0 and config_data['imagen']['channels'] < 5, 'Imagen only support 1 to 4 channels L, LA, RGB, RGBA'\n        if config_data['imagen']['channels'] == 4:\n            channels = 'RGBA' # Color with alpha\n        elif config_data['imagen']['channels'] == 2:\n            channels == 'LA' # Luminance (Greyscale) with alpha\n        elif config_data['imagen']['channels'] == 1:\n            channels = 'L' # Luminance (Greyscale)\n\n\n    assert 'batch_size' in config_data['dataset'], 'A batch_size is required in the config file'\n    \n    # load and add train dataset and valid dataset\n    ds = load_dataset(config_data['dataset_name'])\n    trainer.add_train_dataset(\n        ds = ds['train'],\n        collate_fn = Collator(\n            image_size = size,\n            image_label = config_data['image_label'],\n            text_label = config_data['text_label'],\n            url_label = config_data['url_label'],\n            name = imagen.text_encoder_name,\n            channels = channels\n        ),\n        **config_data['dataset']\n    )\n\n\n    if not trainer.split_valid_from_train and valid != 0:\n        assert 'valid' in ds, 'There is no validation split in the dataset'\n        trainer.add_valid_dataset(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_105-155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        \n    if torch.cuda.is_available():\n        trainer = trainer.cuda()\n\n    size = config_data['imagen']['image_sizes'][unet-1]\n\n    max_batch_size = config_data['max_batch_size'] if 'max_batch_size' in config_data else 1\n\n    channels = 'RGB'\n    if 'channels' in config_data['imagen']:\n        assert config_data['imagen']['channels'] > 0 and config_data['imagen']['channels'] < 5, 'Imagen only support 1 to 4 channels L, LA, RGB, RGBA'\n        if config_data['imagen']['channels'] == 4:\n            channels = 'RGBA' # Color with alpha\n        elif config_data['imagen']['channels'] == 2:\n            channels == 'LA' # Luminance (Greyscale) with alpha\n        elif config_data['imagen']['channels'] == 1:\n            channels = 'L' # Luminance (Greyscale)\n\n\n    assert 'batch_size' in config_data['dataset'], 'A batch_size is required in the config file'\n    \n    # load and add train dataset and valid dataset\n    ds = load_dataset(config_data['dataset_name'])\n    trainer.add_train_dataset(\n        ds = ds['train'],\n        collate_fn = Collator(\n            image_size = size,\n            image_label = config_data['image_label'],\n            text_label = config_data['text_label'],\n            url_label = config_data['url_label'],\n            name = imagen.text_encoder_name,\n            channels = channels\n        ),\n        **config_data['dataset']\n    )\n\n\n    if not trainer.split_valid_from_train and valid != 0:\n        assert 'valid' in ds, 'There is no validation split in the dataset'\n        trainer.add_valid_dataset(\n            ds = ds['valid'],\n            collate_fn = Collator(\n                image_size = size,\n                image_label = config_data['image_label'],\n                text_label= config_data['text_label'],\n                url_label = config_data['url_label'],\n                name = imagen.text_encoder_name,\n                channels = channels \n            ),\n            **config_data['dataset']", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_115-165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert config_data['imagen']['channels'] > 0 and config_data['imagen']['channels'] < 5, 'Imagen only support 1 to 4 channels L, LA, RGB, RGBA'\n        if config_data['imagen']['channels'] == 4:\n            channels = 'RGBA' # Color with alpha\n        elif config_data['imagen']['channels'] == 2:\n            channels == 'LA' # Luminance (Greyscale) with alpha\n        elif config_data['imagen']['channels'] == 1:\n            channels = 'L' # Luminance (Greyscale)\n\n\n    assert 'batch_size' in config_data['dataset'], 'A batch_size is required in the config file'\n    \n    # load and add train dataset and valid dataset\n    ds = load_dataset(config_data['dataset_name'])\n    trainer.add_train_dataset(\n        ds = ds['train'],\n        collate_fn = Collator(\n            image_size = size,\n            image_label = config_data['image_label'],\n            text_label = config_data['text_label'],\n            url_label = config_data['url_label'],\n            name = imagen.text_encoder_name,\n            channels = channels\n        ),\n        **config_data['dataset']\n    )\n\n\n    if not trainer.split_valid_from_train and valid != 0:\n        assert 'valid' in ds, 'There is no validation split in the dataset'\n        trainer.add_valid_dataset(\n            ds = ds['valid'],\n            collate_fn = Collator(\n                image_size = size,\n                image_label = config_data['image_label'],\n                text_label= config_data['text_label'],\n                url_label = config_data['url_label'],\n                name = imagen.text_encoder_name,\n                channels = channels \n            ),\n            **config_data['dataset']\n        )\n\n    for i in range(epoches):\n        loss = trainer.train_step(unet_number = unet, max_batch_size = max_batch_size)\n        print(f'loss: {loss}')\n\n        if valid != 0 and not (i % valid) and i > 0:\n            valid_loss = trainer.valid_step(unet_number = unet, max_batch_size = max_batch_size)\n            print(f'valid loss: {valid_loss}')\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_125-175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 180, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    \n    # load and add train dataset and valid dataset\n    ds = load_dataset(config_data['dataset_name'])\n    trainer.add_train_dataset(\n        ds = ds['train'],\n        collate_fn = Collator(\n            image_size = size,\n            image_label = config_data['image_label'],\n            text_label = config_data['text_label'],\n            url_label = config_data['url_label'],\n            name = imagen.text_encoder_name,\n            channels = channels\n        ),\n        **config_data['dataset']\n    )\n\n\n    if not trainer.split_valid_from_train and valid != 0:\n        assert 'valid' in ds, 'There is no validation split in the dataset'\n        trainer.add_valid_dataset(\n            ds = ds['valid'],\n            collate_fn = Collator(\n                image_size = size,\n                image_label = config_data['image_label'],\n                text_label= config_data['text_label'],\n                url_label = config_data['url_label'],\n                name = imagen.text_encoder_name,\n                channels = channels \n            ),\n            **config_data['dataset']\n        )\n\n    for i in range(epoches):\n        loss = trainer.train_step(unet_number = unet, max_batch_size = max_batch_size)\n        print(f'loss: {loss}')\n\n        if valid != 0 and not (i % valid) and i > 0:\n            valid_loss = trainer.valid_step(unet_number = unet, max_batch_size = max_batch_size)\n            print(f'valid loss: {valid_loss}')\n\n        if not (i % 100) and i > 0 and trainer.is_main and text is not None:\n            images = trainer.sample(texts = [text], batch_size = 1, return_pil_images = True, stop_at_unet_number = unet)\n            images[0].save(f'./sample-{i // 100}.png')\n\n    trainer.save(model_path)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_135-180"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-cli.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "cli.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 180, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            name = imagen.text_encoder_name,\n            channels = channels\n        ),\n        **config_data['dataset']\n    )\n\n\n    if not trainer.split_valid_from_train and valid != 0:\n        assert 'valid' in ds, 'There is no validation split in the dataset'\n        trainer.add_valid_dataset(\n            ds = ds['valid'],\n            collate_fn = Collator(\n                image_size = size,\n                image_label = config_data['image_label'],\n                text_label= config_data['text_label'],\n                url_label = config_data['url_label'],\n                name = imagen.text_encoder_name,\n                channels = channels \n            ),\n            **config_data['dataset']\n        )\n\n    for i in range(epoches):\n        loss = trainer.train_step(unet_number = unet, max_batch_size = max_batch_size)\n        print(f'loss: {loss}')\n\n        if valid != 0 and not (i % valid) and i > 0:\n            valid_loss = trainer.valid_step(unet_number = unet, max_batch_size = max_batch_size)\n            print(f'valid loss: {valid_loss}')\n\n        if not (i % 100) and i > 0 and trainer.is_main and text is not None:\n            images = trainer.sample(texts = [text], batch_size = 1, return_pil_images = True, stop_at_unet_number = unet)\n            images[0].save(f'./sample-{i // 100}.png')\n\n    trainer.save(model_path)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-cli.py_145-180"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nAST=Module(Import(alias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)Return(IfExp(Call(Name(Load)Name(Load))Name(Load)Name(Load))))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Subscript(Name(Load)Name(Load)Load)Subscript(Name(Load)Name(Load)Load)Load)Load)))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Name(Load)Call(Name(Load)Name(Load))Load)Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n\nAST=Module(Import(alias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)Return(IfExp(Call(Name(Load)Name(Load))Name(Load)Name(Load))))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Subscript(Name(Load)Name(Load)Load)Subscript(Name(Load)Name(Load)Load)Load)Load)))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Name(Load)Call(Name(Load)Name(Load))Load)Load)))ClassDef(Name(Load)Assign(Name(Store)Constant)Assign(Name(Store)Constant))ClassDef(Name(Load)ClassDef(Assign(Name(Store)Constant)Assign(Name(Store)Constant))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import json\nfrom pydantic import BaseModel, validator, root_validator\nfrom typing import List, Iterable, Optional, Union, Tuple, Dict, Any\nfrom enum import Enum\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n\nAST=Module(Import(alias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasaliasaliasaliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)Return(IfExp(Call(Name(Load)Name(Load))Name(Load)Name(Load))))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Subscript(Name(Load)Name(Load)Load)Subscript(Name(Load)Name(Load)Load)Load)Load)))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Name(Load)Call(Name(Load)Name(Load))Load)Load)))ClassDef(Name(Load)Assign(Name(Store)Constant)Assign(Name(Store)Constant))ClassDef(Name(Load)ClassDef(Assign(Name(Store)Constant)Assign(Name(Store)Constant)))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))FunctionDef(arguments(arg)Return(Call(Name(Load)))))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.imagen_pytorch import Imagen, Unet, Unet3D, NullUnet\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.t5 import DEFAULT_T5_NAME, get_encoded_dim\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\n\nAST=Module(ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)Return(IfExp(Call(Name(Load)Name(Load))Name(Load)Name(Load))))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Subscript(Name(Load)Name(Load)Load)Subscript(Name(Load)Name(Load)Load)Load)Load)))FunctionDef(arguments(arg)Return(Subscript(Name(Load)Tuple(Name(Load)Call(Name(Load)Name(Load))Load)Load)))ClassDef(Name(Load)Assign(Name(Store)Constant)Assign(Name(Store)Constant))ClassDef(Name(Load)ClassDef(Assign(Name(Store)Constant)Assign(Name(Store)Constant)))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))FunctionDef(arguments(arg)Return(Call(Name(Load)))))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Call(Name(Load)keyword(Call(Attribute(Name(Load)Load))))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def default(val, d):\n    return val if exists(val) else d\n\ndef ListOrTuple(inner_type):\n    return Union[List[inner_type], Tuple[inner_type]]\n\ndef SingleOrList(inner_type):\n    return Union[inner_type, ListOrTuple(inner_type)]\n\n# noise schedule\n\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass NoiseSchedule(Enum):\n    cosine = 'cosine'\n    linear = 'linear'\n\nclass AllowExtraBaseModel(BaseModel):\n    class Config:\n        extra = \"allow\"\n        use_enum_values = True\n\n# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n\nAST=Module(ClassDef(Name(Load)Assign(Name(Store)Constant)Assign(Name(Store)Constant))ClassDef(Name(Load)ClassDef(Assign(Name(Store)Constant)Assign(Name(Store)Constant)))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))FunctionDef(arguments(arg)Return(Call(Name(Load)))))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Call(Name(Load)keyword(Call(Attribute(Name(Load)Load)))))))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Call(Name(Load)keyword(Call(Attribute(Name(Load)Load)))))))ClassDef(Name(Load)AnnAssign(Name(Store)Call(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)Load)))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Call(Name(Load)Name(Load))Constant)AnnAssign(Name(Store)Call(Name(Load)Name(Load))Constant)AnnAssign(Name(Store)Name(Load)Name(Load))AnnAssign(Name(Store)Name(Load)Constant)))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "# imagen pydantic classes\n\nclass NullUnetConfig(BaseModel):\n    is_null:            bool\n\n    def create(self):\n        return NullUnet()\n\nclass UnetConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n\nAST=Module(ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))FunctionDef(arguments(arg)Return(Call(Name(Load)))))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Call(Name(Load)keyword(Call(Attribute(Name(Load)Load)))))))ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Call(Name(Load)keyword(Call(Attribute(Name(Load)Load)))))))ClassDef(Name(Load)AnnAssign(Name(Store)Call(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)Load)))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Call(Name(Load)Name(Load))Constant)AnnAssign(Name(Store)Call(Name(Load)Name(Load))Constant)AnnAssign(Name(Store)Name(Load)Name(Load))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))If(Compare(Call(Name(Load)Name(Load))NotEqCall(Name(Load)Name(Load)))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Call(Name(Load)Name(Load)))ConstantFormattedValue(Call(Name(Load)Name(Load)))))))Return(Name(Load))Call(Name(Load)Constant))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet(**self.dict())\n\nclass Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class Unet3DConfig(AllowExtraBaseModel):\n    dim:                int\n    dim_mults:          ListOrTuple(int)\n    text_embed_dim:     int = get_encoded_dim(DEFAULT_T5_NAME)\n    cond_dim:           int = None\n    channels:           int = 3\n    attn_dim_head:      int = 32\n    attn_heads:         int = 16\n\n    def create(self):\n        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n\nAST=Module(ClassDef(Name(Load)AnnAssign(Name(Store)Name(Load))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(arg)Return(Call(Name(Load)keyword(Call(Attribute(Name(Load)Load)))))))ClassDef(Name(Load)AnnAssign(Name(Store)Call(Name(Load)Subscript(Name(Load)Tuple(Name(Load)Name(Load)Name(Load)Load)Load)))AnnAssign(Name(Store)Call(Name(Load)Name(Load)))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Call(Name(Load)Name(Load))Constant)AnnAssign(Name(Store)Call(Name(Load)Name(Load))Constant)AnnAssign(Name(Store)Name(Load)Name(Load))AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)AnnAssign(Name(Store)Name(Load)Constant)FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))If(Compare(Call(Name(Load)Name(Load))NotEqCall(Name(Load)Name(Load)))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Call(Name(Load)Name(Load)))ConstantFormattedValue(Call(Name(Load)Name(Load)))))))Return(Name(Load))Call(Name(Load)Constant))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)List(Load))For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Attribute(Name(Load)Load)Name(Load))If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Name(Load))If(Name(Load)Assign(Name(Store)Name(Load))Assign(Name(Store)Name(Load))))Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)keyword(Name(Load))))))Assign(Name(Store)Call(Name(Load)Name(Load)keyword(Name(Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Call(Attribute(Name(Load)Load))Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return Unet3D(**self.dict())\n\nclass ImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    timesteps:              SingleOrList(int) = 1000\n    noise_schedules:        SingleOrList(NoiseSchedule) = 'cosine'\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    loss_type:              str = 'l2'\n    cond_drop_prob:         float = 0.5\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32\n    sigma_min:              SingleOrList(float) = 0.002\n    sigma_max:              SingleOrList(int) = 80\n    sigma_data:             SingleOrList(float) = 0.5\n    rho:                    SingleOrList(int) = 7\n    P_mean:                 SingleOrList(float) = -1.2\n    P_std:                  SingleOrList(float) = 1.2\n    S_churn:                SingleOrList(int) = 80\n    S_tmin:                 SingleOrList(float) = 0.05\n    S_tmax:                 SingleOrList(int) = 50\n    S_noise:                SingleOrList(float) = 1.003", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32\n    sigma_min:              SingleOrList(float) = 0.002\n    sigma_max:              SingleOrList(int) = 80\n    sigma_data:             SingleOrList(float) = 0.5\n    rho:                    SingleOrList(int) = 7\n    P_mean:                 SingleOrList(float) = -1.2\n    P_std:                  SingleOrList(float) = 1.2\n    S_churn:                SingleOrList(int) = 80\n    S_tmin:                 SingleOrList(float) = 0.05\n    S_tmax:                 SingleOrList(int) = 50\n    S_noise:                SingleOrList(float) = 1.003\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = Imagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32\n    sigma_min:              SingleOrList(float) = 0.002\n    sigma_max:              SingleOrList(int) = 80\n    sigma_data:             SingleOrList(float) = 0.5\n    rho:                    SingleOrList(int) = 7\n    P_mean:                 SingleOrList(float) = -1.2\n    P_std:                  SingleOrList(float) = 1.2\n    S_churn:                SingleOrList(int) = 80\n    S_tmin:                 SingleOrList(float) = 0.05\n    S_tmax:                 SingleOrList(int) = 50\n    S_noise:                SingleOrList(float) = 1.003\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unet_klass = Unet3D if is_video else Unet\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_95-145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return imagen\n\nclass ElucidatedImagenConfig(AllowExtraBaseModel):\n    unets:                  ListOrTuple(Union[UnetConfig, Unet3DConfig, NullUnetConfig])\n    image_sizes:            ListOrTuple(int)\n    video:                  bool = False\n    text_encoder_name:      str = DEFAULT_T5_NAME\n    channels:               int = 3\n    cond_drop_prob:         float = 0.5\n    num_sample_steps:       SingleOrList(int) = 32\n    sigma_min:              SingleOrList(float) = 0.002\n    sigma_max:              SingleOrList(int) = 80\n    sigma_data:             SingleOrList(float) = 0.5\n    rho:                    SingleOrList(int) = 7\n    P_mean:                 SingleOrList(float) = -1.2\n    P_std:                  SingleOrList(float) = 1.2\n    S_churn:                SingleOrList(int) = 80\n    S_tmin:                 SingleOrList(float) = 0.05\n    S_tmax:                 SingleOrList(int) = 50\n    S_noise:                SingleOrList(float) = 1.003\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unet_klass = Unet3D if is_video else Unet\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = ElucidatedImagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_105-155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    sigma_min:              SingleOrList(float) = 0.002\n    sigma_max:              SingleOrList(int) = 80\n    sigma_data:             SingleOrList(float) = 0.5\n    rho:                    SingleOrList(int) = 7\n    P_mean:                 SingleOrList(float) = -1.2\n    P_std:                  SingleOrList(float) = 1.2\n    S_churn:                SingleOrList(int) = 80\n    S_tmin:                 SingleOrList(float) = 0.05\n    S_tmax:                 SingleOrList(int) = 50\n    S_noise:                SingleOrList(float) = 1.003\n\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unet_klass = Unet3D if is_video else Unet\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = ElucidatedImagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ImagenTrainerConfig(AllowExtraBaseModel):\n    imagen:                 dict\n    elucidated:             bool = False\n    video:                  bool = False\n    use_ema:                bool = True\n    lr:                     SingleOrList(float) = 1e-4\n    eps:                    SingleOrList(float) = 1e-8\n    beta1:                  float = 0.9", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_115-165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    @validator('image_sizes')\n    def check_image_sizes(cls, image_sizes, values):\n        unets = values.get('unets')\n        if len(image_sizes) != len(unets):\n            raise ValueError(f'image sizes length {len(image_sizes)} must be equivalent to the number of unets {len(unets)}')\n        return image_sizes\n\n    def create(self):\n        decoder_kwargs = self.dict()\n        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unet_klass = Unet3D if is_video else Unet\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = ElucidatedImagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ImagenTrainerConfig(AllowExtraBaseModel):\n    imagen:                 dict\n    elucidated:             bool = False\n    video:                  bool = False\n    use_ema:                bool = True\n    lr:                     SingleOrList(float) = 1e-4\n    eps:                    SingleOrList(float) = 1e-8\n    beta1:                  float = 0.9\n    beta2:                  float = 0.99\n    max_grad_norm:          Optional[float] = None\n    group_wd_params:        bool = True\n    warmup_steps:           SingleOrList(Optional[int]) = None\n    cosine_decay_max_steps: SingleOrList(Optional[int]) = None\n\n    def create(self):\n        trainer_kwargs = self.dict()\n\n        imagen_config = trainer_kwargs.pop('imagen')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_125-175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 181, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        unets_kwargs = decoder_kwargs.pop('unets')\n        is_video = decoder_kwargs.pop('video', False)\n\n        unet_klass = Unet3D if is_video else Unet\n\n        unets = []\n\n        for unet, unet_kwargs in zip(self.unets, unets_kwargs):\n            if isinstance(unet, NullUnetConfig):\n                unet_klass = NullUnet\n            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = ElucidatedImagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ImagenTrainerConfig(AllowExtraBaseModel):\n    imagen:                 dict\n    elucidated:             bool = False\n    video:                  bool = False\n    use_ema:                bool = True\n    lr:                     SingleOrList(float) = 1e-4\n    eps:                    SingleOrList(float) = 1e-8\n    beta1:                  float = 0.9\n    beta2:                  float = 0.99\n    max_grad_norm:          Optional[float] = None\n    group_wd_params:        bool = True\n    warmup_steps:           SingleOrList(Optional[int]) = None\n    cosine_decay_max_steps: SingleOrList(Optional[int]) = None\n\n    def create(self):\n        trainer_kwargs = self.dict()\n\n        imagen_config = trainer_kwargs.pop('imagen')\n        elucidated = trainer_kwargs.pop('elucidated')\n\n        imagen_config_klass = ElucidatedImagenConfig if elucidated else ImagenConfig\n        imagen = imagen_config_klass(**{**imagen_config, 'video': video}).create()\n\n        return ImagenTrainer(imagen, **trainer_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_135-181"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 181, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            elif is_video:\n                unet_klass = Unet3D\n            else:\n                unet_klass = Unet\n\n            unets.append(unet_klass(**unet_kwargs))\n\n        imagen = ElucidatedImagen(unets, **decoder_kwargs)\n\n        imagen._config = self.dict().copy()\n        return imagen\n\nclass ImagenTrainerConfig(AllowExtraBaseModel):\n    imagen:                 dict\n    elucidated:             bool = False\n    video:                  bool = False\n    use_ema:                bool = True\n    lr:                     SingleOrList(float) = 1e-4\n    eps:                    SingleOrList(float) = 1e-8\n    beta1:                  float = 0.9\n    beta2:                  float = 0.99\n    max_grad_norm:          Optional[float] = None\n    group_wd_params:        bool = True\n    warmup_steps:           SingleOrList(Optional[int]) = None\n    cosine_decay_max_steps: SingleOrList(Optional[int]) = None\n\n    def create(self):\n        trainer_kwargs = self.dict()\n\n        imagen_config = trainer_kwargs.pop('imagen')\n        elucidated = trainer_kwargs.pop('elucidated')\n\n        imagen_config_klass = ElucidatedImagenConfig if elucidated else ImagenConfig\n        imagen = imagen_config_klass(**{**imagen_config, 'video': video}).create()\n\n        return ImagenTrainer(imagen, **trainer_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_145-181"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-configs.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "configs.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 181, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return imagen\n\nclass ImagenTrainerConfig(AllowExtraBaseModel):\n    imagen:                 dict\n    elucidated:             bool = False\n    video:                  bool = False\n    use_ema:                bool = True\n    lr:                     SingleOrList(float) = 1e-4\n    eps:                    SingleOrList(float) = 1e-8\n    beta1:                  float = 0.9\n    beta2:                  float = 0.99\n    max_grad_norm:          Optional[float] = None\n    group_wd_params:        bool = True\n    warmup_steps:           SingleOrList(Optional[int]) = None\n    cosine_decay_max_steps: SingleOrList(Optional[int]) = None\n\n    def create(self):\n        trainer_kwargs = self.dict()\n\n        imagen_config = trainer_kwargs.pop('imagen')\n        elucidated = trainer_kwargs.pop('elucidated')\n\n        imagen_config_klass = ElucidatedImagenConfig if elucidated else ImagenConfig\n        imagen = imagen_config_klass(**{**imagen_config, 'video': video}).create()\n\n        return ImagenTrainer(imagen, **trainer_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-configs.py_155-181"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)Assign(Name(Store)Call(Name(Load)))FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(arg)While(ConstantFor(Name(Store)Name(Load)Expr(Yield(Name(Load))))))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)NotEqName(Load))Return(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from pathlib import Path\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T, utils\nimport torch.nn.functional as F\nfrom imagen_pytorch import t5\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom PIL import Image\n\nfrom datasets.utils.file_utils import get_datasets_user_agent\nimport io\nimport urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import urllib\n\nUSER_AGENT = get_datasets_user_agent()\n\n# helpers functions\n\ndef exists(val):\n    return val is not None\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:\n                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))\n\nAST=Module(Import(alias)Assign(Name(Store)Call(Name(Load)))FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(arg)While(ConstantFor(Name(Store)Name(Load)Expr(Yield(Name(Load))))))FunctionDef(arguments(argarg)If(Compare(Attribute(Name(Load)Load)NotEqName(Load))Return(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load)))ClassDef(FunctionDef(arguments(argargargargargargarg)Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Compare(Name(Load)IsNotConstant))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Call(Attribute(Name(Load)Load)Name(Load))Call(Attribute(Name(Load)Load)Name(Load))Call(Attribute(Name(Load)Load))Load))))FunctionDef(arguments(argarg)Assign(Name(Store)List(Load))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Try(If(Attribute(Name(Load)Load)Assign(Name(Store)Call(Attribute(Name(Load)Load)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))Assign(Name(Store)Subscript(Name(Load)Attribute(Name(Load)Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Attribute(Name(Load)Load))))ExceptHandler(Continue))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Subscript(Name(Load)Attribute(Name(Load)Load)Load)Load)keyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    while True:\n        for data in dl:\n            yield data\n\ndef convert_image_to(img_type, image):\n    if image.mode != img_type:\n        return image.convert(img_type)\n    return image\n\n# dataset, dataloader, collator\n\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:\n                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))\n            images.append(image)\n\n        if len(texts) == 0:\n            return None\n        \n        texts = pad_sequence(texts, True)\n\n        newbatch = []\n        for i in range(len(texts)):\n            newbatch.append((images[i], texts[i]))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass Collator:\n    def __init__(self, image_size, url_label, text_label, image_label, name, channels):\n        self.url_label = url_label\n        self.text_label = text_label\n        self.image_label = image_label\n        self.download = url_label is not None\n        self.name = name\n        self.channels = channels\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:\n                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))\n            images.append(image)\n\n        if len(texts) == 0:\n            return None\n        \n        texts = pad_sequence(texts, True)\n\n        newbatch = []\n        for i in range(len(texts)):\n            newbatch.append((images[i], texts[i]))\n\n        return torch.utils.data.dataloader.default_collate(newbatch)\n\n    def fetch_single_image(self, image_url, timeout=1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            T.Resize(image_size),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n        ])\n    def __call__(self, batch):\n\n        texts = []\n        images = []\n        for item in batch:\n            try:\n                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))\n            images.append(image)\n\n        if len(texts) == 0:\n            return None\n        \n        texts = pad_sequence(texts, True)\n\n        newbatch = []\n        for i in range(len(texts)):\n            newbatch.append((images[i], texts[i]))\n\n        return torch.utils.data.dataloader.default_collate(newbatch)\n\n    def fetch_single_image(self, image_url, timeout=1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )\n            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = Image.open(io.BytesIO(req.read())).convert('RGB')\n        except Exception:\n            image = None\n        return image\n\nclass Dataset(Dataset):\n    def __init__(\n        self,\n        folder,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                if self.download:\n                    image = self.fetch_single_image(item[self.url_label])\n                else:\n                    image = item[self.image_label]\n                image = self.transform(image.convert(self.channels))\n            except:\n                continue\n\n            text = t5.t5_encode_text([item[self.text_label]], name=self.name)\n            texts.append(torch.squeeze(text))\n            images.append(image)\n\n        if len(texts) == 0:\n            return None\n        \n        texts = pad_sequence(texts, True)\n\n        newbatch = []\n        for i in range(len(texts)):\n            newbatch.append((images[i], texts[i]))\n\n        return torch.utils.data.dataloader.default_collate(newbatch)\n\n    def fetch_single_image(self, image_url, timeout=1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )\n            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = Image.open(io.BytesIO(req.read())).convert('RGB')\n        except Exception:\n            image = None\n        return image\n\nclass Dataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n        convert_image_to_type = None\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        convert_fn = partial(convert_image_to, convert_image_to_type) if exists(convert_image_to_type) else nn.Identity()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            images.append(image)\n\n        if len(texts) == 0:\n            return None\n        \n        texts = pad_sequence(texts, True)\n\n        newbatch = []\n        for i in range(len(texts)):\n            newbatch.append((images[i], texts[i]))\n\n        return torch.utils.data.dataloader.default_collate(newbatch)\n\n    def fetch_single_image(self, image_url, timeout=1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )\n            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = Image.open(io.BytesIO(req.read())).convert('RGB')\n        except Exception:\n            image = None\n        return image\n\nclass Dataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n        convert_image_to_type = None\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        convert_fn = partial(convert_image_to, convert_image_to_type) if exists(convert_image_to_type) else nn.Identity()\n\n        self.transform = T.Compose([\n            T.Lambda(convert_fn),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        return torch.utils.data.dataloader.default_collate(newbatch)\n\n    def fetch_single_image(self, image_url, timeout=1):\n        try:\n            request = urllib.request.Request(\n                image_url,\n                data=None,\n                headers={\"user-agent\": USER_AGENT},\n            )\n            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = Image.open(io.BytesIO(req.read())).convert('RGB')\n        except Exception:\n            image = None\n        return image\n\nclass Dataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n        convert_image_to_type = None\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        convert_fn = partial(convert_image_to, convert_image_to_type) if exists(convert_image_to_type) else nn.Identity()\n\n        self.transform = T.Compose([\n            T.Lambda(convert_fn),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\ndef get_images_dataloader(\n    folder,\n    *,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            with urllib.request.urlopen(request, timeout=timeout) as req:\n                image = Image.open(io.BytesIO(req.read())).convert('RGB')\n        except Exception:\n            image = None\n        return image\n\nclass Dataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n        convert_image_to_type = None\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        convert_fn = partial(convert_image_to, convert_image_to_type) if exists(convert_image_to_type) else nn.Identity()\n\n        self.transform = T.Compose([\n            T.Lambda(convert_fn),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\ndef get_images_dataloader(\n    folder,\n    *,\n    batch_size,\n    image_size,\n    shuffle = True,\n    cycle_dl = False,\n    pin_memory = True\n):\n    ds = Dataset(folder, image_size)\n    dl = DataLoader(ds, batch_size = batch_size, shuffle = shuffle, pin_memory = pin_memory)\n\n    if cycle_dl:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 137, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        image_size,\n        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n        convert_image_to_type = None\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        convert_fn = partial(convert_image_to, convert_image_to_type) if exists(convert_image_to_type) else nn.Identity()\n\n        self.transform = T.Compose([\n            T.Lambda(convert_fn),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\ndef get_images_dataloader(\n    folder,\n    *,\n    batch_size,\n    image_size,\n    shuffle = True,\n    cycle_dl = False,\n    pin_memory = True\n):\n    ds = Dataset(folder, image_size)\n    dl = DataLoader(ds, batch_size = batch_size, shuffle = shuffle, pin_memory = pin_memory)\n\n    if cycle_dl:\n        dl = cycle(dl)\n    return dl", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_95-137"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-data.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "data.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 137, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.transform = T.Compose([\n            T.Lambda(convert_fn),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\ndef get_images_dataloader(\n    folder,\n    *,\n    batch_size,\n    image_size,\n    shuffle = True,\n    cycle_dl = False,\n    pin_memory = True\n):\n    ds = Dataset(folder, image_size)\n    dl = DataLoader(ds, batch_size = batch_size, shuffle = shuffle, pin_memory = pin_memory)\n\n    if cycle_dl:\n        dl = cycle(dl)\n    return dl", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-data.py_105-137"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from math import sqrt\nfrom random import random\nfrom functools import partial\nfrom contextlib import contextmanager, nullcontext\nfrom typing import List, Union\nfrom collections import namedtuple\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.nn.parallel import DistributedDataParallel\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce\nfrom einops_exts import rearrange_many\n\nfrom imagen_pytorch.imagen_pytorch import (\n    GaussianDiffusionContinuousTimes,\n    Unet,\n    NullUnet,\n    first,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from math import sqrt\nfrom random import random\nfrom functools import partial\nfrom contextlib import contextmanager, nullcontext\nfrom typing import List, Union\nfrom collections import namedtuple\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.nn.parallel import DistributedDataParallel\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce\nfrom einops_exts import rearrange_many\n\nfrom imagen_pytorch.imagen_pytorch import (\n    GaussianDiffusionContinuousTimes,\n    Unet,\n    NullUnet,\n    first,\n    exists,\n    identity,\n    maybe,\n    default,\n    cast_tuple,\n    cast_uint8_images_to_float,\n    eval_decorator,\n    check_shape,\n    pad_tuple_to_length,\n    resize_image_to,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from math import sqrt\nfrom random import random\nfrom functools import partial\nfrom contextlib import contextmanager, nullcontext\nfrom typing import List, Union\nfrom collections import namedtuple\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.nn.parallel import DistributedDataParallel\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce\nfrom einops_exts import rearrange_many\n\nfrom imagen_pytorch.imagen_pytorch import (\n    GaussianDiffusionContinuousTimes,\n    Unet,\n    NullUnet,\n    first,\n    exists,\n    identity,\n    maybe,\n    default,\n    cast_tuple,\n    cast_uint8_images_to_float,\n    eval_decorator,\n    check_shape,\n    pad_tuple_to_length,\n    resize_image_to,\n    calc_all_frame_dims,\n    safe_get_tuple_index,\n    right_pad_dims_to,\n    module_device,\n    normalize_neg_one_to_one,\n    unnormalize_zero_to_one,\n    compact,\n    maybe_transform_dict_key\n)\n\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasalias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from collections import namedtuple\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.nn.parallel import DistributedDataParallel\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce\nfrom einops_exts import rearrange_many\n\nfrom imagen_pytorch.imagen_pytorch import (\n    GaussianDiffusionContinuousTimes,\n    Unet,\n    NullUnet,\n    first,\n    exists,\n    identity,\n    maybe,\n    default,\n    cast_tuple,\n    cast_uint8_images_to_float,\n    eval_decorator,\n    check_shape,\n    pad_tuple_to_length,\n    resize_image_to,\n    calc_all_frame_dims,\n    safe_get_tuple_index,\n    right_pad_dims_to,\n    module_device,\n    normalize_neg_one_to_one,\n    unnormalize_zero_to_one,\n    compact,\n    maybe_transform_dict_key\n)\n\nfrom imagen_pytorch.imagen_video import (\n    Unet3D,\n    resize_video_to,\n    scale_video_time\n)\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# constants\n\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasaliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce\nfrom einops_exts import rearrange_many\n\nfrom imagen_pytorch.imagen_pytorch import (\n    GaussianDiffusionContinuousTimes,\n    Unet,\n    NullUnet,\n    first,\n    exists,\n    identity,\n    maybe,\n    default,\n    cast_tuple,\n    cast_uint8_images_to_float,\n    eval_decorator,\n    check_shape,\n    pad_tuple_to_length,\n    resize_image_to,\n    calc_all_frame_dims,\n    safe_get_tuple_index,\n    right_pad_dims_to,\n    module_device,\n    normalize_neg_one_to_one,\n    unnormalize_zero_to_one,\n    compact,\n    maybe_transform_dict_key\n)\n\nfrom imagen_pytorch.imagen_video import (\n    Unet3D,\n    resize_video_to,\n    scale_video_time\n)\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# constants\n\nHparams_fields = [\n    'num_sample_steps',\n    'sigma_min',\n    'sigma_max',\n    'sigma_data',\n    'rho',\n    'P_mean',\n    'P_std',\n    'S_churn',\n    'S_tmin',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    exists,\n    identity,\n    maybe,\n    default,\n    cast_tuple,\n    cast_uint8_images_to_float,\n    eval_decorator,\n    check_shape,\n    pad_tuple_to_length,\n    resize_image_to,\n    calc_all_frame_dims,\n    safe_get_tuple_index,\n    right_pad_dims_to,\n    module_device,\n    normalize_neg_one_to_one,\n    unnormalize_zero_to_one,\n    compact,\n    maybe_transform_dict_key\n)\n\nfrom imagen_pytorch.imagen_video import (\n    Unet3D,\n    resize_video_to,\n    scale_video_time\n)\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# constants\n\nHparams_fields = [\n    'num_sample_steps',\n    'sigma_min',\n    'sigma_max',\n    'sigma_data',\n    'rho',\n    'P_mean',\n    'P_std',\n    'S_churn',\n    'S_tmin',\n    'S_tmax',\n    'S_noise'\n]\n\nHparams = namedtuple('Hparams', Hparams_fields)\n\n# helper functions\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    calc_all_frame_dims,\n    safe_get_tuple_index,\n    right_pad_dims_to,\n    module_device,\n    normalize_neg_one_to_one,\n    unnormalize_zero_to_one,\n    compact,\n    maybe_transform_dict_key\n)\n\nfrom imagen_pytorch.imagen_video import (\n    Unet3D,\n    resize_video_to,\n    scale_video_time\n)\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# constants\n\nHparams_fields = [\n    'num_sample_steps',\n    'sigma_min',\n    'sigma_max',\n    'sigma_data',\n    'rho',\n    'P_mean',\n    'P_std',\n    'S_churn',\n    'S_tmin',\n    'S_tmax',\n    'S_noise'\n]\n\nHparams = namedtuple('Hparams', Hparams_fields)\n\n# helper functions\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\n# main class\n\nclass ElucidatedImagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.imagen_video import (\n    Unet3D,\n    resize_video_to,\n    scale_video_time\n)\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# constants\n\nHparams_fields = [\n    'num_sample_steps',\n    'sigma_min',\n    'sigma_max',\n    'sigma_data',\n    'rho',\n    'P_mean',\n    'P_std',\n    'S_churn',\n    'S_tmin',\n    'S_tmax',\n    'S_noise'\n]\n\nHparams = namedtuple('Hparams', Hparams_fields)\n\n# helper functions\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\n# main class\n\nclass ElucidatedImagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        cond_drop_prob = 0.1,\n        random_crop_sizes = None,\n        resize_mode = 'nearest',\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "Hparams_fields = [\n    'num_sample_steps',\n    'sigma_min',\n    'sigma_max',\n    'sigma_data',\n    'rho',\n    'P_mean',\n    'P_std',\n    'S_churn',\n    'S_tmin',\n    'S_tmax',\n    'S_noise'\n]\n\nHparams = namedtuple('Hparams', Hparams_fields)\n\n# helper functions\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\n# main class\n\nclass ElucidatedImagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        cond_drop_prob = 0.1,\n        random_crop_sizes = None,\n        resize_mode = 'nearest',\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        lowres_noise_schedule = 'linear',\n        num_sample_steps = 32,                      # number of sampling steps\n        sigma_min = 0.002,                          # min noise level\n        sigma_max = 80,                             # max noise level\n        sigma_data = 0.5,                           # standard deviation of data distribution\n        rho = 7,                                    # controls the sampling schedule", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    'S_tmax',\n    'S_noise'\n]\n\nHparams = namedtuple('Hparams', Hparams_fields)\n\n# helper functions\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\n# main class\n\nclass ElucidatedImagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        cond_drop_prob = 0.1,\n        random_crop_sizes = None,\n        resize_mode = 'nearest',\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        lowres_noise_schedule = 'linear',\n        num_sample_steps = 32,                      # number of sampling steps\n        sigma_min = 0.002,                          # min noise level\n        sigma_max = 80,                             # max noise level\n        sigma_data = 0.5,                           # standard deviation of data distribution\n        rho = 7,                                    # controls the sampling schedule\n        P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n        P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n        S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n        S_tmin = 0.05,\n        S_tmax = 50,\n        S_noise = 1.003,\n    ):\n        super().__init__()\n\n        self.only_train_unet_number = only_train_unet_number", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n# main class\n\nclass ElucidatedImagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        cond_drop_prob = 0.1,\n        random_crop_sizes = None,\n        resize_mode = 'nearest',\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        lowres_noise_schedule = 'linear',\n        num_sample_steps = 32,                      # number of sampling steps\n        sigma_min = 0.002,                          # min noise level\n        sigma_max = 80,                             # max noise level\n        sigma_data = 0.5,                           # standard deviation of data distribution\n        rho = 7,                                    # controls the sampling schedule\n        P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n        P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n        S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n        S_tmin = 0.05,\n        S_tmax = 50,\n        S_noise = 1.003,\n    ):\n        super().__init__()\n\n        self.only_train_unet_number = only_train_unet_number\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargargargargargargargargargargargargargargargargargargargargargargargargargargName(Load)ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantUnaryOp(USubConstant)ConstantConstantConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)UnaryOp(NotName(Load)))Assign(Attribute(Name(Load)Store)Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        text_embed_dim = None,\n        channels = 3,\n        cond_drop_prob = 0.1,\n        random_crop_sizes = None,\n        resize_mode = 'nearest',\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        lowres_noise_schedule = 'linear',\n        num_sample_steps = 32,                      # number of sampling steps\n        sigma_min = 0.002,                          # min noise level\n        sigma_max = 80,                             # max noise level\n        sigma_data = 0.5,                           # standard deviation of data distribution\n        rho = 7,                                    # controls the sampling schedule\n        P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n        P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n        S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n        S_tmin = 0.05,\n        S_tmax = 50,\n        S_noise = 1.003,\n    ):\n        super().__init__()\n\n        self.only_train_unet_number = only_train_unet_number\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        lowres_noise_schedule = 'linear',\n        num_sample_steps = 32,                      # number of sampling steps\n        sigma_min = 0.002,                          # min noise level\n        sigma_max = 80,                             # max noise level\n        sigma_data = 0.5,                           # standard deviation of data distribution\n        rho = 7,                                    # controls the sampling schedule\n        P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n        P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n        S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n        S_tmin = 0.05,\n        S_tmax = 50,\n        S_noise = 1.003,\n    ):\n        super().__init__()\n\n        self.only_train_unet_number = only_train_unet_number\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_95-145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n        P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n        S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n        S_tmin = 0.05,\n        S_tmax = 50,\n        S_noise = 1.003,\n    ):\n        super().__init__()\n\n        self.only_train_unet_number = only_train_unet_number\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_105-155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_115-165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_125-175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # unet image sizes\n\n        self.image_sizes = cast_tuple(image_sizes)\n        assert num_unets == len(self.image_sizes), f'you did not supply the correct number of u-nets ({len(self.unets)}) for resolutions {self.image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # cascading ddpm related stuff\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_135-185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # unet image sizes\n\n        self.image_sizes = cast_tuple(image_sizes)\n        assert num_unets == len(self.image_sizes), f'you did not supply the correct number of u-nets ({len(self.unets)}) for resolutions {self.image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_145-195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # unet image sizes\n\n        self.image_sizes = cast_tuple(image_sizes)\n        assert num_unets == len(self.image_sizes), f'you did not supply the correct number of u-nets ({len(self.unets)}) for resolutions {self.image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_155-205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # unet image sizes\n\n        self.image_sizes = cast_tuple(image_sizes)\n        assert num_unets == len(self.image_sizes), f'you did not supply the correct number of u-nets ({len(self.unets)}) for resolutions {self.image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # temporal interpolations\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_165-215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # unet image sizes\n\n        self.image_sizes = cast_tuple(image_sizes)\n        assert num_unets == len(self.image_sizes), f'you did not supply the correct number of u-nets ({len(self.unets)}) for resolutions {self.image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # temporal interpolations\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # elucidating parameters\n\n        hparams = [\n            num_sample_steps,\n            sigma_min,\n            sigma_max,\n            sigma_data,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_175-225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # temporal interpolations\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # elucidating parameters\n\n        hparams = [\n            num_sample_steps,\n            sigma_min,\n            sigma_max,\n            sigma_data,\n            rho,\n            P_mean,\n            P_std,\n            S_churn,\n            S_tmin,\n            S_tmax,\n            S_noise,\n        ]\n\n        hparams = [cast_tuple(hp, num_unets) for hp in hparams]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_185-235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # temporal interpolations\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # elucidating parameters\n\n        hparams = [\n            num_sample_steps,\n            sigma_min,\n            sigma_max,\n            sigma_data,\n            rho,\n            P_mean,\n            P_std,\n            S_churn,\n            S_tmin,\n            S_tmax,\n            S_noise,\n        ]\n\n        hparams = [cast_tuple(hp, num_unets) for hp in hparams]\n        self.hparams = [Hparams(*unet_hp) for unet_hp in zip(*hparams)]\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_195-245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # temporal interpolations\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # elucidating parameters\n\n        hparams = [\n            num_sample_steps,\n            sigma_min,\n            sigma_max,\n            sigma_data,\n            rho,\n            P_mean,\n            P_std,\n            S_churn,\n            S_tmin,\n            S_tmax,\n            S_noise,\n        ]\n\n        hparams = [cast_tuple(hp, num_unets) for hp in hparams]\n        self.hparams = [Hparams(*unet_hp) for unet_hp in zip(*hparams)]\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_205-255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # elucidating parameters\n\n        hparams = [\n            num_sample_steps,\n            sigma_min,\n            sigma_max,\n            sigma_data,\n            rho,\n            P_mean,\n            P_std,\n            S_churn,\n            S_tmin,\n            S_tmax,\n            S_noise,\n        ]\n\n        hparams = [cast_tuple(hp, num_unets) for hp in hparams]\n        self.hparams = [Hparams(*unet_hp) for unet_hp in zip(*hparams)]\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_215-265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            rho,\n            P_mean,\n            P_std,\n            S_churn,\n            S_tmin,\n            S_tmax,\n            S_noise,\n        ]\n\n        hparams = [cast_tuple(hp, num_unets) for hp in hparams]\n        self.hparams = [Hparams(*unet_hp) for unet_hp in zip(*hparams)]\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_225-275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.hparams = [Hparams(*unet_hp) for unet_hp in zip(*hparams)]\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_235-285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_245-295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_255-305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # dynamic thresholding\n\n    def threshold_x_start(self, x_start, dynamic_threshold = True):\n        if not dynamic_threshold:\n            return x_start.clamp(-1., 1.)\n\n        s = torch.quantile(\n            rearrange(x_start, 'b ... -> b (...)').abs(),\n            self.dynamic_thresholding_percentile,\n            dim = -1", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_265-315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # dynamic thresholding\n\n    def threshold_x_start(self, x_start, dynamic_threshold = True):\n        if not dynamic_threshold:\n            return x_start.clamp(-1., 1.)\n\n        s = torch.quantile(\n            rearrange(x_start, 'b ... -> b (...)').abs(),\n            self.dynamic_thresholding_percentile,\n            dim = -1\n        )\n\n        s.clamp_(min = 1.)\n        s = right_pad_dims_to(x_start, s)\n        return x_start.clamp(-s, s) / s\n\n    # derived preconditioning params - Table 1\n\n    def c_skip(self, sigma_data, sigma):\n        return (sigma_data ** 2) / (sigma ** 2 + sigma_data ** 2)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_275-325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # dynamic thresholding\n\n    def threshold_x_start(self, x_start, dynamic_threshold = True):\n        if not dynamic_threshold:\n            return x_start.clamp(-1., 1.)\n\n        s = torch.quantile(\n            rearrange(x_start, 'b ... -> b (...)').abs(),\n            self.dynamic_thresholding_percentile,\n            dim = -1\n        )\n\n        s.clamp_(min = 1.)\n        s = right_pad_dims_to(x_start, s)\n        return x_start.clamp(-s, s) / s\n\n    # derived preconditioning params - Table 1\n\n    def c_skip(self, sigma_data, sigma):\n        return (sigma_data ** 2) / (sigma ** 2 + sigma_data ** 2)\n\n    def c_out(self, sigma_data, sigma):\n        return sigma * sigma_data * (sigma_data ** 2 + sigma ** 2) ** -0.5\n\n    def c_in(self, sigma_data, sigma):\n        return 1 * (sigma ** 2 + sigma_data ** 2) ** -0.5\n\n    def c_noise(self, sigma):\n        return log(sigma) * 0.25\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_285-335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # dynamic thresholding\n\n    def threshold_x_start(self, x_start, dynamic_threshold = True):\n        if not dynamic_threshold:\n            return x_start.clamp(-1., 1.)\n\n        s = torch.quantile(\n            rearrange(x_start, 'b ... -> b (...)').abs(),\n            self.dynamic_thresholding_percentile,\n            dim = -1\n        )\n\n        s.clamp_(min = 1.)\n        s = right_pad_dims_to(x_start, s)\n        return x_start.clamp(-s, s) / s\n\n    # derived preconditioning params - Table 1\n\n    def c_skip(self, sigma_data, sigma):\n        return (sigma_data ** 2) / (sigma ** 2 + sigma_data ** 2)\n\n    def c_out(self, sigma_data, sigma):\n        return sigma * sigma_data * (sigma_data ** 2 + sigma ** 2) ** -0.5\n\n    def c_in(self, sigma_data, sigma):\n        return 1 * (sigma ** 2 + sigma_data ** 2) ** -0.5\n\n    def c_noise(self, sigma):\n        return log(sigma) * 0.25\n\n   # preconditioned network output\n    # equation (7) in the paper\n\n    def preconditioned_network_forward(\n        self,\n        unet_forward,\n        noised_images,\n        sigma,\n        *,\n        sigma_data,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_295-345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    # dynamic thresholding\n\n    def threshold_x_start(self, x_start, dynamic_threshold = True):\n        if not dynamic_threshold:\n            return x_start.clamp(-1., 1.)\n\n        s = torch.quantile(\n            rearrange(x_start, 'b ... -> b (...)').abs(),\n            self.dynamic_thresholding_percentile,\n            dim = -1\n        )\n\n        s.clamp_(min = 1.)\n        s = right_pad_dims_to(x_start, s)\n        return x_start.clamp(-s, s) / s\n\n    # derived preconditioning params - Table 1\n\n    def c_skip(self, sigma_data, sigma):\n        return (sigma_data ** 2) / (sigma ** 2 + sigma_data ** 2)\n\n    def c_out(self, sigma_data, sigma):\n        return sigma * sigma_data * (sigma_data ** 2 + sigma ** 2) ** -0.5\n\n    def c_in(self, sigma_data, sigma):\n        return 1 * (sigma ** 2 + sigma_data ** 2) ** -0.5\n\n    def c_noise(self, sigma):\n        return log(sigma) * 0.25\n\n   # preconditioned network output\n    # equation (7) in the paper\n\n    def preconditioned_network_forward(\n        self,\n        unet_forward,\n        noised_images,\n        sigma,\n        *,\n        sigma_data,\n        clamp = False,\n        dynamic_threshold = True,\n        **kwargs\n    ):\n        batch, device = noised_images.shape[0], noised_images.device\n\n        if isinstance(sigma, float):\n            sigma = torch.full((batch,), sigma, device = device)\n\n        padded_sigma = self.right_pad_dims_to_datatype(sigma)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_305-355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n        s.clamp_(min = 1.)\n        s = right_pad_dims_to(x_start, s)\n        return x_start.clamp(-s, s) / s\n\n    # derived preconditioning params - Table 1\n\n    def c_skip(self, sigma_data, sigma):\n        return (sigma_data ** 2) / (sigma ** 2 + sigma_data ** 2)\n\n    def c_out(self, sigma_data, sigma):\n        return sigma * sigma_data * (sigma_data ** 2 + sigma ** 2) ** -0.5\n\n    def c_in(self, sigma_data, sigma):\n        return 1 * (sigma ** 2 + sigma_data ** 2) ** -0.5\n\n    def c_noise(self, sigma):\n        return log(sigma) * 0.25\n\n   # preconditioned network output\n    # equation (7) in the paper\n\n    def preconditioned_network_forward(\n        self,\n        unet_forward,\n        noised_images,\n        sigma,\n        *,\n        sigma_data,\n        clamp = False,\n        dynamic_threshold = True,\n        **kwargs\n    ):\n        batch, device = noised_images.shape[0], noised_images.device\n\n        if isinstance(sigma, float):\n            sigma = torch.full((batch,), sigma, device = device)\n\n        padded_sigma = self.right_pad_dims_to_datatype(sigma)\n\n        net_out = unet_forward(\n            self.c_in(sigma_data, padded_sigma) * noised_images,\n            self.c_noise(sigma),\n            **kwargs\n        )\n\n        out = self.c_skip(sigma_data, padded_sigma) * noised_images +  self.c_out(sigma_data, padded_sigma) * net_out\n\n        if not clamp:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_315-365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def c_out(self, sigma_data, sigma):\n        return sigma * sigma_data * (sigma_data ** 2 + sigma ** 2) ** -0.5\n\n    def c_in(self, sigma_data, sigma):\n        return 1 * (sigma ** 2 + sigma_data ** 2) ** -0.5\n\n    def c_noise(self, sigma):\n        return log(sigma) * 0.25\n\n   # preconditioned network output\n    # equation (7) in the paper\n\n    def preconditioned_network_forward(\n        self,\n        unet_forward,\n        noised_images,\n        sigma,\n        *,\n        sigma_data,\n        clamp = False,\n        dynamic_threshold = True,\n        **kwargs\n    ):\n        batch, device = noised_images.shape[0], noised_images.device\n\n        if isinstance(sigma, float):\n            sigma = torch.full((batch,), sigma, device = device)\n\n        padded_sigma = self.right_pad_dims_to_datatype(sigma)\n\n        net_out = unet_forward(\n            self.c_in(sigma_data, padded_sigma) * noised_images,\n            self.c_noise(sigma),\n            **kwargs\n        )\n\n        out = self.c_skip(sigma_data, padded_sigma) * noised_images +  self.c_out(sigma_data, padded_sigma) * net_out\n\n        if not clamp:\n            return out\n\n        return self.threshold_x_start(out, dynamic_threshold)\n\n    # sampling\n\n    # sample schedule\n    # equation (5) in the paper\n\n    def sample_schedule(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_325-375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "   # preconditioned network output\n    # equation (7) in the paper\n\n    def preconditioned_network_forward(\n        self,\n        unet_forward,\n        noised_images,\n        sigma,\n        *,\n        sigma_data,\n        clamp = False,\n        dynamic_threshold = True,\n        **kwargs\n    ):\n        batch, device = noised_images.shape[0], noised_images.device\n\n        if isinstance(sigma, float):\n            sigma = torch.full((batch,), sigma, device = device)\n\n        padded_sigma = self.right_pad_dims_to_datatype(sigma)\n\n        net_out = unet_forward(\n            self.c_in(sigma_data, padded_sigma) * noised_images,\n            self.c_noise(sigma),\n            **kwargs\n        )\n\n        out = self.c_skip(sigma_data, padded_sigma) * noised_images +  self.c_out(sigma_data, padded_sigma) * net_out\n\n        if not clamp:\n            return out\n\n        return self.threshold_x_start(out, dynamic_threshold)\n\n    # sampling\n\n    # sample schedule\n    # equation (5) in the paper\n\n    def sample_schedule(\n        self,\n        num_sample_steps,\n        rho,\n        sigma_min,\n        sigma_max\n    ):\n        N = num_sample_steps\n        inv_rho = 1 / rho\n\n        steps = torch.arange(num_sample_steps, device = self.device, dtype = torch.float32)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_335-385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        clamp = False,\n        dynamic_threshold = True,\n        **kwargs\n    ):\n        batch, device = noised_images.shape[0], noised_images.device\n\n        if isinstance(sigma, float):\n            sigma = torch.full((batch,), sigma, device = device)\n\n        padded_sigma = self.right_pad_dims_to_datatype(sigma)\n\n        net_out = unet_forward(\n            self.c_in(sigma_data, padded_sigma) * noised_images,\n            self.c_noise(sigma),\n            **kwargs\n        )\n\n        out = self.c_skip(sigma_data, padded_sigma) * noised_images +  self.c_out(sigma_data, padded_sigma) * net_out\n\n        if not clamp:\n            return out\n\n        return self.threshold_x_start(out, dynamic_threshold)\n\n    # sampling\n\n    # sample schedule\n    # equation (5) in the paper\n\n    def sample_schedule(\n        self,\n        num_sample_steps,\n        rho,\n        sigma_min,\n        sigma_max\n    ):\n        N = num_sample_steps\n        inv_rho = 1 / rho\n\n        steps = torch.arange(num_sample_steps, device = self.device, dtype = torch.float32)\n        sigmas = (sigma_max ** inv_rho + steps / (N - 1) * (sigma_min ** inv_rho - sigma_max ** inv_rho)) ** rho\n\n        sigmas = F.pad(sigmas, (0, 1), value = 0.) # last step is sigma value of 0.\n        return sigmas\n\n    @torch.no_grad()\n    def one_unet_sample(\n        self,\n        unet,\n        shape,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_345-395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        net_out = unet_forward(\n            self.c_in(sigma_data, padded_sigma) * noised_images,\n            self.c_noise(sigma),\n            **kwargs\n        )\n\n        out = self.c_skip(sigma_data, padded_sigma) * noised_images +  self.c_out(sigma_data, padded_sigma) * net_out\n\n        if not clamp:\n            return out\n\n        return self.threshold_x_start(out, dynamic_threshold)\n\n    # sampling\n\n    # sample schedule\n    # equation (5) in the paper\n\n    def sample_schedule(\n        self,\n        num_sample_steps,\n        rho,\n        sigma_min,\n        sigma_max\n    ):\n        N = num_sample_steps\n        inv_rho = 1 / rho\n\n        steps = torch.arange(num_sample_steps, device = self.device, dtype = torch.float32)\n        sigmas = (sigma_max ** inv_rho + steps / (N - 1) * (sigma_min ** inv_rho - sigma_max ** inv_rho)) ** rho\n\n        sigmas = F.pad(sigmas, (0, 1), value = 0.) # last step is sigma value of 0.\n        return sigmas\n\n    @torch.no_grad()\n    def one_unet_sample(\n        self,\n        unet,\n        shape,\n        *,\n        unet_number,\n        clamp = True,\n        dynamic_threshold = True,\n        cond_scale = 1.,\n        use_tqdm = True,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_355-405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return out\n\n        return self.threshold_x_start(out, dynamic_threshold)\n\n    # sampling\n\n    # sample schedule\n    # equation (5) in the paper\n\n    def sample_schedule(\n        self,\n        num_sample_steps,\n        rho,\n        sigma_min,\n        sigma_max\n    ):\n        N = num_sample_steps\n        inv_rho = 1 / rho\n\n        steps = torch.arange(num_sample_steps, device = self.device, dtype = torch.float32)\n        sigmas = (sigma_max ** inv_rho + steps / (N - 1) * (sigma_min ** inv_rho - sigma_max ** inv_rho)) ** rho\n\n        sigmas = F.pad(sigmas, (0, 1), value = 0.) # last step is sigma value of 0.\n        return sigmas\n\n    @torch.no_grad()\n    def one_unet_sample(\n        self,\n        unet,\n        shape,\n        *,\n        unet_number,\n        clamp = True,\n        dynamic_threshold = True,\n        cond_scale = 1.,\n        use_tqdm = True,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        **kwargs\n    ):\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_365-415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self,\n        num_sample_steps,\n        rho,\n        sigma_min,\n        sigma_max\n    ):\n        N = num_sample_steps\n        inv_rho = 1 / rho\n\n        steps = torch.arange(num_sample_steps, device = self.device, dtype = torch.float32)\n        sigmas = (sigma_max ** inv_rho + steps / (N - 1) * (sigma_min ** inv_rho - sigma_max ** inv_rho)) ** rho\n\n        sigmas = F.pad(sigmas, (0, 1), value = 0.) # last step is sigma value of 0.\n        return sigmas\n\n    @torch.no_grad()\n    def one_unet_sample(\n        self,\n        unet,\n        shape,\n        *,\n        unet_number,\n        clamp = True,\n        dynamic_threshold = True,\n        cond_scale = 1.,\n        use_tqdm = True,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        **kwargs\n    ):\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # get specific sampling hyperparameters for unet\n\n        hp = self.hparams[unet_number - 1]\n\n        sigma_min = default(sigma_min, hp.sigma_min)\n        sigma_max = default(sigma_max, hp.sigma_max)\n\n        # get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_375-425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        sigmas = (sigma_max ** inv_rho + steps / (N - 1) * (sigma_min ** inv_rho - sigma_max ** inv_rho)) ** rho\n\n        sigmas = F.pad(sigmas, (0, 1), value = 0.) # last step is sigma value of 0.\n        return sigmas\n\n    @torch.no_grad()\n    def one_unet_sample(\n        self,\n        unet,\n        shape,\n        *,\n        unet_number,\n        clamp = True,\n        dynamic_threshold = True,\n        cond_scale = 1.,\n        use_tqdm = True,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        **kwargs\n    ):\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # get specific sampling hyperparameters for unet\n\n        hp = self.hparams[unet_number - 1]\n\n        sigma_min = default(sigma_min, hp.sigma_min)\n        sigma_max = default(sigma_max, hp.sigma_max)\n\n        # get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n\n        sigmas = self.sample_schedule(hp.num_sample_steps, hp.rho, sigma_min, sigma_max)\n\n        gammas = torch.where(\n            (sigmas >= hp.S_tmin) & (sigmas <= hp.S_tmax),\n            min(hp.S_churn / hp.num_sample_steps, sqrt(2) - 1),\n            0.\n        )\n\n        sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[:-1]))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_385-435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        unet_number,\n        clamp = True,\n        dynamic_threshold = True,\n        cond_scale = 1.,\n        use_tqdm = True,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        **kwargs\n    ):\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # get specific sampling hyperparameters for unet\n\n        hp = self.hparams[unet_number - 1]\n\n        sigma_min = default(sigma_min, hp.sigma_min)\n        sigma_max = default(sigma_max, hp.sigma_max)\n\n        # get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n\n        sigmas = self.sample_schedule(hp.num_sample_steps, hp.rho, sigma_min, sigma_max)\n\n        gammas = torch.where(\n            (sigmas >= hp.S_tmin) & (sigmas <= hp.S_tmax),\n            min(hp.S_churn / hp.num_sample_steps, sqrt(2) - 1),\n            0.\n        )\n\n        sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[:-1]))\n\n        # images is noise at the beginning\n\n        init_sigma = sigmas[0]\n\n        images = init_sigma * torch.randn(shape, device = self.device)\n\n        # initializing with an image\n\n        if exists(init_images):\n            images += init_images", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_395-445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        **kwargs\n    ):\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # get specific sampling hyperparameters for unet\n\n        hp = self.hparams[unet_number - 1]\n\n        sigma_min = default(sigma_min, hp.sigma_min)\n        sigma_max = default(sigma_max, hp.sigma_max)\n\n        # get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n\n        sigmas = self.sample_schedule(hp.num_sample_steps, hp.rho, sigma_min, sigma_max)\n\n        gammas = torch.where(\n            (sigmas >= hp.S_tmin) & (sigmas <= hp.S_tmax),\n            min(hp.S_churn / hp.num_sample_steps, sqrt(2) - 1),\n            0.\n        )\n\n        sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[:-1]))\n\n        # images is noise at the beginning\n\n        init_sigma = sigmas[0]\n\n        images = init_sigma * torch.randn(shape, device = self.device)\n\n        # initializing with an image\n\n        if exists(init_images):\n            images += init_images\n\n        # keeping track of x0, for self conditioning if needed\n\n        x_start = None\n\n        # prepare inpainting images and mask\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_405-455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # get specific sampling hyperparameters for unet\n\n        hp = self.hparams[unet_number - 1]\n\n        sigma_min = default(sigma_min, hp.sigma_min)\n        sigma_max = default(sigma_max, hp.sigma_max)\n\n        # get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n\n        sigmas = self.sample_schedule(hp.num_sample_steps, hp.rho, sigma_min, sigma_max)\n\n        gammas = torch.where(\n            (sigmas >= hp.S_tmin) & (sigmas <= hp.S_tmax),\n            min(hp.S_churn / hp.num_sample_steps, sqrt(2) - 1),\n            0.\n        )\n\n        sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[:-1]))\n\n        # images is noise at the beginning\n\n        init_sigma = sigmas[0]\n\n        images = init_sigma * torch.randn(shape, device = self.device)\n\n        # initializing with an image\n\n        if exists(init_images):\n            images += init_images\n\n        # keeping track of x0, for self conditioning if needed\n\n        x_start = None\n\n        # prepare inpainting images and mask\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            clamp = clamp,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_415-465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        sigmas = self.sample_schedule(hp.num_sample_steps, hp.rho, sigma_min, sigma_max)\n\n        gammas = torch.where(\n            (sigmas >= hp.S_tmin) & (sigmas <= hp.S_tmax),\n            min(hp.S_churn / hp.num_sample_steps, sqrt(2) - 1),\n            0.\n        )\n\n        sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[:-1]))\n\n        # images is noise at the beginning\n\n        init_sigma = sigmas[0]\n\n        images = init_sigma * torch.randn(shape, device = self.device)\n\n        # initializing with an image\n\n        if exists(init_images):\n            images += init_images\n\n        # keeping track of x0, for self conditioning if needed\n\n        x_start = None\n\n        # prepare inpainting images and mask\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            clamp = clamp,\n            dynamic_threshold = dynamic_threshold,\n            cond_scale = cond_scale,\n            **kwargs\n        )\n\n        # gradually denoise\n\n        initial_step = default(skip_steps, 0)\n        sigmas_and_gammas = sigmas_and_gammas[initial_step:]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_425-475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # images is noise at the beginning\n\n        init_sigma = sigmas[0]\n\n        images = init_sigma * torch.randn(shape, device = self.device)\n\n        # initializing with an image\n\n        if exists(init_images):\n            images += init_images\n\n        # keeping track of x0, for self conditioning if needed\n\n        x_start = None\n\n        # prepare inpainting images and mask\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            clamp = clamp,\n            dynamic_threshold = dynamic_threshold,\n            cond_scale = cond_scale,\n            **kwargs\n        )\n\n        # gradually denoise\n\n        initial_step = default(skip_steps, 0)\n        sigmas_and_gammas = sigmas_and_gammas[initial_step:]\n\n        total_steps = len(sigmas_and_gammas)\n\n        for ind, (sigma, sigma_next, gamma) in tqdm(enumerate(sigmas_and_gammas), total = total_steps, desc = 'sampling time step', disable = not use_tqdm):\n            is_last_timestep = ind == (total_steps - 1)\n\n            sigma, sigma_next, gamma = map(lambda t: t.item(), (sigma, sigma_next, gamma))\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_435-485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # keeping track of x0, for self conditioning if needed\n\n        x_start = None\n\n        # prepare inpainting images and mask\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            clamp = clamp,\n            dynamic_threshold = dynamic_threshold,\n            cond_scale = cond_scale,\n            **kwargs\n        )\n\n        # gradually denoise\n\n        initial_step = default(skip_steps, 0)\n        sigmas_and_gammas = sigmas_and_gammas[initial_step:]\n\n        total_steps = len(sigmas_and_gammas)\n\n        for ind, (sigma, sigma_next, gamma) in tqdm(enumerate(sigmas_and_gammas), total = total_steps, desc = 'sampling time step', disable = not use_tqdm):\n            is_last_timestep = ind == (total_steps - 1)\n\n            sigma, sigma_next, gamma = map(lambda t: t.item(), (sigma, sigma_next, gamma))\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                eps = hp.S_noise * torch.randn(shape, device = self.device) # stochastic sampling\n\n                sigma_hat = sigma + gamma * sigma\n                added_noise = sqrt(sigma_hat ** 2 - sigma ** 2) * eps\n\n                images_hat = images + added_noise\n\n                self_cond = x_start if unet.self_cond else None\n\n                if has_inpainting:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_445-495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            clamp = clamp,\n            dynamic_threshold = dynamic_threshold,\n            cond_scale = cond_scale,\n            **kwargs\n        )\n\n        # gradually denoise\n\n        initial_step = default(skip_steps, 0)\n        sigmas_and_gammas = sigmas_and_gammas[initial_step:]\n\n        total_steps = len(sigmas_and_gammas)\n\n        for ind, (sigma, sigma_next, gamma) in tqdm(enumerate(sigmas_and_gammas), total = total_steps, desc = 'sampling time step', disable = not use_tqdm):\n            is_last_timestep = ind == (total_steps - 1)\n\n            sigma, sigma_next, gamma = map(lambda t: t.item(), (sigma, sigma_next, gamma))\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                eps = hp.S_noise * torch.randn(shape, device = self.device) # stochastic sampling\n\n                sigma_hat = sigma + gamma * sigma\n                added_noise = sqrt(sigma_hat ** 2 - sigma ** 2) * eps\n\n                images_hat = images + added_noise\n\n                self_cond = x_start if unet.self_cond else None\n\n                if has_inpainting:\n                    images_hat = images_hat * ~inpaint_masks + (inpaint_images + added_noise) * inpaint_masks\n\n                model_output = self.preconditioned_network_forward(\n                    unet.forward_with_cond_scale,\n                    images_hat,\n                    sigma_hat,\n                    self_cond = self_cond,\n                    **unet_kwargs\n                )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_455-505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            dynamic_threshold = dynamic_threshold,\n            cond_scale = cond_scale,\n            **kwargs\n        )\n\n        # gradually denoise\n\n        initial_step = default(skip_steps, 0)\n        sigmas_and_gammas = sigmas_and_gammas[initial_step:]\n\n        total_steps = len(sigmas_and_gammas)\n\n        for ind, (sigma, sigma_next, gamma) in tqdm(enumerate(sigmas_and_gammas), total = total_steps, desc = 'sampling time step', disable = not use_tqdm):\n            is_last_timestep = ind == (total_steps - 1)\n\n            sigma, sigma_next, gamma = map(lambda t: t.item(), (sigma, sigma_next, gamma))\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                eps = hp.S_noise * torch.randn(shape, device = self.device) # stochastic sampling\n\n                sigma_hat = sigma + gamma * sigma\n                added_noise = sqrt(sigma_hat ** 2 - sigma ** 2) * eps\n\n                images_hat = images + added_noise\n\n                self_cond = x_start if unet.self_cond else None\n\n                if has_inpainting:\n                    images_hat = images_hat * ~inpaint_masks + (inpaint_images + added_noise) * inpaint_masks\n\n                model_output = self.preconditioned_network_forward(\n                    unet.forward_with_cond_scale,\n                    images_hat,\n                    sigma_hat,\n                    self_cond = self_cond,\n                    **unet_kwargs\n                )\n\n                denoised_over_sigma = (images_hat - model_output) / sigma_hat\n\n                images_next = images_hat + (sigma_next - sigma_hat) * denoised_over_sigma\n\n                # second order correction, if not the last timestep\n\n                has_second_order_correction = sigma_next != 0\n\n                if has_second_order_correction:\n                    self_cond = model_output if unet.self_cond else None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_465-515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        total_steps = len(sigmas_and_gammas)\n\n        for ind, (sigma, sigma_next, gamma) in tqdm(enumerate(sigmas_and_gammas), total = total_steps, desc = 'sampling time step', disable = not use_tqdm):\n            is_last_timestep = ind == (total_steps - 1)\n\n            sigma, sigma_next, gamma = map(lambda t: t.item(), (sigma, sigma_next, gamma))\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                eps = hp.S_noise * torch.randn(shape, device = self.device) # stochastic sampling\n\n                sigma_hat = sigma + gamma * sigma\n                added_noise = sqrt(sigma_hat ** 2 - sigma ** 2) * eps\n\n                images_hat = images + added_noise\n\n                self_cond = x_start if unet.self_cond else None\n\n                if has_inpainting:\n                    images_hat = images_hat * ~inpaint_masks + (inpaint_images + added_noise) * inpaint_masks\n\n                model_output = self.preconditioned_network_forward(\n                    unet.forward_with_cond_scale,\n                    images_hat,\n                    sigma_hat,\n                    self_cond = self_cond,\n                    **unet_kwargs\n                )\n\n                denoised_over_sigma = (images_hat - model_output) / sigma_hat\n\n                images_next = images_hat + (sigma_next - sigma_hat) * denoised_over_sigma\n\n                # second order correction, if not the last timestep\n\n                has_second_order_correction = sigma_next != 0\n\n                if has_second_order_correction:\n                    self_cond = model_output if unet.self_cond else None\n\n                    model_output_next = self.preconditioned_network_forward(\n                        unet.forward_with_cond_scale,\n                        images_next,\n                        sigma_next,\n                        self_cond = self_cond,\n                        **unet_kwargs\n                    )\n\n                    denoised_prime_over_sigma = (images_next - model_output_next) / sigma_next", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_475-525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                eps = hp.S_noise * torch.randn(shape, device = self.device) # stochastic sampling\n\n                sigma_hat = sigma + gamma * sigma\n                added_noise = sqrt(sigma_hat ** 2 - sigma ** 2) * eps\n\n                images_hat = images + added_noise\n\n                self_cond = x_start if unet.self_cond else None\n\n                if has_inpainting:\n                    images_hat = images_hat * ~inpaint_masks + (inpaint_images + added_noise) * inpaint_masks\n\n                model_output = self.preconditioned_network_forward(\n                    unet.forward_with_cond_scale,\n                    images_hat,\n                    sigma_hat,\n                    self_cond = self_cond,\n                    **unet_kwargs\n                )\n\n                denoised_over_sigma = (images_hat - model_output) / sigma_hat\n\n                images_next = images_hat + (sigma_next - sigma_hat) * denoised_over_sigma\n\n                # second order correction, if not the last timestep\n\n                has_second_order_correction = sigma_next != 0\n\n                if has_second_order_correction:\n                    self_cond = model_output if unet.self_cond else None\n\n                    model_output_next = self.preconditioned_network_forward(\n                        unet.forward_with_cond_scale,\n                        images_next,\n                        sigma_next,\n                        self_cond = self_cond,\n                        **unet_kwargs\n                    )\n\n                    denoised_prime_over_sigma = (images_next - model_output_next) / sigma_next\n                    images_next = images_hat + 0.5 * (sigma_next - sigma_hat) * (denoised_over_sigma + denoised_prime_over_sigma)\n\n                images = images_next\n\n                if has_inpainting and not (is_last_resample_step or is_last_timestep):\n                    # renoise in repaint and then resample\n                    repaint_noise = torch.randn(shape, device = self.device)\n                    images = images + (sigma - sigma_next) * repaint_noise\n\n                x_start = model_output if not has_second_order_correction else model_output_next # save model output for self conditioning", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_485-535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    images_hat = images_hat * ~inpaint_masks + (inpaint_images + added_noise) * inpaint_masks\n\n                model_output = self.preconditioned_network_forward(\n                    unet.forward_with_cond_scale,\n                    images_hat,\n                    sigma_hat,\n                    self_cond = self_cond,\n                    **unet_kwargs\n                )\n\n                denoised_over_sigma = (images_hat - model_output) / sigma_hat\n\n                images_next = images_hat + (sigma_next - sigma_hat) * denoised_over_sigma\n\n                # second order correction, if not the last timestep\n\n                has_second_order_correction = sigma_next != 0\n\n                if has_second_order_correction:\n                    self_cond = model_output if unet.self_cond else None\n\n                    model_output_next = self.preconditioned_network_forward(\n                        unet.forward_with_cond_scale,\n                        images_next,\n                        sigma_next,\n                        self_cond = self_cond,\n                        **unet_kwargs\n                    )\n\n                    denoised_prime_over_sigma = (images_next - model_output_next) / sigma_next\n                    images_next = images_hat + 0.5 * (sigma_next - sigma_hat) * (denoised_over_sigma + denoised_prime_over_sigma)\n\n                images = images_next\n\n                if has_inpainting and not (is_last_resample_step or is_last_timestep):\n                    # renoise in repaint and then resample\n                    repaint_noise = torch.randn(shape, device = self.device)\n                    images = images + (sigma - sigma_next) * repaint_noise\n\n                x_start = model_output if not has_second_order_correction else model_output_next # save model output for self conditioning\n\n        images = images.clamp(-1., 1.)\n\n        if has_inpainting:\n            images = images * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        return self.unnormalize_img(images)\n\n    @torch.no_grad()\n    @eval_decorator", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_495-545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                denoised_over_sigma = (images_hat - model_output) / sigma_hat\n\n                images_next = images_hat + (sigma_next - sigma_hat) * denoised_over_sigma\n\n                # second order correction, if not the last timestep\n\n                has_second_order_correction = sigma_next != 0\n\n                if has_second_order_correction:\n                    self_cond = model_output if unet.self_cond else None\n\n                    model_output_next = self.preconditioned_network_forward(\n                        unet.forward_with_cond_scale,\n                        images_next,\n                        sigma_next,\n                        self_cond = self_cond,\n                        **unet_kwargs\n                    )\n\n                    denoised_prime_over_sigma = (images_next - model_output_next) / sigma_next\n                    images_next = images_hat + 0.5 * (sigma_next - sigma_hat) * (denoised_over_sigma + denoised_prime_over_sigma)\n\n                images = images_next\n\n                if has_inpainting and not (is_last_resample_step or is_last_timestep):\n                    # renoise in repaint and then resample\n                    repaint_noise = torch.randn(shape, device = self.device)\n                    images = images + (sigma - sigma_next) * repaint_noise\n\n                x_start = model_output if not has_second_order_correction else model_output_next # save model output for self conditioning\n\n        images = images.clamp(-1., 1.)\n\n        if has_inpainting:\n            images = images * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        return self.unnormalize_img(images)\n\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_505-555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n                    model_output_next = self.preconditioned_network_forward(\n                        unet.forward_with_cond_scale,\n                        images_next,\n                        sigma_next,\n                        self_cond = self_cond,\n                        **unet_kwargs\n                    )\n\n                    denoised_prime_over_sigma = (images_next - model_output_next) / sigma_next\n                    images_next = images_hat + 0.5 * (sigma_next - sigma_hat) * (denoised_over_sigma + denoised_prime_over_sigma)\n\n                images = images_next\n\n                if has_inpainting and not (is_last_resample_step or is_last_timestep):\n                    # renoise in repaint and then resample\n                    repaint_noise = torch.randn(shape, device = self.device)\n                    images = images + (sigma - sigma_next) * repaint_noise\n\n                x_start = model_output if not has_second_order_correction else model_output_next # save model output for self conditioning\n\n        images = images.clamp(-1., 1.)\n\n        if has_inpainting:\n            images = images * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        return self.unnormalize_img(images)\n\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        video_frames = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_515-565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    images_next = images_hat + 0.5 * (sigma_next - sigma_hat) * (denoised_over_sigma + denoised_prime_over_sigma)\n\n                images = images_next\n\n                if has_inpainting and not (is_last_resample_step or is_last_timestep):\n                    # renoise in repaint and then resample\n                    repaint_noise = torch.randn(shape, device = self.device)\n                    images = images + (sigma - sigma_next) * repaint_noise\n\n                x_start = model_output if not has_second_order_correction else model_output_next # save model output for self conditioning\n\n        images = images.clamp(-1., 1.)\n\n        if has_inpainting:\n            images = images * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        return self.unnormalize_img(images)\n\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        video_frames = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        use_tqdm = True,\n        device = None,\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_525-575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        images = images.clamp(-1., 1.)\n\n        if has_inpainting:\n            images = images * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        return self.unnormalize_img(images)\n\n    @torch.no_grad()\n    @eval_decorator\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        video_frames = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        use_tqdm = True,\n        device = None,\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_535-585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        video_frames = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        use_tqdm = True,\n        device = None,\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_545-595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        sigma_min = None,\n        sigma_max = None,\n        video_frames = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        use_tqdm = True,\n        device = None,\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_555-605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        use_tqdm = True,\n        device = None,\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n        cond_scale = cast_tuple(cond_scale, num_unets)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_565-615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # handle video and frame dimension\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        # determine the frame dimensions, if needed\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        # initializing with an image or video\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_575-625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # handle video and frame dimension\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        # determine the frame dimensions, if needed\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        # initializing with an image or video\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        sigma_min = cast_tuple(sigma_min, num_unets)\n        sigma_max = cast_tuple(sigma_max, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_585-635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # handle video and frame dimension\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        # determine the frame dimensions, if needed\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        # initializing with an image or video\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        sigma_min = cast_tuple(sigma_min, num_unets)\n        sigma_max = cast_tuple(sigma_max, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            img = self.resize_to(start_image_or_video, prev_image_size)\n\n        # go through each unet in cascade\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_595-645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # handle video and frame dimension\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        # determine the frame dimensions, if needed\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        # initializing with an image or video\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        sigma_min = cast_tuple(sigma_min, num_unets)\n        sigma_max = cast_tuple(sigma_max, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            img = self.resize_to(start_image_or_video, prev_image_size)\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, unet_hparam, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps, unet_sigma_min, unet_sigma_max in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.hparams, self.dynamic_thresholding, cond_scale, init_images, skip_steps, sigma_min, sigma_max), disable = not use_tqdm):\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'cannot sample from null unet'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                lowres_cond_img = lowres_noise_times = None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_605-655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # handle video and frame dimension\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        # determine the frame dimensions, if needed\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        # initializing with an image or video\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        sigma_min = cast_tuple(sigma_min, num_unets)\n        sigma_max = cast_tuple(sigma_max, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            img = self.resize_to(start_image_or_video, prev_image_size)\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, unet_hparam, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps, unet_sigma_min, unet_sigma_max in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.hparams, self.dynamic_thresholding, cond_scale, init_images, skip_steps, sigma_min, sigma_max), disable = not use_tqdm):\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'cannot sample from null unet'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                lowres_cond_img = lowres_noise_times = None\n\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict()\n                video_kwargs = dict()\n\n                if self.is_video:\n                    resize_kwargs = dict(target_frames = frame_dims[0])\n\n                    video_kwargs = dict(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_615-665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        sigma_min = cast_tuple(sigma_min, num_unets)\n        sigma_max = cast_tuple(sigma_max, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            img = self.resize_to(start_image_or_video, prev_image_size)\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, unet_hparam, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps, unet_sigma_min, unet_sigma_max in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.hparams, self.dynamic_thresholding, cond_scale, init_images, skip_steps, sigma_min, sigma_max), disable = not use_tqdm):\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'cannot sample from null unet'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                lowres_cond_img = lowres_noise_times = None\n\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict()\n                video_kwargs = dict()\n\n                if self.is_video:\n                    resize_kwargs = dict(target_frames = frame_dims[0])\n\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                # handle video conditioning frames\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_625-675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            img = self.resize_to(start_image_or_video, prev_image_size)\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, unet_hparam, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps, unet_sigma_min, unet_sigma_max in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.hparams, self.dynamic_thresholding, cond_scale, init_images, skip_steps, sigma_min, sigma_max), disable = not use_tqdm):\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'cannot sample from null unet'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                lowres_cond_img = lowres_noise_times = None\n\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict()\n                video_kwargs = dict()\n\n                if self.is_video:\n                    resize_kwargs = dict(target_frames = frame_dims[0])\n\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                # handle video conditioning frames\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_635-685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        for unet_number, unet, channel, image_size, frame_dims, unet_hparam, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps, unet_sigma_min, unet_sigma_max in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.hparams, self.dynamic_thresholding, cond_scale, init_images, skip_steps, sigma_min, sigma_max), disable = not use_tqdm):\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'cannot sample from null unet'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                lowres_cond_img = lowres_noise_times = None\n\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict()\n                video_kwargs = dict()\n\n                if self.is_video:\n                    resize_kwargs = dict(target_frames = frame_dims[0])\n\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                # handle video conditioning frames\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.one_unet_sample(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_645-695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict()\n                video_kwargs = dict()\n\n                if self.is_video:\n                    resize_kwargs = dict(target_frames = frame_dims[0])\n\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                # handle video conditioning frames\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.one_unet_sample(\n                    unet,\n                    shape,\n                    unet_number = unet_number,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_655-705"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                # handle video conditioning frames\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.one_unet_sample(\n                    unet,\n                    shape,\n                    unet_number = unet_number,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    sigma_min = unet_sigma_min,\n                    sigma_max = unet_sigma_max,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_665-715"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.one_unet_sample(\n                    unet,\n                    shape,\n                    unet_number = unet_number,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    sigma_min = unet_sigma_min,\n                    sigma_max = unet_sigma_max,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_675-725"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.one_unet_sample(\n                    unet,\n                    shape,\n                    unet_number = unet_number,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    sigma_min = unet_sigma_min,\n                    sigma_max = unet_sigma_max,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'automatically converting video tensor to video file for saving is not built yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_685-735"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    unet,\n                    shape,\n                    unet_number = unet_number,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    sigma_min = unet_sigma_min,\n                    sigma_max = unet_sigma_max,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'automatically converting video tensor to video file for saving is not built yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    # training\n\n    def loss_weight(self, sigma_data, sigma):\n        return (sigma ** 2 + sigma_data ** 2) * (sigma * sigma_data) ** -2\n\n    def noise_distribution(self, P_mean, P_std, batch_size):\n        return (P_mean + P_std * torch.randn((batch_size,), device = self.device)).exp()\n\n    def forward(\n        self,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_695-745"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    skip_steps = unet_skip_steps,\n                    sigma_min = unet_sigma_min,\n                    sigma_max = unet_sigma_max,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'automatically converting video tensor to video file for saving is not built yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    # training\n\n    def loss_weight(self, sigma_data, sigma):\n        return (sigma ** 2 + sigma_data ** 2) * (sigma * sigma_data) ** -2\n\n    def noise_distribution(self, P_mean, P_std, batch_size):\n        return (P_mean + P_std * torch.randn((batch_size,), device = self.device)).exp()\n\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_705-755"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'automatically converting video tensor to video file for saving is not built yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    # training\n\n    def loss_weight(self, sigma_data, sigma):\n        return (sigma ** 2 + sigma_data ** 2) * (sigma * sigma_data) ** -2\n\n    def noise_distribution(self, P_mean, P_std, batch_size):\n        return (P_mean + P_std * torch.randn((batch_size,), device = self.device)).exp()\n\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_715-765"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'automatically converting video tensor to video file for saving is not built yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    # training\n\n    def loss_weight(self, sigma_data, sigma):\n        return (sigma ** 2 + sigma_data ** 2) * (sigma * sigma_data) ** -2\n\n    def noise_distribution(self, P_mean, P_std, batch_size):\n        return (P_mean + P_std * torch.randn((batch_size,), device = self.device)).exp()\n\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n        \n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        target_image_size    = self.image_sizes[unet_index]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_725-775"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    # training\n\n    def loss_weight(self, sigma_data, sigma):\n        return (sigma ** 2 + sigma_data ** 2) * (sigma * sigma_data) ** -2\n\n    def noise_distribution(self, P_mean, P_std, batch_size):\n        return (P_mean + P_std * torch.randn((batch_size,), device = self.device)).exp()\n\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n        \n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n        hp                   = self.hparams[unet_index]\n\n        batch_size, c, *_, h, w, device, is_video = *images.shape, images.device, (images.ndim == 5)\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_735-785"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n        \n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n        hp                   = self.hparams[unet_index]\n\n        batch_size, c, *_, h, w, device, is_video = *images.shape, images.device, (images.ndim == 5)\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        check_shape(images, 'b c ...', c = self.channels)\n\n        assert h >= target_image_size and w >= target_image_size\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_745-795"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n        \n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n        hp                   = self.hparams[unet_index]\n\n        batch_size, c, *_, h, w, device, is_video = *images.shape, images.device, (images.ndim == 5)\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        check_shape(images, 'b c ...', c = self.channels)\n\n        assert h >= target_image_size and w >= target_image_size\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_755-805"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n        \n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n        hp                   = self.hparams[unet_index]\n\n        batch_size, c, *_, h, w, device, is_video = *images.shape, images.device, (images.ndim == 5)\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        check_shape(images, 'b c ...', c = self.channels)\n\n        assert h >= target_image_size and w >= target_image_size\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video conditioning frames\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_765-815"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n        hp                   = self.hparams[unet_index]\n\n        batch_size, c, *_, h, w, device, is_video = *images.shape, images.device, (images.ndim == 5)\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        check_shape(images, 'b c ...', c = self.channels)\n\n        assert h >= target_image_size and w >= target_image_size\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video conditioning frames\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_775-825"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        check_shape(images, 'b c ...', c = self.channels)\n\n        assert h >= target_image_size and w >= target_image_size\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video conditioning frames\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(batch_size, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = batch_size)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        # normalize to [-1, 1]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_785-835"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video conditioning frames\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(batch_size, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = batch_size)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        # normalize to [-1, 1]\n\n        images = self.normalize_img(images)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            if is_video:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_795-845"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video conditioning frames\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(batch_size, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = batch_size)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        # normalize to [-1, 1]\n\n        images = self.normalize_img(images)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), 'b c f h w -> (b f) c h w')\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            images = aug(images)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), '(b f) c h w -> b c f h w', f = frames)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_805-855"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(batch_size, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = batch_size)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        # normalize to [-1, 1]\n\n        images = self.normalize_img(images)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), 'b c f h w -> (b f) c h w')\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            images = aug(images)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), '(b f) c h w -> b c f h w', f = frames)\n\n        # noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # get the sigmas\n\n        sigmas = self.noise_distribution(hp.P_mean, hp.P_std, batch_size)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_815-865"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(batch_size, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = batch_size)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        # normalize to [-1, 1]\n\n        images = self.normalize_img(images)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), 'b c f h w -> (b f) c h w')\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            images = aug(images)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), '(b f) c h w -> b c f h w', f = frames)\n\n        # noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # get the sigmas\n\n        sigmas = self.noise_distribution(hp.P_mean, hp.P_std, batch_size)\n        padded_sigmas = self.right_pad_dims_to_datatype(sigmas)\n\n        # noise\n\n        noise = torch.randn_like(images)\n        noised_images = images + padded_sigmas * noise  # alphas are 1. in the paper\n\n        # unet kwargs\n\n        unet_kwargs = dict(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_825-875"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        images = self.normalize_img(images)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), 'b c f h w -> (b f) c h w')\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            images = aug(images)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), '(b f) c h w -> b c f h w', f = frames)\n\n        # noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # get the sigmas\n\n        sigmas = self.noise_distribution(hp.P_mean, hp.P_std, batch_size)\n        padded_sigmas = self.right_pad_dims_to_datatype(sigmas)\n\n        # noise\n\n        noise = torch.randn_like(images)\n        noised_images = images + padded_sigmas * noise  # alphas are 1. in the paper\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            text_embeds = text_embeds,\n            text_mask = text_masks,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_835-885"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), 'b c f h w -> (b f) c h w')\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            images = aug(images)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n\n            if is_video:\n                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), '(b f) c h w -> b c f h w', f = frames)\n\n        # noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # get the sigmas\n\n        sigmas = self.noise_distribution(hp.P_mean, hp.P_std, batch_size)\n        padded_sigmas = self.right_pad_dims_to_datatype(sigmas)\n\n        # noise\n\n        noise = torch.randn_like(images)\n        noised_images = images + padded_sigmas * noise  # alphas are 1. in the paper\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            text_embeds = text_embeds,\n            text_mask = text_masks,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self conditioning - https://arxiv.org/abs/2208.04202 - training will be 25% slower\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred_x0 = self.preconditioned_network_forward(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_845-895"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # get the sigmas\n\n        sigmas = self.noise_distribution(hp.P_mean, hp.P_std, batch_size)\n        padded_sigmas = self.right_pad_dims_to_datatype(sigmas)\n\n        # noise\n\n        noise = torch.randn_like(images)\n        noised_images = images + padded_sigmas * noise  # alphas are 1. in the paper\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            text_embeds = text_embeds,\n            text_mask = text_masks,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self conditioning - https://arxiv.org/abs/2208.04202 - training will be 25% slower\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred_x0 = self.preconditioned_network_forward(\n                    unet.forward,\n                    noised_images,\n                    sigmas,\n                    **unet_kwargs\n                ).detach()\n\n            unet_kwargs = {**unet_kwargs, 'self_cond': pred_x0}\n\n        # get prediction\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_855-905"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        padded_sigmas = self.right_pad_dims_to_datatype(sigmas)\n\n        # noise\n\n        noise = torch.randn_like(images)\n        noised_images = images + padded_sigmas * noise  # alphas are 1. in the paper\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            sigma_data = hp.sigma_data,\n            text_embeds = text_embeds,\n            text_mask = text_masks,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self conditioning - https://arxiv.org/abs/2208.04202 - training will be 25% slower\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred_x0 = self.preconditioned_network_forward(\n                    unet.forward,\n                    noised_images,\n                    sigmas,\n                    **unet_kwargs\n                ).detach()\n\n            unet_kwargs = {**unet_kwargs, 'self_cond': pred_x0}\n\n        # get prediction\n\n        denoised_images = self.preconditioned_network_forward(\n            unet.forward,\n            noised_images,\n            sigmas,\n            **unet_kwargs\n        )\n\n        # losses\n\n        losses = F.mse_loss(denoised_images, images, reduction = 'none')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_865-915"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 924, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            sigma_data = hp.sigma_data,\n            text_embeds = text_embeds,\n            text_mask = text_masks,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self conditioning - https://arxiv.org/abs/2208.04202 - training will be 25% slower\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred_x0 = self.preconditioned_network_forward(\n                    unet.forward,\n                    noised_images,\n                    sigmas,\n                    **unet_kwargs\n                ).detach()\n\n            unet_kwargs = {**unet_kwargs, 'self_cond': pred_x0}\n\n        # get prediction\n\n        denoised_images = self.preconditioned_network_forward(\n            unet.forward,\n            noised_images,\n            sigmas,\n            **unet_kwargs\n        )\n\n        # losses\n\n        losses = F.mse_loss(denoised_images, images, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # loss weighting\n\n        losses = losses * self.loss_weight(hp.sigma_data, sigmas)\n\n        # return average loss\n\n        return losses.mean()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_875-924"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 924, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # self conditioning - https://arxiv.org/abs/2208.04202 - training will be 25% slower\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred_x0 = self.preconditioned_network_forward(\n                    unet.forward,\n                    noised_images,\n                    sigmas,\n                    **unet_kwargs\n                ).detach()\n\n            unet_kwargs = {**unet_kwargs, 'self_cond': pred_x0}\n\n        # get prediction\n\n        denoised_images = self.preconditioned_network_forward(\n            unet.forward,\n            noised_images,\n            sigmas,\n            **unet_kwargs\n        )\n\n        # losses\n\n        losses = F.mse_loss(denoised_images, images, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # loss weighting\n\n        losses = losses * self.loss_weight(hp.sigma_data, sigmas)\n\n        # return average loss\n\n        return losses.mean()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_885-924"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "elucidated_imagen.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 924, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    unet.forward,\n                    noised_images,\n                    sigmas,\n                    **unet_kwargs\n                ).detach()\n\n            unet_kwargs = {**unet_kwargs, 'self_cond': pred_x0}\n\n        # get prediction\n\n        denoised_images = self.preconditioned_network_forward(\n            unet.forward,\n            noised_images,\n            sigmas,\n            **unet_kwargs\n        )\n\n        # losses\n\n        losses = F.mse_loss(denoised_images, images, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # loss weighting\n\n        losses = losses * self.loss_weight(hp.sigma_data, sigmas)\n\n        # return average loss\n\n        return losses.mean()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-elucidated_imagen.py_895-924"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import math\nimport copy\nfrom random import random\nfrom beartype.typing import List, Union\nfrom beartype import beartype\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch import nn, einsum\nfrom torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n\nAST=Module(ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad)))FunctionDef(arguments(arg)FunctionDef(arguments(arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Name(Load)))Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Constant)))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from torch.cuda.amp import autocast\nfrom torch.special import expm1\nimport torchvision.transforms as T\n\nimport kornia.augmentation as K\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad)))FunctionDef(arguments(arg)FunctionDef(arguments(arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Name(Load)))Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Constant)FunctionDef(arguments(arg)NonlocalIf(Name(Load)Return)Assign(Name(Store)Constant)Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\nfrom imagen_pytorch.imagen_video import Unet3D, resize_video_to, scale_video_time\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n\nAST=Module(ImportFrom(aliasaliasalias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad)))FunctionDef(arguments(arg)FunctionDef(arguments(arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Name(Load)))Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Constant)FunctionDef(arguments(arg)NonlocalIf(Name(Load)Return)Assign(Name(Store)Constant)Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return t\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\n\nAST=Module(FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))Assign(Name(Store)IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)BinOp(Tuple(Name(Load)Load)MultCall(Name(Load)Name(Load)Constant))))If(Call(Name(Load)Name(Load))Assert(Compare(Call(Name(Load)Name(Load))EqName(Load))))Return(Name(Load)))FunctionDef(arguments(arg)Return(DictComp(Name(Load)Name(Load)comprehension(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load))Call(Name(Load)Name(Load))))))FunctionDef(arguments(argargarg)If(Compare(Name(Load)NotInName(Load))Return(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load)Subscript(Name(Load)Name(Load)Load)))Return(Name(Load)))FunctionDef(arguments(arg)If(UnaryOp(NotCompare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load)))Return(Name(Load)))Return(BinOp(Name(Load)DivConstant)))FunctionDef(arguments(arg)Return(Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))If(Call(Name(Load)Attribute(Name(Load)Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))))FunctionDef(arguments(arg)FunctionDef(arguments(argargarg)Assign(Name(Store)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))Return(Name(Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef compact(input_dict):\n    return {key: value for key, value in input_dict.items() if exists(value)}\n\ndef maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def maybe_transform_dict_key(input_dict, key, fn):\n    if key not in input_dict:\n        return input_dict\n\n    copied_dict = input_dict.copy()\n    copied_dict[key] = fn(copied_dict[key])\n    return copied_dict\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\n\nAST=Module(FunctionDef(arguments(argargarg)If(Compare(Name(Load)NotInName(Load))Return(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load)Subscript(Name(Load)Name(Load)Load)))Return(Name(Load)))FunctionDef(arguments(arg)If(UnaryOp(NotCompare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load)))Return(Name(Load)))Return(BinOp(Name(Load)DivConstant)))FunctionDef(arguments(arg)Return(Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))If(Call(Name(Load)Attribute(Name(Load)Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))))FunctionDef(arguments(arg)FunctionDef(arguments(argargarg)Assign(Name(Store)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))Return(Name(Load)))FunctionDef(arguments(argargargConstant)Assign(Name(Store)BinOp(Name(Load)SubCall(Name(Load)Name(Load))))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Tuple(Starred(Name(Load)Load)Starred(BinOp(Tuple(Name(Load)Load)MultName(Load))Load)Load)))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load))))FunctionDef(arguments(argargargarg)Return(Name(Load))))FunctionDef(arguments(argarg(Name(Load))Constant)Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Name(Load)))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_95-145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_105-155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n\nAST=Module(FunctionDef(arguments(argargargConstant)Assign(Name(Store)BinOp(Name(Load)SubCall(Name(Load)Name(Load))))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Tuple(Starred(Name(Load)Load)Starred(BinOp(Tuple(Name(Load)Load)MultName(Load))Load)Load)))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load))))FunctionDef(arguments(argargargarg)Return(Name(Load))))FunctionDef(arguments(argarg(Name(Load))Constant)Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Name(Load))))))FunctionDef(arguments(arg)Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)SubAttribute(Name(Load)Load)))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Call(Attribute(Name(Load)Load)Starred(Attribute(Name(Load)Load)Load)Starred(BinOp(Tuple(ConstantLoad)MultName(Load))Load))))FunctionDef(arguments(argargargConstant)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)keyword(Name(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)UnaryOp(InvertName(Load))Constant))Return(BinOp(Call(Attribute(Name(Load)Load)keyword(Name(Load)))DivCall(Attribute(Name(Load)Load)keyword(Constant)))))FunctionDef(arguments(argargargargConstantConstant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))If(Compare(Name(Load)EqName(Load))Return(Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_115-165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_125-175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n\nAST=Module(FunctionDef(arguments(arg)Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)SubAttribute(Name(Load)Load)))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Call(Attribute(Name(Load)Load)Starred(Attribute(Name(Load)Load)Load)Starred(BinOp(Tuple(ConstantLoad)MultName(Load))Load))))FunctionDef(arguments(argargargConstant)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)keyword(Name(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)UnaryOp(InvertName(Load))Constant))Return(BinOp(Call(Attribute(Name(Load)Load)keyword(Name(Load)))DivCall(Attribute(Name(Load)Load)keyword(Constant)))))FunctionDef(arguments(argargargargConstantConstant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))If(Compare(Name(Load)EqName(Load))Return(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Name(Load))))If(Call(Name(Load)Name(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Starred(Name(Load)Load))))Return(Name(Load)))FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(BinOp(Tuple(Call(Name(Load))Load)MultCall(Name(Load)Name(Load)))))Assign(Name(Store)List(Load))For(Name(Store)Name(Load)Assert(Call(Name(Load)Name(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Tuple(BinOp(Name(Load)FloorDivName(Load))Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_135-185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_image_to(\n    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_145-195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    image,\n    target_image_size,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_image_size = image.shape[-1]\n\n    if orig_image_size == target_image_size:\n        return image\n\n    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_155-205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    out = F.interpolate(image, target_image_size, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n\n    return out\n\ndef calc_all_frame_dims(\n    downsample_factors: List[int],\n    frames\n):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_165-215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "):\n    if not exists(frames):\n        return (tuple(),) * len(downsample_factors)\n\n    all_frame_dims = []\n\n    for divisor in downsample_factors:\n        assert divisible_by(frames, divisor)\n        all_frame_dims.append((frames // divisor,))\n\n    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_175-225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return all_frame_dims\n\ndef safe_get_tuple_index(tup, index, default = None):\n    if len(tup) <= index:\n        return default\n    return tup[index]\n\n# image normalization functions\n# ddpms expect images to be in the range of -1 to 1\n\ndef normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_185-235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def normalize_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_zero_to_one(normed_img):\n    return (normed_img + 1) * 0.5\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_195-245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# gaussian diffusion with continuous time helper functions and classes\n# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n\n@torch.jit.script\ndef beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_205-255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def beta_linear_log_snr(t):\n    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n\n@torch.jit.script\ndef alpha_cosine_log_snr(t, s: float = 0.008):\n    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n\ndef log_snr_to_alpha_sigma(log_snr):\n    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n\nclass GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n\nAST=Module(FunctionDef(arguments(arg)Return(UnaryOp(USubCall(Attribute(Name(Load)Load)Call(Name(Load)BinOp(ConstantAddBinOp(ConstantMultBinOp(Name(Load)PowConstant))))))))FunctionDef(arguments(argarg(Name(Load))Constant)Return(UnaryOp(USubCall(Name(Load)BinOp(BinOp(Call(Attribute(Name(Load)Load)BinOp(BinOp(BinOp(BinOp(Name(Load)AddName(Load))DivBinOp(ConstantAddName(Load)))MultAttribute(Name(Load)Load))MultConstant))PowUnaryOp(USubConstant))SubConstant)keyword(Constant))))Attribute(Attribute(Name(Load)Load)Load))FunctionDef(arguments(arg)Return(Tuple(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)))Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)UnaryOp(USubName(Load))))Load)))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Attribute(Name(Load)Store)Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load)))))))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argargargarg)Return(Call(Attribute(Name(Load)Load)Tuple(Name(Load)Load)Name(Load)keyword(Name(Load))keyword(Attribute(Name(Load)Load)))))FunctionDef(arguments(argargarg)Return(Call(Attribute(Call(Attribute(Call(Attribute(Name(Load)Load)Tuple(Name(Load)Load)keyword(Name(Load)))Load))Load)ConstantConstant)))FunctionDef(arguments(argarg)Return(Call(Call(Name(Load)Attribute(Name(Load)Load))Name(Load))))FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstantBinOp(Attribute(Name(Load)Load)AddConstant)keyword(Name(Load))))Assign(Name(Store)Call(Name(Load)Name(Load)Constantkeyword(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Subscript(Name(Load)Tuple(SliceSlice(UnaryOp(USubConstant))Load)Load)Subscript(Name(Load)Tuple(SliceSlice(Constant)Load)Load)Load)keyword(Constant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(UnaryOp(USubConstant))))Return(Name(Load)))FunctionDef(arguments(argargargargargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Lambda(argumentsCall(Attribute(BinOp(Name(Load)SubBinOp(ConstantDivAttribute(Name(Load)Load)))Load)keyword(Constant)))))Expr(Constant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Tuple(Name(Load)Name(Load)Load)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_215-265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class GaussianDiffusionContinuousTimes(nn.Module):\n    def __init__(self, *, noise_schedule, timesteps = 1000):\n        super().__init__()\n\n        if noise_schedule == \"linear\":\n            self.log_snr = beta_linear_log_snr\n        elif noise_schedule == \"cosine\":\n            self.log_snr = alpha_cosine_log_snr\n        else:\n            raise ValueError(f'invalid noise schedule {noise_schedule}')\n\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_225-275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.num_timesteps = timesteps\n\n    def get_times(self, batch_size, noise_level, *, device):\n        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n\n    def sample_random_times(self, batch_size, *, device):\n        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n\n    def get_condition(self, times):\n        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_235-285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return maybe(self.log_snr)(times)\n\n    def get_sampling_timesteps(self, batch, *, device):\n        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n        times = repeat(times, 't -> b t', b = batch)\n        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n        times = times.unbind(dim = -1)\n        return times\n\n    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_245-295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n\n        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n        log_snr = self.log_snr(t)\n        log_snr_next = self.log_snr(t_next)\n        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n\n        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_255-305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # c - as defined near eq 33\n        c = -expm1(log_snr - log_snr_next)\n        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n\n        # following (eq. 33)\n        posterior_variance = (sigma_next ** 2) * c\n        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def q_sample(self, x_start, t, noise = None):\n        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_265-315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dtype = x_start.dtype\n\n        if isinstance(t, float):\n            batch = x_start.shape[0]\n            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        log_snr = self.log_snr(t).type(dtype)\n        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_275-325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n\n    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n        batch = shape[0]\n\n        if isinstance(from_t, float):\n            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n\n        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_285-335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if isinstance(to_t, float):\n            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n\n        noise = default(noise, lambda: torch.randn_like(x_from))\n\n        log_snr = self.log_snr(from_t)\n        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n\n        log_snr_to = self.log_snr(to_t)\n        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_295-345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n\n        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n\n    def predict_start_from_v(self, x_t, t, v):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return alpha * x_t - sigma * v\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_305-355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def predict_start_from_noise(self, x_t, t, noise):\n        log_snr = self.log_snr(t)\n        log_snr = right_pad_dims_to(x_t, log_snr)\n        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_315-365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(self, feats, stable = False, dim = -1):\n        super().__init__()\n        self.stable = stable\n        self.dim = dim\n\n        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n\n    def forward(self, x):\n        dtype, dim = x.dtype, self.dim\n\n        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_325-375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if self.stable:\n            x = x / x.amax(dim = dim, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = dim, keepdim = True)\n\n        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n\nChanLayerNorm = partial(LayerNorm, dim = -3)\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_335-385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\nAST=Module(ClassDef(FunctionDef(arguments(argarg)Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argargarg)Return(Attribute(Name(Load)Load))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argargarg)Return(BinOp(Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load)))AddName(Load)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Name(Store)ListComp(Call(Name(Load)Name(Load))comprehension(Name(Store)Attribute(Name(Load)Load))))Return(Call(Name(Load)Name(Load)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Name(Store)BinOp(Name(Load)MultName(Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)MultConstant)keyword(Constant)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_345-395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_355-405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_365-415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_375-425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_385-435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_395-445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_405-455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_415-465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_425-475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_435-485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_445-495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_455-505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_465-515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_475-525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_485-535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        context_dim = None,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_495-545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_505-555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_515-565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n    def forward(self, x, context = None, mask = None, attn_bias = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_525-575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_535-585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_545-595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if exists(attn_bias):\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_555-605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_565-615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_575-625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\n\nAST=Module(FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)Name(Load)Name(Load)Constantkeyword(Constant)))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)MultConstant)Constant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load)Constant)))Expr(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)FloorDivConstant)Name(Load)Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultConstant)Name(Load)Constant)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_585-635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass PixelShuffleUpsample(nn.Module):\n    \"\"\"\n    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    \"\"\"\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Expr(Constant)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)MultConstant)Constant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load)Constant)))Expr(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)FloorDivConstant)Name(Load)Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultConstant)Name(Load)Constant))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)FloorDivConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Constant)DivBinOp(Name(Load)SubConstant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))MultUnaryOp(USubName(Load)))))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)Constant)MultCall(Name(Load)Name(Load)Constant))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_595-645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU(),\n            nn.PixelShuffle(2)\n        )\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_605-655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        o, i, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        return self.net(x)\n\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_615-665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef Downsample(dim, dim_out = None):\n    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n    # named SP-conv in the paper, but basically a pixel unshuffle\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (h s1) (w s2) -> b (c s1 s2) h w', s1 = 2, s2 = 2),\n        nn.Conv2d(dim * 4, dim_out, 1)\n    )\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n\nAST=Module(FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Constant)keyword(Constant))Call(Attribute(Name(Load)Load)BinOp(Name(Load)MultConstant)Name(Load)Constant))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)FloorDivConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Constant)DivBinOp(Name(Load)SubConstant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))MultUnaryOp(USubName(Load)))))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)Constant)MultCall(Name(Load)Name(Load)Constant)))Return(Call(Attribute(Name(Load)Load)Tuple(Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load))Load)keyword(UnaryOp(USubConstant))))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)Expr(Constant)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Compare(BinOp(Name(Load)ModConstant)EqConstant))Assign(Name(Store)BinOp(Name(Load)FloorDivConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)BinOp(BinOp(BinOp(Name(Load)MultCall(Name(Load)Attribute(Name(Load)Load)Constant))MultConstant)MultAttribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load))Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Load)keyword(UnaryOp(USubConstant))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)IfExp(Name(Load)Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Call(Name(Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_625-675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)FloorDivConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Constant)DivBinOp(Name(Load)SubConstant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))MultUnaryOp(USubName(Load)))))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)Constant)MultCall(Name(Load)Name(Load)Constant)))Return(Call(Attribute(Name(Load)Load)Tuple(Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load))Load)keyword(UnaryOp(USubConstant))))))ClassDef(Attribute(Name(Load)Load)Expr(Constant)Expr(Constant)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Compare(BinOp(Name(Load)ModConstant)EqConstant))Assign(Name(Store)BinOp(Name(Load)FloorDivConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)BinOp(BinOp(BinOp(Name(Load)MultCall(Name(Load)Attribute(Name(Load)Load)Constant))MultConstant)MultAttribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load))Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Load)keyword(UnaryOp(USubConstant))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)IfExp(Name(Load)Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Call(Name(Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)Constantkeyword(Constant))))FunctionDef(arguments(argargargConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Call(Name(Load)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Name(Load))Assign(Name(Store)BinOp(BinOp(Name(Load)MultBinOp(Name(Load)AddConstant))AddName(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_635-685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_645-695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_655-705"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_665-715"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.project = nn.Conv2d(dim, dim_out, 3, padding = 1)\n\n    def forward(self, x, scale_shift = None):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_675-725"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return self.project(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_685-735"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_695-745"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_705-755"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            self.cross_attn = attn_klass(\n                dim = dim_out,\n                context_dim = cond_dim,\n                **attn_kwargs\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_715-765"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(self, x, time_emb = None, cond = None):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_725-775"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = rearrange(h, 'b c h w -> b h w c')\n            h, ps = pack([h], 'b * c')\n            h = self.cross_attn(h, context = cond) + h\n            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_735-785"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            h, = unpack(h, ps, 'b * c')\n            h = rearrange(h, 'b h w c -> b c h w')\n\n        h = self.block2(h, scale_shift = scale_shift)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_745-795"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_755-805"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_765-815"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_775-825"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_785-835"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_795-845"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # cosine sim attention\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_805-855"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_815-865"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        attn = attn.to(sim.dtype)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_825-875"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_835-885"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_845-895"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_855-905"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_865-915"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 925, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_875-925"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 935, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_885-935"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 945, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Conv2d(dim, inner_dim, 1, bias = False),\n            nn.Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_895-945"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 930, "start_line_no": 905, "end_line_no": 955, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_905-955"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 940, "start_line_no": 915, "end_line_no": 965, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_915-965"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 950, "start_line_no": 925, "end_line_no": 975, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_925-975"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 960, "start_line_no": 935, "end_line_no": 985, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_935-985"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 970, "start_line_no": 945, "end_line_no": 995, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = nn.Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_945-995"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 980, "start_line_no": 955, "end_line_no": 1005, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            nn.SiLU(),\n            nn.Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_955-1005"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 990, "start_line_no": 965, "end_line_no": 1015, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_965-1015"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1000, "start_line_no": 975, "end_line_no": 1025, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        nn.Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        nn.Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_975-1025"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1010, "start_line_no": 985, "end_line_no": 1035, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_985-1035"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1020, "start_line_no": 995, "end_line_no": 1045, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_995-1045"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1030, "start_line_no": 1005, "end_line_no": 1055, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            ]))\n\n    def forward(self, x, context = None):\n        x = rearrange(x, 'b c h w -> b h w c')\n        x, ps = pack([x], 'b * c')\n\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n\n        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1005-1055"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1040, "start_line_no": 1015, "end_line_no": 1065, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        x, = unpack(x, ps, 'b * c')\n        x = rearrange(x, 'b h w c -> b c h w')\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1015-1065"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1050, "start_line_no": 1025, "end_line_no": 1075, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1025-1075"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1060, "start_line_no": 1035, "end_line_no": 1085, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1035-1085"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1070, "start_line_no": 1045, "end_line_no": 1095, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Call(Name(Load)List(Starred(Call(Name(Load)Lambda(arguments(arg)Compare(BinOp(Name(Load)ModConstant)EqBinOp(Name(Load)ModConstant)))Name(Load))Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)ListComp(Call(Name(Load)BinOp(Name(Load)DivBinOp(ConstantPowName(Load))))comprehension(Name(Store)Call(Name(Load)ConstantName(Load)))))Assign(Name(Store)List(Starred(Name(Load)Load)BinOp(Name(Load)SubCall(Name(Load)Name(Load)))Load))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Load)))For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)Name(Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)Name(Load)keyword(Name(Load))keyword(BinOp(BinOp(Name(Load)SubName(Load))FloorDivConstant)))))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Call(Name(Load)Name(Load)))Attribute(Name(Load)Load))))Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantCall(Name(Load))Call(Name(Load)))Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Call(Name(Load)Name(Load))))Assert(Compare(Call(Name(Load)Name(Load))EqCall(Name(Load)Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))If(UnaryOp(NotAttribute(Name(Load)Load))Assign(Attribute(Name(Load)Store)Name(Load))Return)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ListComp(Call(Name(Load)Name(Load)Name(Load))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)Name(Load))))))Assign(Attribute(Name(Load)Store)BinOp(Name(Load)AddIfExp(Compare(Call(Name(Load)Name(Load))GtConstant)Call(Name(Load)Name(Load))Constant))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1045-1095"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1080, "start_line_no": 1055, "end_line_no": 1105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1055-1105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1090, "start_line_no": 1065, "end_line_no": 1115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(nn.Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1065-1115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1100, "start_line_no": 1075, "end_line_no": 1125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1075-1125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1110, "start_line_no": 1085, "end_line_no": 1135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = True,\n        layer_attns_depth = 1,\n        layer_mid_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1085-1135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1120, "start_line_no": 1095, "end_line_no": 1145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = True,\n        layer_attns_depth = 1,\n        layer_mid_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1095-1145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1130, "start_line_no": 1105, "end_line_no": 1155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return torch.cat((x, *outs), dim = 1)\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = True,\n        layer_attns_depth = 1,\n        layer_mid_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        resize_mode = 'nearest',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1105-1155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1140, "start_line_no": 1115, "end_line_no": 1165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = True,\n        layer_attns_depth = 1,\n        layer_mid_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        resize_mode = 'nearest',\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1115-1165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1150, "start_line_no": 1125, "end_line_no": 1175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = True,\n        layer_attns_depth = 1,\n        layer_mid_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        resize_mode = 'nearest',\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        # determine dimensions\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1125-1175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1160, "start_line_no": 1135, "end_line_no": 1185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        resize_mode = 'nearest',\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        self.self_cond = self_cond\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1135-1185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1170, "start_line_no": 1145, "end_line_no": 1195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        resize_mode = 'nearest',\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        self.self_cond = self_cond\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1145-1195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1180, "start_line_no": 1155, "end_line_no": 1205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        self.self_cond = self_cond\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1155-1205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1190, "start_line_no": 1165, "end_line_no": 1215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        self.self_cond = self_cond\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1165-1215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1200, "start_line_no": 1175, "end_line_no": 1225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        self.self_cond = self_cond\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1175-1225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1210, "start_line_no": 1185, "end_line_no": 1235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1185-1235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1220, "start_line_no": 1195, "end_line_no": 1245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1195-1245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1230, "start_line_no": 1205, "end_line_no": 1255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1205-1255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1240, "start_line_no": 1215, "end_line_no": 1265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1215-1265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1250, "start_line_no": 1225, "end_line_no": 1275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1225-1275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1260, "start_line_no": 1235, "end_line_no": 1285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1235-1285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1270, "start_line_no": 1245, "end_line_no": 1295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1245-1295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1280, "start_line_no": 1255, "end_line_no": 1305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1255-1305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1290, "start_line_no": 1265, "end_line_no": 1315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1265-1315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1300, "start_line_no": 1275, "end_line_no": 1325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1275-1325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1310, "start_line_no": 1285, "end_line_no": 1335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1285-1335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1320, "start_line_no": 1295, "end_line_no": 1345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1295-1345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1330, "start_line_no": 1305, "end_line_no": 1355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1305-1355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1340, "start_line_no": 1315, "end_line_no": 1365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1315-1365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1350, "start_line_no": 1325, "end_line_no": 1375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1325-1375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1360, "start_line_no": 1335, "end_line_no": 1385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1335-1385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1370, "start_line_no": 1345, "end_line_no": 1395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                transformer_block_klass = Identity\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1345-1395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1380, "start_line_no": 1355, "end_line_no": 1405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1355-1405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1390, "start_line_no": 1365, "end_line_no": 1415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1365-1415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1400, "start_line_no": 1375, "end_line_no": 1425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = TransformerBlock(mid_dim, depth = layer_mid_attns_depth, **attn_kwargs) if attend_at_middle else None\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1375-1425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1410, "start_line_no": 1385, "end_line_no": 1435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            if layer_attn:\n                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1385-1435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1420, "start_line_no": 1395, "end_line_no": 1445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                transformer_block_klass = TransformerBlock\n            elif layer_use_linear_attn:\n                transformer_block_klass = LinearAttentionTransformerBlock\n            else:\n                transformer_block_klass = Identity\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1395-1445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1430, "start_line_no": 1405, "end_line_no": 1455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1405-1455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1440, "start_line_no": 1415, "end_line_no": 1465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1415-1465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1450, "start_line_no": 1425, "end_line_no": 1475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = nn.Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1425-1475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1460, "start_line_no": 1435, "end_line_no": 1485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1435-1485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1470, "start_line_no": 1445, "end_line_no": 1495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1445-1495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1480, "start_line_no": 1455, "end_line_no": 1505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1455-1505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1490, "start_line_no": 1465, "end_line_no": 1515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1465-1515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1500, "start_line_no": 1475, "end_line_no": 1525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1475-1525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1510, "start_line_no": 1485, "end_line_no": 1535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        self_cond = None,\n        cond_drop_prob = 0.\n    ):\n        batch_size, device = x.shape[0], x.device", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1485-1535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1520, "start_line_no": 1495, "end_line_no": 1545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        self_cond = None,\n        cond_drop_prob = 0.\n    ):\n        batch_size, device = x.shape[0], x.device\n\n        # condition on self\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1495-1545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1530, "start_line_no": 1505, "end_line_no": 1555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        self_cond = None,\n        cond_drop_prob = 0.\n    ):\n        batch_size, device = x.shape[0], x.device\n\n        # condition on self\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1505-1555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1540, "start_line_no": 1515, "end_line_no": 1565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        self_cond = None,\n        cond_drop_prob = 0.\n    ):\n        batch_size, device = x.shape[0], x.device\n\n        # condition on self\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        # init conv residual\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1515-1565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1550, "start_line_no": 1525, "end_line_no": 1575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        self_cond = None,\n        cond_drop_prob = 0.\n    ):\n        batch_size, device = x.shape[0], x.device\n\n        # condition on self\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1525-1575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1560, "start_line_no": 1535, "end_line_no": 1585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # condition on self\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1535-1585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1570, "start_line_no": 1545, "end_line_no": 1595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1545-1595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1580, "start_line_no": 1555, "end_line_no": 1605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n            cond_images = resize_image_to(cond_images, x.shape[-1], mode = self.resize_mode)\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1555-1605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1590, "start_line_no": 1565, "end_line_no": 1615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n\n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1565-1615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1600, "start_line_no": 1575, "end_line_no": 1625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n\n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1575-1625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1610, "start_line_no": 1585, "end_line_no": 1635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n\n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1585-1635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1620, "start_line_no": 1595, "end_line_no": 1645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n\n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1595-1645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1630, "start_line_no": 1605, "end_line_no": 1655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            text_tokens = text_tokens[:, :self.max_text_len]\n\n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1605-1655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1640, "start_line_no": 1615, "end_line_no": 1665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t)\n\n        # go through the layers of the unet, down and up", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1615-1665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1650, "start_line_no": 1625, "end_line_no": 1675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1625-1675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1660, "start_line_no": 1635, "end_line_no": 1685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n            hiddens.append(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1635-1685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1670, "start_line_no": 1645, "end_line_no": 1695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n            hiddens.append(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        x = self.mid_block2(x, t, c)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1645-1695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1680, "start_line_no": 1655, "end_line_no": 1705, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n            hiddens.append(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        x = self.mid_block2(x, t, c)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n            x = add_skip_connection(x)\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t)\n\n            x = attn_block(x, c)\n            up_hiddens.append(x.contiguous())", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1655-1705"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1690, "start_line_no": 1665, "end_line_no": 1715, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n            hiddens.append(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        x = self.mid_block2(x, t, c)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n            x = add_skip_connection(x)\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t)\n\n            x = attn_block(x, c)\n            up_hiddens.append(x.contiguous())\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1665-1715"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1700, "start_line_no": 1675, "end_line_no": 1725, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                x = resnet_block(x, t)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n            hiddens.append(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        x = self.mid_block2(x, t, c)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n            x = add_skip_connection(x)\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t)\n\n            x = attn_block(x, c)\n            up_hiddens.append(x.contiguous())\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        return self.final_conv(x)\n\n# null unet", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1675-1725"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1710, "start_line_no": 1685, "end_line_no": 1735, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        x = self.mid_block2(x, t, c)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n            x = add_skip_connection(x)\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t)\n\n            x = attn_block(x, c)\n            up_hiddens.append(x.contiguous())\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        return self.final_conv(x)\n\n# null unet\n\nclass NullUnet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.lowres_cond = False\n        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n\n    def cast_model_parameters(self, *args, **kwargs):\n        return self\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1685-1735"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1720, "start_line_no": 1695, "end_line_no": 1745, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n            x = add_skip_connection(x)\n            x = init_block(x, t, c)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t)\n\n            x = attn_block(x, c)\n            up_hiddens.append(x.contiguous())\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        return self.final_conv(x)\n\n# null unet\n\nclass NullUnet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.lowres_cond = False\n        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n\n    def cast_model_parameters(self, *args, **kwargs):\n        return self\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# predefined unets, with configs lining up with hyperparameters in appendix of paper\n\nclass BaseUnet64(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 512,\n            dim_mults = (1, 2, 3, 4),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1695-1745"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1730, "start_line_no": 1705, "end_line_no": 1755, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        return self.final_conv(x)\n\n# null unet\n\nclass NullUnet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.lowres_cond = False\n        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n\n    def cast_model_parameters(self, *args, **kwargs):\n        return self\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# predefined unets, with configs lining up with hyperparameters in appendix of paper\n\nclass BaseUnet64(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 512,\n            dim_mults = (1, 2, 3, 4),\n            num_resnet_blocks = 3,\n            layer_attns = (False, True, True, True),\n            layer_cross_attns = (False, True, True, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = False\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet256(Unet):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1705-1755"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1740, "start_line_no": 1715, "end_line_no": 1765, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        return self.final_conv(x)\n\n# null unet\n\nclass NullUnet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.lowres_cond = False\n        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n\n    def cast_model_parameters(self, *args, **kwargs):\n        return self\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# predefined unets, with configs lining up with hyperparameters in appendix of paper\n\nclass BaseUnet64(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 512,\n            dim_mults = (1, 2, 3, 4),\n            num_resnet_blocks = 3,\n            layer_attns = (False, True, True, True),\n            layer_cross_attns = (False, True, True, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = False\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet256(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = (False, False, False, True),\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1715-1765"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1750, "start_line_no": 1725, "end_line_no": 1775, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass NullUnet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.lowres_cond = False\n        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n\n    def cast_model_parameters(self, *args, **kwargs):\n        return self\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# predefined unets, with configs lining up with hyperparameters in appendix of paper\n\nclass BaseUnet64(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 512,\n            dim_mults = (1, 2, 3, 4),\n            num_resnet_blocks = 3,\n            layer_attns = (False, True, True, True),\n            layer_cross_attns = (False, True, True, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = False\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet256(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = (False, False, False, True),\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet1024(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = False,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1725-1775"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1760, "start_line_no": 1735, "end_line_no": 1785, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(self, x, *args, **kwargs):\n        return x\n\n# predefined unets, with configs lining up with hyperparameters in appendix of paper\n\nclass BaseUnet64(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 512,\n            dim_mults = (1, 2, 3, 4),\n            num_resnet_blocks = 3,\n            layer_attns = (False, True, True, True),\n            layer_cross_attns = (False, True, True, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = False\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet256(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = (False, False, False, True),\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet1024(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = False,\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\n# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n\nclass Imagen(nn.Module):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1735-1785"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1770, "start_line_no": 1745, "end_line_no": 1795, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            num_resnet_blocks = 3,\n            layer_attns = (False, True, True, True),\n            layer_cross_attns = (False, True, True, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = False\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet256(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = (False, False, False, True),\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet1024(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = False,\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\n# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n\nclass Imagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1745-1795"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1780, "start_line_no": 1755, "end_line_no": 1805, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = (False, False, False, True),\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet1024(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = False,\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\n# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n\nclass Imagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,\n        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1755-1805"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1790, "start_line_no": 1765, "end_line_no": 1815, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\nclass SRUnet1024(Unet):\n    def __init__(self, *args, **kwargs):\n        default_kwargs = dict(\n            dim = 128,\n            dim_mults = (1, 2, 4, 8),\n            num_resnet_blocks = (2, 4, 8, 8),\n            layer_attns = False,\n            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\n# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n\nclass Imagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,\n        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1765-1815"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1800, "start_line_no": 1775, "end_line_no": 1825, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            layer_cross_attns = (False, False, False, True),\n            attn_heads = 8,\n            ff_mult = 2.,\n            memory_efficient = True\n        )\n        super().__init__(*args, **{**default_kwargs, **kwargs})\n\n# main imagen ddpm class, which is a cascading DDPM from Ho et al.\n\nclass Imagen(nn.Module):\n    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,\n        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1775-1825"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1810, "start_line_no": 1785, "end_line_no": 1835, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(\n        self,\n        unets,\n        *,\n        image_sizes,                                # for cascading ddpm, image size at each stage\n        text_encoder_name = DEFAULT_T5_NAME,\n        text_embed_dim = None,\n        channels = 3,\n        timesteps = 1000,\n        cond_drop_prob = 0.1,\n        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1785-1835"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1820, "start_line_no": 1795, "end_line_no": 1845, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        loss_type = 'l2',\n        noise_schedules = 'cosine',\n        pred_objectives = 'noise',\n        random_crop_sizes = None,\n        lowres_noise_schedule = 'linear',\n        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n        condition_on_text = True,\n        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n        p2_loss_weight_gamma = 0.5,                 # p2 loss weight, from https://arxiv.org/abs/2204.00227 - 0 is equivalent to weight of 1 across time\n        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1795-1845"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1830, "start_line_no": 1805, "end_line_no": 1855, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        p2_loss_weight_k = 1,\n        dynamic_thresholding = True,\n        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n        only_train_unet_number = None,\n        temporal_downsample_factor = 1,\n        resize_cond_video_frames = True,\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1805-1855"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1840, "start_line_no": 1815, "end_line_no": 1865, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # loss\n\n        if loss_type == 'l1':\n            loss_fn = F.l1_loss\n        elif loss_type == 'l2':\n            loss_fn = F.mse_loss\n        elif loss_type == 'huber':\n            loss_fn = F.smooth_l1_loss\n        else:\n            raise NotImplementedError()\n\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1815-1865"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1850, "start_line_no": 1825, "end_line_no": 1875, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.loss_type = loss_type\n        self.loss_fn = loss_fn\n\n        # conditioning hparams\n\n        self.condition_on_text = condition_on_text\n        self.unconditional = not condition_on_text\n\n        # channels\n\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1825-1875"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1860, "start_line_no": 1835, "end_line_no": 1885, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.channels = channels\n\n        # automatically take care of ensuring that first unet is unconditional\n        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n\n        unets = cast_tuple(unets)\n        num_unets = len(unets)\n\n        # determine noise schedules per unet\n\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1835-1885"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1870, "start_line_no": 1845, "end_line_no": 1895, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        timesteps = cast_tuple(timesteps, num_unets)\n\n        # make sure noise schedule defaults to 'cosine', 'cosine', and then 'linear' for rest of super-resoluting unets\n\n        noise_schedules = cast_tuple(noise_schedules)\n        noise_schedules = pad_tuple_to_length(noise_schedules, 2, 'cosine')\n        noise_schedules = pad_tuple_to_length(noise_schedules, num_unets, 'linear')\n\n        # construct noise schedulers\n\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1845-1895"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1880, "start_line_no": 1855, "end_line_no": 1905, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        noise_scheduler_klass = GaussianDiffusionContinuousTimes\n        self.noise_schedulers = nn.ModuleList([])\n\n        for timestep, noise_schedule in zip(timesteps, noise_schedules):\n            noise_scheduler = noise_scheduler_klass(noise_schedule = noise_schedule, timesteps = timestep)\n            self.noise_schedulers.append(noise_scheduler)\n\n        # randomly cropping for upsampler training\n\n        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1855-1905"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1890, "start_line_no": 1865, "end_line_no": 1915, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n\n        # lowres augmentation noise schedule\n\n        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n\n        # ddpm objectives - predicting noise by default\n\n        self.pred_objectives = cast_tuple(pred_objectives, num_unets)\n\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1865-1915"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1900, "start_line_no": 1875, "end_line_no": 1925, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # get text encoder\n\n        self.text_encoder_name = text_encoder_name\n        self.text_embed_dim = default(text_embed_dim, lambda: get_encoded_dim(text_encoder_name))\n\n        self.encode_text = partial(t5_encode_text, name = text_encoder_name)\n\n        # construct unets\n\n        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1875-1925"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1910, "start_line_no": 1885, "end_line_no": 1935, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.unets = nn.ModuleList([])\n\n        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n        self.only_train_unet_number = only_train_unet_number\n\n        for ind, one_unet in enumerate(unets):\n            assert isinstance(one_unet, (Unet, Unet3D, NullUnet))\n            is_first = ind == 0\n\n            one_unet = one_unet.cast_model_parameters(\n                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1885-1935"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1920, "start_line_no": 1895, "end_line_no": 1945, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                lowres_cond = not is_first,\n                cond_on_text = self.condition_on_text,\n                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n                channels = self.channels,\n                channels_out = self.channels\n            )\n\n            self.unets.append(one_unet)\n\n        # unet image sizes\n\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1895-1945"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1930, "start_line_no": 1905, "end_line_no": 1955, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        image_sizes = cast_tuple(image_sizes)\n        self.image_sizes = image_sizes\n\n        assert num_unets == len(image_sizes), f'you did not supply the correct number of u-nets ({len(unets)}) for resolutions {image_sizes}'\n\n        self.sample_channels = cast_tuple(self.channels, num_unets)\n\n        # determine whether we are training on images or video\n\n        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1905-1955"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1940, "start_line_no": 1915, "end_line_no": 1965, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        is_video = any([isinstance(unet, Unet3D) for unet in self.unets])\n        self.is_video = is_video\n\n        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1 1' if not is_video else 'b -> b 1 1 1 1'))\n\n        self.resize_to = resize_video_to if is_video else resize_image_to\n        self.resize_to = partial(self.resize_to, mode = resize_mode)\n\n        # temporal interpolation\n\n        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1915-1965"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1950, "start_line_no": 1925, "end_line_no": 1975, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        temporal_downsample_factor = cast_tuple(temporal_downsample_factor, num_unets)\n        self.temporal_downsample_factor = temporal_downsample_factor\n\n        self.resize_cond_video_frames = resize_cond_video_frames\n        self.temporal_downsample_divisor = temporal_downsample_factor[0]\n\n        assert temporal_downsample_factor[-1] == 1, 'downsample factor of last stage must be 1'\n        assert tuple(sorted(temporal_downsample_factor, reverse = True)) == temporal_downsample_factor, 'temporal downsample factor must be in order of descending'\n\n        # cascading ddpm related stuff\n\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1925-1975"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1960, "start_line_no": 1935, "end_line_no": 1985, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n\n        self.lowres_sample_noise_level = lowres_sample_noise_level\n        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n\n        # classifier free guidance\n\n        self.cond_drop_prob = cond_drop_prob\n        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1935-1985"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1970, "start_line_no": 1945, "end_line_no": 1995, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.can_classifier_guidance = cond_drop_prob > 0.\n\n        # normalize and unnormalize image functions\n\n        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n\n        # dynamic thresholding\n\n        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1945-1995"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1980, "start_line_no": 1955, "end_line_no": 2005, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n\n        # p2 loss weight\n\n        self.p2_loss_weight_k = p2_loss_weight_k\n        self.p2_loss_weight_gamma = cast_tuple(p2_loss_weight_gamma, num_unets)\n\n        assert all([(gamma_value <= 2) for gamma_value in self.p2_loss_weight_gamma]), 'in paper, they noticed any gamma greater than 2 is harmful'\n\n        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1955-2005"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 1990, "start_line_no": 1965, "end_line_no": 2015, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # one temp parameter for keeping track of device\n\n        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n\n        # default to device of unets passed in\n\n        self.to(next(self.unets.parameters()).device)\n\n    def force_unconditional_(self):\n        self.condition_on_text = False\n        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1965-2015"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2000, "start_line_no": 1975, "end_line_no": 2025, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.unconditional = True\n\n        for unet in self.unets:\n            unet.cond_on_text = False\n\n    @property\n    def device(self):\n        return self._temp.device\n\n    def get_unet(self, unet_number):\n        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1975-2025"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2010, "start_line_no": 1985, "end_line_no": 2035, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert 0 < unet_number <= len(self.unets)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.unets]\n            delattr(self, 'unets')\n            self.unets = unets_list\n\n        if index != self.unet_being_trained_index:\n            for unet_index, unet in enumerate(self.unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # gaussian diffusion methods\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1985-2035"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2020, "start_line_no": 1995, "end_line_no": 2045, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.unet_being_trained_index = index\n        return self.unets[index]\n\n    def reset_unets_all_one_device(self, device = None):\n        device = default(device, self.device)\n        self.unets = nn.ModuleList([*self.unets])\n        self.unets.to(device)\n\n        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # gaussian diffusion methods\n\n    def p_mean_variance(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_1995-2045"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2030, "start_line_no": 2005, "end_line_no": 2055, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.unet_being_trained_index = -1\n\n    @contextmanager\n    def one_unet_in_gpu(self, unet_number = None, unet = None):\n        assert exists(unet_number) ^ exists(unet)\n\n        if exists(unet_number):\n            unet = self.unets[unet_number - 1]\n\n        devices = [module_device(unet) for unet in self.unets]\n        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # gaussian diffusion methods\n\n    def p_mean_variance(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        lowres_cond_img = None,\n        self_cond = None,\n        lowres_noise_times = None,\n        cond_scale = 1.,\n        model_output = None,\n        t_next = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2005-2055"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2040, "start_line_no": 2015, "end_line_no": 2065, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.unets.cpu()\n        unet.to(self.device)\n\n        yield\n\n        for unet, device in zip(self.unets, devices):\n            unet.to(device)\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # gaussian diffusion methods\n\n    def p_mean_variance(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        lowres_cond_img = None,\n        self_cond = None,\n        lowres_noise_times = None,\n        cond_scale = 1.,\n        model_output = None,\n        t_next = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2015-2065"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2050, "start_line_no": 2025, "end_line_no": 2075, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # gaussian diffusion methods\n\n    def p_mean_variance(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        lowres_cond_img = None,\n        self_cond = None,\n        lowres_noise_times = None,\n        cond_scale = 1.,\n        model_output = None,\n        t_next = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n            x,\n            noise_scheduler.get_condition(t),\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2025-2075"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2060, "start_line_no": 2035, "end_line_no": 2085, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def p_mean_variance(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        lowres_cond_img = None,\n        self_cond = None,\n        lowres_noise_times = None,\n        cond_scale = 1.,\n        model_output = None,\n        t_next = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n            x,\n            noise_scheduler.get_condition(t),\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),\n            **video_kwargs\n        ))\n\n        if pred_objective == 'noise':\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        elif pred_objective == 'x_start':\n            x_start = pred\n        elif pred_objective == 'v':\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        else:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2035-2085"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2070, "start_line_no": 2045, "end_line_no": 2095, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        cond_video_frames = None,\n        post_cond_video_frames = None,\n        lowres_cond_img = None,\n        self_cond = None,\n        lowres_noise_times = None,\n        cond_scale = 1.,\n        model_output = None,\n        t_next = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n            x,\n            noise_scheduler.get_condition(t),\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),\n            **video_kwargs\n        ))\n\n        if pred_objective == 'noise':\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        elif pred_objective == 'x_start':\n            x_start = pred\n        elif pred_objective == 'v':\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        if dynamic_threshold:\n            # following pseudocode in appendix\n            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n            s = torch.quantile(\n                rearrange(x_start, 'b ... -> b (...)').abs(),\n                self.dynamic_thresholding_percentile,\n                dim = -1\n            )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2045-2095"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2080, "start_line_no": 2055, "end_line_no": 2105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    ):\n        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n            x,\n            noise_scheduler.get_condition(t),\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),\n            **video_kwargs\n        ))\n\n        if pred_objective == 'noise':\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        elif pred_objective == 'x_start':\n            x_start = pred\n        elif pred_objective == 'v':\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        if dynamic_threshold:\n            # following pseudocode in appendix\n            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n            s = torch.quantile(\n                rearrange(x_start, 'b ... -> b (...)').abs(),\n                self.dynamic_thresholding_percentile,\n                dim = -1\n            )\n\n            s.clamp_(min = 1.)\n            s = right_pad_dims_to(x_start, s)\n            x_start = x_start.clamp(-s, s) / s\n        else:\n            x_start.clamp_(-1., 1.)\n\n        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n        return mean_and_variance, x_start\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2055-2105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2090, "start_line_no": 2065, "end_line_no": 2115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        pred = default(model_output, lambda: unet.forward_with_cond_scale(\n            x,\n            noise_scheduler.get_condition(t),\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times),\n            **video_kwargs\n        ))\n\n        if pred_objective == 'noise':\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        elif pred_objective == 'x_start':\n            x_start = pred\n        elif pred_objective == 'v':\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        if dynamic_threshold:\n            # following pseudocode in appendix\n            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n            s = torch.quantile(\n                rearrange(x_start, 'b ... -> b (...)').abs(),\n                self.dynamic_thresholding_percentile,\n                dim = -1\n            )\n\n            s.clamp_(min = 1.)\n            s = right_pad_dims_to(x_start, s)\n            x_start = x_start.clamp(-s, s) / s\n        else:\n            x_start.clamp_(-1., 1.)\n\n        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n        return mean_and_variance, x_start\n\n    @torch.no_grad()\n    def p_sample(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        t_next = None,\n        text_embeds = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2065-2115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2100, "start_line_no": 2075, "end_line_no": 2125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            **video_kwargs\n        ))\n\n        if pred_objective == 'noise':\n            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n        elif pred_objective == 'x_start':\n            x_start = pred\n        elif pred_objective == 'v':\n            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        if dynamic_threshold:\n            # following pseudocode in appendix\n            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n            s = torch.quantile(\n                rearrange(x_start, 'b ... -> b (...)').abs(),\n                self.dynamic_thresholding_percentile,\n                dim = -1\n            )\n\n            s.clamp_(min = 1.)\n            s = right_pad_dims_to(x_start, s)\n            x_start = x_start.clamp(-s, s) / s\n        else:\n            x_start.clamp_(-1., 1.)\n\n        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n        return mean_and_variance, x_start\n\n    @torch.no_grad()\n    def p_sample(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        t_next = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        cond_scale = 1.,\n        self_cond = None,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2075-2125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2110, "start_line_no": 2085, "end_line_no": 2135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            raise ValueError(f'unknown objective {pred_objective}')\n\n        if dynamic_threshold:\n            # following pseudocode in appendix\n            # s is the dynamic threshold, determined by percentile of absolute values of reconstructed sample per batch element\n            s = torch.quantile(\n                rearrange(x_start, 'b ... -> b (...)').abs(),\n                self.dynamic_thresholding_percentile,\n                dim = -1\n            )\n\n            s.clamp_(min = 1.)\n            s = right_pad_dims_to(x_start, s)\n            x_start = x_start.clamp(-s, s) / s\n        else:\n            x_start.clamp_(-1., 1.)\n\n        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n        return mean_and_variance, x_start\n\n    @torch.no_grad()\n    def p_sample(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        t_next = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        cond_scale = 1.,\n        self_cond = None,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        b, *_, device = *x.shape, x.device\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2085-2135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2120, "start_line_no": 2095, "end_line_no": 2145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            s.clamp_(min = 1.)\n            s = right_pad_dims_to(x_start, s)\n            x_start = x_start.clamp(-s, s) / s\n        else:\n            x_start.clamp_(-1., 1.)\n\n        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n        return mean_and_variance, x_start\n\n    @torch.no_grad()\n    def p_sample(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        t_next = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        cond_scale = 1.,\n        self_cond = None,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        b, *_, device = *x.shape, x.device\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n            unet,\n            x = x,\n            t = t,\n            t_next = t_next,\n            noise_scheduler = noise_scheduler,\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2095-2145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2130, "start_line_no": 2105, "end_line_no": 2155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    @torch.no_grad()\n    def p_sample(\n        self,\n        unet,\n        x,\n        t,\n        *,\n        noise_scheduler,\n        t_next = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        cond_scale = 1.,\n        self_cond = None,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        b, *_, device = *x.shape, x.device\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n            unet,\n            x = x,\n            t = t,\n            t_next = t_next,\n            noise_scheduler = noise_scheduler,\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = lowres_noise_times,\n            pred_objective = pred_objective,\n            dynamic_threshold = dynamic_threshold,\n            **video_kwargs\n        )\n\n        noise = torch.randn_like(x)\n        # no noise when t == 0", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2105-2155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2140, "start_line_no": 2115, "end_line_no": 2165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        cond_scale = 1.,\n        self_cond = None,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        pred_objective = 'noise',\n        dynamic_threshold = True\n    ):\n        b, *_, device = *x.shape, x.device\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n            unet,\n            x = x,\n            t = t,\n            t_next = t_next,\n            noise_scheduler = noise_scheduler,\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = lowres_noise_times,\n            pred_objective = pred_objective,\n            dynamic_threshold = dynamic_threshold,\n            **video_kwargs\n        )\n\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(\n        self,\n        unet,\n        shape,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2115-2165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2150, "start_line_no": 2125, "end_line_no": 2175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    ):\n        b, *_, device = *x.shape, x.device\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n            unet,\n            x = x,\n            t = t,\n            t_next = t_next,\n            noise_scheduler = noise_scheduler,\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = lowres_noise_times,\n            pred_objective = pred_objective,\n            dynamic_threshold = dynamic_threshold,\n            **video_kwargs\n        )\n\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(\n        self,\n        unet,\n        shape,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2125-2175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2160, "start_line_no": 2135, "end_line_no": 2185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n            unet,\n            x = x,\n            t = t,\n            t_next = t_next,\n            noise_scheduler = noise_scheduler,\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            cond_scale = cond_scale,\n            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = lowres_noise_times,\n            pred_objective = pred_objective,\n            dynamic_threshold = dynamic_threshold,\n            **video_kwargs\n        )\n\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(\n        self,\n        unet,\n        shape,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        cond_scale = 1,\n        pred_objective = 'noise',\n        dynamic_threshold = True,\n        use_tqdm = True\n    ):\n        device = self.device", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2135-2185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2170, "start_line_no": 2145, "end_line_no": 2195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            lowres_cond_img = lowres_cond_img,\n            self_cond = self_cond,\n            lowres_noise_times = lowres_noise_times,\n            pred_objective = pred_objective,\n            dynamic_threshold = dynamic_threshold,\n            **video_kwargs\n        )\n\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(\n        self,\n        unet,\n        shape,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        cond_scale = 1,\n        pred_objective = 'noise',\n        dynamic_threshold = True,\n        use_tqdm = True\n    ):\n        device = self.device\n\n        batch = shape[0]\n        img = torch.randn(shape, device = device)\n\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2145-2195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2180, "start_line_no": 2155, "end_line_no": 2205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(\n        self,\n        unet,\n        shape,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        cond_scale = 1,\n        pred_objective = 'noise',\n        dynamic_threshold = True,\n        use_tqdm = True\n    ):\n        device = self.device\n\n        batch = shape[0]\n        img = torch.randn(shape, device = device)\n\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initialization with an image or video\n\n        if exists(init_images):\n            img += init_images\n\n        # keep track of x0, for self conditioning\n\n        x_start = None\n\n        # prepare inpainting", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2155-2205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2190, "start_line_no": 2165, "end_line_no": 2215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        cond_scale = 1,\n        pred_objective = 'noise',\n        dynamic_threshold = True,\n        use_tqdm = True\n    ):\n        device = self.device\n\n        batch = shape[0]\n        img = torch.randn(shape, device = device)\n\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initialization with an image or video\n\n        if exists(init_images):\n            img += init_images\n\n        # keep track of x0, for self conditioning\n\n        x_start = None\n\n        # prepare inpainting\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # time", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2165-2215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2200, "start_line_no": 2175, "end_line_no": 2225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        cond_scale = 1,\n        pred_objective = 'noise',\n        dynamic_threshold = True,\n        use_tqdm = True\n    ):\n        device = self.device\n\n        batch = shape[0]\n        img = torch.randn(shape, device = device)\n\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initialization with an image or video\n\n        if exists(init_images):\n            img += init_images\n\n        # keep track of x0, for self conditioning\n\n        x_start = None\n\n        # prepare inpainting\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # time\n\n        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n\n        # whether to skip any steps\n\n        skip_steps = default(skip_steps, 0)\n        timesteps = timesteps[skip_steps:]\n\n        # video conditioning kwargs\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2175-2225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2210, "start_line_no": 2185, "end_line_no": 2235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        batch = shape[0]\n        img = torch.randn(shape, device = device)\n\n        # video\n\n        is_video = len(shape) == 5\n        frames = shape[-3] if is_video else None\n        resize_kwargs = dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initialization with an image or video\n\n        if exists(init_images):\n            img += init_images\n\n        # keep track of x0, for self conditioning\n\n        x_start = None\n\n        # prepare inpainting\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # time\n\n        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n\n        # whether to skip any steps\n\n        skip_steps = default(skip_steps, 0)\n        timesteps = timesteps[skip_steps:]\n\n        # video conditioning kwargs\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n            is_last_timestep = times_next == 0\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2185-2235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2220, "start_line_no": 2195, "end_line_no": 2245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # for initialization with an image or video\n\n        if exists(init_images):\n            img += init_images\n\n        # keep track of x0, for self conditioning\n\n        x_start = None\n\n        # prepare inpainting\n\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # time\n\n        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n\n        # whether to skip any steps\n\n        skip_steps = default(skip_steps, 0)\n        timesteps = timesteps[skip_steps:]\n\n        # video conditioning kwargs\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n            is_last_timestep = times_next == 0\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                if has_inpainting:\n                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n\n                self_cond = x_start if unet.self_cond else None\n\n                img, x_start = self.p_sample(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2195-2245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2230, "start_line_no": 2205, "end_line_no": 2255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n        resample_times = inpaint_resample_times if has_inpainting else 1\n\n        if has_inpainting:\n            inpaint_images = self.normalize_img(inpaint_images)\n            inpaint_images = self.resize_to(inpaint_images, shape[-1], **resize_kwargs)\n            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n\n        # time\n\n        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n\n        # whether to skip any steps\n\n        skip_steps = default(skip_steps, 0)\n        timesteps = timesteps[skip_steps:]\n\n        # video conditioning kwargs\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n            is_last_timestep = times_next == 0\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                if has_inpainting:\n                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n\n                self_cond = x_start if unet.self_cond else None\n\n                img, x_start = self.p_sample(\n                    unet,\n                    img,\n                    times,\n                    t_next = times_next,\n                    text_embeds = text_embeds,\n                    text_mask = text_mask,\n                    cond_images = cond_images,\n                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2205-2255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2240, "start_line_no": 2215, "end_line_no": 2265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n\n        # whether to skip any steps\n\n        skip_steps = default(skip_steps, 0)\n        timesteps = timesteps[skip_steps:]\n\n        # video conditioning kwargs\n\n        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n            is_last_timestep = times_next == 0\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                if has_inpainting:\n                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n\n                self_cond = x_start if unet.self_cond else None\n\n                img, x_start = self.p_sample(\n                    unet,\n                    img,\n                    times,\n                    t_next = times_next,\n                    text_embeds = text_embeds,\n                    text_mask = text_mask,\n                    cond_images = cond_images,\n                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    **video_kwargs\n                )\n\n                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2215-2265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2250, "start_line_no": 2225, "end_line_no": 2275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        video_kwargs = dict()\n        if self.is_video:\n            video_kwargs = dict(\n                cond_video_frames = cond_video_frames,\n                post_cond_video_frames = post_cond_video_frames,\n            )\n\n        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n            is_last_timestep = times_next == 0\n\n            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                if has_inpainting:\n                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n\n                self_cond = x_start if unet.self_cond else None\n\n                img, x_start = self.p_sample(\n                    unet,\n                    img,\n                    times,\n                    t_next = times_next,\n                    text_embeds = text_embeds,\n                    text_mask = text_mask,\n                    cond_images = cond_images,\n                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    **video_kwargs\n                )\n\n                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n\n                    img = torch.where(\n                        self.right_pad_dims_to_datatype(is_last_timestep),\n                        img,\n                        renoised_img\n                    )\n\n        img.clamp_(-1., 1.)\n\n        # final inpainting\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2225-2275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2260, "start_line_no": 2235, "end_line_no": 2285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            for r in reversed(range(resample_times)):\n                is_last_resample_step = r == 0\n\n                if has_inpainting:\n                    noised_inpaint_images, *_ = noise_scheduler.q_sample(inpaint_images, t = times)\n                    img = img * ~inpaint_masks + noised_inpaint_images * inpaint_masks\n\n                self_cond = x_start if unet.self_cond else None\n\n                img, x_start = self.p_sample(\n                    unet,\n                    img,\n                    times,\n                    t_next = times_next,\n                    text_embeds = text_embeds,\n                    text_mask = text_mask,\n                    cond_images = cond_images,\n                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    **video_kwargs\n                )\n\n                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n\n                    img = torch.where(\n                        self.right_pad_dims_to_datatype(is_last_timestep),\n                        img,\n                        renoised_img\n                    )\n\n        img.clamp_(-1., 1.)\n\n        # final inpainting\n\n        if has_inpainting:\n            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n\n    @torch.no_grad()\n    @eval_decorator\n    @beartype\n    def sample(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2235-2285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2270, "start_line_no": 2245, "end_line_no": 2295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    unet,\n                    img,\n                    times,\n                    t_next = times_next,\n                    text_embeds = text_embeds,\n                    text_mask = text_mask,\n                    cond_images = cond_images,\n                    cond_scale = cond_scale,\n                    self_cond = self_cond,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    **video_kwargs\n                )\n\n                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n\n                    img = torch.where(\n                        self.right_pad_dims_to_datatype(is_last_timestep),\n                        img,\n                        renoised_img\n                    )\n\n        img.clamp_(-1., 1.)\n\n        # final inpainting\n\n        if has_inpainting:\n            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n\n    @torch.no_grad()\n    @eval_decorator\n    @beartype\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        video_frames = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2245-2295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2280, "start_line_no": 2255, "end_line_no": 2305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    **video_kwargs\n                )\n\n                if has_inpainting and not (is_last_resample_step or torch.all(is_last_timestep)):\n                    renoised_img = noise_scheduler.q_sample_from_to(img, times_next, times)\n\n                    img = torch.where(\n                        self.right_pad_dims_to_datatype(is_last_timestep),\n                        img,\n                        renoised_img\n                    )\n\n        img.clamp_(-1., 1.)\n\n        # final inpainting\n\n        if has_inpainting:\n            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n\n    @torch.no_grad()\n    @eval_decorator\n    @beartype\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        video_frames = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2255-2305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2290, "start_line_no": 2265, "end_line_no": 2315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    img = torch.where(\n                        self.right_pad_dims_to_datatype(is_last_timestep),\n                        img,\n                        renoised_img\n                    )\n\n        img.clamp_(-1., 1.)\n\n        # final inpainting\n\n        if has_inpainting:\n            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n\n    @torch.no_grad()\n    @eval_decorator\n    @beartype\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        video_frames = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        device = None,\n        use_tqdm = True\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2265-2315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2300, "start_line_no": 2275, "end_line_no": 2325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if has_inpainting:\n            img = img * ~inpaint_masks + inpaint_images * inpaint_masks\n\n        unnormalize_img = self.unnormalize_img(img)\n        return unnormalize_img\n\n    @torch.no_grad()\n    @eval_decorator\n    @beartype\n    def sample(\n        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        video_frames = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        device = None,\n        use_tqdm = True\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2275-2325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2310, "start_line_no": 2285, "end_line_no": 2335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self,\n        texts: List[str] = None,\n        text_masks = None,\n        text_embeds = None,\n        video_frames = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        inpaint_images = None,\n        inpaint_masks = None,\n        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        device = None,\n        use_tqdm = True\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2285-2335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2320, "start_line_no": 2295, "end_line_no": 2345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        inpaint_resample_times = 5,\n        init_images = None,\n        skip_steps = None,\n        batch_size = 1,\n        cond_scale = 1.,\n        lowres_sample_noise_level = None,\n        start_at_unet_number = 1,\n        start_image_or_video = None,\n        stop_at_unet_number = None,\n        return_all_unet_outputs = False,\n        return_pil_images = False,\n        device = None,\n        use_tqdm = True\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2295-2345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2330, "start_line_no": 2305, "end_line_no": 2355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return_pil_images = False,\n        device = None,\n        use_tqdm = True\n    ):\n        device = default(device, self.device)\n        self.reset_unets_all_one_device(device = device)\n\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n\n        # condition scaling\n\n        cond_scale = cast_tuple(cond_scale, num_unets)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2305-2355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2340, "start_line_no": 2315, "end_line_no": 2365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            assert all([*map(len, texts)]), 'text cannot be empty'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n\n        # condition scaling\n\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # add frame dimension for video\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initial image and skipping steps\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2315-2365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2350, "start_line_no": 2325, "end_line_no": 2375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n            batch_size = text_embeds.shape[0]\n\n        if exists(inpaint_images):\n            if self.unconditional:\n                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n                    batch_size = inpaint_images.shape[0]\n\n            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n\n        # condition scaling\n\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # add frame dimension for video\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initial image and skipping steps\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2325-2375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2360, "start_line_no": 2335, "end_line_no": 2385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n\n        outputs = []\n\n        is_cuda = next(self.parameters()).is_cuda\n        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n\n        # condition scaling\n\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # add frame dimension for video\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initial image and skipping steps\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2335-2385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2370, "start_line_no": 2345, "end_line_no": 2395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        device = next(self.parameters()).device\n\n        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n\n        num_unets = len(self.unets)\n\n        # condition scaling\n\n        cond_scale = cast_tuple(cond_scale, num_unets)\n\n        # add frame dimension for video\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initial image and skipping steps\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):\n\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                # video kwargs", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2345-2395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2380, "start_line_no": 2355, "end_line_no": 2405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # add frame dimension for video\n\n        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n\n        all_frame_dims = calc_all_frame_dims(self.temporal_downsample_factor, video_frames)\n\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        # for initial image and skipping steps\n\n        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):\n\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                # video kwargs\n\n                video_kwargs = dict()\n                if self.is_video:\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames,\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2355-2405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2390, "start_line_no": 2365, "end_line_no": 2415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        init_images = cast_tuple(init_images, num_unets)\n        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n\n        skip_steps = cast_tuple(skip_steps, num_unets)\n\n        # handle starting at a unet greater than 1, for training only-upscaler training\n\n        if start_at_unet_number > 1:\n            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):\n\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                # video kwargs\n\n                video_kwargs = dict()\n                if self.is_video:\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames,\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                lowres_cond_img = lowres_noise_times = None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2365-2415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2400, "start_line_no": 2375, "end_line_no": 2425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n\n            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n            prev_frame_size = all_frame_dims[start_at_unet_number - 2][0] if self.is_video else None\n            img = self.resize_to(start_image_or_video, prev_image_size, **frames_to_resize_kwargs(prev_frame_size))\n\n\n        # go through each unet in cascade\n\n        for unet_number, unet, channel, image_size, frame_dims, noise_scheduler, pred_objective, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, all_frame_dims, self.noise_schedulers, self.pred_objectives, self.dynamic_thresholding, cond_scale, init_images, skip_steps), disable = not use_tqdm):\n\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                # video kwargs\n\n                video_kwargs = dict()\n                if self.is_video:\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames,\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                lowres_cond_img = lowres_noise_times = None\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2375-2425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2410, "start_line_no": 2385, "end_line_no": 2435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            if unet_number < start_at_unet_number:\n                continue\n\n            assert not isinstance(unet, NullUnet), 'one cannot sample from null / placeholder unets'\n\n            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n\n            with context:\n                # video kwargs\n\n                video_kwargs = dict()\n                if self.is_video:\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames,\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                lowres_cond_img = lowres_noise_times = None\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                # init images or video\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                # shape of stage\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2385-2435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2420, "start_line_no": 2395, "end_line_no": 2445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n                video_kwargs = dict()\n                if self.is_video:\n                    video_kwargs = dict(\n                        cond_video_frames = cond_video_frames,\n                        post_cond_video_frames = post_cond_video_frames,\n                    )\n\n                    video_kwargs = compact(video_kwargs)\n\n                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                lowres_cond_img = lowres_noise_times = None\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                # init images or video\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                # shape of stage\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2395-2445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2430, "start_line_no": 2405, "end_line_no": 2455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                if self.is_video and self.resize_cond_video_frames:\n                    downsample_scale = self.temporal_downsample_factor[unet_number - 1]\n                    temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'cond_video_frames', temporal_downsample_fn)\n                    video_kwargs = maybe_transform_dict_key(video_kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n                # low resolution conditioning\n\n                lowres_cond_img = lowres_noise_times = None\n                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                # init images or video\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                # shape of stage\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2405-2455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2440, "start_line_no": 2415, "end_line_no": 2465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                shape = (batch_size, channel, *frame_dims, image_size, image_size)\n\n                resize_kwargs = dict(target_frames = frame_dims[0]) if self.is_video else dict()\n\n                if unet.lowres_cond:\n                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n\n                    lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n\n                    lowres_cond_img = self.normalize_img(lowres_cond_img)\n                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                # init images or video\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                # shape of stage\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2415-2465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2450, "start_line_no": 2425, "end_line_no": 2475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_noise_times, noise = torch.randn_like(lowres_cond_img))\n\n                # init images or video\n\n                if exists(unet_init_images):\n                    unet_init_images = self.resize_to(unet_init_images, image_size, **resize_kwargs)\n\n                # shape of stage\n\n                shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2425-2475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2460, "start_line_no": 2435, "end_line_no": 2485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n                img = self.p_sample_loop(\n                    unet,\n                    shape,\n                    text_embeds = text_embeds,\n                    text_mask = text_masks,\n                    cond_images = cond_images,\n                    inpaint_images = inpaint_images,\n                    inpaint_masks = inpaint_masks,\n                    inpaint_resample_times = inpaint_resample_times,\n                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    @beartype\n    def p_losses(\n        self,\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n        x_start,\n        times,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2435-2485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2470, "start_line_no": 2445, "end_line_no": 2495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    init_images = unet_init_images,\n                    skip_steps = unet_skip_steps,\n                    cond_scale = unet_cond_scale,\n                    lowres_cond_img = lowres_cond_img,\n                    lowres_noise_times = lowres_noise_times,\n                    noise_scheduler = noise_scheduler,\n                    pred_objective = pred_objective,\n                    dynamic_threshold = dynamic_threshold,\n                    use_tqdm = use_tqdm,\n                    **video_kwargs\n                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    @beartype\n    def p_losses(\n        self,\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n        x_start,\n        times,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_aug_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        noise = None,\n        times_next = None,\n        pred_objective = 'noise',\n        p2_loss_weight_gamma = 0.,\n        random_crop_size = None,\n        **kwargs", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2445-2495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2480, "start_line_no": 2455, "end_line_no": 2505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                )\n\n                outputs.append(img)\n\n            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n                break\n\n        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n\n        if not return_pil_images:\n            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    @beartype\n    def p_losses(\n        self,\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n        x_start,\n        times,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_aug_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        noise = None,\n        times_next = None,\n        pred_objective = 'noise',\n        p2_loss_weight_gamma = 0.,\n        random_crop_size = None,\n        **kwargs\n    ):\n        is_video = x_start.ndim == 5\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # normalize to [-1, 1]\n\n        x_start = self.normalize_img(x_start)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2455-2505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2490, "start_line_no": 2465, "end_line_no": 2515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return outputs[output_index]\n\n        if not return_all_unet_outputs:\n            outputs = outputs[-1:]\n\n        assert not self.is_video, 'converting sampled video tensor to video file is not supported yet'\n\n        pil_images = list(map(lambda img: list(map(T.ToPILImage(), img.unbind(dim = 0))), outputs))\n\n        return pil_images[output_index] # now you have a bunch of pillow images you can just .save(/where/ever/you/want.png)\n\n    @beartype\n    def p_losses(\n        self,\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n        x_start,\n        times,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_aug_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        noise = None,\n        times_next = None,\n        pred_objective = 'noise',\n        p2_loss_weight_gamma = 0.,\n        random_crop_size = None,\n        **kwargs\n    ):\n        is_video = x_start.ndim == 5\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # normalize to [-1, 1]\n\n        x_start = self.normalize_img(x_start)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            if is_video:\n                frames = x_start.shape[2]\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), 'b c f h w -> (b f) c h w')\n\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2465-2515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2500, "start_line_no": 2475, "end_line_no": 2525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    @beartype\n    def p_losses(\n        self,\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel],\n        x_start,\n        times,\n        *,\n        noise_scheduler,\n        lowres_cond_img = None,\n        lowres_aug_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        noise = None,\n        times_next = None,\n        pred_objective = 'noise',\n        p2_loss_weight_gamma = 0.,\n        random_crop_size = None,\n        **kwargs\n    ):\n        is_video = x_start.ndim == 5\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # normalize to [-1, 1]\n\n        x_start = self.normalize_img(x_start)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            if is_video:\n                frames = x_start.shape[2]\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), 'b c f h w -> (b f) c h w')\n\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            x_start = aug(x_start)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n            noise = aug(noise, params = aug._params)\n\n            if is_video:\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), '(b f) c h w -> b c f h w', f = frames)\n\n        # get x_t", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2475-2525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2510, "start_line_no": 2485, "end_line_no": 2535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        lowres_aug_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        noise = None,\n        times_next = None,\n        pred_objective = 'noise',\n        p2_loss_weight_gamma = 0.,\n        random_crop_size = None,\n        **kwargs\n    ):\n        is_video = x_start.ndim == 5\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # normalize to [-1, 1]\n\n        x_start = self.normalize_img(x_start)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            if is_video:\n                frames = x_start.shape[2]\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), 'b c f h w -> (b f) c h w')\n\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            x_start = aug(x_start)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n            noise = aug(noise, params = aug._params)\n\n            if is_video:\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), '(b f) c h w -> b c f h w', f = frames)\n\n        # get x_t\n\n        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n\n        # also noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_aug_times = default(lowres_aug_times, times)\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2485-2535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2520, "start_line_no": 2495, "end_line_no": 2545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    ):\n        is_video = x_start.ndim == 5\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # normalize to [-1, 1]\n\n        x_start = self.normalize_img(x_start)\n        lowres_cond_img = maybe(self.normalize_img)(lowres_cond_img)\n\n        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            if is_video:\n                frames = x_start.shape[2]\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), 'b c f h w -> (b f) c h w')\n\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            x_start = aug(x_start)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n            noise = aug(noise, params = aug._params)\n\n            if is_video:\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), '(b f) c h w -> b c f h w', f = frames)\n\n        # get x_t\n\n        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n\n        # also noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_aug_times = default(lowres_aug_times, times)\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # time condition\n\n        noise_cond = noise_scheduler.get_condition(times)\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            text_embeds = text_embeds,\n            text_mask = text_mask,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2495-2545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2530, "start_line_no": 2505, "end_line_no": 2555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # random cropping during training\n        # for upsamplers\n\n        if exists(random_crop_size):\n            if is_video:\n                frames = x_start.shape[2]\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), 'b c f h w -> (b f) c h w')\n\n            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n\n            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            x_start = aug(x_start)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n            noise = aug(noise, params = aug._params)\n\n            if is_video:\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), '(b f) c h w -> b c f h w', f = frames)\n\n        # get x_t\n\n        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n\n        # also noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_aug_times = default(lowres_aug_times, times)\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # time condition\n\n        noise_cond = noise_scheduler.get_condition(times)\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self condition if needed\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2505-2555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2540, "start_line_no": 2515, "end_line_no": 2565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            # make sure low res conditioner and image both get augmented the same way\n            # detailed https://kornia.readthedocs.io/en/latest/augmentation.module.html?highlight=randomcrop#kornia.augmentation.RandomCrop\n            x_start = aug(x_start)\n            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n            noise = aug(noise, params = aug._params)\n\n            if is_video:\n                x_start, lowres_cond_img, noise = rearrange_many((x_start, lowres_cond_img, noise), '(b f) c h w -> b c f h w', f = frames)\n\n        # get x_t\n\n        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n\n        # also noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_aug_times = default(lowres_aug_times, times)\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # time condition\n\n        noise_cond = noise_scheduler.get_condition(times)\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self condition if needed\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred = unet.forward(\n                    x_noisy,\n                    noise_cond,\n                    **unet_kwargs", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2515-2565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2550, "start_line_no": 2525, "end_line_no": 2575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n\n        # also noise the lowres conditioning image\n        # at sample time, they then fix the noise level of 0.1 - 0.3\n\n        lowres_cond_img_noisy = None\n        if exists(lowres_cond_img):\n            lowres_aug_times = default(lowres_aug_times, times)\n            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))\n\n        # time condition\n\n        noise_cond = noise_scheduler.get_condition(times)\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self condition if needed\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred = unet.forward(\n                    x_noisy,\n                    noise_cond,\n                    **unet_kwargs\n                ).detach()\n\n                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n\n                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n\n        # get prediction\n\n        pred = unet.forward(\n            x_noisy,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2525-2575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2560, "start_line_no": 2535, "end_line_no": 2585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # time condition\n\n        noise_cond = noise_scheduler.get_condition(times)\n\n        # unet kwargs\n\n        unet_kwargs = dict(\n            text_embeds = text_embeds,\n            text_mask = text_mask,\n            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self condition if needed\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred = unet.forward(\n                    x_noisy,\n                    noise_cond,\n                    **unet_kwargs\n                ).detach()\n\n                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n\n                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n\n        # get prediction\n\n        pred = unet.forward(\n            x_noisy,\n            noise_cond,\n            **unet_kwargs\n        )\n\n        # prediction objective\n\n        if pred_objective == 'noise':\n            target = noise\n        elif pred_objective == 'x_start':\n            target = x_start", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2535-2585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2570, "start_line_no": 2545, "end_line_no": 2595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            cond_images = cond_images,\n            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n            lowres_cond_img = lowres_cond_img_noisy,\n            cond_drop_prob = self.cond_drop_prob,\n            **kwargs\n        )\n\n        # self condition if needed\n\n        # Because 'unet' can be an instance of DistributedDataParallel coming from the\n        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred = unet.forward(\n                    x_noisy,\n                    noise_cond,\n                    **unet_kwargs\n                ).detach()\n\n                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n\n                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n\n        # get prediction\n\n        pred = unet.forward(\n            x_noisy,\n            noise_cond,\n            **unet_kwargs\n        )\n\n        # prediction objective\n\n        if pred_objective == 'noise':\n            target = noise\n        elif pred_objective == 'x_start':\n            target = x_start\n        elif pred_objective == 'v':\n            # derivation detailed in Appendix D of Progressive Distillation paper\n            # https://arxiv.org/abs/2202.00512\n            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n            target = alpha * noise - sigma * x_start\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        # losses\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2545-2595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2580, "start_line_no": 2555, "end_line_no": 2605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # ImagenTrainer.unet_being_trained when invoking ImagenTrainer.forward(), we need to\n        # access the member 'module' of the wrapped unet instance.\n        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet.self_cond\n\n        if self_cond and random() < 0.5:\n            with torch.no_grad():\n                pred = unet.forward(\n                    x_noisy,\n                    noise_cond,\n                    **unet_kwargs\n                ).detach()\n\n                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n\n                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n\n        # get prediction\n\n        pred = unet.forward(\n            x_noisy,\n            noise_cond,\n            **unet_kwargs\n        )\n\n        # prediction objective\n\n        if pred_objective == 'noise':\n            target = noise\n        elif pred_objective == 'x_start':\n            target = x_start\n        elif pred_objective == 'v':\n            # derivation detailed in Appendix D of Progressive Distillation paper\n            # https://arxiv.org/abs/2202.00512\n            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n            target = alpha * noise - sigma * x_start\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        # losses\n\n        losses = self.loss_fn(pred, target, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # p2 loss reweighting\n\n        if p2_loss_weight_gamma > 0:\n            loss_weight = (self.p2_loss_weight_k + log_snr.exp()) ** -p2_loss_weight_gamma\n            losses = losses * loss_weight\n\n        return losses.mean()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2555-2605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2590, "start_line_no": 2565, "end_line_no": 2615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                ).detach()\n\n                x_start = noise_scheduler.predict_start_from_noise(x_noisy, t = times, noise = pred) if pred_objective == 'noise' else pred\n\n                unet_kwargs = {**unet_kwargs, 'self_cond': x_start}\n\n        # get prediction\n\n        pred = unet.forward(\n            x_noisy,\n            noise_cond,\n            **unet_kwargs\n        )\n\n        # prediction objective\n\n        if pred_objective == 'noise':\n            target = noise\n        elif pred_objective == 'x_start':\n            target = x_start\n        elif pred_objective == 'v':\n            # derivation detailed in Appendix D of Progressive Distillation paper\n            # https://arxiv.org/abs/2202.00512\n            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n            target = alpha * noise - sigma * x_start\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        # losses\n\n        losses = self.loss_fn(pred, target, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # p2 loss reweighting\n\n        if p2_loss_weight_gamma > 0:\n            loss_weight = (self.p2_loss_weight_k + log_snr.exp()) ** -p2_loss_weight_gamma\n            losses = losses * loss_weight\n\n        return losses.mean()\n\n    @beartype\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2565-2615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2600, "start_line_no": 2575, "end_line_no": 2625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            noise_cond,\n            **unet_kwargs\n        )\n\n        # prediction objective\n\n        if pred_objective == 'noise':\n            target = noise\n        elif pred_objective == 'x_start':\n            target = x_start\n        elif pred_objective == 'v':\n            # derivation detailed in Appendix D of Progressive Distillation paper\n            # https://arxiv.org/abs/2202.00512\n            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n            target = alpha * noise - sigma * x_start\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        # losses\n\n        losses = self.loss_fn(pred, target, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # p2 loss reweighting\n\n        if p2_loss_weight_gamma > 0:\n            loss_weight = (self.p2_loss_weight_k + log_snr.exp()) ** -p2_loss_weight_gamma\n            losses = losses * loss_weight\n\n        return losses.mean()\n\n    @beartype\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2575-2625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2610, "start_line_no": 2585, "end_line_no": 2635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        elif pred_objective == 'v':\n            # derivation detailed in Appendix D of Progressive Distillation paper\n            # https://arxiv.org/abs/2202.00512\n            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n            target = alpha * noise - sigma * x_start\n        else:\n            raise ValueError(f'unknown objective {pred_objective}')\n\n        # losses\n\n        losses = self.loss_fn(pred, target, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # p2 loss reweighting\n\n        if p2_loss_weight_gamma > 0:\n            loss_weight = (self.p2_loss_weight_k + log_snr.exp()) ** -p2_loss_weight_gamma\n            losses = losses * loss_weight\n\n        return losses.mean()\n\n    @beartype\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n\n        unet = default(unet, lambda: self.get_unet(unet_number))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2585-2635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2620, "start_line_no": 2595, "end_line_no": 2645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        losses = self.loss_fn(pred, target, reduction = 'none')\n        losses = reduce(losses, 'b ... -> b', 'mean')\n\n        # p2 loss reweighting\n\n        if p2_loss_weight_gamma > 0:\n            loss_weight = (self.p2_loss_weight_k + log_snr.exp()) ** -p2_loss_weight_gamma\n            losses = losses * loss_weight\n\n        return losses.mean()\n\n    @beartype\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n\n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        noise_scheduler      = self.noise_schedulers[unet_index]\n        p2_loss_weight_gamma = self.p2_loss_weight_gamma[unet_index]\n        pred_objective       = self.pred_objectives[unet_index]\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2595-2645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2630, "start_line_no": 2605, "end_line_no": 2655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    @beartype\n    def forward(\n        self,\n        images, # rename to images or video\n        unet: Union[Unet, Unet3D, NullUnet, DistributedDataParallel] = None,\n        texts: List[str] = None,\n        text_embeds = None,\n        text_masks = None,\n        unet_number = None,\n        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n\n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        noise_scheduler      = self.noise_schedulers[unet_index]\n        p2_loss_weight_gamma = self.p2_loss_weight_gamma[unet_index]\n        pred_objective       = self.pred_objectives[unet_index]\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n\n        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n\n        check_shape(images, 'b c ...', c = self.channels)\n        assert h >= target_image_size and w >= target_image_size\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2605-2655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2640, "start_line_no": 2615, "end_line_no": 2665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        cond_images = None,\n        **kwargs\n    ):\n        if self.is_video and images.ndim == 4:\n            images = rearrange(images, 'b c h w -> b c 1 h w')\n            kwargs.update(ignore_time = True)\n\n        assert images.shape[-1] == images.shape[-2], f'the images you pass in must be a square, but received dimensions of {images.shape[2]}, {images.shape[-1]}'\n        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n        unet_number = default(unet_number, 1)\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n\n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        noise_scheduler      = self.noise_schedulers[unet_index]\n        p2_loss_weight_gamma = self.p2_loss_weight_gamma[unet_index]\n        pred_objective       = self.pred_objectives[unet_index]\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n\n        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n\n        check_shape(images, 'b c ...', c = self.channels)\n        assert h >= target_image_size and w >= target_image_size\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        times = noise_scheduler.sample_random_times(b, device = device)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2615-2665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2650, "start_line_no": 2625, "end_line_no": 2675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n\n        images = cast_uint8_images_to_float(images)\n        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n\n        assert images.dtype == torch.float, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n\n        unet_index = unet_number - 1\n\n        unet = default(unet, lambda: self.get_unet(unet_number))\n\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        noise_scheduler      = self.noise_schedulers[unet_index]\n        p2_loss_weight_gamma = self.p2_loss_weight_gamma[unet_index]\n        pred_objective       = self.pred_objectives[unet_index]\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n\n        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n\n        check_shape(images, 'b c ...', c = self.channels)\n        assert h >= target_image_size and w >= target_image_size\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        times = noise_scheduler.sample_random_times(b, device = device)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2625-2675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2660, "start_line_no": 2635, "end_line_no": 2685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n\n        noise_scheduler      = self.noise_schedulers[unet_index]\n        p2_loss_weight_gamma = self.p2_loss_weight_gamma[unet_index]\n        pred_objective       = self.pred_objectives[unet_index]\n        target_image_size    = self.image_sizes[unet_index]\n        random_crop_size     = self.random_crop_sizes[unet_index]\n        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n\n        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n\n        check_shape(images, 'b c ...', c = self.channels)\n        assert h >= target_image_size and w >= target_image_size\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        times = noise_scheduler.sample_random_times(b, device = device)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video frame conditioning\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2635-2685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2670, "start_line_no": 2645, "end_line_no": 2695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        b, c, *_, h, w, device, is_video = *images.shape, images.device, images.ndim == 5\n\n        check_shape(images, 'b c ...', c = self.channels)\n        assert h >= target_image_size and w >= target_image_size\n\n        frames              = images.shape[2] if is_video else None\n        all_frame_dims      = tuple(safe_get_tuple_index(el, 0) for el in calc_all_frame_dims(self.temporal_downsample_factor, frames))\n        ignore_time         = kwargs.get('ignore_time', False)\n\n        target_frame_size   = all_frame_dims[unet_index] if is_video and not ignore_time else None\n        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        times = noise_scheduler.sample_random_times(b, device = device)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video frame conditioning\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # handle low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(b, device = device)\n            else:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2645-2695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2680, "start_line_no": 2655, "end_line_no": 2701, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        prev_frame_size     = all_frame_dims[unet_index - 1] if is_video and not ignore_time and unet_index > 0 else None\n        frames_to_resize_kwargs = lambda frames: dict(target_frames = frames) if exists(frames) else dict()\n\n        times = noise_scheduler.sample_random_times(b, device = device)\n\n        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n            assert all([*map(len, texts)]), 'text cannot be empty'\n            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n\n            with autocast(enabled = False):\n                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video frame conditioning\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # handle low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(b, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        return self.p_losses(unet, images, times, text_embeds = text_embeds, text_mask = text_masks, cond_images = cond_images, noise_scheduler = noise_scheduler, lowres_cond_img = lowres_cond_img, lowres_aug_times = lowres_aug_times, pred_objective = pred_objective, p2_loss_weight_gamma = p2_loss_weight_gamma, random_crop_size = random_crop_size, **kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2655-2701"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2690, "start_line_no": 2665, "end_line_no": 2701, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n\n            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n        if not self.unconditional:\n            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\n        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n\n        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video frame conditioning\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # handle low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(b, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        return self.p_losses(unet, images, times, text_embeds = text_embeds, text_mask = text_masks, cond_images = cond_images, noise_scheduler = noise_scheduler, lowres_cond_img = lowres_cond_img, lowres_aug_times = lowres_aug_times, pred_objective = pred_objective, p2_loss_weight_gamma = p2_loss_weight_gamma, random_crop_size = random_crop_size, **kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2665-2701"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_pytorch.py"], "line_no": 2700, "start_line_no": 2675, "end_line_no": 2701, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n\n        # handle video frame conditioning\n\n        if self.is_video and self.resize_cond_video_frames:\n            downsample_scale = self.temporal_downsample_factor[unet_index]\n            temporal_downsample_fn = partial(scale_video_time, downsample_scale = downsample_scale)\n            kwargs = maybe_transform_dict_key(kwargs, 'cond_video_frames', temporal_downsample_fn)\n            kwargs = maybe_transform_dict_key(kwargs, 'post_cond_video_frames', temporal_downsample_fn)\n\n        # handle low resolution conditioning\n\n        lowres_cond_img = lowres_aug_times = None\n        if exists(prev_image_size):\n            lowres_cond_img = self.resize_to(images, prev_image_size, **frames_to_resize_kwargs(prev_frame_size), clamp_range = self.input_image_range)\n            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, **frames_to_resize_kwargs(target_frame_size), clamp_range = self.input_image_range)\n\n            if self.per_sample_random_aug_noise_level:\n                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(b, device = device)\n            else:\n                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n\n        images = self.resize_to(images, target_image_size, **frames_to_resize_kwargs(target_frame_size))\n\n        return self.p_losses(unet, images, times, text_embeds = text_embeds, text_mask = text_masks, cond_images = cond_images, noise_scheduler = noise_scheduler, lowres_cond_img = lowres_cond_img, lowres_aug_times = lowres_aug_times, pred_objective = pred_objective, p2_loss_weight_gamma = p2_loss_weight_gamma, random_crop_size = random_crop_size, **kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_pytorch.py_2675-2701"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import math\nimport copy\nimport operator\nimport functools\nfrom typing import List\nfrom tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\nAST=Module(Import(alias)Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad)))FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(arg)FunctionDef(arguments(arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Name(Load)))Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from tqdm.auto import tqdm\nfrom functools import partial, wraps\nfrom contextlib import contextmanager, nullcontext\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n\nAST=Module(ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)Import(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(aliasaliasaliasaliasalias)ImportFrom(aliasalias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(aliasaliasalias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargarg)Return(Name(Load)))FunctionDef(arguments(argargConstant)If(Compare(Call(Name(Load)Name(Load))EqConstant)Return(Name(Load)))Return(Subscript(Name(Load)ConstantLoad)))FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(arg)FunctionDef(arguments(arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Name(Load)))Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Constant)FunctionDef(arguments(arg)NonlocalIf(Name(Load)Return)Assign(Name(Store)Constant)Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from einops import rearrange, repeat, reduce, pack, unpack\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops_exts import rearrange_many, repeat_many, check_shape\nfrom einops_exts.torch import EinopsToAndFrom\n\nfrom imagen_pytorch.t5 import t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return val is not None\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef first(arr, d = None):\n    if len(arr) == 0:\n        return d\n    return arr[0]\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x):\n        if not exists(x):\n            return x\n        return fn(x)\n    return inner\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n\nAST=Module(FunctionDef(arguments(argarg)Return(Compare(BinOp(Name(Load)ModName(Load))EqConstant)))FunctionDef(arguments(arg)FunctionDef(arguments(arg)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Name(Load)))Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Constant)FunctionDef(arguments(arg)NonlocalIf(Name(Load)Return)Assign(Name(Store)Constant)Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))Assign(Name(Store)IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)BinOp(Tuple(Name(Load)Load)MultCall(Name(Load)Name(Load)Constant))))If(Call(Name(Load)Name(Load))Assert(Compare(Call(Name(Load)Name(Load))EqName(Load))))Return(Name(Load)))FunctionDef(arguments(arg)If(UnaryOp(NotCompare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load)))Return(Name(Load)))Return(BinOp(Name(Load)DivConstant)))FunctionDef(arguments(arg)Return(Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n\nAST=Module(FunctionDef(arguments(arg)Assign(Name(Store)Constant)FunctionDef(arguments(arg)NonlocalIf(Name(Load)Return)Assign(Name(Store)Constant)Return(Call(Name(Load)Name(Load)))Call(Name(Load)Name(Load)))Return(Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))Assign(Name(Store)IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)BinOp(Tuple(Name(Load)Load)MultCall(Name(Load)Name(Load)Constant))))If(Call(Name(Load)Name(Load))Assert(Compare(Call(Name(Load)Name(Load))EqName(Load))))Return(Name(Load)))FunctionDef(arguments(arg)If(UnaryOp(NotCompare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load)))Return(Name(Load)))Return(BinOp(Name(Load)DivConstant)))FunctionDef(arguments(arg)Return(Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))If(Call(Name(Load)Attribute(Name(Load)Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))))FunctionDef(arguments(arg)FunctionDef(arguments(argargarg)Assign(Name(Store)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return inner\n\nprint_once = once(print)\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = None):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if isinstance(val, list):\n        val = tuple(val)\n\n    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n\n    if exists(length):\n        assert len(output) == length\n\n    return output\n\ndef cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def cast_uint8_images_to_float(images):\n    if not images.dtype == torch.uint8:\n        return images\n    return images / 255\n\ndef module_device(module):\n    return next(module.parameters()).device\n\ndef zero_init_(m):\n    nn.init.zeros_(m.weight)\n    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\nAST=Module(FunctionDef(arguments(arg)If(UnaryOp(NotCompare(Attribute(Name(Load)Load)EqAttribute(Name(Load)Load)))Return(Name(Load)))Return(BinOp(Name(Load)DivConstant)))FunctionDef(arguments(arg)Return(Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load)))FunctionDef(arguments(arg)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))If(Call(Name(Load)Attribute(Name(Load)Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Name(Load)Load)))))FunctionDef(arguments(arg)FunctionDef(arguments(argargarg)Assign(Name(Store)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))Return(Name(Load)))FunctionDef(arguments(argargargConstant)Assign(Name(Store)BinOp(Name(Load)SubCall(Name(Load)Name(Load))))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Tuple(Starred(Name(Load)Load)Starred(BinOp(Tuple(Name(Load)Load)MultName(Load))Load)Load)))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load))))FunctionDef(arguments(argargargarg)Return(Name(Load))))FunctionDef(arguments(argarg(Name(Load))Constant)Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Name(Load))))))FunctionDef(arguments(arg)Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)SubAttribute(Name(Load)Load)))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Call(Attribute(Name(Load)Load)Starred(Attribute(Name(Load)Load)Load)Starred(BinOp(Tuple(ConstantLoad)MultName(Load))Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if exists(m.bias):\n        nn.init.zeros_(m.bias)\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return inner\n\ndef pad_tuple_to_length(t, length, fillvalue = None):\n    remain_length = length - len(t)\n    if remain_length <= 0:\n        return t\n    return (*t, *((fillvalue,) * remain_length))\n\n# helper classes\n\nclass Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_video_to(\n    video,\n    target_image_size,\n    target_frames = None,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_video_size = video.shape[-1]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_95-145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class Identity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n# tensor helpers\n\ndef log(t, eps: float = 1e-12):\n    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_video_to(\n    video,\n    target_image_size,\n    target_frames = None,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_video_size = video.shape[-1]\n\n    frames = video.shape[2]\n    target_frames = default(target_frames, frames)\n\n    target_shape = (target_frames, target_image_size, target_image_size)\n\n    if tuple(video.shape[-3:]) == target_shape:\n        return video\n\n    out = F.interpolate(video, target_shape, mode = mode)\n\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargarg)Expr(Call(Attribute(Call(Name(Load))Load))))FunctionDef(arguments(argargargarg)Return(Name(Load))))FunctionDef(arguments(argarg(Name(Load))Constant)Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Name(Load))))))FunctionDef(arguments(arg)Return(Call(Attribute(Name(Load)Load)Name(Load)keyword(UnaryOp(USubConstant)))))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)SubAttribute(Name(Load)Load)))If(Compare(Name(Load)LtEConstant)Return(Name(Load)))Return(Call(Attribute(Name(Load)Load)Starred(Attribute(Name(Load)Load)Load)Starred(BinOp(Tuple(ConstantLoad)MultName(Load))Load))))FunctionDef(arguments(argargargConstant)If(UnaryOp(NotCall(Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)keyword(Name(Load)))))Assign(Name(Store)Call(Attribute(Name(Load)Load)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)UnaryOp(InvertName(Load))Constant))Return(BinOp(Call(Attribute(Name(Load)Load)keyword(Name(Load)))DivCall(Attribute(Name(Load)Load)keyword(Constant)))))FunctionDef(arguments(argargargargargConstantConstantConstant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Tuple(Name(Load)Name(Load)Name(Load)Load))If(Compare(Call(Name(Load)Subscript(Attribute(Name(Load)Load)Slice(UnaryOp(USubConstant))Load))EqName(Load))Return(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)Name(Load)keyword(Name(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_105-155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    return torch.log(t.clamp(min = eps))\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\ndef right_pad_dims_to(x, t):\n    padding_dims = x.ndim - t.ndim\n    if padding_dims <= 0:\n        return t\n    return t.view(*t.shape, *((1,) * padding_dims))\n\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_video_to(\n    video,\n    target_image_size,\n    target_frames = None,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_video_size = video.shape[-1]\n\n    frames = video.shape[2]\n    target_frames = default(target_frames, frames)\n\n    target_shape = (target_frames, target_image_size, target_image_size)\n\n    if tuple(video.shape[-3:]) == target_shape:\n        return video\n\n    out = F.interpolate(video, target_shape, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n        \n    return out\n\ndef scale_video_time(\n    video,\n    downsample_scale = 1,\n    mode = 'nearest'\n):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_115-165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef masked_mean(t, *, dim, mask = None):\n    if not exists(mask):\n        return t.mean(dim = dim)\n\n    denom = mask.sum(dim = dim, keepdim = True)\n    mask = rearrange(mask, 'b n -> b n 1')\n    masked_t = t.masked_fill(~mask, 0.)\n\n    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)\n\ndef resize_video_to(\n    video,\n    target_image_size,\n    target_frames = None,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_video_size = video.shape[-1]\n\n    frames = video.shape[2]\n    target_frames = default(target_frames, frames)\n\n    target_shape = (target_frames, target_image_size, target_image_size)\n\n    if tuple(video.shape[-3:]) == target_shape:\n        return video\n\n    out = F.interpolate(video, target_shape, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n        \n    return out\n\ndef scale_video_time(\n    video,\n    downsample_scale = 1,\n    mode = 'nearest'\n):\n    if downsample_scale == 1:\n        return video\n\n    image_size, frames = video.shape[-1], video.shape[-3]\n    assert divisible_by(frames, downsample_scale), f'trying to temporally downsample a conditioning video frames of length {frames} by {downsample_scale}, however it is not neatly divisible'\n\n    target_frames = frames // downsample_scale\n\n    resized_video = resize_video_to(\n        video,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_125-175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef resize_video_to(\n    video,\n    target_image_size,\n    target_frames = None,\n    clamp_range = None,\n    mode = 'nearest'\n):\n    orig_video_size = video.shape[-1]\n\n    frames = video.shape[2]\n    target_frames = default(target_frames, frames)\n\n    target_shape = (target_frames, target_image_size, target_image_size)\n\n    if tuple(video.shape[-3:]) == target_shape:\n        return video\n\n    out = F.interpolate(video, target_shape, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n        \n    return out\n\ndef scale_video_time(\n    video,\n    downsample_scale = 1,\n    mode = 'nearest'\n):\n    if downsample_scale == 1:\n        return video\n\n    image_size, frames = video.shape[-1], video.shape[-3]\n    assert divisible_by(frames, downsample_scale), f'trying to temporally downsample a conditioning video frames of length {frames} by {downsample_scale}, however it is not neatly divisible'\n\n    target_frames = frames // downsample_scale\n\n    resized_video = resize_video_to(\n        video,\n        image_size,\n        target_frames = target_frames,\n        mode = mode\n    )\n\n    return resized_video\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_135-185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    frames = video.shape[2]\n    target_frames = default(target_frames, frames)\n\n    target_shape = (target_frames, target_image_size, target_image_size)\n\n    if tuple(video.shape[-3:]) == target_shape:\n        return video\n\n    out = F.interpolate(video, target_shape, mode = mode)\n\n    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n        \n    return out\n\ndef scale_video_time(\n    video,\n    downsample_scale = 1,\n    mode = 'nearest'\n):\n    if downsample_scale == 1:\n        return video\n\n    image_size, frames = video.shape[-1], video.shape[-3]\n    assert divisible_by(frames, downsample_scale), f'trying to temporally downsample a conditioning video frames of length {frames} by {downsample_scale}, however it is not neatly divisible'\n\n    target_frames = frames // downsample_scale\n\n    resized_video = resize_video_to(\n        video,\n        image_size,\n        target_frames = target_frames,\n        mode = mode\n    )\n\n    return resized_video\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_145-195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if exists(clamp_range):\n        out = out.clamp(*clamp_range)\n        \n    return out\n\ndef scale_video_time(\n    video,\n    downsample_scale = 1,\n    mode = 'nearest'\n):\n    if downsample_scale == 1:\n        return video\n\n    image_size, frames = video.shape[-1], video.shape[-3]\n    assert divisible_by(frames, downsample_scale), f'trying to temporally downsample a conditioning video frames of length {frames} by {downsample_scale}, however it is not neatly divisible'\n\n    target_frames = frames // downsample_scale\n\n    resized_video = resize_video_to(\n        video,\n        image_size,\n        target_frames = target_frames,\n        mode = mode\n    )\n\n    return resized_video\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_155-205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if downsample_scale == 1:\n        return video\n\n    image_size, frames = video.shape[-1], video.shape[-3]\n    assert divisible_by(frames, downsample_scale), f'trying to temporally downsample a conditioning video frames of length {frames} by {downsample_scale}, however it is not neatly divisible'\n\n    target_frames = frames // downsample_scale\n\n    resized_video = resize_video_to(\n        video,\n        image_size,\n        target_frames = target_frames,\n        mode = mode\n    )\n\n    return resized_video\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_165-215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        image_size,\n        target_frames = target_frames,\n        mode = mode\n    )\n\n    return resized_video\n\n# classifier free guidance functions\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass Always():", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_175-225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\n# norms and residuals\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_185-235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = -1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_195-245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = -1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, stable = False):\n        super().__init__()\n        self.stable = stable\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n\n    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_205-255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(self, x):\n        if self.stable:\n            x = x / x.amax(dim = 1, keepdim = True).detach()\n\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass Always():\n    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_215-265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def __init__(self, val):\n        self.val = val\n\n    def __call__(self, *args, **kwargs):\n        return self.val\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_225-275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass Parallel(nn.Module):\n    def __init__(self, *fns):\n        super().__init__()\n        self.fns = nn.ModuleList(fns)\n\n    def forward(self, x):\n        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_235-285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        outputs = [fn(x) for fn in self.fns]\n        return sum(outputs)\n\n# attention pooling\n\nclass PerceiverAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_245-295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dim_head = 64,\n        heads = 8,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_255-305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.norm = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_265-315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            nn.Linear(inner_dim, dim, bias = False),\n            nn.LayerNorm(dim)\n        )\n\n    def forward(self, x, latents, mask = None):\n        x = self.norm(x)\n        latents = self.norm_latents(latents)\n\n        b, h = x.shape[0], self.heads\n\n        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_275-325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        q = self.to_q(latents)\n\n        # the paper differs from Perceiver in which they also concat the key / values derived from the latents to be attended to\n        kv_input = torch.cat((x, latents), dim = -2)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_285-335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities and masking\n\n        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.scale\n\n        if exists(mask):\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_295-345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('... i j, ... j d -> ... i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_305-355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return self.to_out(out)\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_315-365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        num_latents = 64,\n        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n        max_seq_len = 512,\n        ff_mult = 4\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# main contribution from make-a-video - pseudo conv3d\n# axial space-time convolutions, but made causal to keep in line with the design decisions of imagen-video paper\n\nclass Conv3d(nn.Module):\n    def __init__(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_325-375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_latents_from_mean_pooled_seq = None\n\n        if num_latents_mean_pooled > 0:\n            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n                LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n            )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# main contribution from make-a-video - pseudo conv3d\n# axial space-time convolutions, but made causal to keep in line with the design decisions of imagen-video paper\n\nclass Conv3d(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        kernel_size = 3,\n        *,\n        temporal_kernel_size = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_out = default(dim_out, dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_335-385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, mask = None):\n        n, device = x.shape[1], x.device\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n\n        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# main contribution from make-a-video - pseudo conv3d\n# axial space-time convolutions, but made causal to keep in line with the design decisions of imagen-video paper\n\nclass Conv3d(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        kernel_size = 3,\n        *,\n        temporal_kernel_size = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        temporal_kernel_size = default(temporal_kernel_size, kernel_size)\n\n        self.spatial_conv = nn.Conv2d(dim, dim_out, kernel_size = kernel_size, padding = kernel_size // 2)\n        self.temporal_conv = nn.Conv1d(dim_out, dim_out, kernel_size = temporal_kernel_size) if kernel_size > 1 else None\n        self.kernel_size = kernel_size\n\n        if exists(self.temporal_conv):\n            nn.init.dirac_(self.temporal_conv.weight.data) # initialized to be identity\n            nn.init.zeros_(self.temporal_conv.bias.data)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_345-395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        x_with_pos = x + pos_emb\n\n        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n\n        if exists(self.to_latents_from_mean_pooled_seq):\n            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n\n        for attn, ff in self.layers:\n            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# main contribution from make-a-video - pseudo conv3d\n# axial space-time convolutions, but made causal to keep in line with the design decisions of imagen-video paper\n\nclass Conv3d(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        kernel_size = 3,\n        *,\n        temporal_kernel_size = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        temporal_kernel_size = default(temporal_kernel_size, kernel_size)\n\n        self.spatial_conv = nn.Conv2d(dim, dim_out, kernel_size = kernel_size, padding = kernel_size // 2)\n        self.temporal_conv = nn.Conv1d(dim_out, dim_out, kernel_size = temporal_kernel_size) if kernel_size > 1 else None\n        self.kernel_size = kernel_size\n\n        if exists(self.temporal_conv):\n            nn.init.dirac_(self.temporal_conv.weight.data) # initialized to be identity\n            nn.init.zeros_(self.temporal_conv.bias.data)\n\n    def forward(\n        self,\n        x,\n        ignore_time = False\n    ):\n        b, c, *_, h, w = x.shape\n\n        is_video = x.ndim == 5\n        ignore_time &= is_video\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_355-405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            latents = attn(x_with_pos, latents, mask = mask) + latents\n            latents = ff(latents) + latents\n\n        return latents\n\n# main contribution from make-a-video - pseudo conv3d\n# axial space-time convolutions, but made causal to keep in line with the design decisions of imagen-video paper\n\nclass Conv3d(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        kernel_size = 3,\n        *,\n        temporal_kernel_size = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        temporal_kernel_size = default(temporal_kernel_size, kernel_size)\n\n        self.spatial_conv = nn.Conv2d(dim, dim_out, kernel_size = kernel_size, padding = kernel_size // 2)\n        self.temporal_conv = nn.Conv1d(dim_out, dim_out, kernel_size = temporal_kernel_size) if kernel_size > 1 else None\n        self.kernel_size = kernel_size\n\n        if exists(self.temporal_conv):\n            nn.init.dirac_(self.temporal_conv.weight.data) # initialized to be identity\n            nn.init.zeros_(self.temporal_conv.bias.data)\n\n    def forward(\n        self,\n        x,\n        ignore_time = False\n    ):\n        b, c, *_, h, w = x.shape\n\n        is_video = x.ndim == 5\n        ignore_time &= is_video\n\n        if is_video:\n            x = rearrange(x, 'b c f h w -> (b f) c h w')\n\n        x = self.spatial_conv(x)\n\n        if is_video:\n            x = rearrange(x, '(b f) c h w -> b c f h w', b = b)\n\n        if ignore_time or not exists(self.temporal_conv):\n            return x", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_365-415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self,\n        dim,\n        dim_out = None,\n        kernel_size = 3,\n        *,\n        temporal_kernel_size = None,\n        **kwargs\n    ):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        temporal_kernel_size = default(temporal_kernel_size, kernel_size)\n\n        self.spatial_conv = nn.Conv2d(dim, dim_out, kernel_size = kernel_size, padding = kernel_size // 2)\n        self.temporal_conv = nn.Conv1d(dim_out, dim_out, kernel_size = temporal_kernel_size) if kernel_size > 1 else None\n        self.kernel_size = kernel_size\n\n        if exists(self.temporal_conv):\n            nn.init.dirac_(self.temporal_conv.weight.data) # initialized to be identity\n            nn.init.zeros_(self.temporal_conv.bias.data)\n\n    def forward(\n        self,\n        x,\n        ignore_time = False\n    ):\n        b, c, *_, h, w = x.shape\n\n        is_video = x.ndim == 5\n        ignore_time &= is_video\n\n        if is_video:\n            x = rearrange(x, 'b c f h w -> (b f) c h w')\n\n        x = self.spatial_conv(x)\n\n        if is_video:\n            x = rearrange(x, '(b f) c h w -> b c f h w', b = b)\n\n        if ignore_time or not exists(self.temporal_conv):\n            return x\n\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n\n        # causal temporal convolution - time is causal in imagen-video\n\n        if self.kernel_size > 1:\n            x = F.pad(x, (self.kernel_size - 1, 0))\n\n        x = self.temporal_conv(x)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_375-425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        temporal_kernel_size = default(temporal_kernel_size, kernel_size)\n\n        self.spatial_conv = nn.Conv2d(dim, dim_out, kernel_size = kernel_size, padding = kernel_size // 2)\n        self.temporal_conv = nn.Conv1d(dim_out, dim_out, kernel_size = temporal_kernel_size) if kernel_size > 1 else None\n        self.kernel_size = kernel_size\n\n        if exists(self.temporal_conv):\n            nn.init.dirac_(self.temporal_conv.weight.data) # initialized to be identity\n            nn.init.zeros_(self.temporal_conv.bias.data)\n\n    def forward(\n        self,\n        x,\n        ignore_time = False\n    ):\n        b, c, *_, h, w = x.shape\n\n        is_video = x.ndim == 5\n        ignore_time &= is_video\n\n        if is_video:\n            x = rearrange(x, 'b c f h w -> (b f) c h w')\n\n        x = self.spatial_conv(x)\n\n        if is_video:\n            x = rearrange(x, '(b f) c h w -> b c f h w', b = b)\n\n        if ignore_time or not exists(self.temporal_conv):\n            return x\n\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n\n        # causal temporal convolution - time is causal in imagen-video\n\n        if self.kernel_size > 1:\n            x = F.pad(x, (self.kernel_size - 1, 0))\n\n        x = self.temporal_conv(x)\n\n        x = rearrange(x, '(b h w) c f -> b c f h w', h = h, w = w)\n\n        return x\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_385-435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(\n        self,\n        x,\n        ignore_time = False\n    ):\n        b, c, *_, h, w = x.shape\n\n        is_video = x.ndim == 5\n        ignore_time &= is_video\n\n        if is_video:\n            x = rearrange(x, 'b c f h w -> (b f) c h w')\n\n        x = self.spatial_conv(x)\n\n        if is_video:\n            x = rearrange(x, '(b f) c h w -> b c f h w', b = b)\n\n        if ignore_time or not exists(self.temporal_conv):\n            return x\n\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n\n        # causal temporal convolution - time is causal in imagen-video\n\n        if self.kernel_size > 1:\n            x = F.pad(x, (self.kernel_size - 1, 0))\n\n        x = self.temporal_conv(x)\n\n        x = rearrange(x, '(b h w) c f -> b c f h w', h = h, w = w)\n\n        return x\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        context_dim = None,\n        rel_pos_bias = False,\n        rel_pos_bias_mlp_depth = 2,\n        init_zero = False,\n        scale = 8\n    ):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_395-445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if is_video:\n            x = rearrange(x, 'b c f h w -> (b f) c h w')\n\n        x = self.spatial_conv(x)\n\n        if is_video:\n            x = rearrange(x, '(b f) c h w -> b c f h w', b = b)\n\n        if ignore_time or not exists(self.temporal_conv):\n            return x\n\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n\n        # causal temporal convolution - time is causal in imagen-video\n\n        if self.kernel_size > 1:\n            x = F.pad(x, (self.kernel_size - 1, 0))\n\n        x = self.temporal_conv(x)\n\n        x = rearrange(x, '(b h w) c f -> b c f h w', h = h, w = w)\n\n        return x\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        context_dim = None,\n        rel_pos_bias = False,\n        rel_pos_bias_mlp_depth = 2,\n        init_zero = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n\n        self.rel_pos_bias = DynamicPositionBias(dim = dim, heads = heads, depth = rel_pos_bias_mlp_depth) if rel_pos_bias else None\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_405-455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n\n        # causal temporal convolution - time is causal in imagen-video\n\n        if self.kernel_size > 1:\n            x = F.pad(x, (self.kernel_size - 1, 0))\n\n        x = self.temporal_conv(x)\n\n        x = rearrange(x, '(b h w) c f -> b c f h w', h = h, w = w)\n\n        return x\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        context_dim = None,\n        rel_pos_bias = False,\n        rel_pos_bias_mlp_depth = 2,\n        init_zero = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n\n        self.rel_pos_bias = DynamicPositionBias(dim = dim, heads = heads, depth = rel_pos_bias_mlp_depth) if rel_pos_bias else None\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_attn_bias = nn.Parameter(torch.randn(heads))\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_415-465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        x = rearrange(x, '(b h w) c f -> b c f h w', h = h, w = w)\n\n        return x\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        context_dim = None,\n        rel_pos_bias = False,\n        rel_pos_bias_mlp_depth = 2,\n        init_zero = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n\n        self.rel_pos_bias = DynamicPositionBias(dim = dim, heads = heads, depth = rel_pos_bias_mlp_depth) if rel_pos_bias else None\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_attn_bias = nn.Parameter(torch.randn(heads))\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n        if init_zero:\n            nn.init.zeros_(self.to_out[-1].g)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_425-475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        context_dim = None,\n        rel_pos_bias = False,\n        rel_pos_bias_mlp_depth = 2,\n        init_zero = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n\n        self.rel_pos_bias = DynamicPositionBias(dim = dim, heads = heads, depth = rel_pos_bias_mlp_depth) if rel_pos_bias else None\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_attn_bias = nn.Parameter(torch.randn(heads))\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n        if init_zero:\n            nn.init.zeros_(self.to_out[-1].g)\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        attn_bias = None\n    ):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_435-485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        super().__init__()\n        self.scale = scale\n        self.causal = causal\n\n        self.rel_pos_bias = DynamicPositionBias(dim = dim, heads = heads, depth = rel_pos_bias_mlp_depth) if rel_pos_bias else None\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = LayerNorm(dim)\n\n        self.null_attn_bias = nn.Parameter(torch.randn(heads))\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n        if init_zero:\n            nn.init.zeros_(self.to_out[-1].g)\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        attn_bias = None\n    ):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_445-495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.null_attn_bias = nn.Parameter(torch.randn(heads))\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n        if init_zero:\n            nn.init.zeros_(self.to_out[-1].g)\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        attn_bias = None\n    ):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_455-505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n        if init_zero:\n            nn.init.zeros_(self.to_out[-1].g)\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        attn_bias = None\n    ):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_465-515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        attn_bias = None\n    ):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if not exists(attn_bias) and exists(self.rel_pos_bias):\n            attn_bias = self.rel_pos_bias(n, device = device, dtype = q.dtype)\n\n        if exists(attn_bias):\n            null_attn_bias = repeat(self.null_attn_bias, 'h -> h n 1', n = n)\n            attn_bias = torch.cat((null_attn_bias, attn_bias), dim = -1)\n            sim = sim + attn_bias\n\n        # masking\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_475-525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if not exists(attn_bias) and exists(self.rel_pos_bias):\n            attn_bias = self.rel_pos_bias(n, device = device, dtype = q.dtype)\n\n        if exists(attn_bias):\n            null_attn_bias = repeat(self.null_attn_bias, 'h -> h n 1', n = n)\n            attn_bias = torch.cat((null_attn_bias, attn_bias), dim = -1)\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_485-535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # add text conditioning, if present\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            k = torch.cat((ck, k), dim = -2)\n            v = torch.cat((cv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if not exists(attn_bias) and exists(self.rel_pos_bias):\n            attn_bias = self.rel_pos_bias(n, device = device, dtype = q.dtype)\n\n        if exists(attn_bias):\n            null_attn_bias = repeat(self.null_attn_bias, 'h -> h n 1', n = n)\n            attn_bias = torch.cat((null_attn_bias, attn_bias), dim = -1)\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_495-545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # calculate query / key similarities\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.scale\n\n        # relative positional encoding (T5 style)\n\n        if not exists(attn_bias) and exists(self.rel_pos_bias):\n            attn_bias = self.rel_pos_bias(n, device = device, dtype = q.dtype)\n\n        if exists(attn_bias):\n            null_attn_bias = repeat(self.null_attn_bias, 'h -> h n 1', n = n)\n            attn_bias = torch.cat((null_attn_bias, attn_bias), dim = -1)\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# pseudo conv2d that uses conv3d but with kernel size of 1 across frames dimension\n\ndef Conv2d(dim_in, dim_out, kernel, stride = 1, padding = 0, **kwargs):\n    kernel = cast_tuple(kernel, 2)\n    stride = cast_tuple(stride, 2)\n    padding = cast_tuple(padding, 2)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_505-555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if not exists(attn_bias) and exists(self.rel_pos_bias):\n            attn_bias = self.rel_pos_bias(n, device = device, dtype = q.dtype)\n\n        if exists(attn_bias):\n            null_attn_bias = repeat(self.null_attn_bias, 'h -> h n 1', n = n)\n            attn_bias = torch.cat((null_attn_bias, attn_bias), dim = -1)\n            sim = sim + attn_bias\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# pseudo conv2d that uses conv3d but with kernel size of 1 across frames dimension\n\ndef Conv2d(dim_in, dim_out, kernel, stride = 1, padding = 0, **kwargs):\n    kernel = cast_tuple(kernel, 2)\n    stride = cast_tuple(stride, 2)\n    padding = cast_tuple(padding, 2)\n\n    if len(kernel) == 2:\n        kernel = (1, *kernel)\n\n    if len(stride) == 2:\n        stride = (1, *stride)\n\n    if len(padding) == 2:\n        padding = (0, *padding)\n\n    return nn.Conv3d(dim_in, dim_out, kernel, stride = stride, padding = padding, **kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_515-565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# pseudo conv2d that uses conv3d but with kernel size of 1 across frames dimension\n\ndef Conv2d(dim_in, dim_out, kernel, stride = 1, padding = 0, **kwargs):\n    kernel = cast_tuple(kernel, 2)\n    stride = cast_tuple(stride, 2)\n    padding = cast_tuple(padding, 2)\n\n    if len(kernel) == 2:\n        kernel = (1, *kernel)\n\n    if len(stride) == 2:\n        stride = (1, *stride)\n\n    if len(padding) == 2:\n        padding = (0, *padding)\n\n    return nn.Conv3d(dim_in, dim_out, kernel, stride = stride, padding = padding, **kwargs)\n\nclass Pad(nn.Module):\n    def __init__(self, padding, value = 0.):\n        super().__init__()\n        self.padding = padding\n        self.value = value\n\n    def forward(self, x):\n        return F.pad(x, self.padding, value = self.value)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_525-575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            sim = sim.masked_fill(~mask, max_neg_value)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n\n        # aggregate values\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# pseudo conv2d that uses conv3d but with kernel size of 1 across frames dimension\n\ndef Conv2d(dim_in, dim_out, kernel, stride = 1, padding = 0, **kwargs):\n    kernel = cast_tuple(kernel, 2)\n    stride = cast_tuple(stride, 2)\n    padding = cast_tuple(padding, 2)\n\n    if len(kernel) == 2:\n        kernel = (1, *kernel)\n\n    if len(stride) == 2:\n        stride = (1, *stride)\n\n    if len(padding) == 2:\n        padding = (0, *padding)\n\n    return nn.Conv3d(dim_in, dim_out, kernel, stride = stride, padding = padding, **kwargs)\n\nclass Pad(nn.Module):\n    def __init__(self, padding, value = 0.):\n        super().__init__()\n        self.padding = padding\n        self.value = value\n\n    def forward(self, x):\n        return F.pad(x, self.padding, value = self.value)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        Conv2d(dim, dim_out, 3, padding = 1)\n    )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_535-585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# pseudo conv2d that uses conv3d but with kernel size of 1 across frames dimension\n\ndef Conv2d(dim_in, dim_out, kernel, stride = 1, padding = 0, **kwargs):\n    kernel = cast_tuple(kernel, 2)\n    stride = cast_tuple(stride, 2)\n    padding = cast_tuple(padding, 2)\n\n    if len(kernel) == 2:\n        kernel = (1, *kernel)\n\n    if len(stride) == 2:\n        stride = (1, *stride)\n\n    if len(padding) == 2:\n        padding = (0, *padding)\n\n    return nn.Conv3d(dim_in, dim_out, kernel, stride = stride, padding = padding, **kwargs)\n\nclass Pad(nn.Module):\n    def __init__(self, padding, value = 0.):\n        super().__init__()\n        self.padding = padding\n        self.value = value\n\n    def forward(self, x):\n        return F.pad(x, self.padding, value = self.value)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_545-595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if len(kernel) == 2:\n        kernel = (1, *kernel)\n\n    if len(stride) == 2:\n        stride = (1, *stride)\n\n    if len(padding) == 2:\n        padding = (0, *padding)\n\n    return nn.Conv3d(dim_in, dim_out, kernel, stride = stride, padding = padding, **kwargs)\n\nclass Pad(nn.Module):\n    def __init__(self, padding, value = 0.):\n        super().__init__()\n        self.padding = padding\n        self.value = value\n\n    def forward(self, x):\n        return F.pad(x, self.padding, value = self.value)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, f, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_555-605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass Pad(nn.Module):\n    def __init__(self, padding, value = 0.):\n        super().__init__()\n        self.padding = padding\n        self.value = value\n\n    def forward(self, x):\n        return F.pad(x, self.padding, value = self.value)\n\n# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, f, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        out = self.net(x)\n        frames = x.shape[2]\n        out = rearrange(out, 'b c f h w -> (b f) c h w')\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b f) c h w -> b c f h w', f = frames)\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load)Attribute(Name(Load)Load)keyword(Attribute(Name(Load)Load))))))FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Name(Load)Name(Load)Name(Load)Constantkeyword(Constant)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)BinOp(Name(Load)MultConstant)Constant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Attribute(Name(Load)Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)FloorDivConstant)Name(Load)Name(Load)Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Call(Name(Load)Name(Load)Constantkeyword(Name(Load)))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_565-615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "# decoder\n\ndef Upsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        Conv2d(dim, dim_out, 3, padding = 1)\n    )\n\nclass PixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, f, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        out = self.net(x)\n        frames = x.shape[2]\n        out = rearrange(out, 'b c f h w -> (b f) c h w')\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b f) c h w -> b c f h w', f = frames)\n\ndef Downsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c f (h p1) (w p2) -> b (c p1 p2) f h w', p1 = 2, p2 = 2),\n        Conv2d(dim * 4, dim_out, 1)\n    )\n\n# temporal up and downsamples\n\n\nAST=Module(FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)keyword(Constant)keyword(Constant))Call(Name(Load)Name(Load)Name(Load)Constantkeyword(Constant)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)BinOp(Name(Load)MultConstant)Constant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Attribute(Name(Load)Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Constant))Expr(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)FloorDivConstant)Name(Load)Name(Load)Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Subscript(Attribute(Name(Load)Load)ConstantLoad))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Call(Name(Load)Name(Load)Constantkeyword(Name(Load))))))FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Constant)keyword(Constant))Call(Name(Load)BinOp(Name(Load)MultConstant)Name(Load)Constant)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_575-625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class PixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        conv = Conv2d(dim, dim_out * 4, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, f, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        out = self.net(x)\n        frames = x.shape[2]\n        out = rearrange(out, 'b c f h w -> (b f) c h w')\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b f) c h w -> b c f h w', f = frames)\n\ndef Downsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c f (h p1) (w p2) -> b (c p1 p2) f h w', p1 = 2, p2 = 2),\n        Conv2d(dim * 4, dim_out, 1)\n    )\n\n# temporal up and downsamples\n\nclass TemporalPixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None, stride = 2):\n        super().__init__()\n        self.stride = stride\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv1d(dim, dim_out * stride, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_585-635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f, h, w = conv.weight.shape\n        conv_weight = torch.empty(o // 4, i, f, h, w)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        out = self.net(x)\n        frames = x.shape[2]\n        out = rearrange(out, 'b c f h w -> (b f) c h w')\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b f) c h w -> b c f h w', f = frames)\n\ndef Downsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c f (h p1) (w p2) -> b (c p1 p2) f h w', p1 = 2, p2 = 2),\n        Conv2d(dim * 4, dim_out, 1)\n    )\n\n# temporal up and downsamples\n\nclass TemporalPixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None, stride = 2):\n        super().__init__()\n        self.stride = stride\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv1d(dim, dim_out * stride, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = Rearrange('b (c r) n -> b c (n r)', r = stride)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f = conv.weight.shape\n        conv_weight = torch.empty(o // self.stride, i, f)\n        nn.init.kaiming_uniform_(conv_weight)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_595-645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        out = self.net(x)\n        frames = x.shape[2]\n        out = rearrange(out, 'b c f h w -> (b f) c h w')\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b f) c h w -> b c f h w', f = frames)\n\ndef Downsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c f (h p1) (w p2) -> b (c p1 p2) f h w', p1 = 2, p2 = 2),\n        Conv2d(dim * 4, dim_out, 1)\n    )\n\n# temporal up and downsamples\n\nclass TemporalPixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None, stride = 2):\n        super().__init__()\n        self.stride = stride\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv1d(dim, dim_out * stride, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = Rearrange('b (c r) n -> b c (n r)', r = stride)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f = conv.weight.shape\n        conv_weight = torch.empty(o // self.stride, i, f)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o r) ...', r = self.stride)\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        b, c, f, h, w = x.shape\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n        out = self.net(x)\n        out = self.pixel_shuffle(out)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_605-655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef Downsample(dim, dim_out = None):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c f (h p1) (w p2) -> b (c p1 p2) f h w', p1 = 2, p2 = 2),\n        Conv2d(dim * 4, dim_out, 1)\n    )\n\n# temporal up and downsamples\n\nclass TemporalPixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None, stride = 2):\n        super().__init__()\n        self.stride = stride\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv1d(dim, dim_out * stride, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = Rearrange('b (c r) n -> b c (n r)', r = stride)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f = conv.weight.shape\n        conv_weight = torch.empty(o // self.stride, i, f)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o r) ...', r = self.stride)\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        b, c, f, h, w = x.shape\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n        out = self.net(x)\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b h w) c f -> b c f h w', h = h, w = w)\n\ndef TemporalDownsample(dim, dim_out = None, stride = 2):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (f p) h w -> b (c p) f h w', p = stride),\n        Conv2d(dim * stride, dim_out, 1)\n    )\n\n# positional embedding\n\nAST=Module(FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Constant)keyword(Constant))Call(Name(Load)BinOp(Name(Load)MultConstant)Name(Load)Constant))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)MultName(Load))Constant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Attribute(Name(Load)Load))))Assign(Attribute(Name(Load)Store)Call(Name(Load)Constantkeyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)FloorDivAttribute(Name(Load)Load))Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Constantkeyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Call(Name(Load)Name(Load)Constantkeyword(Name(Load))keyword(Name(Load))))))FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Name(Load)))Call(Name(Load)BinOp(Name(Load)MultName(Load))Name(Load)Constant)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_615-665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class TemporalPixelShuffleUpsample(nn.Module):\n    def __init__(self, dim, dim_out = None, stride = 2):\n        super().__init__()\n        self.stride = stride\n        dim_out = default(dim_out, dim)\n        conv = nn.Conv1d(dim, dim_out * stride, 1)\n\n        self.net = nn.Sequential(\n            conv,\n            nn.SiLU()\n        )\n\n        self.pixel_shuffle = Rearrange('b (c r) n -> b c (n r)', r = stride)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f = conv.weight.shape\n        conv_weight = torch.empty(o // self.stride, i, f)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o r) ...', r = self.stride)\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        b, c, f, h, w = x.shape\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n        out = self.net(x)\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b h w) c f -> b c f h w', h = h, w = w)\n\ndef TemporalDownsample(dim, dim_out = None, stride = 2):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (f p) h w -> b (c p) f h w', p = stride),\n        Conv2d(dim * stride, dim_out, 1)\n    )\n\n# positional embedding\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)BinOp(Name(Load)MultName(Load))Constant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Name(Load)Call(Attribute(Name(Load)Load))))Assign(Attribute(Name(Load)Store)Call(Name(Load)Constantkeyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Store)Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Name(Load)FloorDivAttribute(Name(Load)Load))Name(Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Constantkeyword(Attribute(Name(Load)Load))))Expr(Call(Attribute(Attribute(Attribute(Name(Load)Load)Load)Load)Name(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Attribute(Attribute(Name(Load)Load)Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Name(Store)Name(Store)Name(Store)Store)Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Call(Name(Load)Name(Load)Constantkeyword(Name(Load))keyword(Name(Load))))))FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Name(Load)Name(Load)Name(Load)))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Constantkeyword(Name(Load)))Call(Name(Load)BinOp(Name(Load)MultName(Load))Name(Load)Constant))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)FloorDivConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Constant)DivBinOp(Name(Load)SubConstant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))MultUnaryOp(USubName(Load))))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_625-675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n        self.pixel_shuffle = Rearrange('b (c r) n -> b c (n r)', r = stride)\n\n        self.init_conv_(conv)\n\n    def init_conv_(self, conv):\n        o, i, f = conv.weight.shape\n        conv_weight = torch.empty(o // self.stride, i, f)\n        nn.init.kaiming_uniform_(conv_weight)\n        conv_weight = repeat(conv_weight, 'o ... -> (o r) ...', r = self.stride)\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        b, c, f, h, w = x.shape\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n        out = self.net(x)\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b h w) c f -> b c f h w', h = h, w = w)\n\ndef TemporalDownsample(dim, dim_out = None, stride = 2):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (f p) h w -> b (c p) f h w', p = stride),\n        Conv2d(dim * stride, dim_out, 1)\n    )\n\n# positional embedding\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_635-685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        conv_weight = repeat(conv_weight, 'o ... -> (o r) ...', r = self.stride)\n\n        conv.weight.data.copy_(conv_weight)\n        nn.init.zeros_(conv.bias.data)\n\n    def forward(self, x):\n        b, c, f, h, w = x.shape\n        x = rearrange(x, 'b c f h w -> (b h w) c f')\n        out = self.net(x)\n        out = self.pixel_shuffle(out)\n        return rearrange(out, '(b h w) c f -> b c f h w', h = h, w = w)\n\ndef TemporalDownsample(dim, dim_out = None, stride = 2):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (f p) h w -> b (c p) f h w', p = stride),\n        Conv2d(dim * stride, dim_out, 1)\n    )\n\n# positional embedding\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_645-695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return rearrange(out, '(b h w) c f -> b c f h w', h = h, w = w)\n\ndef TemporalDownsample(dim, dim_out = None, stride = 2):\n    dim_out = default(dim_out, dim)\n    return nn.Sequential(\n        Rearrange('b c (f p) h w -> b (c p) f h w', p = stride),\n        Conv2d(dim * stride, dim_out, 1)\n    )\n\n# positional embedding\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = Conv3d(dim, dim_out, 3, padding = 1)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_655-705"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = Conv3d(dim, dim_out, 3, padding = 1)\n\n    def forward(\n        self,\n        x,\n        scale_shift = None,\n        ignore_time = False\n    ):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Name(Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Attribute(Name(Load)Load)FloorDivConstant))Assign(Name(Store)BinOp(Call(Attribute(Name(Load)Load)Constant)DivBinOp(Name(Load)SubConstant)))Assign(Name(Store)Call(Attribute(Name(Load)Load)BinOp(Call(Attribute(Name(Load)Load)Name(Load)keyword(Attribute(Name(Load)Load)))MultUnaryOp(USubName(Load)))))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)Constant)MultCall(Name(Load)Name(Load)Constant)))Return(Call(Attribute(Name(Load)Load)Tuple(Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load))Load)keyword(UnaryOp(USubConstant))))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(Compare(BinOp(Name(Load)ModConstant)EqConstant))Assign(Name(Store)BinOp(Name(Load)FloorDivConstant))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)BinOp(BinOp(BinOp(Name(Load)MultCall(Name(Load)Attribute(Name(Load)Load)Constant))MultConstant)MultAttribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Call(Attribute(Name(Load)Load))Call(Attribute(Name(Load)Load))Load)keyword(UnaryOp(USubConstant))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Tuple(Name(Load)Name(Load)Load)keyword(UnaryOp(USubConstant))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)IfExp(Name(Load)Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Call(Name(Load))))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)))Assign(Attribute(Name(Load)Store)Call(Name(Load)Name(Load)Name(Load)Constantkeyword(Constant))))FunctionDef(arguments(argargargargConstantConstant)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))If(Call(Name(Load)Name(Load))Assign(Tuple(Name(Store)Name(Store)Store)Name(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_665-715"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n\nclass LearnedSinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim))\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = Conv3d(dim, dim_out, 3, padding = 1)\n\n    def forward(\n        self,\n        x,\n        scale_shift = None,\n        ignore_time = False\n    ):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x, ignore_time = ignore_time)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_675-725"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = Conv3d(dim, dim_out, 3, padding = 1)\n\n    def forward(\n        self,\n        x,\n        scale_shift = None,\n        ignore_time = False\n    ):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x, ignore_time = ignore_time)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_685-735"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dim,\n        dim_out,\n        groups = 8,\n        norm = True\n    ):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n        self.activation = nn.SiLU()\n        self.project = Conv3d(dim, dim_out, 3, padding = 1)\n\n    def forward(\n        self,\n        x,\n        scale_shift = None,\n        ignore_time = False\n    ):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x, ignore_time = ignore_time)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_695-745"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(\n        self,\n        x,\n        scale_shift = None,\n        ignore_time = False\n    ):\n        x = self.groupnorm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x, ignore_time = ignore_time)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = EinopsToAndFrom(\n                'b c f h w',\n                'b (f h w) c',\n                attn_klass(\n                    dim = dim_out,\n                    context_dim = cond_dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_705-755"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            x = x * (scale + 1) + shift\n\n        x = self.activation(x)\n        return self.project(x, ignore_time = ignore_time)\n\nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = EinopsToAndFrom(\n                'b c f h w',\n                'b (f h w) c',\n                attn_klass(\n                    dim = dim_out,\n                    context_dim = cond_dim,\n                    **attn_kwargs\n                )\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_715-765"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        cond_dim = None,\n        time_cond_dim = None,\n        groups = 8,\n        linear_attn = False,\n        use_gca = False,\n        squeeze_excite = False,\n        **attn_kwargs\n    ):\n        super().__init__()\n\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = EinopsToAndFrom(\n                'b c f h w',\n                'b (f h w) c',\n                attn_klass(\n                    dim = dim_out,\n                    context_dim = cond_dim,\n                    **attn_kwargs\n                )\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(\n        self,\n        x,\n        time_emb = None,\n        cond = None,\n        ignore_time = False\n    ):\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_725-775"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.time_mlp = None\n\n        if exists(time_cond_dim):\n            self.time_mlp = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, dim_out * 2)\n            )\n\n        self.cross_attn = None\n\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = EinopsToAndFrom(\n                'b c f h w',\n                'b (f h w) c',\n                attn_klass(\n                    dim = dim_out,\n                    context_dim = cond_dim,\n                    **attn_kwargs\n                )\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(\n        self,\n        x,\n        time_emb = None,\n        cond = None,\n        ignore_time = False\n    ):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, ignore_time = ignore_time)\n\n        if exists(self.cross_attn):\n            assert exists(cond)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_735-785"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if exists(cond_dim):\n            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n\n            self.cross_attn = EinopsToAndFrom(\n                'b c f h w',\n                'b (f h w) c',\n                attn_klass(\n                    dim = dim_out,\n                    context_dim = cond_dim,\n                    **attn_kwargs\n                )\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(\n        self,\n        x,\n        time_emb = None,\n        cond = None,\n        ignore_time = False\n    ):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, ignore_time = ignore_time)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = self.cross_attn(h, context = cond) + h\n\n        h = self.block2(h, scale_shift = scale_shift, ignore_time = ignore_time)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_745-795"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    **attn_kwargs\n                )\n            )\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n\n        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n\n        self.res_conv = Conv2d(dim, dim_out, 1) if dim != dim_out else Identity()\n\n\n    def forward(\n        self,\n        x,\n        time_emb = None,\n        cond = None,\n        ignore_time = False\n    ):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, ignore_time = ignore_time)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = self.cross_attn(h, context = cond) + h\n\n        h = self.block2(h, scale_shift = scale_shift, ignore_time = ignore_time)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_755-805"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n\n    def forward(\n        self,\n        x,\n        time_emb = None,\n        cond = None,\n        ignore_time = False\n    ):\n\n        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, ignore_time = ignore_time)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = self.cross_attn(h, context = cond) + h\n\n        h = self.block2(h, scale_shift = scale_shift, ignore_time = ignore_time)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_765-815"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        scale_shift = None\n        if exists(self.time_mlp) and exists(time_emb):\n            time_emb = self.time_mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, ignore_time = ignore_time)\n\n        if exists(self.cross_attn):\n            assert exists(cond)\n            h = self.cross_attn(h, context = cond) + h\n\n        h = self.block2(h, scale_shift = scale_shift, ignore_time = ignore_time)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_775-825"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            h = self.cross_attn(h, context = cond) + h\n\n        h = self.block2(h, scale_shift = scale_shift, ignore_time = ignore_time)\n\n        h = h * self.gca(h)\n\n        return h + self.res_conv(x)\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_785-835"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self,\n        dim,\n        *,\n        context_dim = None,\n        dim_head = 64,\n        heads = 8,\n        norm_context = False,\n        scale = 8\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # qk rmsnorm", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_795-845"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.scale = scale\n\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n\n        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_805-855"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n\n        self.q_scale = nn.Parameter(torch.ones(dim_head))\n        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias = False),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_815-865"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_825-875"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # qk rmsnorm\n\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_835-885"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        q, k = map(l2norm, (q, k))\n        q = q * self.q_scale\n        k = k * self.k_scale\n\n        # similarities\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n        # masking\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_845-895"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1, dtype = torch.float32)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_855-905"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass LinearCrossAttention(CrossAttention):\n    def forward(self, x, context, mask = None):\n        b, n, device = *x.shape[:2], x.device\n\n        x = self.norm(x)\n        context = self.norm_context(context)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_865-915"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 925, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n\n        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n\n        # add null key / value for classifier free guidance in prior net\n\n        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n\n        k = torch.cat((nk, k), dim = -2)\n        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_875-925"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 935, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        v = torch.cat((nv, v), dim = -2)\n\n        # masking\n\n        max_neg_value = -torch.finfo(x.dtype).max\n\n        if exists(mask):\n            mask = F.pad(mask, (1, 0), value = True)\n            mask = rearrange(mask, 'b n -> b n 1')\n            k = k.masked_fill(~mask, max_neg_value)\n            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_885-935"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 945, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            v = v.masked_fill(~mask, 0.)\n\n        # linear attention\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_895-945"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 930, "start_line_no": 905, "end_line_no": 955, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        heads = 8,\n        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_905-955"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 940, "start_line_no": 915, "end_line_no": 965, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dropout = 0.05,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n        self.norm = ChanLayerNorm(dim)\n\n        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_915-965"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 950, "start_line_no": 925, "end_line_no": 975, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.nonlin = nn.SiLU()\n\n        self.to_q = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_k = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_925-975"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 960, "start_line_no": 935, "end_line_no": 985, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_v = nn.Sequential(\n            nn.Dropout(dropout),\n            Conv2d(dim, inner_dim, 1, bias = False),\n            Conv2d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n        )\n\n        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_935-985"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 970, "start_line_no": 945, "end_line_no": 995, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n\n        self.to_out = nn.Sequential(\n            Conv2d(inner_dim, dim, 1, bias = False),\n            ChanLayerNorm(dim)\n        )\n\n    def forward(self, fmap, context = None):\n        h, x, y = self.heads, *fmap.shape[-2:]\n\n        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            Conv2d(hidden_dim, dim_out, 1),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_945-995"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 980, "start_line_no": 955, "end_line_no": 1005, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        fmap = self.norm(fmap)\n        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n\n        if exists(context):\n            assert exists(self.to_context)\n            ck, cv = self.to_context(context).chunk(2, dim = -1)\n            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n            k = torch.cat((k, ck), dim = -2)\n            v = torch.cat((v, cv), dim = -2)\n\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1 1')\n        return self.net(out)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_955-1005"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 990, "start_line_no": 965, "end_line_no": 1015, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        q = q.softmax(dim = -1)\n        k = k.softmax(dim = -2)\n\n        q = q * self.scale\n\n        context = einsum('b n d, b n e -> b d e', k, v)\n        out = einsum('b n d, b d e -> b n e', q, context)\n        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n\n        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_965-1015"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1000, "start_line_no": 975, "end_line_no": 1025, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        out = self.nonlin(out)\n        return self.to_out(out)\n\nclass GlobalContext(nn.Module):\n    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim_in,\n        dim_out\n    ):\n        super().__init__()\n        self.to_k = Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_975-1025"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1010, "start_line_no": 985, "end_line_no": 1035, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dim_out\n    ):\n        super().__init__()\n        self.to_k = Conv2d(dim_in, 1, 1)\n        hidden_dim = max(3, dim_out // 2)\n\n        self.net = nn.Sequential(\n            Conv2d(dim_in, hidden_dim, 1),\n            nn.SiLU(),\n            Conv2d(hidden_dim, dim_out, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_985-1035"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1020, "start_line_no": 995, "end_line_no": 1045, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        context = self.to_k(x)\n        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n        out = rearrange(out, '... -> ... 1 1')\n        return self.net(out)\n\ndef FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                EinopsToAndFrom('b c f h w', 'b (f h w) c', Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim)),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_995-1045"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1030, "start_line_no": 1005, "end_line_no": 1055, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def FeedForward(dim, mult = 2):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        LayerNorm(hidden_dim),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\ndef ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                EinopsToAndFrom('b c f h w', 'b (f h w) c', Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim)),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1005-1055"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1040, "start_line_no": 1015, "end_line_no": 1065, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        ChanLayerNorm(dim),\n        Conv2d(dim, hidden_dim, 1, bias = False),\n        nn.GELU(),\n        ChanLayerNorm(hidden_dim),\n        Conv2d(hidden_dim, dim, 1, bias = False)\n    )\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                EinopsToAndFrom('b c f h w', 'b (f h w) c', Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim)),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\nAST=Module(FunctionDef(arguments(argargConstant)Assign(Name(Store)Call(Name(Load)BinOp(Name(Load)MultName(Load))))Return(Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))Call(Name(Load)Name(Load)Name(Load)Constantkeyword(Constant))Call(Attribute(Name(Load)Load))Call(Name(Load)Name(Load))Call(Name(Load)Name(Load)Name(Load)Constantkeyword(Constant)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargargargConstantConstantConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Load)))For(Name(Store)Call(Name(Load)Name(Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)List(Call(Name(Load)ConstantConstantCall(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))Load))))))FunctionDef(arguments(argargargConstant)For(Tuple(Name(Store)Name(Store)Store)Attribute(Name(Load)Load)Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)keyword(Name(Load)))AddName(Load)))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load))AddName(Load))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargargargConstantConstantConstantConstantConstantarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1015-1065"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1050, "start_line_no": 1025, "end_line_no": 1075, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                EinopsToAndFrom('b c f h w', 'b (f h w) c', Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim)),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargargargConstantConstantConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Load)))For(Name(Store)Call(Name(Load)Name(Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)List(Call(Name(Load)ConstantConstantCall(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))))Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))Load))))))FunctionDef(arguments(argargargConstant)For(Tuple(Name(Store)Name(Store)Store)Attribute(Name(Load)Load)Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)keyword(Name(Load)))AddName(Load)))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load))AddName(Load))))Return(Name(Load))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargargargConstantConstantConstantConstantConstantarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Load)))For(Name(Store)Call(Name(Load)Name(Load))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)List(Call(Name(Load)keyword(Name(Load))keyword(Name(Load))keyword(Name(Load))keyword(Name(Load)))Call(Name(Load)keyword(Name(Load))keyword(Name(Load)))Load))))))FunctionDef(arguments(argargargConstant)For(Tuple(Name(Store)Name(Store)Store)Attribute(Name(Load)Load)Assign(Name(Store)BinOp(Call(Name(Load)Name(Load)keyword(Name(Load)))AddName(Load)))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1025-1075"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1060, "start_line_no": 1035, "end_line_no": 1085, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                EinopsToAndFrom('b c f h w', 'b (f h w) c', Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim)),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1035-1085"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1070, "start_line_no": 1045, "end_line_no": 1095, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass LinearAttentionTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1045-1095"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1080, "start_line_no": 1055, "end_line_no": 1105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        depth = 1,\n        heads = 8,\n        dim_head = 32,\n        ff_mult = 2,\n        context_dim = None,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1055-1105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1090, "start_line_no": 1065, "end_line_no": 1115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n                ChanFeedForward(dim = dim, mult = ff_mult)\n            ]))\n\n    def forward(self, x, context = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context) + x\n            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1065-1115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1100, "start_line_no": 1075, "end_line_no": 1125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            x = ff(x) + x\n        return x\n\nclass CrossEmbedLayer(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        kernel_sizes,\n        dim_out = None,\n        stride = 2\n    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1075-1125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1110, "start_line_no": 1085, "end_line_no": 1135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    ):\n        super().__init__()\n        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n        dim_out = default(dim_out, dim_in)\n\n        kernel_sizes = sorted(kernel_sizes)\n        num_scales = len(kernel_sizes)\n\n        # calculate the dimension at each scale\n        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1085-1135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1120, "start_line_no": 1095, "end_line_no": 1145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n\n        self.convs = nn.ModuleList([])\n        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n            self.convs.append(Conv2d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n\n    def forward(self, x):\n        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n        return torch.cat(fmaps, dim = 1)\n\nclass UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_video_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass DynamicPositionBias(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1095-1145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1130, "start_line_no": 1105, "end_line_no": 1155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "class UpsampleCombiner(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        enabled = False,\n        dim_ins = tuple(),\n        dim_outs = tuple()\n    ):\n        super().__init__()\n        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_video_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass DynamicPositionBias(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads,\n        depth\n    ):\n        super().__init__()\n        self.mlp = nn.ModuleList([])\n\n        self.mlp.append(nn.Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim),\n            nn.SiLU()\n        ))\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargargargConstantCall(Name(Load))Call(Name(Load)))Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Call(Name(Load)Name(Load))))Assert(Compare(Call(Name(Load)Name(Load))EqCall(Name(Load)Name(Load))))Assign(Attribute(Name(Load)Store)Name(Load))If(UnaryOp(NotAttribute(Name(Load)Load))Assign(Attribute(Name(Load)Store)Name(Load))Return)Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)ListComp(Call(Name(Load)Name(Load)Name(Load))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)Name(Load))))))Assign(Attribute(Name(Load)Store)BinOp(Name(Load)AddIfExp(Compare(Call(Name(Load)Name(Load))GtConstant)Call(Name(Load)Name(Load))Constant))))FunctionDef(arguments(argargargConstant)Assign(Name(Store)Subscript(Attribute(Name(Load)Load)UnaryOp(USubConstant)Load))Assign(Name(Store)Call(Name(Load)Name(Load)Call(Name(Load))))If(BoolOp(OrUnaryOp(NotAttribute(Name(Load)Load))Compare(Call(Name(Load)Name(Load))EqConstant)Compare(Call(Name(Load)Attribute(Name(Load)Load))EqConstant))Return(Name(Load)))Assign(Name(Store)ListComp(Call(Name(Load)Name(Load)Name(Load))comprehension(Name(Store)Name(Load))))Assign(Name(Store)ListComp(Call(Name(Load)Name(Load))comprehension(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))))Return(Call(Attribute(Name(Load)Load)Tuple(Name(Load)Starred(Name(Load)Load)Load)keyword(Constant)))))ClassDef(Attribute(Name(Load)Load)FunctionDef(arguments(argargargarg)Expr(Call(Attribute(Call(Name(Load))Load)))Assign(Attribute(Name(Load)Store)Call(Attribute(Name(Load)Load)List(Load)))Expr(Call(Attribute(Attribute(Name(Load)Load)Load)Call(Attribute(Name(Load)Load)Call(Attribute(Name(Load)Load)ConstantName(Load))Call(Name(Load)Name(Load))Call(Attribute(Name(Load)Load))))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1105-1155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1140, "start_line_no": 1115, "end_line_no": 1165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n        assert len(dim_ins) == len(dim_outs)\n\n        self.enabled = enabled\n\n        if not self.enabled:\n            self.dim_out = dim\n            return\n\n        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_video_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass DynamicPositionBias(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads,\n        depth\n    ):\n        super().__init__()\n        self.mlp = nn.ModuleList([])\n\n        self.mlp.append(nn.Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim),\n            nn.SiLU()\n        ))\n\n        for _ in range(max(depth - 1, 0)):\n            self.mlp.append(nn.Sequential(\n                nn.Linear(dim, dim),\n                LayerNorm(dim),\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1115-1165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1150, "start_line_no": 1125, "end_line_no": 1175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n\n    def forward(self, x, fmaps = None):\n        target_size = x.shape[-1]\n\n        fmaps = default(fmaps, tuple())\n\n        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n            return x\n\n        fmaps = [resize_video_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass DynamicPositionBias(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads,\n        depth\n    ):\n        super().__init__()\n        self.mlp = nn.ModuleList([])\n\n        self.mlp.append(nn.Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim),\n            nn.SiLU()\n        ))\n\n        for _ in range(max(depth - 1, 0)):\n            self.mlp.append(nn.Sequential(\n                nn.Linear(dim, dim),\n                LayerNorm(dim),\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    def forward(self, n, device, dtype):\n        i = torch.arange(n, device = device)\n        j = torch.arange(n, device = device)\n\n        indices = rearrange(i, 'i -> i 1') - rearrange(j, 'j -> 1 j')\n        indices += (n - 1)\n\n        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n        pos = rearrange(pos, '... -> ... 1')\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1125-1175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1160, "start_line_no": 1135, "end_line_no": 1185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        fmaps = [resize_video_to(fmap, target_size) for fmap in fmaps]\n        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n        return torch.cat((x, *outs), dim = 1)\n\nclass DynamicPositionBias(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads,\n        depth\n    ):\n        super().__init__()\n        self.mlp = nn.ModuleList([])\n\n        self.mlp.append(nn.Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim),\n            nn.SiLU()\n        ))\n\n        for _ in range(max(depth - 1, 0)):\n            self.mlp.append(nn.Sequential(\n                nn.Linear(dim, dim),\n                LayerNorm(dim),\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    def forward(self, n, device, dtype):\n        i = torch.arange(n, device = device)\n        j = torch.arange(n, device = device)\n\n        indices = rearrange(i, 'i -> i 1') - rearrange(j, 'j -> 1 j')\n        indices += (n - 1)\n\n        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n        pos = rearrange(pos, '... -> ... 1')\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias\n\nclass Unet3D(nn.Module):\n    def __init__(\n        self,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1135-1185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1170, "start_line_no": 1145, "end_line_no": 1195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        depth\n    ):\n        super().__init__()\n        self.mlp = nn.ModuleList([])\n\n        self.mlp.append(nn.Sequential(\n            nn.Linear(1, dim),\n            LayerNorm(dim),\n            nn.SiLU()\n        ))\n\n        for _ in range(max(depth - 1, 0)):\n            self.mlp.append(nn.Sequential(\n                nn.Linear(dim, dim),\n                LayerNorm(dim),\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    def forward(self, n, device, dtype):\n        i = torch.arange(n, device = device)\n        j = torch.arange(n, device = device)\n\n        indices = rearrange(i, 'i -> i 1') - rearrange(j, 'j -> 1 j')\n        indices += (n - 1)\n\n        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n        pos = rearrange(pos, '... -> ... 1')\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias\n\nclass Unet3D(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults = (1, 2, 4, 8),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1145-1195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1180, "start_line_no": 1155, "end_line_no": 1205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        for _ in range(max(depth - 1, 0)):\n            self.mlp.append(nn.Sequential(\n                nn.Linear(dim, dim),\n                LayerNorm(dim),\n                nn.SiLU()\n            ))\n\n        self.mlp.append(nn.Linear(dim, heads))\n\n    def forward(self, n, device, dtype):\n        i = torch.arange(n, device = device)\n        j = torch.arange(n, device = device)\n\n        indices = rearrange(i, 'i -> i 1') - rearrange(j, 'j -> 1 j')\n        indices += (n - 1)\n\n        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n        pos = rearrange(pos, '... -> ... 1')\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias\n\nclass Unet3D(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults = (1, 2, 4, 8),\n        temporal_strides = 1,\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = False,\n        layer_attns_depth = 1,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1155-1205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1190, "start_line_no": 1165, "end_line_no": 1215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(self, n, device, dtype):\n        i = torch.arange(n, device = device)\n        j = torch.arange(n, device = device)\n\n        indices = rearrange(i, 'i -> i 1') - rearrange(j, 'j -> 1 j')\n        indices += (n - 1)\n\n        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n        pos = rearrange(pos, '... -> ... 1')\n\n        for layer in self.mlp:\n            pos = layer(pos)\n\n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias\n\nclass Unet3D(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults = (1, 2, 4, 8),\n        temporal_strides = 1,\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = False,\n        layer_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        time_rel_pos_bias_depth = 2,\n        time_causal_attn = True,\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1165-1215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1200, "start_line_no": 1175, "end_line_no": 1225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        for layer in self.mlp:\n            pos = layer(pos)\n\n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias\n\nclass Unet3D(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults = (1, 2, 4, 8),\n        temporal_strides = 1,\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = False,\n        layer_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        time_rel_pos_bias_depth = 2,\n        time_causal_attn = True,\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1175-1225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1210, "start_line_no": 1185, "end_line_no": 1235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        *,\n        dim,\n        text_embed_dim = get_encoded_dim(DEFAULT_T5_NAME),\n        num_resnet_blocks = 1,\n        cond_dim = None,\n        num_image_tokens = 4,\n        num_time_tokens = 2,\n        learned_sinu_pos_emb_dim = 16,\n        out_dim = None,\n        dim_mults = (1, 2, 4, 8),\n        temporal_strides = 1,\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = False,\n        layer_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        time_rel_pos_bias_depth = 2,\n        time_causal_attn = True,\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n        resize_mode = 'nearest'\n    ):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1185-1235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1220, "start_line_no": 1195, "end_line_no": 1245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        temporal_strides = 1,\n        cond_images_channels = 0,\n        channels = 3,\n        channels_out = None,\n        attn_dim_head = 64,\n        attn_heads = 8,\n        ff_mult = 2.,\n        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n        layer_attns = False,\n        layer_attns_depth = 1,\n        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        time_rel_pos_bias_depth = 2,\n        time_causal_attn = True,\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1195-1245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1230, "start_line_no": 1205, "end_line_no": 1255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n        time_rel_pos_bias_depth = 2,\n        time_causal_attn = True,\n        layer_cross_attns = True,\n        use_linear_attn = False,\n        use_linear_cross_attn = False,\n        cond_on_text = True,\n        max_text_len = 256,\n        init_dim = None,\n        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        self.self_cond = self_cond\n\n        # determine dimensions\n\n        self.channels = channels", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1205-1255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1240, "start_line_no": 1215, "end_line_no": 1265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        resnet_groups = 8,\n        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n        init_cross_embed = True,\n        init_cross_embed_kernel_sizes = (3, 7, 15),\n        cross_embed_downsample = False,\n        cross_embed_downsample_kernel_sizes = (2, 4),\n        attn_pool_text = True,\n        attn_pool_num_latents = 32,\n        dropout = 0.,\n        memory_efficient = False,\n        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        self.self_cond = self_cond\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1215-1265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1250, "start_line_no": 1225, "end_line_no": 1275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        init_conv_to_final_conv_residual = False,\n        use_global_context_attn = True,\n        scale_skip_connection = True,\n        final_resnet_block = True,\n        final_conv_kernel_size = 3,\n        self_cond = False,\n        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n        pixel_shuffle_upsample = True,       # may address checkboard artifacts\n        resize_mode = 'nearest'\n    ):\n        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        self.self_cond = self_cond\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1225-1275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1260, "start_line_no": 1235, "end_line_no": 1285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        super().__init__()\n\n        # guide researchers\n\n        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n\n        if dim < 128:\n            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n\n        # save locals to take care of some hyperparameters for cascading DDPM\n\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        self.self_cond = self_cond\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1235-1285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1270, "start_line_no": 1245, "end_line_no": 1295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self._locals = locals()\n        self._locals.pop('self', None)\n        self._locals.pop('__class__', None)\n\n        self.self_cond = self_cond\n\n        # determine dimensions\n\n        self.channels = channels\n        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1245-1295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1280, "start_line_no": 1255, "end_line_no": 1305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.channels_out = default(channels_out, channels)\n\n        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n        # (2) in self conditioning, one appends the predict x0 (x_start)\n        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n        init_dim = default(init_dim, dim)\n\n        # optional image conditioning\n\n        self.has_cond_image = cond_images_channels > 0\n        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1255-1305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1290, "start_line_no": 1265, "end_line_no": 1315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.cond_images_channels = cond_images_channels\n\n        init_channels += cond_images_channels\n\n        # initial convolution\n\n        self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else Conv2d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1265-1315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1300, "start_line_no": 1275, "end_line_no": 1325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # time conditioning\n\n        cond_dim = default(cond_dim, dim)\n        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n\n        # embedding time for log(snr) noise from continuous version\n\n        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1275-1325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1310, "start_line_no": 1285, "end_line_no": 1335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.to_time_hiddens = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n            nn.SiLU()\n        )\n\n        self.to_time_cond = nn.Sequential(\n            nn.Linear(time_cond_dim, time_cond_dim)\n        )\n\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1285-1335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1320, "start_line_no": 1295, "end_line_no": 1345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # project to time tokens as well as time hiddens\n\n        self.to_time_tokens = nn.Sequential(\n            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n        )\n\n        # low res aug noise conditioning\n\n        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1295-1345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1330, "start_line_no": 1305, "end_line_no": 1355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.lowres_cond = lowres_cond\n\n        if lowres_cond:\n            self.to_lowres_time_hiddens = nn.Sequential(\n                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n                nn.SiLU()\n            )\n\n            self.to_lowres_time_cond = nn.Sequential(\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1305-1355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1340, "start_line_no": 1315, "end_line_no": 1365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n            self.to_lowres_time_tokens = nn.Sequential(\n                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n            )\n\n        # normalizations\n\n        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1315-1365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1350, "start_line_no": 1325, "end_line_no": 1375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.norm_cond = nn.LayerNorm(cond_dim)\n\n        # text encoding conditioning (optional)\n\n        self.text_to_cond = None\n\n        if cond_on_text:\n            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n            self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n\n        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # temporal attention - attention across video frames\n\n        temporal_peg_padding = (0, 0, 0, 0, 2, 0) if time_causal_attn else (0, 0, 0, 0, 1, 1)\n        temporal_peg = lambda dim: Residual(nn.Sequential(Pad(temporal_peg_padding), nn.Conv3d(dim, dim, (3, 1, 1), groups = dim)))\n\n        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', '(b h w) f c', Residual(Attention(dim, **{**attn_kwargs, 'causal': time_causal_attn, 'init_zero': True, 'rel_pos_bias': True})))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1325-1375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1360, "start_line_no": 1335, "end_line_no": 1385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # finer control over whether to condition on text encodings\n\n        self.cond_on_text = cond_on_text\n\n        # attention pooling\n\n        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, dim_head = attn_dim_head, heads = attn_heads, num_latents = attn_pool_num_latents) if attn_pool_text else None\n\n        # for classifier free guidance\n\n        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # temporal attention - attention across video frames\n\n        temporal_peg_padding = (0, 0, 0, 0, 2, 0) if time_causal_attn else (0, 0, 0, 0, 1, 1)\n        temporal_peg = lambda dim: Residual(nn.Sequential(Pad(temporal_peg_padding), nn.Conv3d(dim, dim, (3, 1, 1), groups = dim)))\n\n        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', '(b h w) f c', Residual(Attention(dim, **{**attn_kwargs, 'causal': time_causal_attn, 'init_zero': True, 'rel_pos_bias': True})))\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1335-1385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1370, "start_line_no": 1345, "end_line_no": 1395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.max_text_len = max_text_len\n\n        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n\n        # for non-attention based text conditioning at all points in the network where time is also conditioned\n\n        self.to_text_non_attn_cond = None\n\n        if cond_on_text:\n            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # temporal attention - attention across video frames\n\n        temporal_peg_padding = (0, 0, 0, 0, 2, 0) if time_causal_attn else (0, 0, 0, 0, 1, 1)\n        temporal_peg = lambda dim: Residual(nn.Sequential(Pad(temporal_peg_padding), nn.Conv3d(dim, dim, (3, 1, 1), groups = dim)))\n\n        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', '(b h w) f c', Residual(Attention(dim, **{**attn_kwargs, 'causal': time_causal_attn, 'init_zero': True, 'rel_pos_bias': True})))\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # temporal downsample config\n\n        temporal_strides = cast_tuple(temporal_strides, num_layers)\n        self.total_temporal_divisor = functools.reduce(operator.mul, temporal_strides, 1)\n\n        # downsample klass\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1345-1395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1380, "start_line_no": 1355, "end_line_no": 1405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            self.to_text_non_attn_cond = nn.Sequential(\n                nn.LayerNorm(cond_dim),\n                nn.Linear(cond_dim, time_cond_dim),\n                nn.SiLU(),\n                nn.Linear(time_cond_dim, time_cond_dim)\n            )\n\n        # attention related params\n\n        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head)\n\n        num_layers = len(in_out)\n\n        # temporal attention - attention across video frames\n\n        temporal_peg_padding = (0, 0, 0, 0, 2, 0) if time_causal_attn else (0, 0, 0, 0, 1, 1)\n        temporal_peg = lambda dim: Residual(nn.Sequential(Pad(temporal_peg_padding), nn.Conv3d(dim, dim, (3, 1, 1), groups = dim)))\n\n        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', '(b h w) f c', Residual(Attention(dim, **{**attn_kwargs, 'causal': time_causal_attn, 'init_zero': True, 'rel_pos_bias': True})))\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # temporal downsample config\n\n        temporal_strides = cast_tuple(temporal_strides, num_layers)\n        self.total_temporal_divisor = functools.reduce(operator.mul, temporal_strides, 1)\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        self.init_temporal_peg = temporal_peg(init_dim)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1355-1405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1390, "start_line_no": 1365, "end_line_no": 1415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        num_layers = len(in_out)\n\n        # temporal attention - attention across video frames\n\n        temporal_peg_padding = (0, 0, 0, 0, 2, 0) if time_causal_attn else (0, 0, 0, 0, 1, 1)\n        temporal_peg = lambda dim: Residual(nn.Sequential(Pad(temporal_peg_padding), nn.Conv3d(dim, dim, (3, 1, 1), groups = dim)))\n\n        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', '(b h w) f c', Residual(Attention(dim, **{**attn_kwargs, 'causal': time_causal_attn, 'init_zero': True, 'rel_pos_bias': True})))\n\n        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # temporal downsample config\n\n        temporal_strides = cast_tuple(temporal_strides, num_layers)\n        self.total_temporal_divisor = functools.reduce(operator.mul, temporal_strides, 1)\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        self.init_temporal_peg = temporal_peg(init_dim)\n        self.init_temporal_attn = temporal_attn(init_dim)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1365-1415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1400, "start_line_no": 1375, "end_line_no": 1425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # resnet block klass\n\n        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n        resnet_groups = cast_tuple(resnet_groups, num_layers)\n\n        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n\n        layer_attns = cast_tuple(layer_attns, num_layers)\n        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # temporal downsample config\n\n        temporal_strides = cast_tuple(temporal_strides, num_layers)\n        self.total_temporal_divisor = functools.reduce(operator.mul, temporal_strides, 1)\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        self.init_temporal_peg = temporal_peg(init_dim)\n        self.init_temporal_attn = temporal_attn(init_dim)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, temporal_strides]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(in_out, *layer_params)):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1375-1425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1410, "start_line_no": 1385, "end_line_no": 1435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n\n        # temporal downsample config\n\n        temporal_strides = cast_tuple(temporal_strides, num_layers)\n        self.total_temporal_divisor = functools.reduce(operator.mul, temporal_strides, 1)\n\n        # downsample klass\n\n        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        self.init_temporal_peg = temporal_peg(init_dim)\n        self.init_temporal_attn = temporal_attn(init_dim)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, temporal_strides]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1385-1435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1420, "start_line_no": 1395, "end_line_no": 1445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        downsample_klass = Downsample\n\n        if cross_embed_downsample:\n            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n\n        # initial resnet block (for memory efficient unet)\n\n        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n\n        self.init_temporal_peg = temporal_peg(init_dim)\n        self.init_temporal_attn = temporal_attn(init_dim)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, temporal_strides]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1395-1445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1430, "start_line_no": 1405, "end_line_no": 1455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.init_temporal_attn = temporal_attn(init_dim)\n\n        # scale for resnet skip connections\n\n        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, temporal_strides]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(Conv2d(dim_in, dim_out, 3, padding = 1), Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1405-1455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1440, "start_line_no": 1415, "end_line_no": 1465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        num_resolutions = len(in_out)\n\n        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, temporal_strides]\n        reversed_layer_params = list(map(reversed, layer_params))\n\n        # downsampling layers\n\n        skip_connect_dims = [] # keep track of skip connection dimensions\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(in_out, *layer_params)):\n            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(Conv2d(dim_in, dim_out, 3, padding = 1), Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(current_dim),\n                temporal_attn(current_dim),\n                TemporalDownsample(current_dim, stride = temporal_stride) if temporal_stride > 1 else None,\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1415-1465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1450, "start_line_no": 1425, "end_line_no": 1475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            is_last = ind >= (num_resolutions - 1)\n\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            current_dim = dim_in\n\n            # whether to pre-downsample, from memory efficient unet\n\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(Conv2d(dim_in, dim_out, 3, padding = 1), Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(current_dim),\n                temporal_attn(current_dim),\n                TemporalDownsample(current_dim, stride = temporal_stride) if temporal_stride > 1 else None,\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c f h w', 'b (f h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_temporal_peg = temporal_peg(mid_dim)\n        self.mid_temporal_attn = temporal_attn(mid_dim)\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1425-1475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1460, "start_line_no": 1435, "end_line_no": 1485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            pre_downsample = None\n\n            if memory_efficient:\n                pre_downsample = downsample_klass(dim_in, dim_out)\n                current_dim = dim_out\n\n            skip_connect_dims.append(current_dim)\n\n            # whether to do post-downsample, for non-memory efficient unet\n\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(Conv2d(dim_in, dim_out, 3, padding = 1), Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(current_dim),\n                temporal_attn(current_dim),\n                TemporalDownsample(current_dim, stride = temporal_stride) if temporal_stride > 1 else None,\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c f h w', 'b (f h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_temporal_peg = temporal_peg(mid_dim)\n        self.mid_temporal_attn = temporal_attn(mid_dim)\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1435-1485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1470, "start_line_no": 1445, "end_line_no": 1495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            post_downsample = None\n            if not memory_efficient:\n                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(Conv2d(dim_in, dim_out, 3, padding = 1), Conv2d(dim_in, dim_out, 1))\n\n            self.downs.append(nn.ModuleList([\n                pre_downsample,\n                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(current_dim),\n                temporal_attn(current_dim),\n                TemporalDownsample(current_dim, stride = temporal_stride) if temporal_stride > 1 else None,\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c f h w', 'b (f h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_temporal_peg = temporal_peg(mid_dim)\n        self.mid_temporal_attn = temporal_attn(mid_dim)\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(dim_out),\n                temporal_attn(dim_out),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1445-1495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1480, "start_line_no": 1455, "end_line_no": 1505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                temporal_peg(current_dim),\n                temporal_attn(current_dim),\n                TemporalDownsample(current_dim, stride = temporal_stride) if temporal_stride > 1 else None,\n                post_downsample\n            ]))\n\n        # middle layers\n\n        mid_dim = dims[-1]\n\n        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c f h w', 'b (f h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_temporal_peg = temporal_peg(mid_dim)\n        self.mid_temporal_attn = temporal_attn(mid_dim)\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(dim_out),\n                temporal_attn(dim_out),\n                TemporalPixelShuffleUpsample(dim_out, stride = temporal_stride) if temporal_stride > 1 else None,\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1455-1505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1490, "start_line_no": 1465, "end_line_no": 1515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n        self.mid_attn = EinopsToAndFrom('b c f h w', 'b (f h w) c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n        self.mid_temporal_peg = temporal_peg(mid_dim)\n        self.mid_temporal_attn = temporal_attn(mid_dim)\n        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n\n        # upsample klass\n\n        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n\n        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(dim_out),\n                temporal_attn(dim_out),\n                TemporalPixelShuffleUpsample(dim_out, stride = temporal_stride) if temporal_stride > 1 else None,\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1465-1515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1500, "start_line_no": 1475, "end_line_no": 1525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # upsampling layers\n\n        upsample_fmap_dims = []\n\n        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, temporal_stride) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n            is_last = ind == (len(in_out) - 1)\n            layer_use_linear_cross_attn = not layer_cross_attn and use_linear_cross_attn\n            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n            transformer_block_klass = TransformerBlock if layer_attn else (LinearAttentionTransformerBlock if use_linear_attn else Identity)\n\n            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(dim_out),\n                temporal_attn(dim_out),\n                TemporalPixelShuffleUpsample(dim_out, stride = temporal_stride) if temporal_stride > 1 else None,\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1475-1525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1510, "start_line_no": 1485, "end_line_no": 1535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            skip_connect_dim = skip_connect_dims.pop()\n\n            upsample_fmap_dims.append(dim_out)\n\n            self.ups.append(nn.ModuleList([\n                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n                temporal_peg(dim_out),\n                temporal_attn(dim_out),\n                TemporalPixelShuffleUpsample(dim_out, stride = temporal_stride) if temporal_stride > 1 else None,\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1485-1535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1520, "start_line_no": 1495, "end_line_no": 1545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                TemporalPixelShuffleUpsample(dim_out, stride = temporal_stride) if temporal_stride > 1 else None,\n                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n            ]))\n\n        # whether to combine feature maps from all upsample blocks before final resnet block out\n\n        self.upsample_combiner = UpsampleCombiner(\n            dim = dim,\n            enabled = combine_upsample_fmaps,\n            dim_ins = upsample_fmap_dims,\n            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1495-1545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1530, "start_line_no": 1505, "end_line_no": 1555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            dim_outs = dim\n        )\n\n        # whether to do a final residual from initial conv to the final resnet block out\n\n        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n\n        # final optional resnet block and convolution out\n\n        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1505-1555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1540, "start_line_no": 1515, "end_line_no": 1565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n\n        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n        final_conv_dim_in += (channels if lowres_cond else 0)\n\n        self.final_conv = Conv2d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n\n        zero_init_(self.final_conv)\n\n        # resize mode\n\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1515-1565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1550, "start_line_no": 1525, "end_line_no": 1575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.resize_mode = resize_mode\n\n    # if the current settings for the unet are not correct\n    # for cascading DDPM, then reinit the unet with the right settings\n    def cast_model_parameters(\n        self,\n        *,\n        lowres_cond,\n        text_embed_dim,\n        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1525-1575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1560, "start_line_no": 1535, "end_line_no": 1585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        channels,\n        channels_out,\n        cond_on_text\n    ):\n        if lowres_cond == self.lowres_cond and \\\n            channels == self.channels and \\\n            cond_on_text == self.cond_on_text and \\\n            text_embed_dim == self._locals['text_embed_dim'] and \\\n            channels_out == self.channels_out:\n            return self\n\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1535-1585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1570, "start_line_no": 1545, "end_line_no": 1595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        updated_kwargs = dict(\n            lowres_cond = lowres_cond,\n            text_embed_dim = text_embed_dim,\n            channels = channels,\n            channels_out = channels_out,\n            cond_on_text = cond_on_text\n        )\n\n        return self.__class__(**{**self._locals, **updated_kwargs})\n\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1545-1595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1580, "start_line_no": 1555, "end_line_no": 1605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    # methods for returning the full unet config as well as its parameter state\n\n    def to_config_and_state_dict(self):\n        return self._locals, self.state_dict()\n\n    # class method for rehydrating the unet from its config and state dict\n\n    @classmethod\n    def from_config_and_state_dict(klass, config, state_dict):\n        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1555-1605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1590, "start_line_no": 1565, "end_line_no": 1615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        unet = klass(**config)\n        unet.load_state_dict(state_dict)\n        return unet\n\n    # methods for persisting unet to disk\n\n    def persist_to_file(self, path):\n        path = Path(path)\n        path.parents[0].mkdir(exist_ok = True, parents = True)\n\n        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1565-1615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1600, "start_line_no": 1575, "end_line_no": 1625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        config, state_dict = self.to_config_and_state_dict()\n        pkg = dict(config = config, state_dict = state_dict)\n        torch.save(pkg, str(path))\n\n    # class method for rehydrating the unet from file saved with `persist_to_file`\n\n    @classmethod\n    def hydrate_from_file(klass, path):\n        path = Path(path)\n        assert path.exists()\n        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        self_cond = None,\n        cond_drop_prob = 0.,\n        ignore_time = False\n    ):\n        assert x.ndim == 5, 'input to 3d unet must have 5 dimensions (batch, channels, time, height, width)'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1575-1625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1610, "start_line_no": 1585, "end_line_no": 1635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        pkg = torch.load(str(path))\n\n        assert 'config' in pkg and 'state_dict' in pkg\n        config, state_dict = pkg['config'], pkg['state_dict']\n\n        return Unet.from_config_and_state_dict(config, state_dict)\n\n    # forward with classifier free guidance\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        self_cond = None,\n        cond_drop_prob = 0.,\n        ignore_time = False\n    ):\n        assert x.ndim == 5, 'input to 3d unet must have 5 dimensions (batch, channels, time, height, width)'\n\n        batch_size, frames, device, dtype = x.shape[0], x.shape[2], x.device, x.dtype\n\n        assert ignore_time or divisible_by(frames, self.total_temporal_divisor), f'number of input frames {frames} must be divisible by {self.total_temporal_divisor}'\n\n        # add self conditioning if needed\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1585-1635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1620, "start_line_no": 1595, "end_line_no": 1645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self,\n        *args,\n        cond_scale = 1.,\n        **kwargs\n    ):\n        logits = self.forward(*args, **kwargs)\n\n        if cond_scale == 1:\n            return logits\n\n        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        self_cond = None,\n        cond_drop_prob = 0.,\n        ignore_time = False\n    ):\n        assert x.ndim == 5, 'input to 3d unet must have 5 dimensions (batch, channels, time, height, width)'\n\n        batch_size, frames, device, dtype = x.shape[0], x.shape[2], x.device, x.dtype\n\n        assert ignore_time or divisible_by(frames, self.total_temporal_divisor), f'number of input frames {frames} must be divisible by {self.total_temporal_divisor}'\n\n        # add self conditioning if needed\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n            if exists(cond_video_frames):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1595-1645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1630, "start_line_no": 1605, "end_line_no": 1655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        *,\n        lowres_cond_img = None,\n        lowres_noise_times = None,\n        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        self_cond = None,\n        cond_drop_prob = 0.,\n        ignore_time = False\n    ):\n        assert x.ndim == 5, 'input to 3d unet must have 5 dimensions (batch, channels, time, height, width)'\n\n        batch_size, frames, device, dtype = x.shape[0], x.shape[2], x.device, x.dtype\n\n        assert ignore_time or divisible_by(frames, self.total_temporal_divisor), f'number of input frames {frames} must be divisible by {self.total_temporal_divisor}'\n\n        # add self conditioning if needed\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n            if exists(cond_video_frames):\n                lowres_cond_img = torch.cat((cond_video_frames, lowres_cond_img), dim = 2)\n                cond_video_frames = torch.cat((cond_video_frames, cond_video_frames), dim = 1)\n\n            if exists(post_cond_video_frames):\n                lowres_cond_img = torch.cat((lowres_cond_img, post_cond_video_frames), dim = 2)\n                post_cond_video_frames = torch.cat((post_cond_video_frames, post_cond_video_frames), dim = 1)\n\n        # conditioning on video frames as a prompt\n\n        num_preceding_frames = 0", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1605-1655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1640, "start_line_no": 1615, "end_line_no": 1665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        text_embeds = None,\n        text_mask = None,\n        cond_images = None,\n        cond_video_frames = None,\n        post_cond_video_frames = None,\n        self_cond = None,\n        cond_drop_prob = 0.,\n        ignore_time = False\n    ):\n        assert x.ndim == 5, 'input to 3d unet must have 5 dimensions (batch, channels, time, height, width)'\n\n        batch_size, frames, device, dtype = x.shape[0], x.shape[2], x.device, x.dtype\n\n        assert ignore_time or divisible_by(frames, self.total_temporal_divisor), f'number of input frames {frames} must be divisible by {self.total_temporal_divisor}'\n\n        # add self conditioning if needed\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n            if exists(cond_video_frames):\n                lowres_cond_img = torch.cat((cond_video_frames, lowres_cond_img), dim = 2)\n                cond_video_frames = torch.cat((cond_video_frames, cond_video_frames), dim = 1)\n\n            if exists(post_cond_video_frames):\n                lowres_cond_img = torch.cat((lowres_cond_img, post_cond_video_frames), dim = 2)\n                post_cond_video_frames = torch.cat((post_cond_video_frames, post_cond_video_frames), dim = 1)\n\n        # conditioning on video frames as a prompt\n\n        num_preceding_frames = 0\n        if exists(cond_video_frames):\n            cond_video_frames_len = cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            cond_video_frames = resize_video_to(cond_video_frames, x.shape[-1])\n            x = torch.cat((cond_video_frames, x), dim = 2)\n\n            num_preceding_frames = cond_video_frames_len\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1615-1665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1650, "start_line_no": 1625, "end_line_no": 1675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        batch_size, frames, device, dtype = x.shape[0], x.shape[2], x.device, x.dtype\n\n        assert ignore_time or divisible_by(frames, self.total_temporal_divisor), f'number of input frames {frames} must be divisible by {self.total_temporal_divisor}'\n\n        # add self conditioning if needed\n\n        if self.self_cond:\n            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x, self_cond), dim = 1)\n\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n            if exists(cond_video_frames):\n                lowres_cond_img = torch.cat((cond_video_frames, lowres_cond_img), dim = 2)\n                cond_video_frames = torch.cat((cond_video_frames, cond_video_frames), dim = 1)\n\n            if exists(post_cond_video_frames):\n                lowres_cond_img = torch.cat((lowres_cond_img, post_cond_video_frames), dim = 2)\n                post_cond_video_frames = torch.cat((post_cond_video_frames, post_cond_video_frames), dim = 1)\n\n        # conditioning on video frames as a prompt\n\n        num_preceding_frames = 0\n        if exists(cond_video_frames):\n            cond_video_frames_len = cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            cond_video_frames = resize_video_to(cond_video_frames, x.shape[-1])\n            x = torch.cat((cond_video_frames, x), dim = 2)\n\n            num_preceding_frames = cond_video_frames_len\n\n        # conditioning on video frames as a prompt\n\n        num_succeeding_frames = 0\n        if exists(post_cond_video_frames):\n            cond_video_frames_len = post_cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            post_cond_video_frames = resize_video_to(post_cond_video_frames, x.shape[-1])\n            x = torch.cat((post_cond_video_frames, x), dim = 2)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1625-1675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1660, "start_line_no": 1635, "end_line_no": 1685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # add low resolution conditioning, if present\n\n        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n            if exists(cond_video_frames):\n                lowres_cond_img = torch.cat((cond_video_frames, lowres_cond_img), dim = 2)\n                cond_video_frames = torch.cat((cond_video_frames, cond_video_frames), dim = 1)\n\n            if exists(post_cond_video_frames):\n                lowres_cond_img = torch.cat((lowres_cond_img, post_cond_video_frames), dim = 2)\n                post_cond_video_frames = torch.cat((post_cond_video_frames, post_cond_video_frames), dim = 1)\n\n        # conditioning on video frames as a prompt\n\n        num_preceding_frames = 0\n        if exists(cond_video_frames):\n            cond_video_frames_len = cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            cond_video_frames = resize_video_to(cond_video_frames, x.shape[-1])\n            x = torch.cat((cond_video_frames, x), dim = 2)\n\n            num_preceding_frames = cond_video_frames_len\n\n        # conditioning on video frames as a prompt\n\n        num_succeeding_frames = 0\n        if exists(post_cond_video_frames):\n            cond_video_frames_len = post_cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            post_cond_video_frames = resize_video_to(post_cond_video_frames, x.shape[-1])\n            x = torch.cat((post_cond_video_frames, x), dim = 2)\n\n            num_succeeding_frames = cond_video_frames_len\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.ndim == 4, 'conditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead'\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1635-1685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1670, "start_line_no": 1645, "end_line_no": 1695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                lowres_cond_img = torch.cat((cond_video_frames, lowres_cond_img), dim = 2)\n                cond_video_frames = torch.cat((cond_video_frames, cond_video_frames), dim = 1)\n\n            if exists(post_cond_video_frames):\n                lowres_cond_img = torch.cat((lowres_cond_img, post_cond_video_frames), dim = 2)\n                post_cond_video_frames = torch.cat((post_cond_video_frames, post_cond_video_frames), dim = 1)\n\n        # conditioning on video frames as a prompt\n\n        num_preceding_frames = 0\n        if exists(cond_video_frames):\n            cond_video_frames_len = cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            cond_video_frames = resize_video_to(cond_video_frames, x.shape[-1])\n            x = torch.cat((cond_video_frames, x), dim = 2)\n\n            num_preceding_frames = cond_video_frames_len\n\n        # conditioning on video frames as a prompt\n\n        num_succeeding_frames = 0\n        if exists(post_cond_video_frames):\n            cond_video_frames_len = post_cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            post_cond_video_frames = resize_video_to(post_cond_video_frames, x.shape[-1])\n            x = torch.cat((post_cond_video_frames, x), dim = 2)\n\n            num_succeeding_frames = cond_video_frames_len\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.ndim == 4, 'conditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead'\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n\n            cond_images = repeat(cond_images, 'b c h w -> b c f h w', f = x.shape[2])\n            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)\n\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # ignoring time in pseudo 3d resnet blocks\n\n        conv_kwargs = dict(\n            ignore_time = ignore_time", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1645-1695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1680, "start_line_no": 1655, "end_line_no": 1705, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if exists(cond_video_frames):\n            cond_video_frames_len = cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            cond_video_frames = resize_video_to(cond_video_frames, x.shape[-1])\n            x = torch.cat((cond_video_frames, x), dim = 2)\n\n            num_preceding_frames = cond_video_frames_len\n\n        # conditioning on video frames as a prompt\n\n        num_succeeding_frames = 0\n        if exists(post_cond_video_frames):\n            cond_video_frames_len = post_cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            post_cond_video_frames = resize_video_to(post_cond_video_frames, x.shape[-1])\n            x = torch.cat((post_cond_video_frames, x), dim = 2)\n\n            num_succeeding_frames = cond_video_frames_len\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.ndim == 4, 'conditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead'\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n\n            cond_images = repeat(cond_images, 'b c h w -> b c f h w', f = x.shape[2])\n            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)\n\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # ignoring time in pseudo 3d resnet blocks\n\n        conv_kwargs = dict(\n            ignore_time = ignore_time\n        )\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        if not ignore_time:\n            x = self.init_temporal_peg(x)\n            x = self.init_temporal_attn(x)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1655-1705"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1690, "start_line_no": 1665, "end_line_no": 1715, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # conditioning on video frames as a prompt\n\n        num_succeeding_frames = 0\n        if exists(post_cond_video_frames):\n            cond_video_frames_len = post_cond_video_frames.shape[2]\n\n            assert divisible_by(cond_video_frames_len, self.total_temporal_divisor)\n\n            post_cond_video_frames = resize_video_to(post_cond_video_frames, x.shape[-1])\n            x = torch.cat((post_cond_video_frames, x), dim = 2)\n\n            num_succeeding_frames = cond_video_frames_len\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.ndim == 4, 'conditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead'\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n\n            cond_images = repeat(cond_images, 'b c h w -> b c f h w', f = x.shape[2])\n            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)\n\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # ignoring time in pseudo 3d resnet blocks\n\n        conv_kwargs = dict(\n            ignore_time = ignore_time\n        )\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        if not ignore_time:\n            x = self.init_temporal_peg(x)\n            x = self.init_temporal_attn(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1665-1715"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1700, "start_line_no": 1675, "end_line_no": 1725, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            num_succeeding_frames = cond_video_frames_len\n\n        # condition on input image\n\n        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n\n        if exists(cond_images):\n            assert cond_images.ndim == 4, 'conditioning images must have 4 dimensions only, if you want to condition on frames of video, use `cond_video_frames` instead'\n            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n\n            cond_images = repeat(cond_images, 'b c h w -> b c f h w', f = x.shape[2])\n            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)\n\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # ignoring time in pseudo 3d resnet blocks\n\n        conv_kwargs = dict(\n            ignore_time = ignore_time\n        )\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        if not ignore_time:\n            x = self.init_temporal_peg(x)\n            x = self.init_temporal_attn(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1675-1725"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1710, "start_line_no": 1685, "end_line_no": 1735, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            cond_images = repeat(cond_images, 'b c h w -> b c f h w', f = x.shape[2])\n            cond_images = resize_video_to(cond_images, x.shape[-1], mode = self.resize_mode)\n\n            x = torch.cat((cond_images, x), dim = 1)\n\n        # ignoring time in pseudo 3d resnet blocks\n\n        conv_kwargs = dict(\n            ignore_time = ignore_time\n        )\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        if not ignore_time:\n            x = self.init_temporal_peg(x)\n            x = self.init_temporal_attn(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1685-1735"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1720, "start_line_no": 1695, "end_line_no": 1745, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        )\n\n        # initial convolution\n\n        x = self.init_conv(x)\n\n        if not ignore_time:\n            x = self.init_temporal_peg(x)\n            x = self.init_temporal_attn(x)\n\n        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1695-1745"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1730, "start_line_no": 1705, "end_line_no": 1755, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        # init conv residual\n\n        if self.init_conv_to_final_conv_residual:\n            init_conv_residual = x.clone()\n\n        # time conditioning\n\n        time_hiddens = self.to_time_hiddens(time)\n\n        # derive time tokens\n\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n            \n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1705-1755"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1740, "start_line_no": 1715, "end_line_no": 1765, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        time_tokens = self.to_time_tokens(time_hiddens)\n        t = self.to_time_cond(time_hiddens)\n\n        # add lowres time conditioning to time hiddens\n        # and add lowres time tokens along sequence dimension for attention\n\n        if self.lowres_cond:\n            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n            \n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1715-1765"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1750, "start_line_no": 1725, "end_line_no": 1775, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n\n            t = t + lowres_t\n            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n\n        # text conditioning\n\n        text_tokens = None\n\n        if exists(text_embeds) and self.cond_on_text:\n\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n            \n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1725-1775"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1760, "start_line_no": 1735, "end_line_no": 1785, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            # conditional dropout\n\n            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n\n            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n\n            # calculate text embeds\n\n            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n            \n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1735-1785"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1770, "start_line_no": 1745, "end_line_no": 1795, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            text_tokens = self.text_to_cond(text_embeds)\n\n            text_tokens = text_tokens[:, :self.max_text_len]\n            \n            if exists(text_mask):\n                text_mask = text_mask[:, :self.max_text_len]\n\n            text_tokens_len = text_tokens.shape[1]\n            remainder = self.max_text_len - text_tokens_len\n\n            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1745-1795"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1780, "start_line_no": 1755, "end_line_no": 1805, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            if remainder > 0:\n                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n\n            if exists(text_mask):\n                if remainder > 0:\n                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n\n                text_mask = rearrange(text_mask, 'b n -> b n 1')\n                text_keep_mask_embed = text_mask & text_keep_mask_embed\n\n            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t, **conv_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1755-1805"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1790, "start_line_no": 1765, "end_line_no": 1815, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n\n            text_tokens = torch.where(\n                text_keep_mask_embed,\n                text_tokens,\n                null_text_embed\n            )\n\n            if exists(self.attn_pool):\n                text_tokens = self.attn_pool(text_tokens)\n\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t, **conv_kwargs)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c, **conv_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1765-1815"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1800, "start_line_no": 1775, "end_line_no": 1825, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            # extra non-attention conditioning by projecting and then summing text embeddings to time\n            # termed as text hiddens\n\n            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n\n            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n\n            null_text_hidden = self.null_text_hidden.to(t.dtype)\n\n            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t, **conv_kwargs)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, **conv_kwargs)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1775-1825"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1810, "start_line_no": 1785, "end_line_no": 1835, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            text_hiddens = torch.where(\n                text_keep_mask_hidden,\n                text_hiddens,\n                null_text_hidden\n            )\n\n            t = t + text_hiddens\n\n        # main conditioning tokens (c)\n\n        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t, **conv_kwargs)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, **conv_kwargs)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            hiddens.append(x)\n\n            if exists(temporal_downsample) and not ignore_time:\n                x = temporal_downsample(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c, **conv_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1785-1835"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1820, "start_line_no": 1795, "end_line_no": 1845, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n\n        # normalize conditioning tokens\n\n        c = self.norm_cond(c)\n\n        # initial resnet block (for memory efficient unet)\n\n        if exists(self.init_resnet_block):\n            x = self.init_resnet_block(x, t, **conv_kwargs)\n\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, **conv_kwargs)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            hiddens.append(x)\n\n            if exists(temporal_downsample) and not ignore_time:\n                x = temporal_downsample(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c, **conv_kwargs)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        if not ignore_time:\n            x = self.mid_temporal_peg(x)\n            x = self.mid_temporal_attn(x)\n\n        x = self.mid_block2(x, t, c, **conv_kwargs)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1795-1845"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1830, "start_line_no": 1805, "end_line_no": 1855, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # go through the layers of the unet, down and up\n\n        hiddens = []\n\n        for pre_downsample, init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_downsample, post_downsample in self.downs:\n            if exists(pre_downsample):\n                x = pre_downsample(x)\n\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, **conv_kwargs)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            hiddens.append(x)\n\n            if exists(temporal_downsample) and not ignore_time:\n                x = temporal_downsample(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c, **conv_kwargs)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        if not ignore_time:\n            x = self.mid_temporal_peg(x)\n            x = self.mid_temporal_attn(x)\n\n        x = self.mid_block2(x, t, c, **conv_kwargs)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:\n            if exists(temporal_upsample) and not ignore_time:\n                x = temporal_upsample(x)\n\n            x = add_skip_connection(x)\n            x = init_block(x, t, c, **conv_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1805-1855"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1840, "start_line_no": 1815, "end_line_no": 1865, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            for resnet_block in resnet_blocks:\n                x = resnet_block(x, t, **conv_kwargs)\n                hiddens.append(x)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            hiddens.append(x)\n\n            if exists(temporal_downsample) and not ignore_time:\n                x = temporal_downsample(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c, **conv_kwargs)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        if not ignore_time:\n            x = self.mid_temporal_peg(x)\n            x = self.mid_temporal_attn(x)\n\n        x = self.mid_block2(x, t, c, **conv_kwargs)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:\n            if exists(temporal_upsample) and not ignore_time:\n                x = temporal_upsample(x)\n\n            x = add_skip_connection(x)\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t, **conv_kwargs)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1815-1865"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1850, "start_line_no": 1825, "end_line_no": 1875, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            hiddens.append(x)\n\n            if exists(temporal_downsample) and not ignore_time:\n                x = temporal_downsample(x)\n\n            if exists(post_downsample):\n                x = post_downsample(x)\n\n        x = self.mid_block1(x, t, c, **conv_kwargs)\n\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        if not ignore_time:\n            x = self.mid_temporal_peg(x)\n            x = self.mid_temporal_attn(x)\n\n        x = self.mid_block2(x, t, c, **conv_kwargs)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:\n            if exists(temporal_upsample) and not ignore_time:\n                x = temporal_upsample(x)\n\n            x = add_skip_connection(x)\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t, **conv_kwargs)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            up_hiddens.append(x.contiguous())\n\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1825-1875"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1860, "start_line_no": 1835, "end_line_no": 1885, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if exists(self.mid_attn):\n            x = self.mid_attn(x)\n\n        if not ignore_time:\n            x = self.mid_temporal_peg(x)\n            x = self.mid_temporal_attn(x)\n\n        x = self.mid_block2(x, t, c, **conv_kwargs)\n\n        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:\n            if exists(temporal_upsample) and not ignore_time:\n                x = temporal_upsample(x)\n\n            x = add_skip_connection(x)\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t, **conv_kwargs)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            up_hiddens.append(x.contiguous())\n\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t, **conv_kwargs)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1835-1885"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1870, "start_line_no": 1845, "end_line_no": 1894, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n\n        up_hiddens = []\n\n        for init_block, resnet_blocks, attn_block, temporal_peg, temporal_attn, temporal_upsample, upsample in self.ups:\n            if exists(temporal_upsample) and not ignore_time:\n                x = temporal_upsample(x)\n\n            x = add_skip_connection(x)\n            x = init_block(x, t, c, **conv_kwargs)\n\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t, **conv_kwargs)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            up_hiddens.append(x.contiguous())\n\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t, **conv_kwargs)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        out = self.final_conv(x)\n\n        if num_preceding_frames > 0:\n            out = out[:, :, num_preceding_frames:]\n\n        if num_succeeding_frames > 0:\n            out = out[:, :, :-num_succeeding_frames]\n\n        return out", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1845-1894"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1880, "start_line_no": 1855, "end_line_no": 1894, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            for resnet_block in resnet_blocks:\n                x = add_skip_connection(x)\n                x = resnet_block(x, t, **conv_kwargs)\n\n            x = attn_block(x, c)\n\n            if not ignore_time:\n                x = temporal_peg(x)\n                x = temporal_attn(x)\n\n            up_hiddens.append(x.contiguous())\n\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t, **conv_kwargs)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        out = self.final_conv(x)\n\n        if num_preceding_frames > 0:\n            out = out[:, :, num_preceding_frames:]\n\n        if num_succeeding_frames > 0:\n            out = out[:, :, :-num_succeeding_frames]\n\n        return out", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1855-1894"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "imagen_video.py"], "line_no": 1890, "start_line_no": 1865, "end_line_no": 1894, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            up_hiddens.append(x.contiguous())\n\n            x = upsample(x)\n\n        # whether to combine all feature maps from upsample blocks\n\n        x = self.upsample_combiner(x, up_hiddens)\n\n        # final top-most residual if needed\n\n        if self.init_conv_to_final_conv_residual:\n            x = torch.cat((x, init_conv_residual), dim = 1)\n\n        if exists(self.final_res_block):\n            x = self.final_res_block(x, t, **conv_kwargs)\n\n        if exists(lowres_cond_img):\n            x = torch.cat((x, lowres_cond_img), dim = 1)\n\n        out = self.final_conv(x)\n\n        if num_preceding_frames > 0:\n            out = out[:, :, num_preceding_frames:]\n\n        if num_succeeding_frames > 0:\n            out = out[:, :, :-num_succeeding_frames]\n\n        return out", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-imagen_video.py_1865-1894"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)))FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Dict))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import torch\nimport transformers\nfrom typing import List\nfrom transformers import T5Tokenizer, T5EncoderModel, T5Config\nfrom einops import rearrange\n\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\nAST=Module(Import(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)Expr(Call(Attribute(Attribute(Name(Load)Load)Load)))FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Dict)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)GlobalIf(Compare(Name(Load)NotInName(Load))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load))))If(Compare(ConstantNotInSubscript(Name(Load)Name(Load)Load))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load))))If(Compare(ConstantNotInSubscript(Name(Load)Name(Load)Load))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load))))Return(Tuple(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad)Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad)Load))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ntransformers.logging.set_verbosity_error()\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n\nAST=Module(Expr(Call(Attribute(Attribute(Name(Load)Load)Load)))FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))Assign(Name(Store)Constant)Assign(Name(Store)Constant)Assign(Name(Store)Dict)FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Name(Load))))Return(Name(Load)))FunctionDef(arguments(arg)Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))FunctionDef(arguments(arg)GlobalIf(Compare(Name(Load)NotInName(Load))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load))))If(Compare(ConstantNotInSubscript(Name(Load)Name(Load)Load))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load))))If(Compare(ConstantNotInSubscript(Name(Load)Name(Load)Load))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantStore)Call(Name(Load)Name(Load))))Return(Tuple(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad)Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad)Load)))FunctionDef(arguments(arg)If(Compare(Name(Load)NotInName(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load)keyword(Name(Load))))If(Compare(ConstantInSubscript(Name(Load)Name(Load)Load))Assign(Name(Store)Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad))If(Compare(ConstantInSubscript(Name(Load)Name(Load)Load))Assign(Name(Store)Attribute(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad)Load)))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n# config\n\nMAX_LENGTH = 256\n\nDEFAULT_T5_NAME = 'google/t5-v1_1-base'\n\nT5_CONFIGS = {}\n\n# singleton globals\n\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef get_tokenizer(name):\n    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n    return tokenizer\n\ndef get_model(name):\n    model = T5EncoderModel.from_pretrained(name)\n    return model\n\ndef get_model_and_tokenizer(name):\n    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    global T5_CONFIGS\n\n    if name not in T5_CONFIGS:\n        T5_CONFIGS[name] = dict()\n    if \"model\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"model\"] = get_model(name)\n    if \"tokenizer\" not in T5_CONFIGS[name]:\n        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\n    return T5_CONFIGS[name]['model'], T5_CONFIGS[name]['tokenizer']\n\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",\n        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text(", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef get_encoded_dim(name):\n    if name not in T5_CONFIGS:\n        # avoids loading the model if we only want to get the dim\n        config = T5Config.from_pretrained(name)\n        T5_CONFIGS[name] = dict(config=config)\n    elif \"config\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"config\"]\n    elif \"model\" in T5_CONFIGS[name]:\n        config = T5_CONFIGS[name][\"model\"].config\n    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",\n        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text(\n    token_ids,\n    attn_mask = None,\n    pad_id = None,\n    name = DEFAULT_T5_NAME\n):\n    assert exists(attn_mask) or exists(pad_id)\n    t5, _ = get_model_and_tokenizer(name)\n\n    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n\n\nAST=Module(FunctionDef(arguments(arg)If(Compare(Name(Load)NotInName(Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)))Assign(Subscript(Name(Load)Name(Load)Store)Call(Name(Load)keyword(Name(Load))))If(Compare(ConstantInSubscript(Name(Load)Name(Load)Load))Assign(Name(Store)Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad))If(Compare(ConstantInSubscript(Name(Load)Name(Load)Load))Assign(Name(Store)Attribute(Subscript(Subscript(Name(Load)Name(Load)Load)ConstantLoad)Load))Assert(Constant))))Return(Attribute(Name(Load)Load)))FunctionDef(arguments(arg(Subscript(Name(Load)Name(Load)Load))argName(Load))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))If(Call(Attribute(Attribute(Name(Load)Load)Load))Assign(Name(Store)Call(Attribute(Name(Load)Load))))Assign(Name(Store)Attribute(Call(Name(Load)Call(Attribute(Name(Load)Load)))Load))Assign(Name(Store)Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)keyword(Constant)keyword(Name(Load))keyword(Constant)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Assign(Name(Store)Call(Attribute(Attribute(Name(Load)Load)Load)Name(Load)))Return(Tuple(Name(Load)Name(Load)Load)))FunctionDef(arguments(argargargargConstantConstantName(Load))Assert(BoolOp(OrCall(Name(Load)Name(Load))Call(Name(Load)Name(Load))))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Lambda(argumentsCall(Attribute(Compare(Name(Load)NotEqName(Load))Load)))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    else:\n        assert False\n    return config.d_model\n\n# encoding text\n\ndef t5_tokenize(\n    texts: List[str],\n    name = DEFAULT_T5_NAME\n):\n    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",\n        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text(\n    token_ids,\n    attn_mask = None,\n    pad_id = None,\n    name = DEFAULT_T5_NAME\n):\n    assert exists(attn_mask) or exists(pad_id)\n    t5, _ = get_model_and_tokenizer(name)\n\n    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n\n    t5.eval()\n\n    with torch.no_grad():\n        output = t5(input_ids = token_ids, attention_mask = attn_mask)\n        encoded_text = output.last_hidden_state.detach()\n\n    attn_mask = attn_mask.bool()\n\n    encoded_text = encoded_text.masked_fill(~rearrange(attn_mask, '... -> ... 1'), 0.) # just force all embeddings that is padding to be equal to 0.\n    return encoded_text", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    t5, tokenizer = get_model_and_tokenizer(name)\n\n    if torch.cuda.is_available():\n        t5 = t5.cuda()\n\n    device = next(t5.parameters()).device\n\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        return_tensors = \"pt\",\n        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text(\n    token_ids,\n    attn_mask = None,\n    pad_id = None,\n    name = DEFAULT_T5_NAME\n):\n    assert exists(attn_mask) or exists(pad_id)\n    t5, _ = get_model_and_tokenizer(name)\n\n    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n\n    t5.eval()\n\n    with torch.no_grad():\n        output = t5(input_ids = token_ids, attention_mask = attn_mask)\n        encoded_text = output.last_hidden_state.detach()\n\n    attn_mask = attn_mask.bool()\n\n    encoded_text = encoded_text.masked_fill(~rearrange(attn_mask, '... -> ... 1'), 0.) # just force all embeddings that is padding to be equal to 0.\n    return encoded_text\n\ndef t5_encode_text(\n    texts: List[str],\n    name = DEFAULT_T5_NAME,\n    return_attn_mask = False\n):\n    token_ids, attn_mask = t5_tokenize(texts, name = name)\n    encoded_text = t5_encode_tokenized_text(token_ids, attn_mask = attn_mask, name = name)\n\n    if return_attn_mask:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 119, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        padding = 'longest',\n        max_length = MAX_LENGTH,\n        truncation = True\n    )\n\n    input_ids = encoded.input_ids.to(device)\n    attn_mask = encoded.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef t5_encode_tokenized_text(\n    token_ids,\n    attn_mask = None,\n    pad_id = None,\n    name = DEFAULT_T5_NAME\n):\n    assert exists(attn_mask) or exists(pad_id)\n    t5, _ = get_model_and_tokenizer(name)\n\n    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n\n    t5.eval()\n\n    with torch.no_grad():\n        output = t5(input_ids = token_ids, attention_mask = attn_mask)\n        encoded_text = output.last_hidden_state.detach()\n\n    attn_mask = attn_mask.bool()\n\n    encoded_text = encoded_text.masked_fill(~rearrange(attn_mask, '... -> ... 1'), 0.) # just force all embeddings that is padding to be equal to 0.\n    return encoded_text\n\ndef t5_encode_text(\n    texts: List[str],\n    name = DEFAULT_T5_NAME,\n    return_attn_mask = False\n):\n    token_ids, attn_mask = t5_tokenize(texts, name = name)\n    encoded_text = t5_encode_tokenized_text(token_ids, attn_mask = attn_mask, name = name)\n\n    if return_attn_mask:\n        attn_mask = attn_mask.bool()\n        return encoded_text, attn_mask\n\n    return encoded_text", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_75-119"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-t5.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "t5.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 119, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    token_ids,\n    attn_mask = None,\n    pad_id = None,\n    name = DEFAULT_T5_NAME\n):\n    assert exists(attn_mask) or exists(pad_id)\n    t5, _ = get_model_and_tokenizer(name)\n\n    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n\n    t5.eval()\n\n    with torch.no_grad():\n        output = t5(input_ids = token_ids, attention_mask = attn_mask)\n        encoded_text = output.last_hidden_state.detach()\n\n    attn_mask = attn_mask.bool()\n\n    encoded_text = encoded_text.masked_fill(~rearrange(attn_mask, '... -> ... 1'), 0.) # just force all embeddings that is padding to be equal to 0.\n    return encoded_text\n\ndef t5_encode_text(\n    texts: List[str],\n    name = DEFAULT_T5_NAME,\n    return_attn_mask = False\n):\n    token_ids, attn_mask = t5_tokenize(texts, name = name)\n    encoded_text = t5_encode_tokenized_text(token_ids, attn_mask = attn_mask, name = name)\n\n    if return_attn_mask:\n        attn_mask = attn_mask.bool()\n        return encoded_text, attn_mask\n\n    return encoded_text", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-t5.py_85-119"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import os\nimport time\nimport copy\nfrom pathlib import Path\nfrom math import ceil\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\nAST=Module(Import(alias)Import(alias)Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)ImportFrom(alias)Import(alias)ImportFrom(alias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(aliasalias)Import(alias)ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from contextlib import contextmanager, nullcontext\nfrom functools import partial, wraps\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim import Adam\nfrom lion_pytorch import Lion\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 65, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport pytorch_warmup as warmup\n\nfrom imagen_pytorch.imagen_pytorch import Imagen, NullUnet\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\nfrom imagen_pytorch.data import cycle\n\nfrom imagen_pytorch.version import __version__\nfrom packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_15-65"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 75, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from packaging import version\n\nimport numpy as np\n\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n\nfrom fsspec.core import url_to_fs\nfrom fsspec.implementations.local import LocalFileSystem\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\nAST=Module(ImportFrom(alias)Import(alias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))Return(IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)BinOp(Tuple(Name(Load)Load)MultName(Load)))))FunctionDef(arguments(argarg)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load))If(Call(Name(Load)Name(Load))Return(Name(Load))))Return(UnaryOp(USubConstant)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Call(Attribute(Name(Load)Load)Name(Load)))Name(Load))))Return(Call(Name(Load)Call(Name(Load)Name(Load)Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)List(Call(Name(Load))Call(Name(Load))Load))For(Name(Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)UnaryOp(NotName(Load))))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)Name(Load)Store)Subscript(Name(Load)Name(Load)Load)))Return(Tuple(Starred(Name(Load)Load)Load)))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Return(Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_25-75"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 85, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n\nAST=Module(FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argarg)If(Call(Name(Load)Name(Load))Return(Name(Load)))Return(IfExp(Call(Name(Load)Name(Load))Call(Name(Load))Name(Load))))FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))Return(IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)BinOp(Tuple(Name(Load)Load)MultName(Load)))))FunctionDef(arguments(argarg)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load))If(Call(Name(Load)Name(Load))Return(Name(Load))))Return(UnaryOp(USubConstant)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Call(Attribute(Name(Load)Load)Name(Load)))Name(Load))))Return(Call(Name(Load)Call(Name(Load)Name(Load)Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)List(Call(Name(Load))Call(Name(Load))Load))For(Name(Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)UnaryOp(NotName(Load))))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)Name(Load)Store)Subscript(Name(Load)Name(Load)Load)))Return(Tuple(Starred(Name(Load)Load)Load)))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Return(Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Tuple(Subscript(Subscript(Name(Load)ConstantLoad)Slice(Call(Name(Load)Name(Load)))Load)Subscript(Name(Load)ConstantLoad)Load))Call(Name(Load)Call(Attribute(Name(Load)Load))))))Return(Tuple(Name(Load)Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Name(Load)FloorDivName(Load)))Assign(Name(Store)BinOp(Name(Load)ModName(Load)))Assign(Name(Store)BinOp(List(Name(Load)Load)MultName(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_35-85"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 70, "start_line_no": 45, "end_line_no": 95, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef cast_tuple(val, length = 1):\n    if isinstance(val, list):\n        val = tuple(val)\n\n    return val if isinstance(val, tuple) else ((val,) * length)\n\ndef find_first(fn, arr):\n    for ind, el in enumerate(arr):\n        if fn(el):\n            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n\nAST=Module(FunctionDef(arguments(argargConstant)If(Call(Name(Load)Name(Load)Name(Load))Assign(Name(Store)Call(Name(Load)Name(Load))))Return(IfExp(Call(Name(Load)Name(Load)Name(Load))Name(Load)BinOp(Tuple(Name(Load)Load)MultName(Load)))))FunctionDef(arguments(argarg)For(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Name(Load))If(Call(Name(Load)Name(Load))Return(Name(Load))))Return(UnaryOp(USubConstant)))FunctionDef(arguments(argarg)Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Call(Attribute(Name(Load)Load)Name(Load)))Name(Load))))Return(Call(Name(Load)Call(Name(Load)Name(Load)Name(Load)))))FunctionDef(arguments(argarg)Assign(Name(Store)List(Call(Name(Load))Call(Name(Load))Load))For(Name(Store)Call(Attribute(Name(Load)Load))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)UnaryOp(NotName(Load))))Assign(Subscript(Subscript(Name(Load)Name(Load)Load)Name(Load)Store)Subscript(Name(Load)Name(Load)Load)))Return(Tuple(Starred(Name(Load)Load)Load)))FunctionDef(arguments(argarg)Return(Call(Attribute(Name(Load)Load)Name(Load))))FunctionDef(arguments(argarg)Return(Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Name(Load))))FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Tuple(Subscript(Subscript(Name(Load)ConstantLoad)Slice(Call(Name(Load)Name(Load)))Load)Subscript(Name(Load)ConstantLoad)Load))Call(Name(Load)Call(Attribute(Name(Load)Load))))))Return(Tuple(Name(Load)Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Name(Load)FloorDivName(Load)))Assign(Name(Store)BinOp(Name(Load)ModName(Load)))Assign(Name(Store)BinOp(List(Name(Load)Load)MultName(Load)))If(Compare(Name(Load)GtConstant)Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load)))FunctionDef(arguments(arg)If(Compare(ConstantNotInName(Load))Return(Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_45-95"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 80, "start_line_no": 55, "end_line_no": 105, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return ind\n    return -1\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_55-105"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 90, "start_line_no": 65, "end_line_no": 115, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_65-115"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 100, "start_line_no": 75, "end_line_no": 125, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\nAST=Module(FunctionDef(arguments(argarg)Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)Call(Name(Load)Name(Load)Name(Load))Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)Tuple(Subscript(Subscript(Name(Load)ConstantLoad)Slice(Call(Name(Load)Name(Load)))Load)Subscript(Name(Load)ConstantLoad)Load))Call(Name(Load)Call(Attribute(Name(Load)Load))))))Return(Tuple(Name(Load)Name(Load)Load)))FunctionDef(arguments(argarg)Assign(Name(Store)BinOp(Name(Load)FloorDivName(Load)))Assign(Name(Store)BinOp(Name(Load)ModName(Load)))Assign(Name(Store)BinOp(List(Name(Load)Load)MultName(Load)))If(Compare(Name(Load)GtConstant)Expr(Call(Attribute(Name(Load)Load)Name(Load))))Return(Name(Load)))FunctionDef(arguments(arg)If(Compare(ConstantNotInName(Load))Return(Name(Load)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Attribute(Name(Load)Load)Constant))If(Compare(Name(Load)InSet(ConstantConstant))Return(Subscript(Call(Attribute(Name(Load)Load)Constant)ConstantLoad))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))Constant)))))FunctionDef(arguments(arg)FunctionDef(arguments(argargarg)Assign(Name(Store)Attribute(Name(Load)Load))Expr(Call(Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Starred(Name(Load)Load)keyword(Name(Load))))Expr(Call(Attribute(Name(Load)Load)Name(Load)))Return(Name(Load)))Return(Name(Load)))FunctionDef(arguments(argargConstant)FunctionDef(arguments(argargarg)Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantAttribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)ConstantConstant))Assign(Name(Store)BoolOp(AndName(Load)Attribute(Name(Load)Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)))Assign(Name(Store)Tuple(Starred(Name(Load)Load)Starred(Call(Attribute(Name(Load)Load))Load)Load))Assign(Name(Store)BinOp(Call(Name(Load)Name(Load))SubCall(Name(Load)Name(Load))))Assign(Name(Store)Call(Name(Load)Call(Name(Load)Lambda(arguments(arg)IfExp(BoolOp(AndCall(Name(Load)Name(Load))Call(Name(Load)Name(Load)Attribute(Name(Load)Load)))Call(Attribute(Name(Load)Load)Name(Load))Name(Load)))Name(Load))))Call(Name(Load)Name(Load)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_75-125"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 110, "start_line_no": 85, "end_line_no": 135, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# url to fs, bucket, path - for checkpointing to cloud\n\ndef url_to_bucket(url):\n    if '://' not in url:\n        return url\n\n    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_85-135"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 120, "start_line_no": 95, "end_line_no": 145, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    _, suffix = url.split('://')\n\n    if prefix in {'gs', 's3'}:\n        return suffix.split('/')[0]\n    else:\n        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n\n# decorators\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_95-145"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 130, "start_line_no": 105, "end_line_no": 155, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef cast_torch_tensor(fn, cast_fp16 = False):\n    @wraps(fn)\n    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_105-155"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 140, "start_line_no": 115, "end_line_no": 165, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def inner(model, *args, **kwargs):\n        device = kwargs.pop('_device', model.device)\n        cast_device = kwargs.pop('_cast_device', True)\n\n        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n\n        kwargs_keys = kwargs.keys()\n        all_args = (*args, *kwargs.values())\n        split_kwargs_index = len(all_args) - len(kwargs_keys)\n        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_115-165"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 150, "start_line_no": 125, "end_line_no": 175, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if cast_device:\n            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n\n        if should_cast_fp16:\n            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n\n        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n\n        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None\n\ndef split_args_and_kwargs(*args, split_size = None, **kwargs):\n    all_args = (*args, *kwargs.values())\n    len_all_args = len(all_args)\n    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n    assert exists(first_tensor)\n\n    batch_size = len(first_tensor)\n    split_size = default(split_size, batch_size)\n    num_chunks = ceil(batch_size / split_size)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_125-175"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 160, "start_line_no": 135, "end_line_no": 185, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        out = fn(model, *args, **kwargs)\n        return out\n    return inner\n\n# gradient accumulation functions\n\ndef split_iterable(it, split_size):\n    accum = []\n    for ind in range(ceil(len(it) / split_size)):\n        start_index = ind * split_size\n        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None\n\ndef split_args_and_kwargs(*args, split_size = None, **kwargs):\n    all_args = (*args, *kwargs.values())\n    len_all_args = len(all_args)\n    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n    assert exists(first_tensor)\n\n    batch_size = len(first_tensor)\n    split_size = default(split_size, batch_size)\n    num_chunks = ceil(batch_size / split_size)\n\n    dict_len = len(kwargs)\n    dict_keys = kwargs.keys()\n    split_kwargs_index = len_all_args - dict_len\n\n    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n    chunk_sizes = num_to_groups(batch_size, split_size)\n\n    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_135-185"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 170, "start_line_no": 145, "end_line_no": 195, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        accum.append(it[start_index: (start_index + split_size)])\n    return accum\n\ndef split(t, split_size = None):\n    if not exists(split_size):\n        return t\n\n    if isinstance(t, torch.Tensor):\n        return t.split(split_size, dim = 0)\n\n    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None\n\ndef split_args_and_kwargs(*args, split_size = None, **kwargs):\n    all_args = (*args, *kwargs.values())\n    len_all_args = len(all_args)\n    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n    assert exists(first_tensor)\n\n    batch_size = len(first_tensor)\n    split_size = default(split_size, batch_size)\n    num_chunks = ceil(batch_size / split_size)\n\n    dict_len = len(kwargs)\n    dict_keys = kwargs.keys()\n    split_kwargs_index = len_all_args - dict_len\n\n    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n    chunk_sizes = num_to_groups(batch_size, split_size)\n\n    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]\n        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n\n# imagen trainer\n\ndef imagen_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_145-195"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 180, "start_line_no": 155, "end_line_no": 205, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    if isinstance(t, Iterable):\n        return split_iterable(t, split_size)\n\n    return TypeError\n\ndef find_first(cond, arr):\n    for el in arr:\n        if cond(el):\n            return el\n    return None\n\ndef split_args_and_kwargs(*args, split_size = None, **kwargs):\n    all_args = (*args, *kwargs.values())\n    len_all_args = len(all_args)\n    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n    assert exists(first_tensor)\n\n    batch_size = len(first_tensor)\n    split_size = default(split_size, batch_size)\n    num_chunks = ceil(batch_size / split_size)\n\n    dict_len = len(kwargs)\n    dict_keys = kwargs.keys()\n    split_kwargs_index = len_all_args - dict_len\n\n    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n    chunk_sizes = num_to_groups(batch_size, split_size)\n\n    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]\n        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n\n# imagen trainer\n\ndef imagen_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_155-205"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 190, "start_line_no": 165, "end_line_no": 215, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef split_args_and_kwargs(*args, split_size = None, **kwargs):\n    all_args = (*args, *kwargs.values())\n    len_all_args = len(all_args)\n    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n    assert exists(first_tensor)\n\n    batch_size = len(first_tensor)\n    split_size = default(split_size, batch_size)\n    num_chunks = ceil(batch_size / split_size)\n\n    dict_len = len(kwargs)\n    dict_keys = kwargs.keys()\n    split_kwargs_index = len_all_args - dict_len\n\n    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n    chunk_sizes = num_to_groups(batch_size, split_size)\n\n    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]\n        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n\n# imagen trainer\n\ndef imagen_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):\n            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_165-215"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 200, "start_line_no": 175, "end_line_no": 225, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    dict_len = len(kwargs)\n    dict_keys = kwargs.keys()\n    split_kwargs_index = len_all_args - dict_len\n\n    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n    chunk_sizes = num_to_groups(batch_size, split_size)\n\n    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]\n        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n\n# imagen trainer\n\ndef imagen_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):\n            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_175-225"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 210, "start_line_no": 185, "end_line_no": 235, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n        chunk_size_frac = chunk_size / batch_size\n        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n\n# imagen trainer\n\ndef imagen_sample_in_chunks(fn):\n    @wraps(fn)\n    def inner(self, *args, max_batch_size = None, **kwargs):\n        if not exists(max_batch_size):\n            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):\n            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_185-235"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 220, "start_line_no": 195, "end_line_no": 245, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return fn(self, *args, **kwargs)\n\n        if self.imagen.unconditional:\n            batch_size = kwargs.get('batch_size')\n            batch_sizes = num_to_groups(batch_size, max_batch_size)\n            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n        else:\n            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n\n        if isinstance(outputs[0], torch.Tensor):\n            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_195-245"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 230, "start_line_no": 205, "end_line_no": 255, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return torch.cat(outputs, dim = 0)\n\n        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n\n    return inner\n\n\ndef restore_parts(state_dict_target, state_dict_from):\n    for name, param in state_dict_from.items():\n\n        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_205-255"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 240, "start_line_no": 215, "end_line_no": 265, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if name not in state_dict_target:\n            continue\n\n        if param.size() == state_dict_target[name].size():\n            state_dict_target[name].copy_(param)\n        else:\n            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n\n    return state_dict_target\n\n\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_215-265"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 250, "start_line_no": 225, "end_line_no": 275, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\nclass ImagenTrainer(nn.Module):\n    locked = False\n\n    def __init__(\n        self,\n        imagen = None,\n        imagen_checkpoint_path = None,\n        use_ema = True,\n        lr = 1e-4,\n        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\nAST=Module(ClassDef(Attribute(Name(Load)Load)Assign(Name(Store)Constant)FunctionDef(arguments(argargargargargargargargargargargargargargargargargargargargargargargargarg(Name(Load))argargargConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantConstantTuple(ConstantConstantConstantConstantLoad)ConstantConstantConstantConstantConstantConstantConstantConstantConstantConstant)Expr(Call(Attribute(Call(Name(Load))Load)))Assert(UnaryOp(NotAttribute(Name(Load)Load))Constant)Assert(BinOp(Call(Name(Load)Name(Load))BitXorCall(Name(Load)Name(Load)))Constant)Assign(Attribute(Name(Load)Store)Name(Load))If(UnaryOp(NotCall(Name(Load)Attribute(Name(Load)Load)))Assign(Name(Store)Call(Name(Load)Name(Load)Dict))Assign(Tuple(Attribute(Name(Load)Store)Name(Store)Store)Call(Name(Load)Call(Name(Load)Name(Load)Constant)keyword(Name(Load)))))Assert(Call(Name(Load)Name(Load)Tuple(Name(Load)Name(Load)Load)))Assign(Tuple(Name(Store)Name(Store)Store)Call(Name(Load)ConstantName(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_225-275"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 260, "start_line_no": 235, "end_line_no": 285, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        eps = 1e-8,\n        beta1 = 0.9,\n        beta2 = 0.99,\n        max_grad_norm = None,\n        group_wd_params = True,\n        warmup_steps = None,\n        cosine_decay_max_steps = None,\n        only_train_unet_number = None,\n        fp16 = False,\n        precision = None,\n        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_235-285"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 270, "start_line_no": 245, "end_line_no": 295, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        split_batches = True,\n        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n        verbose = True,\n        split_valid_fraction = 0.025,\n        split_valid_from_train = False,\n        split_random_seed = 42,\n        checkpoint_path = None,\n        checkpoint_every = None,\n        checkpoint_fs = None,\n        fs_kwargs: dict = None,\n        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_245-295"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 280, "start_line_no": 255, "end_line_no": 305, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        max_checkpoints_keep = 20,\n        use_lion = False,\n        **kwargs\n    ):\n        super().__init__()\n        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n        assert exists(imagen) ^ exists(imagen_checkpoint_path), 'either imagen instance is passed into the trainer, or a checkpoint path that contains the imagen config'\n\n        # determine filesystem, using fsspec, for saving to local filesystem or cloud\n\n        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_255-305"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 290, "start_line_no": 265, "end_line_no": 315, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.fs = checkpoint_fs\n\n        if not exists(self.fs):\n            fs_kwargs = default(fs_kwargs, {})\n            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n\n        assert isinstance(imagen, (Imagen, ElucidatedImagen))\n        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n\n        # elucidated or not\n\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_265-315"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 300, "start_line_no": 275, "end_line_no": 325, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.is_elucidated = isinstance(imagen, ElucidatedImagen)\n\n        # create accelerator instance\n\n        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n\n        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n\n        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_275-325"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 310, "start_line_no": 285, "end_line_no": 335, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.accelerator = Accelerator(**{\n            'split_batches': split_batches,\n            'mixed_precision': accelerator_mixed_precision,\n            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n        , **accelerate_kwargs})\n\n        ImagenTrainer.locked = self.is_distributed\n\n        # cast data to fp16 at training time if needed\n\n        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_285-335"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 320, "start_line_no": 295, "end_line_no": 345, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n\n        # grad scaler must be managed outside of accelerator\n\n        grad_scaler_enabled = fp16\n\n        # imagen, unets and ema unets\n\n        self.imagen = imagen\n        self.num_unets = len(self.imagen.unets)\n\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_295-345"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 330, "start_line_no": 305, "end_line_no": 355, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.use_ema = use_ema and self.is_main\n        self.ema_unets = nn.ModuleList([])\n\n        # keep track of what unet is being trained on\n        # only going to allow 1 unet training at a time\n\n        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n\n        # data related functions\n\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_305-355"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 340, "start_line_no": 315, "end_line_no": 365, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.train_dl_iter = None\n        self.train_dl = None\n\n        self.valid_dl_iter = None\n        self.valid_dl = None\n\n        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n\n        # auto splitting validation from training, if dataset is passed in\n\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_315-365"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 350, "start_line_no": 325, "end_line_no": 375, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.split_valid_from_train = split_valid_from_train\n\n        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n        self.split_valid_fraction = split_valid_fraction\n        self.split_random_seed = split_random_seed\n\n        # be able to finely customize learning rate, weight decay\n        # per unet\n\n        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_325-375"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 360, "start_line_no": 335, "end_line_no": 385, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n\n        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n\n            if use_lion:\n                optimizer = Lion(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    betas = (beta1, beta2),\n                    use_triton = True\n                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_335-385"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 370, "start_line_no": 345, "end_line_no": 395, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                )\n            else:\n                optimizer = Adam(\n                    unet.parameters(),\n                    lr = unet_lr,\n                    eps = unet_eps,\n                    betas = (beta1, beta2),\n                    **kwargs\n                )\n\n            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_345-395"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 380, "start_line_no": 355, "end_line_no": 405, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            if self.use_ema:\n                self.ema_unets.append(EMA(unet, **ema_kwargs))\n\n            scaler = GradScaler(enabled = grad_scaler_enabled)\n\n            scheduler = warmup_scheduler = None\n\n            if exists(unet_cosine_decay_max_steps):\n                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n\n            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_355-405"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 390, "start_line_no": 365, "end_line_no": 415, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            if exists(unet_warmup_steps):\n                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n\n                if not exists(scheduler):\n                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n\n            # set on object\n\n            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n            setattr(self, f'scaler{ind}', scaler)\n            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_365-415"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 400, "start_line_no": 375, "end_line_no": 425, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            setattr(self, f'scheduler{ind}', scheduler)\n            setattr(self, f'warmup{ind}', warmup_scheduler)\n\n        # gradient clipping if needed\n\n        self.max_grad_norm = max_grad_norm\n\n        # step tracker and misc\n\n        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_375-425"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 410, "start_line_no": 385, "end_line_no": 435, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.verbose = verbose\n\n        # automatic set devices based on what accelerator decided\n\n        self.imagen.to(self.device)\n        self.to(self.device)\n\n        # checkpointing\n\n        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_385-435"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 420, "start_line_no": 395, "end_line_no": 445, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n        self.checkpoint_path = checkpoint_path\n        self.checkpoint_every = checkpoint_every\n        self.max_checkpoints_keep = max_checkpoints_keep\n\n        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n\n        if exists(checkpoint_path) and self.can_checkpoint:\n            bucket = url_to_bucket(checkpoint_path)\n\n            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_395-445"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 430, "start_line_no": 405, "end_line_no": 455, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            if not self.fs.exists(bucket):\n                self.fs.mkdir(bucket)\n\n            self.load_from_checkpoint_folder()\n\n        # only allowing training for unet\n\n        self.only_train_unet_number = only_train_unet_number\n        self.prepared = False\n\n\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_405-455"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 440, "start_line_no": 415, "end_line_no": 465, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def prepare(self):\n        assert not self.prepared, f'The trainer is allready prepared'\n        self.validate_and_set_unet_being_trained(self.only_train_unet_number)\n        self.prepared = True\n    # computed values\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_415-465"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 450, "start_line_no": 425, "end_line_no": 475, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    @property\n    def is_distributed(self):\n        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n\n    @property\n    def is_main(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_425-475"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 460, "start_line_no": 435, "end_line_no": 485, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def is_local_main(self):\n        return self.accelerator.is_local_main_process\n\n    @property\n    def unwrapped_unet(self):\n        return self.accelerator.unwrap_model(self.unet_being_trained)\n\n    # optimizer helper functions\n\n    def get_lr(self, unet_number):\n        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_435-485"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 470, "start_line_no": 445, "end_line_no": 495, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.validate_unet_number(unet_number)\n        unet_index = unet_number - 1\n\n        optim = getattr(self, f'optim{unet_index}')\n\n        return optim.param_groups[0]['lr']\n\n    # function for allowing only one unet from being trained at a time\n\n    def validate_and_set_unet_being_trained(self, unet_number = None):\n        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_445-495"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 480, "start_line_no": 455, "end_line_no": 505, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if exists(unet_number):\n            self.validate_unet_number(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n\n        self.only_train_unet_number = unet_number\n        self.imagen.only_train_unet_number = unet_number\n\n        if not exists(unet_number):\n            return\n\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_455-505"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 490, "start_line_no": 465, "end_line_no": 515, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.wrap_unet(unet_number)\n\n    def wrap_unet(self, unet_number):\n        if hasattr(self, 'one_unet_wrapped'):\n            return\n\n        unet = self.imagen.get_unet(unet_number)\n        unet_index = unet_number - 1\n\n        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_465-515"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 500, "start_line_no": 475, "end_line_no": 525, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        optimizer = getattr(self, f'optim{unet_index}')\n        scheduler = getattr(self, f'scheduler{unet_index}')\n\n        if self.train_dl:\n            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)\n        else:\n            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)\n\n        if exists(scheduler):\n            scheduler = self.accelerator.prepare(scheduler)\n\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_475-525"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 510, "start_line_no": 485, "end_line_no": 535, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        setattr(self, f'optim{unet_index}', optimizer)\n        setattr(self, f'scheduler{unet_index}', scheduler)\n\n        self.one_unet_wrapped = True\n\n    # hacking accelerator due to not having separate gradscaler per optimizer\n\n    def set_accelerator_scaler(self, unet_number):\n        unet_number = self.validate_unet_number(unet_number)\n        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_485-535"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 520, "start_line_no": 495, "end_line_no": 545, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        scaler = getattr(self, f'scaler{unet_number - 1}')\n\n        self.accelerator.scaler = scaler\n        for optimizer in self.accelerator._optimizers:\n            optimizer.scaler = scaler\n\n    # helper print\n\n    def print(self, msg):\n        if not self.is_main:\n            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_495-545"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 530, "start_line_no": 505, "end_line_no": 555, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return\n\n        if not self.verbose:\n            return\n\n        return self.accelerator.print(msg)\n\n    # validating the unet number\n\n    def validate_unet_number(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_505-555"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 540, "start_line_no": 515, "end_line_no": 565, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n        return unet_number\n\n    # number of training steps taken\n\n    def num_steps_taken(self, unet_number = None):\n        if self.num_unets == 1:\n            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_515-565"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 550, "start_line_no": 525, "end_line_no": 575, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            unet_number = default(unet_number, 1)\n\n        return self.steps[unet_number - 1].item()\n\n    def print_untrained_unets(self):\n        print_final_error = False\n\n        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n            if steps > 0 or isinstance(unet, NullUnet):\n                continue\n\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_525-575"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 560, "start_line_no": 535, "end_line_no": 585, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            self.print(f'unet {ind + 1} has not been trained')\n            print_final_error = True\n\n        if print_final_error:\n            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n\n    # data related functions\n\n    def add_train_dataloader(self, dl = None):\n        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_535-585"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 570, "start_line_no": 545, "end_line_no": 595, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if not exists(dl):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.train_dl = dl\n\n    def add_valid_dataloader(self, dl):\n        if not exists(dl):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_545-595"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 580, "start_line_no": 555, "end_line_no": 605, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n        assert not self.prepared, f'You need to add the dataset before preperation'\n        self.valid_dl = dl\n\n    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.train_dl), 'training dataloader was already added'\n\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_555-605"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 590, "start_line_no": 565, "end_line_no": 615, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        valid_ds = None\n        if self.split_valid_from_train:\n            train_size = int((1 - self.split_valid_fraction) * len(ds))\n            valid_size = len(ds) - train_size\n\n            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_565-615"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 600, "start_line_no": 575, "end_line_no": 625, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.add_train_dataloader(dl)\n\n        if not self.split_valid_from_train:\n            return\n\n        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n\n    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n        if not exists(ds):\n            return\n\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_575-625"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 610, "start_line_no": 585, "end_line_no": 635, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        assert not exists(self.valid_dl), 'validation dataloader was already added'\n\n        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n        self.add_valid_dataloader(dl)\n\n    def create_train_iter(self):\n        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n\n        if exists(self.train_dl_iter):\n            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_585-635"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 620, "start_line_no": 595, "end_line_no": 645, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return\n\n        self.train_dl_iter = cycle(self.train_dl)\n\n    def create_valid_iter(self):\n        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n\n        if exists(self.valid_dl_iter):\n            return\n\n        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_595-645"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 630, "start_line_no": 605, "end_line_no": 655, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.valid_dl_iter = cycle(self.valid_dl)\n\n    def train_step(self, unet_number = None, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_train_iter()\n        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n        self.update(unet_number = unet_number)\n        return loss\n\n    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_605-655"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 640, "start_line_no": 615, "end_line_no": 665, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    @torch.no_grad()\n    @eval_decorator\n    def valid_step(self, **kwargs):\n        if not self.prepared:\n            self.prepare()\n        self.create_valid_iter()\n        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n        with context():\n            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n        return loss\n\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_615-665"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 650, "start_line_no": 625, "end_line_no": 675, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def step_with_dl_iter(self, dl_iter, **kwargs):\n        dl_tuple_output = cast_tuple(next(dl_iter))\n        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n        loss = self.forward(**{**kwargs, **model_input})\n        return loss\n\n    # checkpointing functions\n\n    @property\n    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_625-675"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 660, "start_line_no": 635, "end_line_no": 685, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def all_checkpoints_sorted(self):\n        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n        checkpoints = self.fs.glob(glob_pattern)\n        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n        return sorted_checkpoints\n\n    def load_from_checkpoint_folder(self, last_total_steps = -1):\n        if last_total_steps != -1:\n            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n            self.load(filepath)\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_635-685"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 670, "start_line_no": 645, "end_line_no": 695, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n\n        if len(sorted_checkpoints) == 0:\n            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n            return\n\n        last_checkpoint = sorted_checkpoints[0]\n        self.load(last_checkpoint)\n\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_645-695"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 680, "start_line_no": 655, "end_line_no": 705, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    def save_to_checkpoint_folder(self):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        total_steps = int(self.steps.sum().item())\n        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n\n        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_655-705"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 690, "start_line_no": 665, "end_line_no": 715, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.save(filepath)\n\n        if self.max_checkpoints_keep <= 0:\n            return\n\n        sorted_checkpoints = self.all_checkpoints_sorted\n        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n\n        for checkpoint in checkpoints_to_discard:\n            self.fs.rm(checkpoint)\n\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_665-715"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 700, "start_line_no": 675, "end_line_no": 725, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    # saving and loading functions\n\n    def save(\n        self,\n        path,\n        overwrite = True,\n        without_optim_and_sched = False,\n        **kwargs\n    ):\n        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_675-725"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 710, "start_line_no": 685, "end_line_no": 735, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.accelerator.wait_for_everyone()\n\n        if not self.can_checkpoint:\n            return\n\n        fs = self.fs\n\n        assert not (fs.exists(path) and not overwrite)\n\n        self.reset_ema_unets_all_one_device()\n\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_685-735"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 720, "start_line_no": 695, "end_line_no": 745, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        save_obj = dict(\n            model = self.imagen.state_dict(),\n            version = __version__,\n            steps = self.steps.cpu(),\n            **kwargs\n        )\n\n        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n\n        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_695-745"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 730, "start_line_no": 705, "end_line_no": 755, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        for ind in save_optim_and_sched_iter:\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_705-755"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 740, "start_line_no": 715, "end_line_no": 765, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n            if exists(scheduler):\n                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n\n            if exists(warmup_scheduler):\n                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n\n            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n\n        if self.use_ema:\n            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_715-765"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 750, "start_line_no": 725, "end_line_no": 775, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n\n        # determine if imagen config is available\n\n        if hasattr(self.imagen, '_config'):\n            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n\n            save_obj = {\n                **save_obj,\n                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_725-775"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 760, "start_line_no": 735, "end_line_no": 785, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                'imagen_params': self.imagen._config\n            }\n\n        #save to path\n\n        with fs.open(path, 'wb') as f:\n            torch.save(save_obj, f)\n\n        self.print(f'checkpoint saved to {path}')\n\n    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_735-785"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 770, "start_line_no": 745, "end_line_no": 795, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n        fs = self.fs\n\n        if noop_if_not_exist and not fs.exists(path):\n            self.print(f'trainer checkpoint not found at {str(path)}')\n            return\n\n        assert fs.exists(path), f'{path} does not exist'\n\n        self.reset_ema_unets_all_one_device()\n\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_745-795"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 780, "start_line_no": 755, "end_line_no": 805, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # to avoid extra GPU memory usage in main process when using Accelerate\n\n        with fs.open(path) as f:\n            loaded_obj = torch.load(f, map_location='cpu')\n\n        if version.parse(__version__) != version.parse(loaded_obj['version']):\n            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n\n        try:\n            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_755-805"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 790, "start_line_no": 765, "end_line_no": 815, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n        except RuntimeError:\n            print(\"Failed loading state dict. Trying partial load\")\n            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n                                                      loaded_obj['model']))\n\n        if only_model:\n            return loaded_obj\n\n        self.steps.copy_(loaded_obj['steps'])\n\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_765-815"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 800, "start_line_no": 775, "end_line_no": 825, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        for ind in range(0, self.num_unets):\n            scaler_key = f'scaler{ind}'\n            optimizer_key = f'optim{ind}'\n            scheduler_key = f'scheduler{ind}'\n            warmup_scheduler_key = f'warmup{ind}'\n\n            scaler = getattr(self, scaler_key)\n            optimizer = getattr(self, optimizer_key)\n            scheduler = getattr(self, scheduler_key)\n            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_775-825"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 810, "start_line_no": 785, "end_line_no": 835, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            warmup_scheduler = getattr(self, warmup_scheduler_key)\n\n            if exists(scheduler) and scheduler_key in loaded_obj:\n                scheduler.load_state_dict(loaded_obj[scheduler_key])\n\n            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n\n            if exists(optimizer):\n                try:\n                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_785-835"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 820, "start_line_no": 795, "end_line_no": 845, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n                    scaler.load_state_dict(loaded_obj[scaler_key])\n                except:\n                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n\n        if self.use_ema:\n            assert 'ema' in loaded_obj\n            try:\n                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n            except RuntimeError:\n                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_795-845"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 830, "start_line_no": 805, "end_line_no": 855, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "                print(\"Failed loading state dict. Trying partial load\")\n                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n                                                             loaded_obj['ema']))\n\n        self.print(f'checkpoint loaded from {path}')\n        return loaded_obj\n\n    # managing ema unets and their devices\n\n    @property\n    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_805-855"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 840, "start_line_no": 815, "end_line_no": 865, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def unets(self):\n        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n\n    def get_ema_unet(self, unet_number = None):\n        if not self.use_ema:\n            return\n\n        unet_number = self.validate_unet_number(unet_number)\n        index = unet_number - 1\n\n        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_815-865"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 850, "start_line_no": 825, "end_line_no": 875, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        if isinstance(self.unets, nn.ModuleList):\n            unets_list = [unet for unet in self.ema_unets]\n            delattr(self, 'ema_unets')\n            self.ema_unets = unets_list\n\n        if index != self.ema_unet_being_trained_index:\n            for unet_index, unet in enumerate(self.ema_unets):\n                unet.to(self.device if unet_index == index else 'cpu')\n\n        self.ema_unet_being_trained_index = index\n        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_825-875"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 860, "start_line_no": 835, "end_line_no": 885, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        return self.ema_unets[index]\n\n    def reset_ema_unets_all_one_device(self, device = None):\n        if not self.use_ema:\n            return\n\n        device = default(device, self.device)\n        self.ema_unets = nn.ModuleList([*self.ema_unets])\n        self.ema_unets.to(device)\n\n        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):\n            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_835-885"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 870, "start_line_no": 845, "end_line_no": 895, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.ema_unet_being_trained_index = -1\n\n    @torch.no_grad()\n    @contextmanager\n    def use_ema_unets(self):\n        if not self.use_ema:\n            output = yield\n            return output\n\n        self.reset_ema_unets_all_one_device()\n        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):\n            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_845-895"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 880, "start_line_no": 855, "end_line_no": 905, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.imagen.reset_unets_all_one_device()\n\n        self.unets.eval()\n\n        trainable_unets = self.imagen.unets\n        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n\n        output = yield\n\n        self.imagen.unets = trainable_unets             # restore original training unets\n\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):\n            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_855-905"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 890, "start_line_no": 865, "end_line_no": 915, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # cast the ema_model unets back to original device\n        for ema in self.ema_unets:\n            ema.restore_ema_model_device()\n\n        return output\n\n    def print_unet_devices(self):\n        self.print('unet devices:')\n        for i, unet in enumerate(self.imagen.unets):\n            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_865-915"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 900, "start_line_no": 875, "end_line_no": 925, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            device = next(unet.parameters()).device\n            self.print(f'\\tunet {i}: {device}')\n\n        if not self.use_ema:\n            return\n\n        self.print('\\nema unet devices:')\n        for i, ema_unet in enumerate(self.ema_unets):\n            device = next(ema_unet.parameters()).device\n            self.print(f'\\tema unet {i}: {device}')\n\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')\n\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_875-925"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 910, "start_line_no": 885, "end_line_no": 935, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    # overriding state dict functions\n\n    def state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        self.reset_ema_unets_all_one_device()\n        return super().load_state_dict(*args, **kwargs)\n\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')\n\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:\n            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_885-935"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 920, "start_line_no": 895, "end_line_no": 945, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    # encoding text functions\n\n    def encode_text(self, text, **kwargs):\n        return self.imagen.encode_text(text, **kwargs)\n\n    # forwarding functions and gradient step updates\n\n    def update(self, unet_number = None):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')\n\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:\n            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()\n\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_895-945"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 930, "start_line_no": 905, "end_line_no": 955, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        index = unet_number - 1\n        unet = self.unet_being_trained\n\n        optimizer = getattr(self, f'optim{index}')\n        scaler = getattr(self, f'scaler{index}')\n        scheduler = getattr(self, f'scheduler{index}')\n        warmup_scheduler = getattr(self, f'warmup{index}')\n\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:\n            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()\n\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return\n\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_905-955"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 940, "start_line_no": 915, "end_line_no": 965, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        # set the grad scaler on the accelerator, since we are managing one per u-net\n\n        if exists(self.max_grad_norm):\n            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if self.use_ema:\n            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()\n\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return\n\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()\n\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_915-965"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 950, "start_line_no": 925, "end_line_no": 975, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "            ema_unet = self.get_ema_unet(unet_number)\n            ema_unet.update()\n\n        # scheduler, if needed\n\n        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n\n        with maybe_warmup_context:\n            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n                scheduler.step()\n\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return\n\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()\n\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_925-975"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 960, "start_line_no": 935, "end_line_no": 985, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n\n        if not exists(self.checkpoint_path):\n            return\n\n        total_steps = int(self.steps.sum().item())\n\n        if total_steps % self.checkpoint_every:\n            return\n\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()\n\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_935-985"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 970, "start_line_no": 945, "end_line_no": 991, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        self.save_to_checkpoint_folder()\n\n    @torch.no_grad()\n    @cast_torch_tensor\n    @imagen_sample_in_chunks\n    def sample(self, *args, **kwargs):\n        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n\n        self.print_untrained_unets()\n\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n\n            total_loss += loss.item()\n\n            if self.training:\n                self.accelerator.backward(loss)\n\n        return total_loss", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_945-991"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 980, "start_line_no": 955, "end_line_no": 991, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n        if not self.is_main:\n            kwargs['use_tqdm'] = False\n\n        with context():\n            output = self.imagen.sample(*args, device = self.device, **kwargs)\n\n        return output\n\n    @partial(cast_torch_tensor, cast_fp16 = True)\n    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n\n            total_loss += loss.item()\n\n            if self.training:\n                self.accelerator.backward(loss)\n\n        return total_loss", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_955-991"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "trainer.py"], "line_no": 990, "start_line_no": 965, "end_line_no": 991, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    def forward(\n        self,\n        *args,\n        unet_number = None,\n        max_batch_size = None,\n        **kwargs\n    ):\n        unet_number = self.validate_unet_number(unet_number)\n        self.validate_and_set_unet_being_trained(unet_number)\n        self.set_accelerator_scaler(unet_number)\n\n        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n\n        total_loss = 0.\n\n        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n            with self.accelerator.autocast():\n                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)\n                loss = loss * chunk_size_frac\n\n            total_loss += loss.item()\n\n            if self.training:\n                self.accelerator.backward(loss)\n\n        return total_loss", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-trainer.py_965-991"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import torch\nfrom torch import nn\nfrom functools import reduce\nfrom pathlib import Path\n\nfrom imagen_pytorch.configs import ImagenConfig, ElucidatedImagenConfig\nfrom ema_pytorch import EMA\n\ndef exists(val):\n    return val is not None\n\ndef safeget(dictionary, keys, default = None):\n    return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split('.'), dictionary)\n\ndef load_imagen_from_checkpoint(\n    checkpoint_path,\n    load_weights = True,\n    load_ema_if_available = False\n):\n    model_path = Path(checkpoint_path)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'checkpoint not found at {full_model_path}'\n    loaded = torch.load(str(model_path), map_location='cpu')\n\n    imagen_params = safeget(loaded, 'imagen_params')\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargargConstant)Return(Call(Name(Load)Lambda(arguments(argarg)IfExp(Call(Name(Load)Name(Load)Name(Load))Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Name(Load)))Call(Attribute(Name(Load)Load)Constant)Name(Load))))FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import torch\nfrom torch import nn\nfrom functools import reduce\nfrom pathlib import Path\n\nfrom imagen_pytorch.configs import ImagenConfig, ElucidatedImagenConfig\nfrom ema_pytorch import EMA\n\ndef exists(val):\n    return val is not None\n\ndef safeget(dictionary, keys, default = None):\n    return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split('.'), dictionary)\n\ndef load_imagen_from_checkpoint(\n    checkpoint_path,\n    load_weights = True,\n    load_ema_if_available = False\n):\n    model_path = Path(checkpoint_path)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'checkpoint not found at {full_model_path}'\n    loaded = torch.load(str(model_path), map_location='cpu')\n\n    imagen_params = safeget(loaded, 'imagen_params')\n    imagen_type = safeget(loaded, 'imagen_type')\n\n    if imagen_type == 'original':\n        imagen_klass = ImagenConfig\n    elif imagen_type == 'elucidated':\n        imagen_klass = ElucidatedImagenConfig\n    else:\n        raise ValueError(f'unknown imagen type {imagen_type} - you need to instantiate your Imagen with configurations, using classes ImagenConfig or ElucidatedImagenConfig')\n\n    assert exists(imagen_params) and exists(imagen_type), 'imagen type and configuration not saved in this checkpoint'\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargargConstant)Return(Call(Name(Load)Lambda(arguments(argarg)IfExp(Call(Name(Load)Name(Load)Name(Load))Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Name(Load)))Call(Attribute(Name(Load)Load)Constant)Name(Load))))FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))Constant)))))Assert(BoolOp(AndCall(Name(Load)Name(Load))Call(Name(Load)Name(Load)))Constant)))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "import torch\nfrom torch import nn\nfrom functools import reduce\nfrom pathlib import Path\n\nfrom imagen_pytorch.configs import ImagenConfig, ElucidatedImagenConfig\nfrom ema_pytorch import EMA\n\ndef exists(val):\n    return val is not None\n\ndef safeget(dictionary, keys, default = None):\n    return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split('.'), dictionary)\n\ndef load_imagen_from_checkpoint(\n    checkpoint_path,\n    load_weights = True,\n    load_ema_if_available = False\n):\n    model_path = Path(checkpoint_path)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'checkpoint not found at {full_model_path}'\n    loaded = torch.load(str(model_path), map_location='cpu')\n\n    imagen_params = safeget(loaded, 'imagen_params')\n    imagen_type = safeget(loaded, 'imagen_type')\n\n    if imagen_type == 'original':\n        imagen_klass = ImagenConfig\n    elif imagen_type == 'elucidated':\n        imagen_klass = ElucidatedImagenConfig\n    else:\n        raise ValueError(f'unknown imagen type {imagen_type} - you need to instantiate your Imagen with configurations, using classes ImagenConfig or ElucidatedImagenConfig')\n\n    assert exists(imagen_params) and exists(imagen_type), 'imagen type and configuration not saved in this checkpoint'\n\n    imagen = imagen_klass(**imagen_params).create()\n\n    if not load_weights:\n        return imagen\n\n    has_ema = 'ema' in loaded\n    should_load_ema = has_ema and load_ema_if_available\n\n    imagen.load_state_dict(loaded['model'])\n\nAST=Module(Import(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasalias)ImportFrom(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargargConstant)Return(Call(Name(Load)Lambda(arguments(argarg)IfExp(Call(Name(Load)Name(Load)Name(Load))Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Name(Load)))Call(Attribute(Name(Load)Load)Constant)Name(Load))))FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))Constant)))))Assert(BoolOp(AndCall(Name(Load)Name(Load))Call(Name(Load)Name(Load)))Constant)Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Name(Load)))Load)))If(UnaryOp(NotName(Load))Return(Name(Load)))Assign(Name(Store)Compare(ConstantInName(Load)))Assign(Name(Store)BoolOp(AndName(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 55, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.configs import ImagenConfig, ElucidatedImagenConfig\nfrom ema_pytorch import EMA\n\ndef exists(val):\n    return val is not None\n\ndef safeget(dictionary, keys, default = None):\n    return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split('.'), dictionary)\n\ndef load_imagen_from_checkpoint(\n    checkpoint_path,\n    load_weights = True,\n    load_ema_if_available = False\n):\n    model_path = Path(checkpoint_path)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'checkpoint not found at {full_model_path}'\n    loaded = torch.load(str(model_path), map_location='cpu')\n\n    imagen_params = safeget(loaded, 'imagen_params')\n    imagen_type = safeget(loaded, 'imagen_type')\n\n    if imagen_type == 'original':\n        imagen_klass = ImagenConfig\n    elif imagen_type == 'elucidated':\n        imagen_klass = ElucidatedImagenConfig\n    else:\n        raise ValueError(f'unknown imagen type {imagen_type} - you need to instantiate your Imagen with configurations, using classes ImagenConfig or ElucidatedImagenConfig')\n\n    assert exists(imagen_params) and exists(imagen_type), 'imagen type and configuration not saved in this checkpoint'\n\n    imagen = imagen_klass(**imagen_params).create()\n\n    if not load_weights:\n        return imagen\n\n    has_ema = 'ema' in loaded\n    should_load_ema = has_ema and load_ema_if_available\n\n    imagen.load_state_dict(loaded['model'])\n\n    if not should_load_ema:\n        print('loading non-EMA version of unets')\n        return imagen\n\n    ema_unets = nn.ModuleList([])\n    for unet in imagen.unets:\n        ema_unets.append(EMA(unet))\n\n    ema_unets.load_state_dict(loaded['ema'])\n\nAST=Module(ImportFrom(aliasalias)ImportFrom(alias)FunctionDef(arguments(arg)Return(Compare(Name(Load)IsNotConstant)))FunctionDef(arguments(argargargConstant)Return(Call(Name(Load)Lambda(arguments(argarg)IfExp(Call(Name(Load)Name(Load)Name(Load))Call(Attribute(Name(Load)Load)Name(Load)Name(Load))Name(Load)))Call(Attribute(Name(Load)Load)Constant)Name(Load))))FunctionDef(arguments(argargargConstantConstant)Assign(Name(Store)Call(Name(Load)Name(Load)))Assign(Name(Store)Call(Name(Load)Call(Attribute(Name(Load)Load))))Assert(Call(Attribute(Name(Load)Load))JoinedStr(ConstantFormattedValue(Name(Load))))Assign(Name(Store)Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load))keyword(Constant)))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))Assign(Name(Store)Call(Name(Load)Name(Load)Constant))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Name(Load))If(Compare(Name(Load)EqConstant)Assign(Name(Store)Name(Load))Raise(Call(Name(Load)JoinedStr(ConstantFormattedValue(Name(Load))Constant)))))Assert(BoolOp(AndCall(Name(Load)Name(Load))Call(Name(Load)Name(Load)))Constant)Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Name(Load)))Load)))If(UnaryOp(NotName(Load))Return(Name(Load)))Assign(Name(Store)Compare(ConstantInName(Load)))Assign(Name(Store)BoolOp(AndName(Load)Name(Load)))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))If(UnaryOp(NotName(Load))Expr(Call(Name(Load)Constant))Return(Name(Load)))Assign(Name(Store)Call(Attribute(Name(Load)Load)List(Load)))For(Name(Store)Attribute(Name(Load)Load)Expr(Call(Attribute(Name(Load)Load)Call(Name(Load)Name(Load)))))Expr(Call(Attribute(Name(Load)Load)Subscript(Name(Load)ConstantLoad)))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_5-55"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 61, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    checkpoint_path,\n    load_weights = True,\n    load_ema_if_available = False\n):\n    model_path = Path(checkpoint_path)\n    full_model_path = str(model_path.resolve())\n    assert model_path.exists(), f'checkpoint not found at {full_model_path}'\n    loaded = torch.load(str(model_path), map_location='cpu')\n\n    imagen_params = safeget(loaded, 'imagen_params')\n    imagen_type = safeget(loaded, 'imagen_type')\n\n    if imagen_type == 'original':\n        imagen_klass = ImagenConfig\n    elif imagen_type == 'elucidated':\n        imagen_klass = ElucidatedImagenConfig\n    else:\n        raise ValueError(f'unknown imagen type {imagen_type} - you need to instantiate your Imagen with configurations, using classes ImagenConfig or ElucidatedImagenConfig')\n\n    assert exists(imagen_params) and exists(imagen_type), 'imagen type and configuration not saved in this checkpoint'\n\n    imagen = imagen_klass(**imagen_params).create()\n\n    if not load_weights:\n        return imagen\n\n    has_ema = 'ema' in loaded\n    should_load_ema = has_ema and load_ema_if_available\n\n    imagen.load_state_dict(loaded['model'])\n\n    if not should_load_ema:\n        print('loading non-EMA version of unets')\n        return imagen\n\n    ema_unets = nn.ModuleList([])\n    for unet in imagen.unets:\n        ema_unets.append(EMA(unet))\n\n    ema_unets.load_state_dict(loaded['ema'])\n\n    for unet, ema_unet in zip(imagen.unets, ema_unets):\n        unet.load_state_dict(ema_unet.ema_model.state_dict())\n\n    print('loaded EMA version of unets')\n    return imagen", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_15-61"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 61, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "    imagen_type = safeget(loaded, 'imagen_type')\n\n    if imagen_type == 'original':\n        imagen_klass = ImagenConfig\n    elif imagen_type == 'elucidated':\n        imagen_klass = ElucidatedImagenConfig\n    else:\n        raise ValueError(f'unknown imagen type {imagen_type} - you need to instantiate your Imagen with configurations, using classes ImagenConfig or ElucidatedImagenConfig')\n\n    assert exists(imagen_params) and exists(imagen_type), 'imagen type and configuration not saved in this checkpoint'\n\n    imagen = imagen_klass(**imagen_params).create()\n\n    if not load_weights:\n        return imagen\n\n    has_ema = 'ema' in loaded\n    should_load_ema = has_ema and load_ema_if_available\n\n    imagen.load_state_dict(loaded['model'])\n\n    if not should_load_ema:\n        print('loading non-EMA version of unets')\n        return imagen\n\n    ema_unets = nn.ModuleList([])\n    for unet in imagen.unets:\n        ema_unets.append(EMA(unet))\n\n    ema_unets.load_state_dict(loaded['ema'])\n\n    for unet, ema_unet in zip(imagen.unets, ema_unets):\n        unet.load_state_dict(ema_unet.ema_model.state_dict())\n\n    print('loaded EMA version of unets')\n    return imagen", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_25-61"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-utils.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "utils.py"], "line_no": 60, "start_line_no": 35, "end_line_no": 61, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    imagen = imagen_klass(**imagen_params).create()\n\n    if not load_weights:\n        return imagen\n\n    has_ema = 'ema' in loaded\n    should_load_ema = has_ema and load_ema_if_available\n\n    imagen.load_state_dict(loaded['model'])\n\n    if not should_load_ema:\n        print('loading non-EMA version of unets')\n        return imagen\n\n    ema_unets = nn.ModuleList([])\n    for unet in imagen.unets:\n        ema_unets.append(EMA(unet))\n\n    ema_unets.load_state_dict(loaded['ema'])\n\n    for unet, ema_unet in zip(imagen.unets, ema_unets):\n        unet.load_state_dict(ema_unet.ema_model.state_dict())\n\n    print('loaded EMA version of unets')\n    return imagen", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-utils.py_35-61"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-version.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "version.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "__version__ = '1.22.2'\n\nAST=Module(Assign(Name(Store)Constant))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-version.py_0-1"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-__init__.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 21, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}, {"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "__init__.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 21, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}, {"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "__init__.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 21, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.imagen_pytorch import Imagen, Unet\nfrom imagen_pytorch.imagen_pytorch import NullUnet\nfrom imagen_pytorch.imagen_pytorch import BaseUnet64, SRUnet256, SRUnet1024\nfrom imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.version import __version__\n\n# imagen using the elucidated ddpm from Tero Karras' new paper\n\nfrom imagen_pytorch.elucidated_imagen import ElucidatedImagen\n\n# config driven creation of imagen instances\n\nfrom imagen_pytorch.configs import UnetConfig, ImagenConfig, ElucidatedImagenConfig, ImagenTrainerConfig\n\n# utils\n\nfrom imagen_pytorch.utils import load_imagen_from_checkpoint\n\n# video\n\nfrom imagen_pytorch.imagen_video import Unet3D\n\nAST=Module(ImportFrom(aliasalias)ImportFrom(alias)ImportFrom(aliasaliasalias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(aliasaliasaliasalias)ImportFrom(alias)ImportFrom(alias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-__init__.py_0-21"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "test_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 25, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.configs import ImagenConfig\nfrom imagen_pytorch.t5 import t5_encode_text\nfrom torch.utils.data import Dataset\nimport torch\n\ndef test_trainer_instantiation():\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n\n\nAST=Module(ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)ImportFrom(alias)Import(alias)FunctionDef(argumentsAssign(Name(Store)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Tuple(Name(Load)Load))keyword(Tuple(ConstantLoad)))Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py_0-25"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "test_trainer.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 35, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.configs import ImagenConfig\nfrom imagen_pytorch.t5 import t5_encode_text\nfrom torch.utils.data import Dataset\nimport torch\n\ndef test_trainer_instantiation():\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n\ndef test_trainer_step():\n    class TestDataset(Dataset):\n        def __init__(self):\n            super().__init__()\n        def __len__(self):\n            return 16\n        def __getitem__(self, index):\n            return (torch.zeros(3, 64, 64), torch.zeros(6, 768))\n    unet1 = dict(\n        dim = 8,", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py_0-35"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "test_trainer.py"], "line_no": 20, "start_line_no": 0, "end_line_no": 45, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.trainer import ImagenTrainer\nfrom imagen_pytorch.configs import ImagenConfig\nfrom imagen_pytorch.t5 import t5_encode_text\nfrom torch.utils.data import Dataset\nimport torch\n\ndef test_trainer_instantiation():\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n\ndef test_trainer_step():\n    class TestDataset(Dataset):\n        def __init__(self):\n            super().__init__()\n        def __len__(self):\n            return 16\n        def __getitem__(self, index):\n            return (torch.zeros(3, 64, 64), torch.zeros(6, 768))\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py_0-45"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "test_trainer.py"], "line_no": 30, "start_line_no": 5, "end_line_no": 54, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\ndef test_trainer_instantiation():\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n\ndef test_trainer_step():\n    class TestDataset(Dataset):\n        def __init__(self):\n            super().__init__()\n        def __len__(self):\n            return 16\n        def __getitem__(self, index):\n            return (torch.zeros(3, 64, 64), torch.zeros(6, 768))\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n    ds = TestDataset()\n    trainer.add_train_dataset(ds, batch_size=8)\n    trainer.train_step(1)\n    assert trainer.num_steps_taken(1) == 1\n\nAST=Module(FunctionDef(argumentsAssign(Name(Store)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Tuple(Name(Load)Load))keyword(Tuple(ConstantLoad)))Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load)))))FunctionDef(argumentsClassDef(Name(Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load))))FunctionDef(arguments(arg)Return(Constant))FunctionDef(arguments(argarg)Return(Tuple(Call(Attribute(Name(Load)Load)ConstantConstantConstant)Call(Attribute(Name(Load)Load)ConstantConstant)Load))))Assign(Name(Store)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Tuple(Name(Load)Load))keyword(Tuple(ConstantLoad)))Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Constant))Assert(Compare(Call(Attribute(Name(Load)Load)Constant)EqConstant))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py_5-54"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "test_trainer.py"], "line_no": 40, "start_line_no": 15, "end_line_no": 54, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n\ndef test_trainer_step():\n    class TestDataset(Dataset):\n        def __init__(self):\n            super().__init__()\n        def __len__(self):\n            return 16\n        def __getitem__(self, index):\n            return (torch.zeros(3, 64, 64), torch.zeros(6, 768))\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n    ds = TestDataset()\n    trainer.add_train_dataset(ds, batch_size=8)\n    trainer.train_step(1)\n    assert trainer.num_steps_taken(1) == 1", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py_15-54"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "test_trainer.py"], "line_no": 50, "start_line_no": 25, "end_line_no": 54, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "def test_trainer_step():\n    class TestDataset(Dataset):\n        def __init__(self):\n            super().__init__()\n        def __len__(self):\n            return 16\n        def __getitem__(self, index):\n            return (torch.zeros(3, 64, 64), torch.zeros(6, 768))\n    unet1 = dict(\n        dim = 8,\n        dim_mults = (1, 1, 1, 1),\n        num_resnet_blocks = 1,\n        layer_attns = False,\n        layer_cross_attns = False,\n        attn_heads = 2\n    )\n\n    imagen = ImagenConfig(\n        unets=(unet1,),\n        image_sizes=(64,),\n    ).create()\n\n    trainer = ImagenTrainer(\n        imagen=imagen\n    )\n    ds = TestDataset()\n    trainer.add_train_dataset(ds, batch_size=8)\n    trainer.train_step(1)\n    assert trainer.num_steps_taken(1) == 1\n\nAST=Module(FunctionDef(argumentsClassDef(Name(Load)FunctionDef(arguments(arg)Expr(Call(Attribute(Call(Name(Load))Load))))FunctionDef(arguments(arg)Return(Constant))FunctionDef(arguments(argarg)Return(Tuple(Call(Attribute(Name(Load)Load)ConstantConstantConstant)Call(Attribute(Name(Load)Load)ConstantConstant)Load))))Assign(Name(Store)Call(Name(Load)keyword(Constant)keyword(Tuple(ConstantConstantConstantConstantLoad))keyword(Constant)keyword(Constant)keyword(Constant)keyword(Constant)))Assign(Name(Store)Call(Attribute(Call(Name(Load)keyword(Tuple(Name(Load)Load))keyword(Tuple(ConstantLoad)))Load)))Assign(Name(Store)Call(Name(Load)keyword(Name(Load))))Assign(Name(Store)Call(Name(Load)))Expr(Call(Attribute(Name(Load)Load)Name(Load)keyword(Constant)))Expr(Call(Attribute(Name(Load)Load)Constant))Assert(Compare(Call(Attribute(Name(Load)Load)Constant)EqConstant))))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-test_trainer.py_25-54"}
{"title": "lucidrains_imagen-pytorch-imagen_pytorch-test-__init__.py", "metadata": [{"fpath_tuple": ["lucidrains_imagen-pytorch", "imagen_pytorch", "test", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 1, "window_size": 50, "repo": "lucidrains_imagen-pytorch", "slice_size": 5}], "contents": "from imagen_pytorch.test import test_trainer\n\nAST=Module(ImportFrom(alias))", "id": "lucidrains_imagen-pytorch_lucidrains_imagen-pytorch-imagen_pytorch-test-__init__.py_0-1"}
